{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e517f244ef71482897fe95f574bb20e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3ee75a919074003aebf4e741b18e9c1",
              "IPY_MODEL_d7e0be4915434e2a9b0ee2804098220b",
              "IPY_MODEL_2551dba898d6441ca52bf9a37e256ddc"
            ],
            "layout": "IPY_MODEL_87ee1f2d41424e93a170c7e7db477df4"
          }
        },
        "e3ee75a919074003aebf4e741b18e9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_109e5c44a3754a0487bfc3ad9630f8bc",
            "placeholder": "​",
            "style": "IPY_MODEL_0b90c3d0e4bd4b8885431d4a5e8a26e3",
            "value": "Converting files: 100%"
          }
        },
        "d7e0be4915434e2a9b0ee2804098220b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd20fbbf7b9a485d9b8c540f1fa78a0f",
            "max": 499,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9267ebea6ecf430c9cfd9e14e3ce84f8",
            "value": 499
          }
        },
        "2551dba898d6441ca52bf9a37e256ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3db597ef5ca847e99c937c5dc2b61a29",
            "placeholder": "​",
            "style": "IPY_MODEL_9d501b9ba32c49e1bb254156cdb30b03",
            "value": " 499/499 [00:02&lt;00:00, 303.42it/s]"
          }
        },
        "87ee1f2d41424e93a170c7e7db477df4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "109e5c44a3754a0487bfc3ad9630f8bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b90c3d0e4bd4b8885431d4a5e8a26e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd20fbbf7b9a485d9b8c540f1fa78a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9267ebea6ecf430c9cfd9e14e3ce84f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3db597ef5ca847e99c937c5dc2b61a29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d501b9ba32c49e1bb254156cdb30b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d21b56beca2848d4a9e1bf6ce34b80a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b31e25ad35604dd18b11cb1c89520090",
              "IPY_MODEL_3ce9e38571ba40578415df7ee096450a",
              "IPY_MODEL_b68ee6df17ee4cc994ac892e8c20de67"
            ],
            "layout": "IPY_MODEL_6439051843cd430d9ab8e8189e53e4a7"
          }
        },
        "b31e25ad35604dd18b11cb1c89520090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90548ba6b60c4032bca6b7edd4edfc48",
            "placeholder": "​",
            "style": "IPY_MODEL_c012c7fe23f14c31815d482f46702cec",
            "value": "Updating BM25 representation...: 100%"
          }
        },
        "3ce9e38571ba40578415df7ee096450a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85ba9ee8b91a442e8d0422d25ad1948c",
            "max": 499,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f9516f928b54426acca6c04d2c21fbc",
            "value": 499
          }
        },
        "b68ee6df17ee4cc994ac892e8c20de67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b187cf767f946b7acafcb70e715ec13",
            "placeholder": "​",
            "style": "IPY_MODEL_662d21e3ec78415eb57d8744a2cf60bb",
            "value": " 499/499 [00:00&lt;00:00, 2075.47 docs/s]"
          }
        },
        "6439051843cd430d9ab8e8189e53e4a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90548ba6b60c4032bca6b7edd4edfc48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c012c7fe23f14c31815d482f46702cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85ba9ee8b91a442e8d0422d25ad1948c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f9516f928b54426acca6c04d2c21fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b187cf767f946b7acafcb70e715ec13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "662d21e3ec78415eb57d8744a2cf60bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9193c8ccb6504e0d84d54358bdbc3b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff2f0fd9914b4097bd2a411a394a1ce2",
              "IPY_MODEL_5d3048aeb0f648108bc665a7925c1ef5",
              "IPY_MODEL_de2940844f5943ee83a3b8d68c384684"
            ],
            "layout": "IPY_MODEL_81a763d0b72a41d2b0dc5ab5b667e7b4"
          }
        },
        "ff2f0fd9914b4097bd2a411a394a1ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e05eba20f04240f48455fc79bbb0f13c",
            "placeholder": "​",
            "style": "IPY_MODEL_0d5949a7b40249519427e622f52346a1",
            "value": "Inferencing Samples: 100%"
          }
        },
        "5d3048aeb0f648108bc665a7925c1ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2dbab82e4fa487ca1010bf747ad9cb7",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff9b2831c8c94e40a98da5a40eca89a3",
            "value": 4
          }
        },
        "de2940844f5943ee83a3b8d68c384684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5026d6d9e364daab5126e2b261687da",
            "placeholder": "​",
            "style": "IPY_MODEL_2ac3ab887e1145219c91020dd7d5161a",
            "value": " 4/4 [00:02&lt;00:00,  1.85 Batches/s]"
          }
        },
        "81a763d0b72a41d2b0dc5ab5b667e7b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e05eba20f04240f48455fc79bbb0f13c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d5949a7b40249519427e622f52346a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2dbab82e4fa487ca1010bf747ad9cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9b2831c8c94e40a98da5a40eca89a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5026d6d9e364daab5126e2b261687da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ac3ab887e1145219c91020dd7d5161a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f84b3569b60413daf27c63c2aa21da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4791441ebfc640cfa9568fa2d4050734",
              "IPY_MODEL_45b5b21f42564bcd8b9c44e25cca1f4f",
              "IPY_MODEL_f1a33b30de714ff8a203918c9b26e46d"
            ],
            "layout": "IPY_MODEL_b55edc76851241a9915d5ca7d9a82599"
          }
        },
        "4791441ebfc640cfa9568fa2d4050734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28a72be7a8614a0494887649b8d278a3",
            "placeholder": "​",
            "style": "IPY_MODEL_afba4c2f2ddf465f9f09382887c6267d",
            "value": "Inferencing Samples: 100%"
          }
        },
        "45b5b21f42564bcd8b9c44e25cca1f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d840ca71b4a40cf872b83555d830ac4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c2f63ce30f24cc486822db52b6d8f96",
            "value": 3
          }
        },
        "f1a33b30de714ff8a203918c9b26e46d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7fab93ee2824a8d9ba187165b137626",
            "placeholder": "​",
            "style": "IPY_MODEL_f30af434954844abaa11b0c4519bee26",
            "value": " 3/3 [00:01&lt;00:00,  1.98 Batches/s]"
          }
        },
        "b55edc76851241a9915d5ca7d9a82599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28a72be7a8614a0494887649b8d278a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afba4c2f2ddf465f9f09382887c6267d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d840ca71b4a40cf872b83555d830ac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c2f63ce30f24cc486822db52b6d8f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7fab93ee2824a8d9ba187165b137626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f30af434954844abaa11b0c4519bee26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34f26466771340e0bf0621a7583b270d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d03d0b3ffe7346e382a0c6c36101615d",
              "IPY_MODEL_ccffe0a0f4ae4c5cbf3b6654e10a05d2",
              "IPY_MODEL_a7b84123f02245f99c7fdb89f3bd1dfd"
            ],
            "layout": "IPY_MODEL_6c40c3dc5a7a4808a86879413364e508"
          }
        },
        "d03d0b3ffe7346e382a0c6c36101615d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0da2005fdc14898b480e674c2057ebc",
            "placeholder": "​",
            "style": "IPY_MODEL_fd0658dcba9840dabb4d84e8c5276e7e",
            "value": "Inferencing Samples: 100%"
          }
        },
        "ccffe0a0f4ae4c5cbf3b6654e10a05d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5108be4d29774dd9856c32d10fc966d9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2dbd977e89544ccb3de4e5cac040a71",
            "value": 2
          }
        },
        "a7b84123f02245f99c7fdb89f3bd1dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_160ec109e6b44f0c921ffbe466b9dbad",
            "placeholder": "​",
            "style": "IPY_MODEL_2f3f9440366943b6aa9c506978913022",
            "value": " 2/2 [00:01&lt;00:00,  1.27 Batches/s]"
          }
        },
        "6c40c3dc5a7a4808a86879413364e508": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0da2005fdc14898b480e674c2057ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd0658dcba9840dabb4d84e8c5276e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5108be4d29774dd9856c32d10fc966d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2dbd977e89544ccb3de4e5cac040a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "160ec109e6b44f0c921ffbe466b9dbad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3f9440366943b6aa9c506978913022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "074bd854a151409b9e1d7a2c83cf8bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da76f25ca308457a82290e1a197c8eb2",
              "IPY_MODEL_f32763713f7341fd9dd2985e38ccb874",
              "IPY_MODEL_9463abfdc1994f18851510adce69a389"
            ],
            "layout": "IPY_MODEL_209dc3b3256045688d3743a97d3e4b0e"
          }
        },
        "da76f25ca308457a82290e1a197c8eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31f2e0a46c3048619abab6494458bda4",
            "placeholder": "​",
            "style": "IPY_MODEL_4de5587107824a99af6cee8a639d2250",
            "value": "Inferencing Samples: 100%"
          }
        },
        "f32763713f7341fd9dd2985e38ccb874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_451c346561a84c26aed5dda5c915a7ce",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_917ba160f6044036bc5490e132a20b5f",
            "value": 1
          }
        },
        "9463abfdc1994f18851510adce69a389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfa87fdc65fc41d19268f593b9a25ab3",
            "placeholder": "​",
            "style": "IPY_MODEL_2645ace625e64b108f9b333187c86e56",
            "value": " 1/1 [00:00&lt;00:00,  1.32 Batches/s]"
          }
        },
        "209dc3b3256045688d3743a97d3e4b0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31f2e0a46c3048619abab6494458bda4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4de5587107824a99af6cee8a639d2250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "451c346561a84c26aed5dda5c915a7ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "917ba160f6044036bc5490e132a20b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfa87fdc65fc41d19268f593b9a25ab3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2645ace625e64b108f9b333187c86e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d95789192329418b89cf13603c31fca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21eb804970f44eb79ee4ce3e2ead9d13",
              "IPY_MODEL_fb7ec0004241406e91da814b9db510c2",
              "IPY_MODEL_304de57a3e5744a0b2efdc236a744e34"
            ],
            "layout": "IPY_MODEL_24a9992cd00a469ba3ce9a4c6469505d"
          }
        },
        "21eb804970f44eb79ee4ce3e2ead9d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3368d79c89a54c8e9ff4db38c727567d",
            "placeholder": "​",
            "style": "IPY_MODEL_6b9359320d5041228c4a761fcb394c4d",
            "value": "Inferencing Samples: 100%"
          }
        },
        "fb7ec0004241406e91da814b9db510c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d391a741c7b463baef88e32f212bba7",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8d56c8676a847d387211ea923015261",
            "value": 3
          }
        },
        "304de57a3e5744a0b2efdc236a744e34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f25f9bcc621e4d13939a79321e8be7a8",
            "placeholder": "​",
            "style": "IPY_MODEL_912707223b7147bb96dc15994bd953df",
            "value": " 3/3 [00:01&lt;00:00,  1.26 Batches/s]"
          }
        },
        "24a9992cd00a469ba3ce9a4c6469505d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3368d79c89a54c8e9ff4db38c727567d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b9359320d5041228c4a761fcb394c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d391a741c7b463baef88e32f212bba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8d56c8676a847d387211ea923015261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f25f9bcc621e4d13939a79321e8be7a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "912707223b7147bb96dc15994bd953df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNlfLnQISvHa",
        "outputId": "fb48a8a9-5cc2-4140-a246-ccc105bac507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n",
            "Collecting pip\n",
            "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 16.7 MB/s eta 0:00:00\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.4\n",
            "    Uninstalling pip-22.0.4:\n",
            "      Successfully uninstalled pip-22.0.4\n",
            "Successfully installed pip-23.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting farm-haystack[colab]\n",
            "  Downloading farm_haystack-1.14.0-py3-none-any.whl (640 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 640.4/640.4 kB 5.5 MB/s eta 0:00:00\n",
            "Collecting transformers[torch]==4.25.1\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 35.0 MB/s eta 0:00:00\n",
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Collecting posthog\n",
            "  Downloading posthog-2.3.1-py2.py3-none-any.whl (34 kB)\n",
            "Collecting tiktoken>=0.1.2\n",
            "  Downloading tiktoken-0.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 63.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (3.0)\n",
            "Collecting tika\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (3.7)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (3.19.6)\n",
            "Collecting quantulum3\n",
            "  Downloading quantulum3-0.8.1-py3-none-any.whl (10.7 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/10.7 MB 108.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (2.25.1)\n",
            "Collecting huggingface-hub>=0.5.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.3/190.3 kB 26.2 MB/s eta 0:00:00\n",
            "Collecting mmh3\n",
            "  Downloading mmh3-3.0.0-cp38-cp38-manylinux2010_x86_64.whl (50 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.0/50.0 kB 7.2 MB/s eta 0:00:00\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 16.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.3.5)\n",
            "Collecting elasticsearch<8,>=7.7\n",
            "  Downloading elasticsearch-7.17.9-py2.py3-none-any.whl (385 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 386.0/386.0 kB 43.2 MB/s eta 0:00:00\n",
            "Collecting azure-ai-formrecognizer>=3.2.0b2\n",
            "  Downloading azure_ai_formrecognizer-3.2.0-py3-none-any.whl (228 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.4/228.4 kB 27.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (8.2.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (4.3.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.10.5)\n",
            "Collecting sentence-transformers>=2.2.0\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.0/86.0 kB 4.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 85.9 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (9.1.0)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-2.2.1-py3-none-any.whl (17.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/17.6 MB 90.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.10.1)\n",
            "Collecting rapidfuzz<2.8.0,>=2.0.15\n",
            "  Downloading rapidfuzz-2.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 81.5 MB/s eta 0:00:00\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 6.3 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (4.64.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 57.8 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: pillow<=9.0.0 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (8.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 90.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (3.9.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (1.13.1+cu116)\n",
            "Collecting msrest>=0.6.21\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.4/85.4 kB 10.5 MB/s eta 0:00:00\n",
            "Collecting azure-common~=1.1\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting azure-core<2.0.0,>=1.23.0\n",
            "  Downloading azure_core-1.26.3-py3-none-any.whl (174 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 174.5/174.5 kB 22.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (4.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (1.26.14)\n",
            "Collecting jarowinkler<2.0.0,>=1.2.0\n",
            "  Downloading jarowinkler-1.2.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.1/114.1 kB 14.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (3.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[colab]) (0.14.1+cu116)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 86.5 MB/s eta 0:00:00\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 9.2 MB/s eta 0:00:00\n",
            "Collecting blobfile>=2\n",
            "  Downloading blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.5/73.5 kB 11.3 MB/s eta 0:00:00\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farm-haystack[colab]) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->farm-haystack[colab]) (3.0.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->farm-haystack[colab]) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->farm-haystack[colab]) (5.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->farm-haystack[colab]) (22.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect->farm-haystack[colab]) (1.15.0)\n",
            "Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (9.0.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (1.4.46)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (0.4)\n",
            "Collecting shap<1,>=0.40\n",
            "  Downloading shap-0.41.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (575 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 575.9/575.9 kB 57.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (3.4.1)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (2.2.1)\n",
            "Collecting docker<7,>=4.0.0\n",
            "  Downloading docker-6.0.1-py3-none-any.whl (147 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.5/147.5 kB 20.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (0.4.3)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (3.1.2)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (2.2.3)\n",
            "Collecting querystring-parser<2\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting gunicorn<21\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 12.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (2022.7.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (8.1.3)\n",
            "Collecting gitpython<4,>=2.1.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 25.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (3.5.3)\n",
            "Collecting alembic<2\n",
            "  Downloading alembic-1.10.0-py3-none-any.whl (211 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 25.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (6.0.0)\n",
            "Collecting databricks-cli<1,>=0.8.7\n",
            "  Downloading databricks-cli-0.17.4.tar.gz (82 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.3/82.3 kB 12.5 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->farm-haystack[colab]) (2.8.2)\n",
            "Collecting backoff>=1.10.0\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from python-docx->farm-haystack[colab]) (4.9.2)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.2/125.2 kB 18.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.8/dist-packages (from quantulum3->farm-haystack[colab]) (2.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tika->farm-haystack[colab]) (57.4.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 12.0 MB/s eta 0:00:00\n",
            "Collecting pycryptodomex~=3.8\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 98.4 MB/s eta 0:00:00\n",
            "Collecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (0.8.10)\n",
            "Collecting websocket-client>=0.32.0\n",
            "  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.9/55.9 kB 7.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.8/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (2.2.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.8/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (2.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 8.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow->farm-haystack[colab]) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2<4,>=2.11->mlflow->farm-haystack[colab]) (2.1.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab]) (4.38.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab]) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab]) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab]) (0.11.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.3.1)\n",
            "Collecting isodate>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 5.9 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from shap<1,>=0.40->mlflow->farm-haystack[colab]) (0.56.4)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow->farm-haystack[colab]) (2.0.2)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->shap<1,>=0.40->mlflow->farm-haystack[colab]) (0.39.1)\n",
            "Building wheels for collected packages: sentence-transformers, langdetect, python-docx, seqeval, tika, databricks-cli, docopt\n",
            "  Building wheel for sentence-transformers (setup.py): started\n",
            "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=06df0ec101799b9c825792b0ba8b0d24865115670c39d4664c3ea54c6f67c7d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "  Building wheel for langdetect (setup.py): started\n",
            "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=0807f5094ffaa71689e7226c59085d4c1328529e08e292473571f00d85b508c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "  Building wheel for python-docx (setup.py): started\n",
            "  Building wheel for python-docx (setup.py): finished with status 'done'\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184505 sha256=ef701f6245492fdb7aa69f4dcc37875b1e05cd3b6e57565ec31c089b43051553\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/b8/b2/c4c2b95765e615fe139b0b17b5ea7c0e1b6519b0a9ec8fb34d\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=5cc58bbac374650c153aa7a29b4523d6aad6dd8865a876b4983b66303b37b70b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "  Building wheel for tika (setup.py): started\n",
            "  Building wheel for tika (setup.py): finished with status 'done'\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32642 sha256=d551909d951d2c01d1cc797944b48ce964477b928be583e94a38297c68596f9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/bd/74/313abcb9271e041e30734880e6813385150dd93627e9659de5\n",
            "  Building wheel for databricks-cli (setup.py): started\n",
            "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.4-py3-none-any.whl size=142894 sha256=3d3be4488c5145d6f21f6b6974a70565e1ed2d606c77fe0e225f9220baf73033\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/7c/6e/4bf2c1748c7ecf994ca951591de81674ed6bf633e1e337d873\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=fced16bbcebd68fd617745e7db106ee35161b284185bbdbf90b7c699a644e88f\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "Successfully built sentence-transformers langdetect python-docx seqeval tika databricks-cli docopt\n",
            "Installing collected packages: tokenizers, sentencepiece, monotonic, mmh3, docopt, azure-common, websocket-client, smmap, slicer, requests, rank-bm25, querystring-parser, python-docx, pyjwt, pycryptodomex, num2words, Mako, langdetect, jarowinkler, isodate, gunicorn, elasticsearch, dill, backoff, tika, rapidfuzz, quantulum3, posthog, huggingface-hub, gitdb, docker, databricks-cli, blobfile, azure-core, alembic, transformers, tiktoken, shap, seqeval, msrest, gitpython, sentence-transformers, mlflow, azure-ai-formrecognizer, farm-haystack\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "Successfully installed Mako-1.2.4 alembic-1.10.0 azure-ai-formrecognizer-3.2.0 azure-common-1.1.28 azure-core-1.26.3 backoff-2.2.1 blobfile-2.0.1 databricks-cli-0.17.4 dill-0.3.6 docker-6.0.1 docopt-0.6.2 elasticsearch-7.17.9 farm-haystack-1.14.0 gitdb-4.0.10 gitpython-3.1.31 gunicorn-20.1.0 huggingface-hub-0.12.1 isodate-0.6.1 jarowinkler-1.2.3 langdetect-1.0.9 mlflow-2.2.1 mmh3-3.0.0 monotonic-1.6 msrest-0.7.1 num2words-0.5.12 posthog-2.3.1 pycryptodomex-3.17 pyjwt-2.6.0 python-docx-0.8.11 quantulum3-0.8.1 querystring-parser-1.2.4 rank-bm25-0.2.2 rapidfuzz-2.7.0 requests-2.28.2 sentence-transformers-2.2.2 sentencepiece-0.1.97 seqeval-1.2.2 shap-0.41.0 slicer-0.0.7 smmap-5.0.0 tika-2.6.0 tiktoken-0.3.0 tokenizers-0.13.2 transformers-4.25.1 websocket-client-1.5.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you have a GPU running\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOZs8-rhu8yL",
        "outputId": "e4e017e1-5d64-4aa2-e4a3-7e93f1bcebf0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar  6 07:03:52 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    32W /  70W |   2621MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest release of Haystack in your own environment\n",
        "#! pip install farm-haystack\n",
        "\n",
        "# Install the latest main of Haystack\n",
        "!pip install --upgrade pip\n",
        "!pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSnbnq4-vR8o",
        "outputId": "ddbf6732-c0f0-464e-b91a-4b014ccae5c4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (23.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting farm-haystack[colab]\n",
            "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-install-brwbo0vf/farm-haystack_1718f2a44aea463a92c614c4af723c9e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git /tmp/pip-install-brwbo0vf/farm-haystack_1718f2a44aea463a92c614c4af723c9e\n",
            "  Resolved https://github.com/deepset-ai/haystack.git to commit 1a421669786c80feee5ef17511202529c9c30c84\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.10.5)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.0.9)\n",
            "Requirement already satisfied: transformers[torch]==4.25.1 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (4.25.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (0.8.11)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (0.3.6)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (4.64.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (9.1.0)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (0.2.2)\n",
            "Requirement already satisfied: tika in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (2.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (3.19.6)\n",
            "Requirement already satisfied: elasticsearch<8,>=7.7 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (7.17.9)\n",
            "Requirement already satisfied: mmh3 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (3.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (3.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.2.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (3.7)\n",
            "Requirement already satisfied: quantulum3 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (0.8.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.2.0 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (2.2.2)\n",
            "Requirement already satisfied: rapidfuzz<2.8.0,>=2.0.15 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (2.7.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.3.5)\n",
            "Requirement already satisfied: tiktoken>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (0.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (2.28.2)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (8.2.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (4.3.3)\n",
            "Requirement already satisfied: azure-ai-formrecognizer>=3.2.0b2 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (3.2.0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (1.10.1)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (2.2.1)\n",
            "Requirement already satisfied: pillow<=9.0.0 in /usr/local/lib/python3.8/dist-packages (from farm-haystack[colab]) (8.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (2022.6.2)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]==4.25.1->farm-haystack[colab]) (1.13.1+cu116)\n",
            "Requirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.8/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.1.28)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (4.5.0)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in /usr/local/lib/python3.8/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.26.3)\n",
            "Requirement already satisfied: msrest>=0.6.21 in /usr/local/lib/python3.8/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (0.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from elasticsearch<8,>=7.7->farm-haystack[colab]) (1.26.14)\n",
            "Requirement already satisfied: jarowinkler<2.0.0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from rapidfuzz<2.8.0,>=2.0.15->farm-haystack[colab]) (1.2.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->farm-haystack[colab]) (3.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[colab]) (0.14.1+cu116)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[colab]) (0.1.97)\n",
            "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.8/dist-packages (from tiktoken>=0.2.0->farm-haystack[colab]) (2.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->farm-haystack[colab]) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->farm-haystack[colab]) (3.0.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->farm-haystack[colab]) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->farm-haystack[colab]) (5.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->farm-haystack[colab]) (22.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect->farm-haystack[colab]) (1.15.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (3.4.1)\n",
            "Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (6.0.1)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (3.5.3)\n",
            "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (0.17.4)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (1.4.46)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (0.4.3)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (3.1.2)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (6.0.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (8.1.3)\n",
            "Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (9.0.0)\n",
            "Requirement already satisfied: shap<1,>=0.40 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (0.41.0)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (0.4)\n",
            "Requirement already satisfied: alembic<2 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (1.10.0)\n",
            "Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (2022.7.1)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (3.1.31)\n",
            "Requirement already satisfied: gunicorn<21 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (20.1.0)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (2.2.1)\n",
            "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (1.2.4)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.8/dist-packages (from mlflow->farm-haystack[colab]) (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->farm-haystack[colab]) (2.8.2)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from posthog->farm-haystack[colab]) (2.2.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.8/dist-packages (from posthog->farm-haystack[colab]) (1.6)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from python-docx->farm-haystack[colab]) (4.9.2)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.8/dist-packages (from quantulum3->farm-haystack[colab]) (0.5.12)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.8/dist-packages (from quantulum3->farm-haystack[colab]) (2.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tika->farm-haystack[colab]) (57.4.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.8/dist-packages (from alembic<2->mlflow->farm-haystack[colab]) (1.2.4)\n",
            "Requirement already satisfied: pycryptodomex~=3.8 in /usr/local/lib/python3.8/dist-packages (from blobfile>=2->tiktoken>=0.2.0->farm-haystack[colab]) (3.17)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (2.6.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (0.8.10)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack[colab]) (3.2.2)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from docker<7,>=4.0.0->mlflow->farm-haystack[colab]) (1.5.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.8/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (2.2.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.8/dist-packages (from Flask<3->mlflow->farm-haystack[colab]) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython<4,>=2.1.0->mlflow->farm-haystack[colab]) (4.0.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow->farm-haystack[colab]) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2<4,>=2.11->mlflow->farm-haystack[colab]) (2.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab]) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab]) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab]) (4.38.0)\n",
            "Requirement already satisfied: isodate>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab]) (1.3.1)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.8/dist-packages (from shap<1,>=0.40->mlflow->farm-haystack[colab]) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from shap<1,>=0.40->mlflow->farm-haystack[colab]) (0.56.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow->farm-haystack[colab]) (2.0.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.8/dist-packages (from num2words->quantulum3->farm-haystack[colab]) (0.6.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow->farm-haystack[colab]) (5.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->shap<1,>=0.40->mlflow->farm-haystack[colab]) (0.39.1)\n",
            "Building wheels for collected packages: farm-haystack\n",
            "  Building wheel for farm-haystack (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for farm-haystack: filename=farm_haystack-1.15.0rc0-py3-none-any.whl size=649534 sha256=84c1d3453a71e4423049441316e3425559b1a547facdf71ddf703c74373acaa7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f2a3o28e/wheels/97/0d/ae/77cde17929fbf66c8320f19b30789acfe52e2312bb1d125be1\n",
            "Successfully built farm-haystack\n",
            "Installing collected packages: farm-haystack\n",
            "  Attempting uninstall: farm-haystack\n",
            "    Found existing installation: farm-haystack 1.14.0\n",
            "    Uninstalling farm-haystack-1.14.0:\n",
            "      Successfully uninstalled farm-haystack-1.14.0\n",
            "Successfully installed farm-haystack-1.15.0rc0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Jh_hcsAlw71y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9558fdb3-26f1-4817-c176-888893d7a2e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "bP9Y8wtXVyTh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.document_stores import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore(use_bm25=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9BMjfKKV5JQ",
        "outputId": "3d09fd96-522a-4f70-922b-2e5e7c2af267"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the document: Add DocOps Document here"
      ],
      "metadata": {
        "id": "sQlL0Mn9WBCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_zip_file = 'drive/My Drive/MF/'\n",
        "directory_to_extract_to = 'Opsb_Corpus'\n",
        "doc_dir = path_to_zip_file+'/'+directory_to_extract_to\n",
        "\n"
      ],
      "metadata": {
        "id": "Z5WvKU8_WHEL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.nodes import TextConverter, PreProcessor\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "text_converter = TextConverter()\n",
        "preprocessor = PreProcessor(\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=True,\n",
        "    clean_empty_lines=True,\n",
        "    split_by=\"word\",\n",
        "    split_length=500,\n",
        "    split_overlap=50,\n",
        "    split_respect_sentence_boundary=True,\n",
        ")"
      ],
      "metadata": {
        "id": "9TVJgLKVpVSK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "indexing_pipeline.add_node(component=text_converter, name=\"TextConverter\", inputs=[\"File\"])\n",
        "#indexing_pipeline.add_node(component=preprocessor, name=\"PreProcessor\", inputs=[\"TextConverter\"])\n",
        "indexing_pipeline.add_node(component=document_store, name=\"DocumentStore\", inputs=[\"TextConverter\"])"
      ],
      "metadata": {
        "id": "vnbsb2lrpd5A"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
        "indexing_pipeline.run_batch(file_paths=files_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e517f244ef71482897fe95f574bb20e0",
            "e3ee75a919074003aebf4e741b18e9c1",
            "d7e0be4915434e2a9b0ee2804098220b",
            "2551dba898d6441ca52bf9a37e256ddc",
            "87ee1f2d41424e93a170c7e7db477df4",
            "109e5c44a3754a0487bfc3ad9630f8bc",
            "0b90c3d0e4bd4b8885431d4a5e8a26e3",
            "fd20fbbf7b9a485d9b8c540f1fa78a0f",
            "9267ebea6ecf430c9cfd9e14e3ce84f8",
            "3db597ef5ca847e99c937c5dc2b61a29",
            "9d501b9ba32c49e1bb254156cdb30b03",
            "d21b56beca2848d4a9e1bf6ce34b80a0",
            "b31e25ad35604dd18b11cb1c89520090",
            "3ce9e38571ba40578415df7ee096450a",
            "b68ee6df17ee4cc994ac892e8c20de67",
            "6439051843cd430d9ab8e8189e53e4a7",
            "90548ba6b60c4032bca6b7edd4edfc48",
            "c012c7fe23f14c31815d482f46702cec",
            "85ba9ee8b91a442e8d0422d25ad1948c",
            "7f9516f928b54426acca6c04d2c21fbc",
            "3b187cf767f946b7acafcb70e715ec13",
            "662d21e3ec78415eb57d8744a2cf60bb"
          ]
        },
        "id": "O5pGTlpjpk6A",
        "outputId": "8c4692dc-8163-4054-faea-bfda76772593"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.pipelines.base:It seems that an indexing Pipeline is run, so using the nodes' run method instead of run_batch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting files:   0%|          | 0/499 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e517f244ef71482897fe95f574bb20e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Updating BM25 representation...:   0%|          | 0/499 [00:00<?, ? docs/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d21b56beca2848d4a9e1bf6ce34b80a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'documents': [<Document: {'content': 'Get started\\nContainerized Operations Bridge offers a monitoring solution that consolidates data from existing tools. It applies automated\\ndiscovery, monitoring, analytics, and remediation to data across traditional, private, public, multi cloud, and container based\\ninfrastructure. Using this solution, you can achieve the following goals:\\nEvent consolidation - You can manage events from all your domains by consolidating them in Operations Bridge Manager\\n(OBM). This consolidated view helps your IT operators to identify the causes of IT incidents and rectify issues. You can also\\ncorrelate similar events, suppress unwanted events, or promote events based on your business requirements. \\nNoise reduction - You can also get automatically correlated events in OBM. Driven by artificial intelligence (AI), automatic\\ncorrelation reduces event related noise.\\nCross domain business service driven out-of-the-box Reports. You can collect metrics, topology, and events from data\\nsources like SiteScope, Business Process Monitor, Real User Monitor, and OBM (classic and containerized) and send them\\nto a central data lake - OPTIC Data Lake. You can view reports for resource, event, and response time reporting across\\nserver, network, and application environments using Business Value Dashboard (BVD) and Performance Dashboard.\\nFlexible design driven reporting  - You can adjust the design of the variety of out of the box dashboards. In addition, you\\ncan also create reports over a well defined public data schema (Operations Bridge Data Model). \\n \\nContainerized Operations Bridge 2022.11\\nPage \\n113\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Get started\\nContainerized Operations Bridge offers a monitoring solution that consolidates data from existing tools. It applies automated\\ndiscovery, monitoring, analytics, and remediation to data across traditional, private, public, multi cloud, and container based\\ninfrastructure. Using this solution, you can achieve the following goals:\\nEvent consolidation - You can manage events from all your domains by consolidating them in Operations Bridge Manager\\n(OBM). This consolidated view helps your IT operators to identify the causes of IT incidents and rectify issues. You can also\\ncorrelate similar events, suppress unwanted events, or promote events based on your business requirements. \\nNoise reduction - You can also get automatically correlated events in OBM. Driven by artificial intelligence (AI), automatic\\ncorrelation reduces event related noise.\\nCross domain business service driven out-of-the-box Reports. You can collect metrics, topology, and events from data\\nsources like SiteScope, Business Process Monitor, Real User Monitor, and OBM (classic and containerized) and send them\\nto a central data lake - OPTIC Data Lake. You can view reports for resource, event, and response time reporting across\\nserver, network, and application environments using Business Value Dashboard (BVD) and Performance Dashboard.\\nFlexible design driven reporting  - You can adjust the design of the variety of out of the box dashboards. In addition, you\\ncan also create reports over a well defined public data schema (Operations Bridge Data Model). \\n \\nContainerized Operations Bridge 2022.11\\nPage \\n113\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Get started\\nContainerized Operations Bridge offers a monitoring solution that consolidates data from existing tools. It applies automated\\ndiscovery, monitoring, analytics, and remediation to data across traditional, private, public, multi cloud, and container based\\ninfrastructure. Using this solution, you can achieve the following goals:\\nEvent consolidation - You can manage events from all your domains by consolidating them in Operations Bridge Manager\\n(OBM). This consolidated view helps your IT operators to identify the causes of IT incidents and rectify issues. You can also\\ncorrelate similar events, suppress unwanted events, or promote events based on your business requirements. \\nNoise reduction - You can also get automatically correlated events in OBM. Driven by artificial intelligence (AI), automatic\\ncorrelation reduces event related noise.\\nCross domain business service driven out-of-the-box Reports. You can collect metrics, topology, and events from data\\nsources like SiteScope, Business Process Monitor, Real User Monitor, and OBM (classic and containerized) and send them\\nto a central data lake - OPTIC Data Lake. You can view reports for resource, event, and response time reporting across\\nserver, network, and application environments using Business Value Dashboard (BVD) and Performance Dashboard.\\nFlexible design driven reporting  - You can adjust the design of the variety of out of the box dashboards. In addition, you\\ncan also create reports over a well defined public data schema (Operations Bridge Data Model). \\n \\nContainerized Operations Bridge 2022.11\\nPage \\n113\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f1123634d31c46ef20d080a1e270d0ea'}>,\n",
              "  <Document: {'content': \"Key capabilities and concepts\\nContainerized Operations Bridge provides the following capabilities:\\nStakeholder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerized OBM\\nAnomaly Detection\\n* The administration and configuration of these capabilities are supported through Monitoring Control Center (MCC). \\nStakeholder Dashboards \\nThe stakeholder dashboard gives you visual information of live data using tables, charts, and widgets. You can stream real-\\ntime data from any data source in JSON format via HTTP post. Stakeholder Dashboards can integrate with a variety of data\\nsources and enable you to represent data in several valuable perspectives. You can create custom dashboards, near real-time\\ndashboards, receive real-time updates, and access information from any device with a browser. \\nOPTIC Reporting \\nThe OPTIC Reporting capability provides you with all the artifacts required for IT infrastructure and event management. It\\nconsists of the following components - Operations Bridge Reporting Content, BVD Reports, Flex Reports and OPTIC Data\\nLake. It consolidates performance metrics, event metrics, and response time data into tables, graphs, and dashboards and\\nenables you to visualize and analyze your IT environment. OPTIC Reporting uses OPTIC One to create reports and visualize the\\nvisualize the real time data of the IT infrastructure. You can create the following reports. \\nBVD reports: BVD reports display visual information of historical or recorded data using tables, charts, and widgets. You\\ncan use Visio to create dashboards and attach data from data queries, which can retrieve data from any Vertica database\\nusing SQL queries. BVD reports are created in OPTIC One. \\nFlex reports: Flex Reports is web-based authoring that allows you to create dashboards and reports (eliminates the use of\\ntools such as visio). You can create and customize reports and dashboards based on your business needs.  Flex reports\\nare created in OPTIC One.\\nBYOBI reports: You can create custom reports using using the Business Intelligence (BI) tool of your choice. \\nYou can send data (performance metrics, events, and topology) from different sources to OPTIC Data Lake. You can represent\\nthis data graphically either on BVD or any other business intelligence tool (BYOBI) of your choice.\\nOperations Bridge offers several out of the box reports that you can view on BVD. These reports are available in the following\\ncategories: \\nSystem Infrastructure reports\\n: Use these reports to view information about the availability and performance of\\nphysical systems in your environment. You can also view information about the average utilization of resources for a\\nchosen period of time. You can use the Agent Metric Collector, Metric Streaming policies, or SiteScope to collect system\\ninfrastructure metrics. You can view historical data collected by AMC in the System Infrastructure reports. \\nSystem Infrastructure reports are downtime aware. The metrics received during planned downtime are excluded from\\ncalculations performed while aggregating data. \\nDowntime aware reports are currently available for system infrastructure metrics collected through AMC and SiteScope.\\nThey're not supported for Metric Streaming policies, BPM, and RUM.\\nEvent reports\\n: Use these reports to view statistics about the events that come into OBM from various data sources like\\nAgent metric collector, SiteScope, RUM, and BPM. Using these reports, you can gain insight into event trends, the severity\\nof events, and event assignments.\\nSynthetic Transaction reports\\n: Use these reports to view information about synthetic polling of end user experience,\\navailability, and performance of applications. You can use Business Process Monitor (BPM) to collect synthetic transaction\\nmetrics. \\nReal User Monitor reports\\n: Use these reports to view information about real end user experience, availability, and\\nperformance of applications. You can use Real User Monitor (RUM) to monitor real user behavior and collect the metrics.\\nAutomatic Event Correlation\\nThe Automatic Event Correlation capability provides you with all the artifacts required for event management, event\\nconsolidation, and noise reduction. This capability consists of OPTIC Data Lake.\\nAutomatic Event Correlation offers the facility to automatically correlate events coming from OBM, by analyzing patterns. This\\nautomatic event correlation (AEC) uses a machine learning algorithm to group related events into a single event and sends the\\ncorrelated event back to OBM. Thus, as an operator viewing the event console in OBM, you will be able to identify key events\\nthat can solve many underlying events.\\nMonitoring Control Center\\nMonitoring Control Center (MCC) is a new section of the Operations Bridge UI that consolidates the administration and\\nconfiguration of monitoring capabilities into a central user experience. MCC provides the following capabilities:\\nAgentless Monitoring\\nHyperscale Observability\\nAgentless Monitoring\\nAgentless Monitoring enables you to gain an overview about the health, availability, and performance of a hybrid set of\\nsystems and applications deployed on-premise and cloud in your infrastructure. The solution provides a single view of the\\nhealth, availability, and performance metrics from multiple infrastructure monitoring solutions, such as SiteScope.\\nContainerized Operations Bridge 2022.11\\nPage \\n114\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Key capabilities and concepts\\nContainerized Operations Bridge provides the following capabilities:\\nStakeholder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerized OBM\\nAnomaly Detection\\n* The administration and configuration of these capabilities are supported through Monitoring Control Center (MCC). \\nStakeholder Dashboards \\nThe stakeholder dashboard gives you visual information of live data using tables, charts, and widgets. You can stream real-\\ntime data from any data source in JSON format via HTTP post. Stakeholder Dashboards can integrate with a variety of data\\nsources and enable you to represent data in several valuable perspectives. You can create custom dashboards, near real-time\\ndashboards, receive real-time updates, and access information from any device with a browser. \\nOPTIC Reporting \\nThe OPTIC Reporting capability provides you with all the artifacts required for IT infrastructure and event management. It\\nconsists of the following components - Operations Bridge Reporting Content, BVD Reports, Flex Reports and OPTIC Data\\nLake. It consolidates performance metrics, event metrics, and response time data into tables, graphs, and dashboards and\\nenables you to visualize and analyze your IT environment. OPTIC Reporting uses OPTIC One to create reports and visualize the\\nvisualize the real time data of the IT infrastructure. You can create the following reports. \\nBVD reports: BVD reports display visual information of historical or recorded data using tables, charts, and widgets. You\\ncan use Visio to create dashboards and attach data from data queries, which can retrieve data from any Vertica database\\nusing SQL queries. BVD reports are created in OPTIC One. \\nFlex reports: Flex Reports is web-based authoring that allows you to create dashboards and reports (eliminates the use of\\ntools such as visio). You can create and customize reports and dashboards based on your business needs.  Flex reports\\nare created in OPTIC One.\\nBYOBI reports: You can create custom reports using using the Business Intelligence (BI) tool of your choice. \\nYou can send data (performance metrics, events, and topology) from different sources to OPTIC Data Lake. You can represent\\nthis data graphically either on BVD or any other business intelligence tool (BYOBI) of your choice.\\nOperations Bridge offers several out of the box reports that you can view on BVD. These reports are available in the following\\ncategories: \\nSystem Infrastructure reports\\n: Use these reports to view information about the availability and performance of\\nphysical systems in your environment. You can also view information about the average utilization of resources for a\\nchosen period of time. You can use the Agent Metric Collector, Metric Streaming policies, or SiteScope to collect system\\ninfrastructure metrics. You can view historical data collected by AMC in the System Infrastructure reports. \\nSystem Infrastructure reports are downtime aware. The metrics received during planned downtime are excluded from\\ncalculations performed while aggregating data. \\nDowntime aware reports are currently available for system infrastructure metrics collected through AMC and SiteScope.\\nThey're not supported for Metric Streaming policies, BPM, and RUM.\\nEvent reports\\n: Use these reports to view statistics about the events that come into OBM from various data sources like\\nAgent metric collector, SiteScope, RUM, and BPM. Using these reports, you can gain insight into event trends, the severity\\nof events, and event assignments.\\nSynthetic Transaction reports\\n: Use these reports to view information about synthetic polling of end user experience,\\navailability, and performance of applications. You can use Business Process Monitor (BPM) to collect synthetic transaction\\nmetrics. \\nReal User Monitor reports\\n: Use these reports to view information about real end user experience, availability, and\\nperformance of applications. You can use Real User Monitor (RUM) to monitor real user behavior and collect the metrics.\\nAutomatic Event Correlation\\nThe Automatic Event Correlation capability provides you with all the artifacts required for event management, event\\nconsolidation, and noise reduction. This capability consists of OPTIC Data Lake.\\nAutomatic Event Correlation offers the facility to automatically correlate events coming from OBM, by analyzing patterns. This\\nautomatic event correlation (AEC) uses a machine learning algorithm to group related events into a single event and sends the\\ncorrelated event back to OBM. Thus, as an operator viewing the event console in OBM, you will be able to identify key events\\nthat can solve many underlying events.\\nMonitoring Control Center\\nMonitoring Control Center (MCC) is a new section of the Operations Bridge UI that consolidates the administration and\\nconfiguration of monitoring capabilities into a central user experience. MCC provides the following capabilities:\\nAgentless Monitoring\\nHyperscale Observability\\nAgentless Monitoring\\nAgentless Monitoring enables you to gain an overview about the health, availability, and performance of a hybrid set of\\nsystems and applications deployed on-premise and cloud in your infrastructure. The solution provides a single view of the\\nhealth, availability, and performance metrics from multiple infrastructure monitoring solutions, such as SiteScope.\\nContainerized Operations Bridge 2022.11\\nPage \\n114\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Key capabilities and concepts\\nContainerized Operations Bridge provides the following capabilities:\\nStakeholder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerized OBM\\nAnomaly Detection\\n* The administration and configuration of these capabilities are supported through Monitoring Control Center (MCC). \\nStakeholder Dashboards \\nThe stakeholder dashboard gives you visual information of live data using tables, charts, and widgets. You can stream real-\\ntime data from any data source in JSON format via HTTP post. Stakeholder Dashboards can integrate with a variety of data\\nsources and enable you to represent data in several valuable perspectives. You can create custom dashboards, near real-time\\ndashboards, receive real-time updates, and access information from any device with a browser. \\nOPTIC Reporting \\nThe OPTIC Reporting capability provides you with all the artifacts required for IT infrastructure and event management. It\\nconsists of the following components - Operations Bridge Reporting Content, BVD Reports, Flex Reports and OPTIC Data\\nLake. It consolidates performance metrics, event metrics, and response time data into tables, graphs, and dashboards and\\nenables you to visualize and analyze your IT environment. OPTIC Reporting uses OPTIC One to create reports and visualize the\\nvisualize the real time data of the IT infrastructure. You can create the following reports. \\nBVD reports: BVD reports display visual information of historical or recorded data using tables, charts, and widgets. You\\ncan use Visio to create dashboards and attach data from data queries, which can retrieve data from any Vertica database\\nusing SQL queries. BVD reports are created in OPTIC One. \\nFlex reports: Flex Reports is web-based authoring that allows you to create dashboards and reports (eliminates the use of\\ntools such as visio). You can create and customize reports and dashboards based on your business needs.  Flex reports\\nare created in OPTIC One.\\nBYOBI reports: You can create custom reports using using the Business Intelligence (BI) tool of your choice. \\nYou can send data (performance metrics, events, and topology) from different sources to OPTIC Data Lake. You can represent\\nthis data graphically either on BVD or any other business intelligence tool (BYOBI) of your choice.\\nOperations Bridge offers several out of the box reports that you can view on BVD. These reports are available in the following\\ncategories: \\nSystem Infrastructure reports\\n: Use these reports to view information about the availability and performance of\\nphysical systems in your environment. You can also view information about the average utilization of resources for a\\nchosen period of time. You can use the Agent Metric Collector, Metric Streaming policies, or SiteScope to collect system\\ninfrastructure metrics. You can view historical data collected by AMC in the System Infrastructure reports. \\nSystem Infrastructure reports are downtime aware. The metrics received during planned downtime are excluded from\\ncalculations performed while aggregating data. \\nDowntime aware reports are currently available for system infrastructure metrics collected through AMC and SiteScope.\\nThey're not supported for Metric Streaming policies, BPM, and RUM.\\nEvent reports\\n: Use these reports to view statistics about the events that come into OBM from various data sources like\\nAgent metric collector, SiteScope, RUM, and BPM. Using these reports, you can gain insight into event trends, the severity\\nof events, and event assignments.\\nSynthetic Transaction reports\\n: Use these reports to view information about synthetic polling of end user experience,\\navailability, and performance of applications. You can use Business Process Monitor (BPM) to collect synthetic transaction\\nmetrics. \\nReal User Monitor reports\\n: Use these reports to view information about real end user experience, availability, and\\nperformance of applications. You can use Real User Monitor (RUM) to monitor real user behavior and collect the metrics.\\nAutomatic Event Correlation\\nThe Automatic Event Correlation capability provides you with all the artifacts required for event management, event\\nconsolidation, and noise reduction. This capability consists of OPTIC Data Lake.\\nAutomatic Event Correlation offers the facility to automatically correlate events coming from OBM, by analyzing patterns. This\\nautomatic event correlation (AEC) uses a machine learning algorithm to group related events into a single event and sends the\\ncorrelated event back to OBM. Thus, as an operator viewing the event console in OBM, you will be able to identify key events\\nthat can solve many underlying events.\\nMonitoring Control Center\\nMonitoring Control Center (MCC) is a new section of the Operations Bridge UI that consolidates the administration and\\nconfiguration of monitoring capabilities into a central user experience. MCC provides the following capabilities:\\nAgentless Monitoring\\nHyperscale Observability\\nAgentless Monitoring\\nAgentless Monitoring enables you to gain an overview about the health, availability, and performance of a hybrid set of\\nsystems and applications deployed on-premise and cloud in your infrastructure. The solution provides a single view of the\\nhealth, availability, and performance metrics from multiple infrastructure monitoring solutions, such as SiteScope.\\nContainerized Operations Bridge 2022.11\\nPage \\n114\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5fc52982237960a004fc3919c6add461'}>,\n",
              "  <Document: {'content': \"Using Agentless Monitoring, you can now view and manage monitoring data from multiple providers in a single interface. For\\nexample, Agentless Monitoring users can now view and manage multiple SiteScope instances in a single interface.\\nThe Agentless Monitoring solution contains the following main components:\\nMonitoring configuration service:\\n Registration and access control for monitoring operations\\nUI Service: \\nAn HTML user interface for users to view, create, and manage the monitoring operations.\\nThe following are the limitations in installation and configuration:\\nNo proxy between Monitor UI and SiteScope\\nNo Client certificate authentication\\nBefore you begin, it's important to understand the following concepts. \\nAgentless Monitoring User Interface \\nThe Agentless Monitoring user interface provides an interface where you can view and monitor your infrastructure across\\nmultiple providers. For more information, see \\nUse\\n.\\n1\\n. \\nProvider Group: \\nA provider group is a logical group of a specific type of provider providing monitoring ability of a\\nspecific provider type.  For example, a group of SiteScope instances is a provider group.\\n2\\n. \\nProvider\\n: A provider is a collector or a data source that enables you to monitor health, availability, and performance of\\nyour infrastructure, applications, or cloud systems.  For example, a single SiteScope instance is a provider. \\n3\\n. \\nMonitor Group\\n: A monitor group is a logical group of a specific set of monitors that monitor your infrastructure or\\napplication component(s). Use Monitor Groups to organize and administer monitoring artifacts.\\n4\\n. \\nMonitor:\\n A monitor is a logical configuration that groups a set of availability and performance measurements,\\nthresholds, and associated configuration item topology for infrastructure and applications.\\n5\\n. \\nActions\\n: Actions you can perform on a monitor: Edit, Copy, Alerts, and Delete.  \\n6\\n. \\nToolbar\\n: Use the toolbar to manage the monitor tree.  Search, Filter, or Add new entries as needed.  \\n7\\n. \\nDetails:\\n Select an entity in the tree, and then view or modify it in the details pane. \\nHyperscale Observability\\nHyperscale Observability capability provides scalable monitoring of your AWS, Azure, and Kubernetes resources. It brings with\\nit the dynamic detection of new resources as they get added to your AWS or Azure subscription, or Kubernetes account.\\nHyperscale Observability supports the discovery and monitoring of AWS services, Azure services, and Kubernetes objects. For\\nmore details about the supported AWS services, Azure services, and Kubernetes objects, see \\nSupported AWS\\nservices\\n, \\nSupported Azure services\\n, and \\nSupported Kubernetes objects\\n.\\nHyperscale Observability helps to collect metrics, set thresholds, and monitor the metrics for any breaches. If there is a breach\\nin threshold, an event gets generated in OBM. You can visualize the monitored data in OBM using performance dashboards.\\nYou can also view the generated events in the Event Browser in OBM.\\nFor end-to-end use cases of monitoring AWS, Azure, and Kubernetes using Hyperscale Observability, see \\nMonitor AWS using\\nHyperscale Observability\\n, \\nMonitor Azure using Hyperscale Observability\\n, and \\nMonitor Kubernetes using Hyperscale\\nObservability\\n topics.\\nContainerized Operations Bridge Manager\\nOBM is also available as a container for Helm based deployment combines events from application management components\\nwith events from the system and network products. This enables you to keep track of all the events that occur in your\\nmonitored environment. Watch the video below to see how OBM, as part of the Operations Bridge, helps you see all your data\\nthrough a single pane of glass:\\nOperations Bridge demo\\nAdditionally, you can also install Management Packs on OBM. They deliver automatic and complete monitoring solutions for\\ninfrastructure and applications. Using Management Packs, you can monitor, detect, troubleshoot, and remediate issues in the\\nImportant:\\n Agentless Monitoring is supported on OMT, Embedded Kubernetes, and K3S.\\nUsing Agentless Monitoring, you can now view and manage monitoring data from multiple providers in a single interface. For\\nexample, Agentless Monitoring users can now view and manage multiple SiteScope instances in a single interface.\\nThe Agentless Monitoring solution contains the following main components:\\nMonitoring configuration service:\\n Registration and access control for monitoring operations\\nUI Service: \\nAn HTML user interface for users to view, create, and manage the monitoring operations.\\nThe following are the limitations in installation and configuration:\\nNo proxy between Monitor UI and SiteScope\\nNo Client certificate authentication\\nBefore you begin, it's important to understand the following concepts. \\nAgentless Monitoring User Interface \\nThe Agentless Monitoring user interface provides an interface where you can view and monitor your infrastructure across\\nmultiple providers. For more information, see \\nUse\\n.\\n1\\n. \\nProvider Group: \\nA provider group is a logical group of a specific type of provider providing monitoring ability of a\\nspecific provider type.  For example, a group of SiteScope instances is a provider group.\\n2\\n. \\nProvider\\n: A provider is a collector or a data source that enables you to monitor health, availability, and performance of\\nyour infrastructure, applications, or cloud systems.  For example, a single SiteScope instance is a provider. \\n3\\n. \\nMonitor Group\\n: A monitor group is a logical group of a specific set of monitors that monitor your infrastructure or\\napplication component(s). Use Monitor Groups to organize and administer monitoring artifacts.\\n4\\n. \\nMonitor:\\n A monitor is a logical configuration that groups a set of availability and performance measurements,\\nthresholds, and associated configuration item topology for infrastructure and applications.\\n5\\n. \\nActions\\n: Actions you can perform on a monitor: Edit, Copy, Alerts, and Delete.  \\n6\\n. \\nToolbar\\n: Use the toolbar to manage the monitor tree.  Search, Filter, or Add new entries as needed.  \\n7\\n. \\nDetails:\\n Select an entity in the tree, and then view or modify it in the details pane. \\nHyperscale Observability\\nHyperscale Observability capability provides scalable monitoring of your AWS, Azure, and Kubernetes resources. It brings with\\nit the dynamic detection of new resources as they get added to your AWS or Azure subscription, or Kubernetes account.\\nHyperscale Observability supports the discovery and monitoring of AWS services, Azure services, and Kubernetes objects. For\\nmore details about the supported AWS services, Azure services, and Kubernetes objects, see \\nSupported AWS\\nservices\\n, \\nSupported Azure services\\n, and \\nSupported Kubernetes objects\\n.\\nHyperscale Observability helps to collect metrics, set thresholds, and monitor the metrics for any breaches. If there is a breach\\nin threshold, an event gets generated in OBM. You can visualize the monitored data in OBM using performance dashboards.\\nYou can also view the generated events in the Event Browser in OBM.\\nFor end-to-end use cases of monitoring AWS, Azure, and Kubernetes using Hyperscale Observability, see \\nMonitor AWS using\\nHyperscale Observability\\n, \\nMonitor Azure using Hyperscale Observability\\n, and \\nMonitor Kubernetes using Hyperscale\\nObservability\\n topics.\\nContainerized Operations Bridge Manager\\nOBM is also available as a container for Helm based deployment combines events from application management components\\nwith events from the system and network products. This enables you to keep track of all the events that occur in your\\nmonitored environment. Watch the video below to see how OBM, as part of the Operations Bridge, helps you see all your data\\nthrough a single pane of glass:\\nOperations Bridge demo\\nAdditionally, you can also install Management Packs on OBM. They deliver automatic and complete monitoring solutions for\\ninfrastructure and applications. Using Management Packs, you can monitor, detect, troubleshoot, and remediate issues in the\\nImportant:\\n Agentless Monitoring is supported on OMT, Embedded Kubernetes, and K3S.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n115\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Using Agentless Monitoring, you can now view and manage monitoring data from multiple providers in a single interface. For\\nexample, Agentless Monitoring users can now view and manage multiple SiteScope instances in a single interface.\\nThe Agentless Monitoring solution contains the following main components:\\nMonitoring configuration service:\\n Registration and access control for monitoring operations\\nUI Service: \\nAn HTML user interface for users to view, create, and manage the monitoring operations.\\nThe following are the limitations in installation and configuration:\\nNo proxy between Monitor UI and SiteScope\\nNo Client certificate authentication\\nBefore you begin, it's important to understand the following concepts. \\nAgentless Monitoring User Interface \\nThe Agentless Monitoring user interface provides an interface where you can view and monitor your infrastructure across\\nmultiple providers. For more information, see \\nUse\\n.\\n1\\n. \\nProvider Group: \\nA provider group is a logical group of a specific type of provider providing monitoring ability of a\\nspecific provider type.  For example, a group of SiteScope instances is a provider group.\\n2\\n. \\nProvider\\n: A provider is a collector or a data source that enables you to monitor health, availability, and performance of\\nyour infrastructure, applications, or cloud systems.  For example, a single SiteScope instance is a provider. \\n3\\n. \\nMonitor Group\\n: A monitor group is a logical group of a specific set of monitors that monitor your infrastructure or\\napplication component(s). Use Monitor Groups to organize and administer monitoring artifacts.\\n4\\n. \\nMonitor:\\n A monitor is a logical configuration that groups a set of availability and performance measurements,\\nthresholds, and associated configuration item topology for infrastructure and applications.\\n5\\n. \\nActions\\n: Actions you can perform on a monitor: Edit, Copy, Alerts, and Delete.  \\n6\\n. \\nToolbar\\n: Use the toolbar to manage the monitor tree.  Search, Filter, or Add new entries as needed.  \\n7\\n. \\nDetails:\\n Select an entity in the tree, and then view or modify it in the details pane. \\nHyperscale Observability\\nHyperscale Observability capability provides scalable monitoring of your AWS, Azure, and Kubernetes resources. It brings with\\nit the dynamic detection of new resources as they get added to your AWS or Azure subscription, or Kubernetes account.\\nHyperscale Observability supports the discovery and monitoring of AWS services, Azure services, and Kubernetes objects. For\\nmore details about the supported AWS services, Azure services, and Kubernetes objects, see \\nSupported AWS\\nservices\\n, \\nSupported Azure services\\n, and \\nSupported Kubernetes objects\\n.\\nHyperscale Observability helps to collect metrics, set thresholds, and monitor the metrics for any breaches. If there is a breach\\nin threshold, an event gets generated in OBM. You can visualize the monitored data in OBM using performance dashboards.\\nYou can also view the generated events in the Event Browser in OBM.\\nFor end-to-end use cases of monitoring AWS, Azure, and Kubernetes using Hyperscale Observability, see \\nMonitor AWS using\\nHyperscale Observability\\n, \\nMonitor Azure using Hyperscale Observability\\n, and \\nMonitor Kubernetes using Hyperscale\\nObservability\\n topics.\\nContainerized Operations Bridge Manager\\nOBM is also available as a container for Helm based deployment combines events from application management components\\nwith events from the system and network products. This enables you to keep track of all the events that occur in your\\nmonitored environment. Watch the video below to see how OBM, as part of the Operations Bridge, helps you see all your data\\nthrough a single pane of glass:\\nOperations Bridge demo\\nAdditionally, you can also install Management Packs on OBM. They deliver automatic and complete monitoring solutions for\\ninfrastructure and applications. Using Management Packs, you can monitor, detect, troubleshoot, and remediate issues in the\\nImportant:\\n Agentless Monitoring is supported on OMT, Embedded Kubernetes, and K3S.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n115\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2f5f43fb62e7c095563a3fc382d94601'}>,\n",
              "  <Document: {'content': \"Deployment options\\nYou can install the Operations Bridge on the following Environments:\\nEmbedded Kubernetes: Packaged with \\nOPTIC Management Toolkit (OMT)\\n v 2022.05.\\nExternal Kubernetes: On Amazon Web Services (AWS) and Azure, RedHat OpenShift, Rancher \\nYou can use the OPTIC AppHub or the CLI commands to deploy Operations Bridge.\\nUsing OPTIC AppHub \\nOPTIC AppHub is a Web-based management portal for Helm-based ITOM applications. From this portal, administrators can\\ndrive day-one and day-two operations such as editing, deploying and upgrading ITOM applications. OPTIC AppHub delivers a\\ncommon user experience for the entire ITOM portfolio. Each OPTIC AppHub application is a curated set of Helm charts with\\nadditional metadata that's rendered and managed via the portal.\\nThe \\nApplications\\n page provides you with a consolidated list of all application releases that you can deploy in your\\ncontainerized environment. You can use the UI to configure the applications.\\nThe \\nDeployments\\n page provides application owners the ability to manage the application deployment lifecycle including\\nthe ability to edit, upgrade, and rollback applications.\\nUsing Helm CLI\\nYou can use the Helm CLI commands to install and manage ITOM applications.\\nSetting up cloud infrastructure\\nTo deploy Operations Bridge on cloud platforms (AWS or Azure) you may either use the ITOM Product Deployment Toolkit or\\nother methods.\\nITOM Product Deployment Toolkit - ITOM Product Deployment Toolkit for Cloud allows customers to provision both AWS\\nand Azure infrastructure for deploying Service Assurance products to the cloud.  Terraform is the tool to manage the\\nentire lifecycle of infrastructure. You must update the infrastructure components in the configuration files given in the\\ntoolkit. Terraform uses this information to provision, adjust, and tear down infrastructure in various cloud providers.    \\nOther methods to provision cloud infrastructure - You can choose to deploy the AWS and Azure infrastructure using cloud\\ninfrastructure management applications.  \\nTo provision the AWS infrastructure, you can use the CloudFormation templates available on the Micro Focus Marketplace.\\nFor more information about how to install your application See \\nInstall\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n121\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Deployment options\\nYou can install the Operations Bridge on the following Environments:\\nEmbedded Kubernetes: Packaged with \\nOPTIC Management Toolkit (OMT)\\n v 2022.05.\\nExternal Kubernetes: On Amazon Web Services (AWS) and Azure, RedHat OpenShift, Rancher \\nYou can use the OPTIC AppHub or the CLI commands to deploy Operations Bridge.\\nUsing OPTIC AppHub \\nOPTIC AppHub is a Web-based management portal for Helm-based ITOM applications. From this portal, administrators can\\ndrive day-one and day-two operations such as editing, deploying and upgrading ITOM applications. OPTIC AppHub delivers a\\ncommon user experience for the entire ITOM portfolio. Each OPTIC AppHub application is a curated set of Helm charts with\\nadditional metadata that's rendered and managed via the portal.\\nThe \\nApplications\\n page provides you with a consolidated list of all application releases that you can deploy in your\\ncontainerized environment. You can use the UI to configure the applications.\\nThe \\nDeployments\\n page provides application owners the ability to manage the application deployment lifecycle including\\nthe ability to edit, upgrade, and rollback applications.\\nUsing Helm CLI\\nYou can use the Helm CLI commands to install and manage ITOM applications.\\nSetting up cloud infrastructure\\nTo deploy Operations Bridge on cloud platforms (AWS or Azure) you may either use the ITOM Product Deployment Toolkit or\\nother methods.\\nITOM Product Deployment Toolkit - ITOM Product Deployment Toolkit for Cloud allows customers to provision both AWS\\nand Azure infrastructure for deploying Service Assurance products to the cloud.  Terraform is the tool to manage the\\nentire lifecycle of infrastructure. You must update the infrastructure components in the configuration files given in the\\ntoolkit. Terraform uses this information to provision, adjust, and tear down infrastructure in various cloud providers.    \\nOther methods to provision cloud infrastructure - You can choose to deploy the AWS and Azure infrastructure using cloud\\ninfrastructure management applications.  \\nTo provision the AWS infrastructure, you can use the CloudFormation templates available on the Micro Focus Marketplace.\\nFor more information about how to install your application See \\nInstall\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n121\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a64aa83ea50c7e53468d76351e7da05'}>,\n",
              "  <Document: {'content': 'identify all the events related to the same problem.  As a result, the total number of events needing attention by your operator\\ngets reduced. Secondly, if your operator solves one of the grouped events, the entire group of events gets closed. Note that\\nyou can set up AEC to work with classic and containerized OBM. \\nData visualization\\nThe OPTIC Reporting capability enables you to integrate data from Operations Agent, SiteScope, Business Process Monitor\\n(BPM), and Real User Monitor (RUM) into the OPTIC Data Lake. You can represent this data as:\\nReports using the Business Value Dashboard (BVD) or any other Business Intelligence tool\\nDashboards using Performance Dashboard\\nOut of the box reports with BVD\\nAs part of OPTIC Reporting, the suite offers the following categories of reports:\\nSystem Infrastructure \\nEvent \\nSynthetic Transaction \\nReal User Monitor \\nApplication\\nBring your own Business Intelligence (BYOBI)\\nYou can use third-party Business Intelligence (BI) tools to create reports and dashboards on the data available in OPTIC Data\\nLake. For information about OPTIC Data Lake schema, see \\nOperations Bridge Data Model\\n.\\nPerformance dashboards\\nYou can use the data in the OPTIC Data Lake to create dashboards using Performance Dashboard (PD). PD is available as part\\nof both classic and containerized OBM. \\nRelated topics\\nFor details about installation, see \\nInstall\\n.\\n \\nContainerized Operations Bridge 2022.11\\nPage \\n118\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.identify all the events related to the same problem.  As a result, the total number of events needing attention by your operator\\ngets reduced. Secondly, if your operator solves one of the grouped events, the entire group of events gets closed. Note that\\nyou can set up AEC to work with classic and containerized OBM. \\nData visualization\\nThe OPTIC Reporting capability enables you to integrate data from Operations Agent, SiteScope, Business Process Monitor\\n(BPM), and Real User Monitor (RUM) into the OPTIC Data Lake. You can represent this data as:\\nReports using the Business Value Dashboard (BVD) or any other Business Intelligence tool\\nDashboards using Performance Dashboard\\nOut of the box reports with BVD\\nAs part of OPTIC Reporting, the suite offers the following categories of reports:\\nSystem Infrastructure \\nEvent \\nSynthetic Transaction \\nReal User Monitor \\nApplication\\nBring your own Business Intelligence (BYOBI)\\nYou can use third-party Business Intelligence (BI) tools to create reports and dashboards on the data available in OPTIC Data\\nLake. For information about OPTIC Data Lake schema, see \\nOperations Bridge Data Model\\n.\\nPerformance dashboards\\nYou can use the data in the OPTIC Data Lake to create dashboards using Performance Dashboard (PD). PD is available as part\\nof both classic and containerized OBM. \\nRelated topics\\nFor details about installation, see \\nInstall\\n.\\n \\nContainerized Operations Bridge 2022.11\\nPage \\n118\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bcd422f5476478db186938529574d177'}>,\n",
              "  <Document: {'content': \"and rollbacks. You can install various capabilities on top of this platform. \\nOPTIC Data Lake refers to the central data lake that you can integrate with different data sources. This data lake can\\nreceive and process data that's high in volume and velocity.\\nAutomated event correlation - This component comprises the algorithms to group events based on patterns and create\\ncorrelation events. \\nOperations Bridge Reporting content - This component comprises all the artifacts required to generate out of the box\\nreports in BVD.\\nBVD - Business Value Dashboard (BVD) is a data visualization tool. You can create custom, flexible dashboards to visualize\\ninformation from different sources in an informative and appealing way. Business Value Dashboard provides the following\\ncapabilities:\\nReporting\\n: Reporting gives you visual information of historical or recorded data using tables, charts, and\\nwidgets. You can create dashboards in Visio and attach data from data queries, which can retrieve data from\\nany Vertica database using SQL queries.  \\nStakeholder Dashboard\\n: The stakeholder dashboard gives you visual information of live data using tables, charts,\\nand widgets. You can configure to stream real time data from any data source in JSON format via HTTP post. \\nContainerized OBM - Operations Bridge Manager is also available as a containerized capability. You can integrate it with\\nother components like OPTIC Data Lake and BVD. In addition, you can also set up automatic event correlation and OPTIC\\nReporting to work with containerized OBM.\\nHyperscale Observability -  It's a container only capability to support a high scale and dynamic discovery, monitoring, and\\nreporting of cloud resources. \\nNote: \\nYou can concurrently set up classic and containerized OBM instances in your environment. You can also\\nhave both classic and containerized OBM integrated with OPTIC Data Lake. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n120\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.and rollbacks. You can install various capabilities on top of this platform. \\nOPTIC Data Lake refers to the central data lake that you can integrate with different data sources. This data lake can\\nreceive and process data that's high in volume and velocity.\\nAutomated event correlation - This component comprises the algorithms to group events based on patterns and create\\ncorrelation events. \\nOperations Bridge Reporting content - This component comprises all the artifacts required to generate out of the box\\nreports in BVD.\\nBVD - Business Value Dashboard (BVD) is a data visualization tool. You can create custom, flexible dashboards to visualize\\ninformation from different sources in an informative and appealing way. Business Value Dashboard provides the following\\ncapabilities:\\nReporting\\n: Reporting gives you visual information of historical or recorded data using tables, charts, and\\nwidgets. You can create dashboards in Visio and attach data from data queries, which can retrieve data from\\nany Vertica database using SQL queries.  \\nStakeholder Dashboard\\n: The stakeholder dashboard gives you visual information of live data using tables, charts,\\nand widgets. You can configure to stream real time data from any data source in JSON format via HTTP post. \\nContainerized OBM - Operations Bridge Manager is also available as a containerized capability. You can integrate it with\\nother components like OPTIC Data Lake and BVD. In addition, you can also set up automatic event correlation and OPTIC\\nReporting to work with containerized OBM.\\nHyperscale Observability -  It's a container only capability to support a high scale and dynamic discovery, monitoring, and\\nreporting of cloud resources. \\nNote: \\nYou can concurrently set up classic and containerized OBM instances in your environment. You can also\\nhave both classic and containerized OBM integrated with OPTIC Data Lake. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n120\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c28305e73dde7e1a73b942fce4d2afa'}>,\n",
              "  <Document: {'content': 'IT domain. They increase your productivity by optimizing and automating various tasks and also reduce the mean time to\\nresolve (MTTR) incidents.\\nAnomaly Detection\\nAnomalies are major changes in the amount or value of collected data. After an anomaly has been detected, users can drill\\ndown to view the details of the anomaly lifecycle, and use log and event analytics, along with interactive text search, to\\nefficiently find data and information about the problem.\\nKey concepts\\nThe capabilities provided by containerized Operations Bridge help you to monitor your environment by providing data insights.\\nMonitoring comprises data collection, data storage, data analytics, and data visualization.\\nData collection\\nYou can collect metrics, events, topology, and log file data from data sources like Operations Agent, SiteScope, Business\\nProcess Monitor (BPM), and Real User Monitor (RUM). \\nAgent based data collection\\nYou can collect system performance metrics from Operations Agent using the Agent Metric Collector or the metric streaming\\npolicies. You can use Agent Metric Collector if your environment has Operations Agent versions between 12.00 and higher. You\\ncan use metrics streaming policies only if your environment has Operations Agent version 12.14 and higher.\\nYou can install a wide variety of Management Packs on target nodes that have Operations Agent on them. Using these\\nManagement Packs, you can further automate discovery and monitoring, create domain specific rules, and collect cross\\ndomain metrics.\\nMonitoring Service Edge\\nMonitoring Service Edge is a service that enables the collection of system infrastructure and custom metrics from the\\nOperations Agent nodes and Management Packs on different networks or subnets, and forwards the collected metrics to the\\nOPTIC DL on the central OpsBridge deployment that it is integrated into. The figure below depicts a deployment architecture\\nfor the Monitoring Service Edge, with the corresponding local OBM and Operations Agent nodes, integrated into the central\\nOpsBridge.\\nYou can install the Monitoring Service Edge service on the same network where the Operations Agent nodes are available.  You\\nmust integrate the service with OBM to discover the Operations Agents. If integrated with the local OBM, you must integrate\\nthe local OBM with the OpsBridge deployment before deploying the Edge chart to establish trust. You can deploy and integrate\\nmultiple instances of the Monitoring Service Edge, in a different network, into the same OpsBridge deployment, enabling\\ndistribution of the metrics collection from the Operations Agents, and where network architecture prohibits collection directly\\nfrom the OpsBridge deployment on a different network.\\nSee the \\nIntegrate OpsBridge with Monitoring Service Edge\\n for deployment options of the Monitoring Service Edge and\\nintegration details.\\nAgentless data collection\\nSiteScope supplies agentless data collection. SiteScope has a new centralized UI for Agentless Monitoring that enables you to\\nunderstand the health, availability, and performance of a hybrid set of systems and applications deployed on-premise and\\ncloud in your infrastructure. The solution provides a single view of the health, availability, and performance metrics from\\nmultiple infrastructure monitoring solutions, such as SiteScope. \\nData storage\\nOPTIC Data Lake provides a central data lake to store data from many sources. OPTIC Data Lake can receive and process data\\nthat are high in volume and velocity, from several independent data sources. Different data sources collect data about\\ntopology, events, and metrics from physical, virtual, on-premise, and cloud infrastructure deployments and stream to OPTIC\\nContainerized Operations Bridge 2022.11\\nPage \\n116\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.IT domain. They increase your productivity by optimizing and automating various tasks and also reduce the mean time to\\nresolve (MTTR) incidents.\\nAnomaly Detection\\nAnomalies are major changes in the amount or value of collected data. After an anomaly has been detected, users can drill\\ndown to view the details of the anomaly lifecycle, and use log and event analytics, along with interactive text search, to\\nefficiently find data and information about the problem.\\nKey concepts\\nThe capabilities provided by containerized Operations Bridge help you to monitor your environment by providing data insights.\\nMonitoring comprises data collection, data storage, data analytics, and data visualization.\\nData collection\\nYou can collect metrics, events, topology, and log file data from data sources like Operations Agent, SiteScope, Business\\nProcess Monitor (BPM), and Real User Monitor (RUM). \\nAgent based data collection\\nYou can collect system performance metrics from Operations Agent using the Agent Metric Collector or the metric streaming\\npolicies. You can use Agent Metric Collector if your environment has Operations Agent versions between 12.00 and higher. You\\ncan use metrics streaming policies only if your environment has Operations Agent version 12.14 and higher.\\nYou can install a wide variety of Management Packs on target nodes that have Operations Agent on them. Using these\\nManagement Packs, you can further automate discovery and monitoring, create domain specific rules, and collect cross\\ndomain metrics.\\nMonitoring Service Edge\\nMonitoring Service Edge is a service that enables the collection of system infrastructure and custom metrics from the\\nOperations Agent nodes and Management Packs on different networks or subnets, and forwards the collected metrics to the\\nOPTIC DL on the central OpsBridge deployment that it is integrated into. The figure below depicts a deployment architecture\\nfor the Monitoring Service Edge, with the corresponding local OBM and Operations Agent nodes, integrated into the central\\nOpsBridge.\\nYou can install the Monitoring Service Edge service on the same network where the Operations Agent nodes are available.  You\\nmust integrate the service with OBM to discover the Operations Agents. If integrated with the local OBM, you must integrate\\nthe local OBM with the OpsBridge deployment before deploying the Edge chart to establish trust. You can deploy and integrate\\nmultiple instances of the Monitoring Service Edge, in a different network, into the same OpsBridge deployment, enabling\\ndistribution of the metrics collection from the Operations Agents, and where network architecture prohibits collection directly\\nfrom the OpsBridge deployment on a different network.\\nSee the \\nIntegrate OpsBridge with Monitoring Service Edge\\n for deployment options of the Monitoring Service Edge and\\nintegration details.\\nAgentless data collection\\nSiteScope supplies agentless data collection. SiteScope has a new centralized UI for Agentless Monitoring that enables you to\\nunderstand the health, availability, and performance of a hybrid set of systems and applications deployed on-premise and\\ncloud in your infrastructure. The solution provides a single view of the health, availability, and performance metrics from\\nmultiple infrastructure monitoring solutions, such as SiteScope. \\nData storage\\nOPTIC Data Lake provides a central data lake to store data from many sources. OPTIC Data Lake can receive and process data\\nthat are high in volume and velocity, from several independent data sources. Different data sources collect data about\\ntopology, events, and metrics from physical, virtual, on-premise, and cloud infrastructure deployments and stream to OPTIC\\nContainerized Operations Bridge 2022.11\\nPage \\n116\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9fe35dcbc8bcf9275577ccb8f2b397f5'}>,\n",
              "  <Document: {'content': \"Architecture and components\\nUsing the architecture of containerized Operations Bridge architecture, you can ingest events and metric data from different\\ndata sources into OPTIC Data Lake. These data sources include Operations Bridge Manager (OBM), Operations Agent,\\nSiteScope, Business Process Monitor (BPM), and Real User Monitor (RUM). \\nThe following diagram shows a typical deployment of classic products along with a containerized Operations Bridge:\\nData sources\\nYou can integrate the containerized Operations Bridge with the following data sources.\\nOperations Bridge Manager\\nOperations Bridge Manager (OBM) is a management server and an event management tool. The central event console of OBM\\nshows all event and performance management data originating from servers, networks, applications, storage, and other IT\\nsilos in your infrastructure. The console displays monitoring alerts to the appropriate team of operators. Operations Agents and\\nthe streaming policies enable the data sources (SiteScope, BPM, and RUM) to collect and stream performance metrics to OBM\\nand then to OPTIC Data Lake. You can then enable automatic event correlation, generate event reports in BVD, or create\\ncustom reports using these metrics.\\nOperations Agent\\nOperations Agent collects metrics that indicate the health, performance, resource utilization, and availability of essential\\nelements of the system. To monitor your environment with Operations Agent you must deploy the agent software on the\\nservers that you want to monitor. With its embedded data collector, Operations Agent continuously collects performance and\\nhealth data across your system and stores the collected data in the \\nMetrics Datastore\\n. You can then configure to send this\\nto the OPTIC Data Lake.\\nSiteScope\\nSiteScope provides simple and powerful agentless infrastructure and application monitoring solutions. It's a classic product\\nthat you can integrate with containerized Operations Bridge. Agentless monitoring means that you can monitor your\\nenvironment without deploying the agent software on the servers that you want to monitor. SiteScope collects system\\nperformance metrics that drive the System Performance data model for out of the box reports and custom reporting in\\nBVD.  SiteScope has a new centralized UI for Agentless Monitoring that provides a single view of the health, availability, and\\nperformance metrics from multiple infrastructure monitoring solutions, such as SiteScope.\\nBusiness Process Monitor \\nBusiness Process Monitor (BPM) enables you to run synthetic transactions and collect metrics about the availability of\\napplications. It's a classic product that you can integrate with Operations Bridge.  You can send the synthetic transaction\\nmetrics to OPTIC Data Lake and create custom reports in BVD.  \\nReal User Monitor \\nReal User Monitor (RUM) monitors the real user traffic and collect metrics about the availability and performance of\\napplications. It's a classic product that you can integrate with Operations Bridge. The metrics collected by RUM drives the Real\\nUser Monitoring data model for out of the box reports and custom reporting in BVD.\\nContainerized deployment\\nContainerized deployment depicts the capabilities that you can deploy on a Kubernetes cluster.\\nOMT - OMT is a platform built to manage the application lifecycle, standardize deployment, upgrade, patching, scaling,\\nContainerized Operations Bridge 2022.11\\nPage \\n119\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Architecture and components\\nUsing the architecture of containerized Operations Bridge architecture, you can ingest events and metric data from different\\ndata sources into OPTIC Data Lake. These data sources include Operations Bridge Manager (OBM), Operations Agent,\\nSiteScope, Business Process Monitor (BPM), and Real User Monitor (RUM). \\nThe following diagram shows a typical deployment of classic products along with a containerized Operations Bridge:\\nData sources\\nYou can integrate the containerized Operations Bridge with the following data sources.\\nOperations Bridge Manager\\nOperations Bridge Manager (OBM) is a management server and an event management tool. The central event console of OBM\\nshows all event and performance management data originating from servers, networks, applications, storage, and other IT\\nsilos in your infrastructure. The console displays monitoring alerts to the appropriate team of operators. Operations Agents and\\nthe streaming policies enable the data sources (SiteScope, BPM, and RUM) to collect and stream performance metrics to OBM\\nand then to OPTIC Data Lake. You can then enable automatic event correlation, generate event reports in BVD, or create\\ncustom reports using these metrics.\\nOperations Agent\\nOperations Agent collects metrics that indicate the health, performance, resource utilization, and availability of essential\\nelements of the system. To monitor your environment with Operations Agent you must deploy the agent software on the\\nservers that you want to monitor. With its embedded data collector, Operations Agent continuously collects performance and\\nhealth data across your system and stores the collected data in the \\nMetrics Datastore\\n. You can then configure to send this\\nto the OPTIC Data Lake.\\nSiteScope\\nSiteScope provides simple and powerful agentless infrastructure and application monitoring solutions. It's a classic product\\nthat you can integrate with containerized Operations Bridge. Agentless monitoring means that you can monitor your\\nenvironment without deploying the agent software on the servers that you want to monitor. SiteScope collects system\\nperformance metrics that drive the System Performance data model for out of the box reports and custom reporting in\\nBVD.  SiteScope has a new centralized UI for Agentless Monitoring that provides a single view of the health, availability, and\\nperformance metrics from multiple infrastructure monitoring solutions, such as SiteScope.\\nBusiness Process Monitor \\nBusiness Process Monitor (BPM) enables you to run synthetic transactions and collect metrics about the availability of\\napplications. It's a classic product that you can integrate with Operations Bridge.  You can send the synthetic transaction\\nmetrics to OPTIC Data Lake and create custom reports in BVD.  \\nReal User Monitor \\nReal User Monitor (RUM) monitors the real user traffic and collect metrics about the availability and performance of\\napplications. It's a classic product that you can integrate with Operations Bridge. The metrics collected by RUM drives the Real\\nUser Monitoring data model for out of the box reports and custom reporting in BVD.\\nContainerized deployment\\nContainerized deployment depicts the capabilities that you can deploy on a Kubernetes cluster.\\nOMT - OMT is a platform built to manage the application lifecycle, standardize deployment, upgrade, patching, scaling,\\nContainerized Operations Bridge 2022.11\\nPage \\n119\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36451b3f808240b5868948364c6cbbbc'}>,\n",
              "  <Document: {'content': \"Data Lake as under:\\n1\\n. \\nSystem infrastructure metrics from Operations Agent and SiteScope\\n2\\n. \\nSynthetic monitoring metrics from Business Process Monitor (BPM)\\n3\\n. \\nReal user metrics from Real User Monitor (RUM) \\n4\\n. \\nApplication metrics from Management Packs\\n5\\n. \\nTopology information (from OBM/RTSM via Data Flow Probe)\\n6\\n. \\nNetwork Management data (in case of shared OPTIC Data Lake between Operations Bridge and NOM)\\n7\\n. \\nDCA data (in case of shared OPTIC Data Lake between Operations Bridge and DCA)\\n8\\n. \\nCustom Data Ingestion (e.g. using Open Data Ingestion API or Custom Metric Ingestion Tool)\\nData enrichment \\nA component that receives the metrics from data sources, enriches them with CI and downtime information, and forwards\\nthem to OPTIC Data Lake.\\nIt enriches data for the following data collectors:\\nManagement Packs\\nSiteScope\\nFor more information on data enrichment, see \\nEnrich fields in OPTIC Data Lake using Data Enrichment Service\\n.\\nData retention\\nOPTIC Data Lake uses Vertica database to manage such a huge volume of data. Over time, the Vertica database accumulates\\nan enormous amount of data from various data sources. You may need to delete the old and unused data periodically to free\\nup storage space.\\nYou can configure retention profiles per source or content category such that data older than the configured period is\\nautomatically deleted from Vertica database. \\nFor more information about configuring data retention, see \\nCustomize Data Retention\\n.\\nRAW and Aggregated data (e.g. hourly, daily, forecasting) are stored in Vertica to ease the reporting. The data aggregation\\nand forecasting are configured by corresponding taskflows triggered by OPTIC Data Lake.\\nFor more information about Reporting data and task flows, see \\nDataAndTaskFlows\\n.\\nData source resiliency\\nWhen the Optic DL is unavailable and therefore unable to ingest metrics collected by Agent Metric Collector (AMC), AWS\\ncollector, Azure Collector, or Kubernetes collectors, the metrics are temporarily stored in the Store and forward database. After\\nthe OPTIC DL is up and running, the metrics stored in the Store and forward database are streamed to the Optic DL.\\nFor other data sources (Operations Agent, SiteScope, Management Packs, BPM, and RUM), the metrics are stored in their local\\nbuffers during OPTIC DL outage and are streamed to the OPTIC DL after it's up and running.\\nThe buffering mechanism for other data types is as follows:\\nHealth Indicator, KPI, and Event data are buffered in OBM’s relational database.\\nTopology data is forwarded to OPTIC DL using the Data Flow Probe. The Data Flow Probe stores the timestamp of the last\\nsuccessful sync. Once OPTIC DL starts accepting data again, the Data Flow Probe reads the topology data from the RTSM\\nserver and forwards it to OPTIC DL.\\nDowntime data is buffered in OBM's message bus.\\nFor information about configuring resiliency, see \\nConfigure data resiliency\\n. \\nEvent and performance management \\nAll events and metrics collected by different Operations Agent instances are sent to OBM. OBM stores these events and\\nmetrics in its own database. You can view the performance metrics using Performance Dashboards and events in the Events\\nconsole. This consolidated view helps you identify, isolate, and remediate issues before they become showstoppers.\\nData analytics\\nYou can generate meaningful insights from all the data stored in OPTIC Data Lake using analytics.\\nEvent correlation\\nIn a large environment, one of the biggest challenges is how to manage the large number of events that originate from a\\nvariety of sources. The aim is to identify the events that have a significant impact on business services. While it's essential to\\nminimize the number of events that appear, it's even more important to highlight the events that, if not managed\\nappropriately, could cause a breach in service level agreements (SLAs) and generate incidents in your help desk system.\\nEvent correlation entails the grouping of events based on some common pattern or identifier. There are two types of\\ncorrelation:\\nRule based event correlation - In this case, you can specify the rules or parameters based on which events get grouped or\\ncorrelated. You can use two rules to correlate events - one based on topology and the other based on the event stream.\\nRule based correlation is available in both classic and containerized OBM.  \\nAutomatic event correlation - In this case, you don't need to specify any rules. The Automatic Event Correlation service\\n(AEC) is a part of the Automatic Event Correlation capability. It identifies patterns among disparate events and creates a\\nsingle correlated event for a group of events. AEC is available if you install the Automatic Event Correlation capability.\\nAutomatic Event Correlation\\nAutomatic Event Correlation identifies repeated patterns in the incoming event flow over a period of time. It creates a single\\ncorrelated event for all events within a pattern to represent the entire group of events. This grouping or correlation helps to\\nContainerized Operations Bridge 2022.11\\nPage \\n117\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Data Lake as under:\\n1\\n. \\nSystem infrastructure metrics from Operations Agent and SiteScope\\n2\\n. \\nSynthetic monitoring metrics from Business Process Monitor (BPM)\\n3\\n. \\nReal user metrics from Real User Monitor (RUM) \\n4\\n. \\nApplication metrics from Management Packs\\n5\\n. \\nTopology information (from OBM/RTSM via Data Flow Probe)\\n6\\n. \\nNetwork Management data (in case of shared OPTIC Data Lake between Operations Bridge and NOM)\\n7\\n. \\nDCA data (in case of shared OPTIC Data Lake between Operations Bridge and DCA)\\n8\\n. \\nCustom Data Ingestion (e.g. using Open Data Ingestion API or Custom Metric Ingestion Tool)\\nData enrichment \\nA component that receives the metrics from data sources, enriches them with CI and downtime information, and forwards\\nthem to OPTIC Data Lake.\\nIt enriches data for the following data collectors:\\nManagement Packs\\nSiteScope\\nFor more information on data enrichment, see \\nEnrich fields in OPTIC Data Lake using Data Enrichment Service\\n.\\nData retention\\nOPTIC Data Lake uses Vertica database to manage such a huge volume of data. Over time, the Vertica database accumulates\\nan enormous amount of data from various data sources. You may need to delete the old and unused data periodically to free\\nup storage space.\\nYou can configure retention profiles per source or content category such that data older than the configured period is\\nautomatically deleted from Vertica database. \\nFor more information about configuring data retention, see \\nCustomize Data Retention\\n.\\nRAW and Aggregated data (e.g. hourly, daily, forecasting) are stored in Vertica to ease the reporting. The data aggregation\\nand forecasting are configured by corresponding taskflows triggered by OPTIC Data Lake.\\nFor more information about Reporting data and task flows, see \\nDataAndTaskFlows\\n.\\nData source resiliency\\nWhen the Optic DL is unavailable and therefore unable to ingest metrics collected by Agent Metric Collector (AMC), AWS\\ncollector, Azure Collector, or Kubernetes collectors, the metrics are temporarily stored in the Store and forward database. After\\nthe OPTIC DL is up and running, the metrics stored in the Store and forward database are streamed to the Optic DL.\\nFor other data sources (Operations Agent, SiteScope, Management Packs, BPM, and RUM), the metrics are stored in their local\\nbuffers during OPTIC DL outage and are streamed to the OPTIC DL after it's up and running.\\nThe buffering mechanism for other data types is as follows:\\nHealth Indicator, KPI, and Event data are buffered in OBM’s relational database.\\nTopology data is forwarded to OPTIC DL using the Data Flow Probe. The Data Flow Probe stores the timestamp of the last\\nsuccessful sync. Once OPTIC DL starts accepting data again, the Data Flow Probe reads the topology data from the RTSM\\nserver and forwards it to OPTIC DL.\\nDowntime data is buffered in OBM's message bus.\\nFor information about configuring resiliency, see \\nConfigure data resiliency\\n. \\nEvent and performance management \\nAll events and metrics collected by different Operations Agent instances are sent to OBM. OBM stores these events and\\nmetrics in its own database. You can view the performance metrics using Performance Dashboards and events in the Events\\nconsole. This consolidated view helps you identify, isolate, and remediate issues before they become showstoppers.\\nData analytics\\nYou can generate meaningful insights from all the data stored in OPTIC Data Lake using analytics.\\nEvent correlation\\nIn a large environment, one of the biggest challenges is how to manage the large number of events that originate from a\\nvariety of sources. The aim is to identify the events that have a significant impact on business services. While it's essential to\\nminimize the number of events that appear, it's even more important to highlight the events that, if not managed\\nappropriately, could cause a breach in service level agreements (SLAs) and generate incidents in your help desk system.\\nEvent correlation entails the grouping of events based on some common pattern or identifier. There are two types of\\ncorrelation:\\nRule based event correlation - In this case, you can specify the rules or parameters based on which events get grouped or\\ncorrelated. You can use two rules to correlate events - one based on topology and the other based on the event stream.\\nRule based correlation is available in both classic and containerized OBM.  \\nAutomatic event correlation - In this case, you don't need to specify any rules. The Automatic Event Correlation service\\n(AEC) is a part of the Automatic Event Correlation capability. It identifies patterns among disparate events and creates a\\nsingle correlated event for a group of events. AEC is available if you install the Automatic Event Correlation capability.\\nAutomatic Event Correlation\\nAutomatic Event Correlation identifies repeated patterns in the incoming event flow over a period of time. It creates a single\\ncorrelated event for all events within a pattern to represent the entire group of events. This grouping or correlation helps to\\nContainerized Operations Bridge 2022.11\\nPage \\n117\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a774b8fc941f72f1a4e02f767573a42c'}>,\n",
              "  <Document: {'content': \"OpsBridge user interface\\nThe Containerized Operations Bridge consists of OPTIC One and classic user interface.\\nOPTIC One is available for the following capabilities:\\nAgentless Monitoring*\\nAutomatic Event Correlation (AEC)\\nOPTIC Reporting\\nStakeholder Dashboards  (Business Value Dashboard)\\nHyperscale Observability*\\nAnomaly Detection (Limited Edition Technical Preview)\\n* The administration and configuration of these capabilities are supported through Monitoring Control Center (MCC).\\nClassic user interface is available with Containerized OBM.\\nThe OPTIC One delivers unified workflow across capabilities and domains and gives consistent drill-down behavior. After you\\nlogin to the OPTIC One, as default you will see a page which is a common home page. It's one centralized user interface where\\nyou see the operations and administration workflow of all capabilities that you have chosen to deploy. The content you will\\nsee, such as menu entries, charts, and more, will depend on the deployed capabilities.\\nNote\\n: Based on your roles and assigned permissions, you may not able to access all deployed capabilities.\\nIn this section, the general user interface components are explained in detail. \\nFor example, when you select a page from the side navigation panel, you can see a page similar to this. \\nThese UI components are building a common base UI. The following table describes each UI element:\\nNumber\\nUI element\\n1\\nSide navigation panel\\n2\\nHome page icon. Clicking on the suite icon takes you to the home page of the user interface.\\n3\\nWorkflow icon. This icon suggesting you the next possible steps in the workflow.\\n4\\nMasthead\\n5\\nTime range selector\\n6\\nPersonal user settings and to logout from the application\\n7\\nNotification bell icon. Displays the notification counter. \\n8\\nTo launch Help and About pages\\nContainerized Operations Bridge 2022.11\\nPage \\n124\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.OpsBridge user interface\\nThe Containerized Operations Bridge consists of OPTIC One and classic user interface.\\nOPTIC One is available for the following capabilities:\\nAgentless Monitoring*\\nAutomatic Event Correlation (AEC)\\nOPTIC Reporting\\nStakeholder Dashboards  (Business Value Dashboard)\\nHyperscale Observability*\\nAnomaly Detection (Limited Edition Technical Preview)\\n* The administration and configuration of these capabilities are supported through Monitoring Control Center (MCC).\\nClassic user interface is available with Containerized OBM.\\nThe OPTIC One delivers unified workflow across capabilities and domains and gives consistent drill-down behavior. After you\\nlogin to the OPTIC One, as default you will see a page which is a common home page. It's one centralized user interface where\\nyou see the operations and administration workflow of all capabilities that you have chosen to deploy. The content you will\\nsee, such as menu entries, charts, and more, will depend on the deployed capabilities.\\nNote\\n: Based on your roles and assigned permissions, you may not able to access all deployed capabilities.\\nIn this section, the general user interface components are explained in detail. \\nFor example, when you select a page from the side navigation panel, you can see a page similar to this. \\nThese UI components are building a common base UI. The following table describes each UI element:\\nNumber\\nUI element\\n1\\nSide navigation panel\\n2\\nHome page icon. Clicking on the suite icon takes you to the home page of the user interface.\\n3\\nWorkflow icon. This icon suggesting you the next possible steps in the workflow.\\n4\\nMasthead\\n5\\nTime range selector\\n6\\nPersonal user settings and to logout from the application\\n7\\nNotification bell icon. Displays the notification counter. \\n8\\nTo launch Help and About pages\\nContainerized Operations Bridge 2022.11\\nPage \\n124\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f4315e75c536ce80e6ccf3e331a0f1d5'}>,\n",
              "  <Document: {'content': 'Use cases\\nOperations Bridge enables you to dynamically discover resources and activate domain specific agent and agentless monitoring\\nof on premise, private, and public cloud environments. It encompasses dashboards and reporting, which represent key aspects\\nof your infrastructure and brings your data to life.\\nThe dashboards and reports merges performance and event metrics into tables and graphs and thus enable you to view data\\nfrom several valuable and unique perspectives. It helps you to predict infrastructure resource utilization, detect problems, and\\ntake corrective actions before critical business availability gets impacted.\\nThis section provides you with a few use cases that enable you to get started with data collection and visualization.\\nContainerized Operations Bridge 2022.11\\nPage \\n127\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Use cases\\nOperations Bridge enables you to dynamically discover resources and activate domain specific agent and agentless monitoring\\nof on premise, private, and public cloud environments. It encompasses dashboards and reporting, which represent key aspects\\nof your infrastructure and brings your data to life.\\nThe dashboards and reports merges performance and event metrics into tables and graphs and thus enable you to view data\\nfrom several valuable and unique perspectives. It helps you to predict infrastructure resource utilization, detect problems, and\\ntake corrective actions before critical business availability gets impacted.\\nThis section provides you with a few use cases that enable you to get started with data collection and visualization.\\nContainerized Operations Bridge 2022.11\\nPage \\n127\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e4de0132ffc878c08b9ec6cc8c57283f'}>,\n",
              "  <Document: {'content': 'License and entitlement\\nContainerized Operations Bridge is available with the following license editions:\\nExpress (Agentless Monitoring user interface only)\\nPremium\\nUltimate\\nThe following capabilities are available:\\nCapabilities\\nLicense edition\\nComponents\\nAgentless Monitoring\\nExpress*, Premium, and\\nUltimate\\nAgentless Monitoring, BVD\\nContainerized Operations Bridge Manager\\n(OBM)\\nPremium and Ultimate \\nOBM\\nHyperscale Observability\\nPremium and Ultimate\\nHyperscale Observability and OPTIC Data\\nLake\\nOPTIC Reporting\\nPremium and Ultimate\\nOPTIC Data Lake, Collection Services, BVD\\nStakeholder Dashboard\\nPremium and Ultimate\\nBusiness Value Dashboard (BVD)\\nEvent Analytics (Automatic Event\\nCorrelation) \\nUltimate\\nOPTIC Data Lake, AEC services\\n* Only User Interface component containerized\\nOperations Bridge uses the OPTIC Data Lake for several use cases such as data consolidation, reporting, and analytics. OPTIC\\nData Lake entitlement for Operations Bridge is only available when licensed under the UNIT-based structure. Customers who\\nown a license for Operations Bridge under the NODE-based structure must migrate to the UNIT-based structure to obtain OPTIC\\nData Lake perpetual license keys. There is no additional charge for a customer to migrate their contract from NODE- to UNIT-\\nbased licensing structures, nor are there any physical product (or technical) changes to a customer environment required to\\nmigrate to the UNIT-based license structure. Customers who require time to make the transition from NODE- to UNIT-based\\nlicensing may request a temporary license key to use for a limited period of time. They should use the instructions for\\nacquiring a temporary license key provided in the \\nentitlements portal\\n for OpsBridge.\\nContainerized Operations Bridge 2022.11\\nPage \\n122\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.License and entitlement\\nContainerized Operations Bridge is available with the following license editions:\\nExpress (Agentless Monitoring user interface only)\\nPremium\\nUltimate\\nThe following capabilities are available:\\nCapabilities\\nLicense edition\\nComponents\\nAgentless Monitoring\\nExpress*, Premium, and\\nUltimate\\nAgentless Monitoring, BVD\\nContainerized Operations Bridge Manager\\n(OBM)\\nPremium and Ultimate \\nOBM\\nHyperscale Observability\\nPremium and Ultimate\\nHyperscale Observability and OPTIC Data\\nLake\\nOPTIC Reporting\\nPremium and Ultimate\\nOPTIC Data Lake, Collection Services, BVD\\nStakeholder Dashboard\\nPremium and Ultimate\\nBusiness Value Dashboard (BVD)\\nEvent Analytics (Automatic Event\\nCorrelation) \\nUltimate\\nOPTIC Data Lake, AEC services\\n* Only User Interface component containerized\\nOperations Bridge uses the OPTIC Data Lake for several use cases such as data consolidation, reporting, and analytics. OPTIC\\nData Lake entitlement for Operations Bridge is only available when licensed under the UNIT-based structure. Customers who\\nown a license for Operations Bridge under the NODE-based structure must migrate to the UNIT-based structure to obtain OPTIC\\nData Lake perpetual license keys. There is no additional charge for a customer to migrate their contract from NODE- to UNIT-\\nbased licensing structures, nor are there any physical product (or technical) changes to a customer environment required to\\nmigrate to the UNIT-based license structure. Customers who require time to make the transition from NODE- to UNIT-based\\nlicensing may request a temporary license key to use for a limited period of time. They should use the instructions for\\nacquiring a temporary license key provided in the \\nentitlements portal\\n for OpsBridge.\\nContainerized Operations Bridge 2022.11\\nPage \\n122\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2e136bc176bc6b7310799bb0427fb863'}>,\n",
              "  <Document: {'content': \"browser determines what's displayed in the Health Top View. The selected CI in the Health Top View determines what's\\ndisplayed in the Health Indicators pane. For details, see the \\nHealth perspective\\n.\\nPerformance perspective\\nIn the Performance Perspective tab, Performance Dashboard enables you to visualize performance metrics in the form of\\nperformance dashboards. By default, Performance Dashboard comprises out-of-the-box dashboards. In addition, you can\\nalso create and customize performance dashboards for the Configuration Items (CIs) that you are monitoring. For details,\\nsee the \\nPerformance perspective\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n126\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.browser determines what's displayed in the Health Top View. The selected CI in the Health Top View determines what's\\ndisplayed in the Health Indicators pane. For details, see the \\nHealth perspective\\n.\\nPerformance perspective\\nIn the Performance Perspective tab, Performance Dashboard enables you to visualize performance metrics in the form of\\nperformance dashboards. By default, Performance Dashboard comprises out-of-the-box dashboards. In addition, you can\\nalso create and customize performance dashboards for the Configuration Items (CIs) that you are monitoring. For details,\\nsee the \\nPerformance perspective\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n126\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '715c3475e41643a6bccb2d7481862a2'}>,\n",
              "  <Document: {'content': '9\\nPage actions\\n10\\nWidget actions\\nNumber\\nUI element\\nFor more information see, \\nOPTIC One components\\n.\\nStakeholder Dashboard\\nThe stakeholder dashboard uses the Business Value Dashboard (BVD) user interface to give you visual information of live data\\nusing tables, charts, and widgets. \\nUse BVD\\n.\\nOPTIC Reporting \\nThe OPTIC Reporting capability uses OPTIC One. After configuring this capability you may log into the OPTIC One to view the\\nsystem infrastructure, event, synthetic transaction, real user monitoring, or application reports. For details, see \\nUse OPTIC\\nReporting\\n.\\nAutomatic Event Correlation\\nAfter configuring the Automatic Event Correlation (AEC) capability, you can view the correlated events in the event console of\\nOperations Bridge Manager (OBM).\\nOn OBM, go to \\nWorkspaces > Operations Console > Event Perspective. \\nCheck the OBM event browser for a new event\\nwith the title: “Automatically Correlated Event: \". \\nSample:\\nMonitoring Control Center\\nMonitoring Control Center (MCC) is a new section of the Operations Bridge UI that consolidates the administration and\\nconfiguration of monitoring capabilities into a central user experience. MCC supports the following capabilities:\\nAgentless Monitoring\\nHyperscale Observability\\nAgentless Monitoring\\nThe new centralized UI, Agentless Monitoring, provides users a unified monitoring experience across multiple SiteScope\\ninstances. See, \\nUse Agentless Monitoring\\n.\\nHyperscale Observability\\nAfter configuring Hyperscale Observability capability:\\nTo view the monitored data in OBM using Performance Dashboards, see \\nAWS metrics and the Performance Dashboard\\n.\\nTo view performance metrics using Performance Troubleshooting dashboards, see \\nPerformance Troubleshooting\\n.\\nTo view the generated events in the Event Browser in OBM, see \\nAWS events and the Event Browser\\n.\\nContainerized OBM\\nOBM has several tabs that enable you to view specific data:\\nEvent perspective\\nThis tab helps you view and better manage the events that occur in your IT environment. For details, see the \\nEvent\\nperspective\\n.\\nHealth perspective\\nThe Health Perspective tab displays the health of related CIs in the context of events. The event selected in the Event\\nNote\\n: Containerized OBM uses OBM user interface to view data; does not use the OPTIC One.\\nContainerized Operations Bridge 2022.11\\nPage \\n125\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.9\\nPage actions\\n10\\nWidget actions\\nNumber\\nUI element\\nFor more information see, \\nOPTIC One components\\n.\\nStakeholder Dashboard\\nThe stakeholder dashboard uses the Business Value Dashboard (BVD) user interface to give you visual information of live data\\nusing tables, charts, and widgets. \\nUse BVD\\n.\\nOPTIC Reporting \\nThe OPTIC Reporting capability uses OPTIC One. After configuring this capability you may log into the OPTIC One to view the\\nsystem infrastructure, event, synthetic transaction, real user monitoring, or application reports. For details, see \\nUse OPTIC\\nReporting\\n.\\nAutomatic Event Correlation\\nAfter configuring the Automatic Event Correlation (AEC) capability, you can view the correlated events in the event console of\\nOperations Bridge Manager (OBM).\\nOn OBM, go to \\nWorkspaces > Operations Console > Event Perspective. \\nCheck the OBM event browser for a new event\\nwith the title: “Automatically Correlated Event: \". \\nSample:\\nMonitoring Control Center\\nMonitoring Control Center (MCC) is a new section of the Operations Bridge UI that consolidates the administration and\\nconfiguration of monitoring capabilities into a central user experience. MCC supports the following capabilities:\\nAgentless Monitoring\\nHyperscale Observability\\nAgentless Monitoring\\nThe new centralized UI, Agentless Monitoring, provides users a unified monitoring experience across multiple SiteScope\\ninstances. See, \\nUse Agentless Monitoring\\n.\\nHyperscale Observability\\nAfter configuring Hyperscale Observability capability:\\nTo view the monitored data in OBM using Performance Dashboards, see \\nAWS metrics and the Performance Dashboard\\n.\\nTo view performance metrics using Performance Troubleshooting dashboards, see \\nPerformance Troubleshooting\\n.\\nTo view the generated events in the Event Browser in OBM, see \\nAWS events and the Event Browser\\n.\\nContainerized OBM\\nOBM has several tabs that enable you to view specific data:\\nEvent perspective\\nThis tab helps you view and better manage the events that occur in your IT environment. For details, see the \\nEvent\\nperspective\\n.\\nHealth perspective\\nThe Health Perspective tab displays the health of related CIs in the context of events. The event selected in the Event\\nNote\\n: Containerized OBM uses OBM user interface to view data; does not use the OPTIC One.\\nContainerized Operations Bridge 2022.11\\nPage \\n125\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '37b93188ba128fe8b1a2d93918f8dd97'}>,\n",
              "  <Document: {'content': 'User roles\\nInstalling Operations Bridge includes sequential procedures and involves different user roles in your organization. Following\\nare the user roles involved:\\nApplication owner\\nThe application owner can install and configure the application irrespective of the Kubernetes distribution where you install the\\napplication. For example, if you want to deploy Operations Bridge on Amazon Elastic Kubernetes Services (EKS), the\\napplication owner can create a user to use S3 services or change the service quota. They have all application administration\\npermissions and privileges to perform the tasks.\\nCluster administrator\\nCluster administrators can create and configure a cluster. They can also install the OPTIC Management Toolkit (OMT) with\\nembedded Kubernetes and create the registry required for the application. They have all administration permissions and\\nprivileges to perform the tasks.\\nDatabase administrator\\nDatabase administrators can install and configure external databases. They can also tune the embedded databases with\\nparameters required for the application. They have all database administration permissions and privileges to perform the\\ntasks.\\nNetwork administrator\\nNetwork administrators can set up all essential network requirements including opening ports applicable to application\\ncapabilities. They have all network administration permissions and privileges to perform the tasks.\\nStorage administrator\\nStorage administrators perform all disk space and storage related tasks such as creating a namespace to deploy the\\napplication or configure local storage volumes. They have all administration permissions and privileges to perform the tasks. \\nContainerized Operations Bridge 2022.11\\nPage \\n123\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.User roles\\nInstalling Operations Bridge includes sequential procedures and involves different user roles in your organization. Following\\nare the user roles involved:\\nApplication owner\\nThe application owner can install and configure the application irrespective of the Kubernetes distribution where you install the\\napplication. For example, if you want to deploy Operations Bridge on Amazon Elastic Kubernetes Services (EKS), the\\napplication owner can create a user to use S3 services or change the service quota. They have all application administration\\npermissions and privileges to perform the tasks.\\nCluster administrator\\nCluster administrators can create and configure a cluster. They can also install the OPTIC Management Toolkit (OMT) with\\nembedded Kubernetes and create the registry required for the application. They have all administration permissions and\\nprivileges to perform the tasks.\\nDatabase administrator\\nDatabase administrators can install and configure external databases. They can also tune the embedded databases with\\nparameters required for the application. They have all database administration permissions and privileges to perform the\\ntasks.\\nNetwork administrator\\nNetwork administrators can set up all essential network requirements including opening ports applicable to application\\ncapabilities. They have all network administration permissions and privileges to perform the tasks.\\nStorage administrator\\nStorage administrators perform all disk space and storage related tasks such as creating a namespace to deploy the\\napplication or configure local storage volumes. They have all administration permissions and privileges to perform the tasks. \\nContainerized Operations Bridge 2022.11\\nPage \\n123\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f813f24ec4aef268364d190dbf8c667c'}>,\n",
              "  <Document: {'content': 'View alerts from OMT Prometheus in Event Browser\\nOPTIC Management Toolkit (OMT) packages Prometheus to collect metrics for Containerized Operations Bridge application and\\nthe underlying Kubernetes infrastructure. OMT offers out-of-the-box (OOTB) alerting rules to trigger alerts whenever a\\nthreshold breach occurs. Prometheus Alertmanager manages the alerting rules. You need to integrate Prometheus\\nAlertmanager with OBM to view these alerts in OBM Event Browser. \\nThis use case describes the end-to-end task flow to view pod CPU usage alerts from OMT Prometheus Alertmanager in\\nContainerized OBM Event Browser. You can use this as an example to send alerts from any other Prometheus Alertmanager in\\nyour environment, to Containerized OBM.\\nIt considers the following OOTB alerting rule for pod CPU usage.\\n- alert: KubernetesPodHighCPU\\n      annotations:\\n        description: The CPU usage of pod {{ $labels.namespace }}/{{ $labels.pod}}\\n          in instance {{ $labels.instance }} is high\\n          VALUE = {{ $value }} LABELS = {{ $labels }}\\n        summary: The CPU usage of pod {{ $labels.namespace }}/{{ $labels.pod}} is\\n          high\\n      expr: 100 * sum(rate(container_cpu_usage_seconds_total{container!=\"\", image!=\"\"}[1m]))\\n        by (pod) / sum(kube_pod_container_resource_limits{resource=\"cpu\"}) by (pod)\\n        > 80\\n      for: 5m\\n      labels:\\n        severity: info\\nThe severity \\ninfo\\n in the alerting rule corresponds to \\nNormal\\n severity in OBM Event Browser. When the CPU usage of a pod\\nexceeds the configured threshold (80), an event of \\nNormal\\n severity appears in the OBM Event Browser. \\nPrerequisites\\nMake sure you\\'ve installed and configured Monitoring capability in OMT. For more information, refer \\nOMT documentation\\n. \\nMake sure that you have deployed the Containerized OBM capability.\\nMake sure that you have the permissions to run \\nkubectl\\n commands on the OMT cluster.\\nSend Prometheus alerts to OBM\\nYou must integrate Prometheus Alertmanager with OBM to send the Prometheus alerts to OBM. Perform the following steps to\\nsend high pod CPU usage alerts to OBM:\\nStep 1: Import Prometheus Alertmanager content pack into OBM\\nPrometheus Alertmanager content pack includes the Prometheus Alertmanager policy. This policy contains rules to map the\\nPrometheus Alertmanager alerts to the OBM events.\\nPerform the following steps to import the Prometheus Alertmanager content pack into OBM:\\n1\\n. \\nDownload the Prometheus Alertmanager content pack from the following location:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessPort>/staticfiles/Monitoring_Service_Prometheus_Alert_Manager_Content_Pack_xxxx.xx.xxx.zip \\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/Monitoring_Service_Prometheus_Alert_Manager_Content_Pack_xxxx.xx.xxx.zip\\n2\\n. \\nOn OBM user interface, go to \\nAdministration\\n > \\nSETUP AND MAINTENANCE\\n > \\nContent Packs\\n.\\n3\\n. \\nClick \\nImport\\n. The Import Content Pack window appears.\\n4\\n. \\nBrowse to the location where you have saved the Prometheus Alertmanager content pack and then click \\nImport\\n. The\\nPrometheus Alertmanager content pack gets imported. Click \\nClose\\n.\\nStep 2: Deploy Prometheus Alertmanager Integration policy\\nFollow these steps to deploy Prometheus Alertmanager Integration policy:\\n1\\n. \\nOn OBM, go to \\nAdministration\\n > \\nMonitoring\\n > \\nPolicy Templates\\n.\\n2\\n. \\nSelect \\nTemplate by Type\\n > \\nEvents\\n > \\nEvent from REST Web Service\\n.\\n3\\n. \\nIn the middle pane, expand \\nPrometheus Alertmanager Integration\\n and select the version.\\n4\\n. \\nClick \\nAssign\\n \\nand Deploy\\n. The Assign and Deploy window opens.\\n5\\n. \\nIn the \\nConfiguration Item\\n tab, select \\nomi-0\\n, or select both \\nomi-0\\n and \\nomi-1\\n if you have OBM High Availability (HA)\\ndeployment.\\nTip: \\nTo view a list of available OOTB alerting rules, run the following command:\\n                \\nkubectl get prometheusrules -n core\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n128\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.View alerts from OMT Prometheus in Event Browser\\nOPTIC Management Toolkit (OMT) packages Prometheus to collect metrics for Containerized Operations Bridge application and\\nthe underlying Kubernetes infrastructure. OMT offers out-of-the-box (OOTB) alerting rules to trigger alerts whenever a\\nthreshold breach occurs. Prometheus Alertmanager manages the alerting rules. You need to integrate Prometheus\\nAlertmanager with OBM to view these alerts in OBM Event Browser. \\nThis use case describes the end-to-end task flow to view pod CPU usage alerts from OMT Prometheus Alertmanager in\\nContainerized OBM Event Browser. You can use this as an example to send alerts from any other Prometheus Alertmanager in\\nyour environment, to Containerized OBM.\\nIt considers the following OOTB alerting rule for pod CPU usage.\\n- alert: KubernetesPodHighCPU\\n      annotations:\\n        description: The CPU usage of pod {{ $labels.namespace }}/{{ $labels.pod}}\\n          in instance {{ $labels.instance }} is high\\n          VALUE = {{ $value }} LABELS = {{ $labels }}\\n        summary: The CPU usage of pod {{ $labels.namespace }}/{{ $labels.pod}} is\\n          high\\n      expr: 100 * sum(rate(container_cpu_usage_seconds_total{container!=\"\", image!=\"\"}[1m]))\\n        by (pod) / sum(kube_pod_container_resource_limits{resource=\"cpu\"}) by (pod)\\n        > 80\\n      for: 5m\\n      labels:\\n        severity: info\\nThe severity \\ninfo\\n in the alerting rule corresponds to \\nNormal\\n severity in OBM Event Browser. When the CPU usage of a pod\\nexceeds the configured threshold (80), an event of \\nNormal\\n severity appears in the OBM Event Browser. \\nPrerequisites\\nMake sure you\\'ve installed and configured Monitoring capability in OMT. For more information, refer \\nOMT documentation\\n. \\nMake sure that you have deployed the Containerized OBM capability.\\nMake sure that you have the permissions to run \\nkubectl\\n commands on the OMT cluster.\\nSend Prometheus alerts to OBM\\nYou must integrate Prometheus Alertmanager with OBM to send the Prometheus alerts to OBM. Perform the following steps to\\nsend high pod CPU usage alerts to OBM:\\nStep 1: Import Prometheus Alertmanager content pack into OBM\\nPrometheus Alertmanager content pack includes the Prometheus Alertmanager policy. This policy contains rules to map the\\nPrometheus Alertmanager alerts to the OBM events.\\nPerform the following steps to import the Prometheus Alertmanager content pack into OBM:\\n1\\n. \\nDownload the Prometheus Alertmanager content pack from the following location:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessPort>/staticfiles/Monitoring_Service_Prometheus_Alert_Manager_Content_Pack_xxxx.xx.xxx.zip \\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/Monitoring_Service_Prometheus_Alert_Manager_Content_Pack_xxxx.xx.xxx.zip\\n2\\n. \\nOn OBM user interface, go to \\nAdministration\\n > \\nSETUP AND MAINTENANCE\\n > \\nContent Packs\\n.\\n3\\n. \\nClick \\nImport\\n. The Import Content Pack window appears.\\n4\\n. \\nBrowse to the location where you have saved the Prometheus Alertmanager content pack and then click \\nImport\\n. The\\nPrometheus Alertmanager content pack gets imported. Click \\nClose\\n.\\nStep 2: Deploy Prometheus Alertmanager Integration policy\\nFollow these steps to deploy Prometheus Alertmanager Integration policy:\\n1\\n. \\nOn OBM, go to \\nAdministration\\n > \\nMonitoring\\n > \\nPolicy Templates\\n.\\n2\\n. \\nSelect \\nTemplate by Type\\n > \\nEvents\\n > \\nEvent from REST Web Service\\n.\\n3\\n. \\nIn the middle pane, expand \\nPrometheus Alertmanager Integration\\n and select the version.\\n4\\n. \\nClick \\nAssign\\n \\nand Deploy\\n. The Assign and Deploy window opens.\\n5\\n. \\nIn the \\nConfiguration Item\\n tab, select \\nomi-0\\n, or select both \\nomi-0\\n and \\nomi-1\\n if you have OBM High Availability (HA)\\ndeployment.\\nTip: \\nTo view a list of available OOTB alerting rules, run the following command:\\n                \\nkubectl get prometheusrules -n core\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n128\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb57d912bbad8c8bc1a28188de13d775'}>,\n",
              "  <Document: {'content': \"2\\n. \\nRun the following command to upload the configuration file:\\n./alertmanager config put -i <alertmanagerConfig.yaml>\\nAfter you complete these steps, OBM will start receiving the Prometheus alert corresponding to high pod CPU usage. You\\ncan view this alert in OBM Event Browser. \\nView high CPU usage alert in Event Browser\\n1\\n. \\nLog in to OBM and go to\\n Workspaces > Operations Console > Event Perspective\\n.\\n2\\n. \\nSelect the event corresponding to high pod CPU usage alert. You'll find more details about the event in the \\nEvent\\nDetails\\n pane.\\nContainerized Operations Bridge 2022.11\\nPage \\n130\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.2\\n. \\nRun the following command to upload the configuration file:\\n./alertmanager config put -i <alertmanagerConfig.yaml>\\nAfter you complete these steps, OBM will start receiving the Prometheus alert corresponding to high pod CPU usage. You\\ncan view this alert in OBM Event Browser. \\nView high CPU usage alert in Event Browser\\n1\\n. \\nLog in to OBM and go to\\n Workspaces > Operations Console > Event Perspective\\n.\\n2\\n. \\nSelect the event corresponding to high pod CPU usage alert. You'll find more details about the event in the \\nEvent\\nDetails\\n pane.\\nContainerized Operations Bridge 2022.11\\nPage \\n130\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '804d961c2b3607b6e01406be9cb251a5'}>,\n",
              "  <Document: {'content': '6\\n. \\nClick \\nAssign\\n.\\nStep 3: Configure credentials to access OBM endpoint\\nREST Web Service Listener policy enables you to receive alerts from Prometheus Alertmanager by using a REST Web Service\\nendpoint. Follow these steps to configure credential to access the OBM REST Web Service endpoint:\\n1\\n. \\nRun the following command to check if the credential is already configured:\\nkubectl  -n <namespace> describe secret omi-restws-policy-auth \\nExample\\n:\\nkubectl  -n opsb-helm describe secret omi-restws-policy-auth \\nIf the credential is configured, the \\nusername\\n and \\npassword \\nwill be displayed in base-64 encoded format. Convert the\\ncredential in clear text format and note it down. You\\'ll need this credential for webhook configuration. If the credential is\\nempty, proceed to the next step to configure it.\\n2\\n. \\nRun the following command to configure the secret:\\nkubectl  -n <namespace> edit secret omi-restws-policy-auth\\nExample\\n:\\nkubectl  -n opsb-helm edit secret omi-restws-policy-auth\\nSet the \\nusername\\n and \\npassword \\nand encode them in base-64 format.\\nExample\\n:\\nRun the following command to generate \\nusername\\n and \\npassword\\n in base-64 format.\\n \\necho myusername | base64\\necho mypassword | base64\\nStep 4: Configure webhook in Prometheus Alertmanager\\nAfter deploying the Prometheus Alertmanager Integration policy, you must configure the REST Web Service endpoint and\\ncredential in the Prometheus Alertmanager configuration file.\\nFollow these steps on the suite master node:\\n1\\n. \\nNavigate to the install directory and run the following command to get the configuration file:\\n./alertmanager config get\\nThe configuration yaml file will be displayed if it\\'s configured. Edit the yaml file to configure the REST Web Service\\nendpoint  and credential. If it\\'s not configured, you need to create it. Refer to the following example to create or edit the\\nconfiguration file:\\nIn the \\nbasic_auth\\n section, specify the \\nusername \\nand \\npassword\\n in clear text format, that you noted down earlier. \\nReplace the \\nurl\\n parameter with the URL of the REST Web Service OBM endpoint.\\nReplace the host with the external access host.\\nReplace the port number with the external access port number.\\nExample webhook configuration\\n:\\nglobal:\\n  resolve_timeout: 5m\\nreceivers:\\n- name: \"null\"\\n- \\nname: webhook\\n  webhook_configs: \\n  - http_config:\\n      tls_config:\\n        insecure_skip_verify: true\\n     \\n basic_auth:\\n        username: admin\\n        password: admin1\\n    url: https://\\n<host>:<port>/bsmc/rest/events/prometheus \\nroute:\\n  group_by:\\n  - \\'...\\'\\n  group_interval: 5m\\n  group_wait: 30s\\n  \\nreceiver: webhook\\n  repeat_interval: 12h\\n  routes:\\n  - match:\\n      alertname: Watchdogs\\n    receiver: \"null\"\\ntemplates:\\n- /etc/alertmanager/config/*.tmpl\\nContainerized Operations Bridge 2022.11\\nPage \\n129\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.6\\n. \\nClick \\nAssign\\n.\\nStep 3: Configure credentials to access OBM endpoint\\nREST Web Service Listener policy enables you to receive alerts from Prometheus Alertmanager by using a REST Web Service\\nendpoint. Follow these steps to configure credential to access the OBM REST Web Service endpoint:\\n1\\n. \\nRun the following command to check if the credential is already configured:\\nkubectl  -n <namespace> describe secret omi-restws-policy-auth \\nExample\\n:\\nkubectl  -n opsb-helm describe secret omi-restws-policy-auth \\nIf the credential is configured, the \\nusername\\n and \\npassword \\nwill be displayed in base-64 encoded format. Convert the\\ncredential in clear text format and note it down. You\\'ll need this credential for webhook configuration. If the credential is\\nempty, proceed to the next step to configure it.\\n2\\n. \\nRun the following command to configure the secret:\\nkubectl  -n <namespace> edit secret omi-restws-policy-auth\\nExample\\n:\\nkubectl  -n opsb-helm edit secret omi-restws-policy-auth\\nSet the \\nusername\\n and \\npassword \\nand encode them in base-64 format.\\nExample\\n:\\nRun the following command to generate \\nusername\\n and \\npassword\\n in base-64 format.\\n \\necho myusername | base64\\necho mypassword | base64\\nStep 4: Configure webhook in Prometheus Alertmanager\\nAfter deploying the Prometheus Alertmanager Integration policy, you must configure the REST Web Service endpoint and\\ncredential in the Prometheus Alertmanager configuration file.\\nFollow these steps on the suite master node:\\n1\\n. \\nNavigate to the install directory and run the following command to get the configuration file:\\n./alertmanager config get\\nThe configuration yaml file will be displayed if it\\'s configured. Edit the yaml file to configure the REST Web Service\\nendpoint  and credential. If it\\'s not configured, you need to create it. Refer to the following example to create or edit the\\nconfiguration file:\\nIn the \\nbasic_auth\\n section, specify the \\nusername \\nand \\npassword\\n in clear text format, that you noted down earlier. \\nReplace the \\nurl\\n parameter with the URL of the REST Web Service OBM endpoint.\\nReplace the host with the external access host.\\nReplace the port number with the external access port number.\\nExample webhook configuration\\n:\\nglobal:\\n  resolve_timeout: 5m\\nreceivers:\\n- name: \"null\"\\n- \\nname: webhook\\n  webhook_configs: \\n  - http_config:\\n      tls_config:\\n        insecure_skip_verify: true\\n     \\n basic_auth:\\n        username: admin\\n        password: admin1\\n    url: https://\\n<host>:<port>/bsmc/rest/events/prometheus \\nroute:\\n  group_by:\\n  - \\'...\\'\\n  group_interval: 5m\\n  group_wait: 30s\\n  \\nreceiver: webhook\\n  repeat_interval: 12h\\n  routes:\\n  - match:\\n      alertname: Watchdogs\\n    receiver: \"null\"\\ntemplates:\\n- /etc/alertmanager/config/*.tmpl\\nContainerized Operations Bridge 2022.11\\nPage \\n129\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b2a923106614c6bfcb2416f7244ca547'}>,\n",
              "  <Document: {'content': 'Monitor AWS using Hyperscale Observability\\nThe \\nHyperscale Observability\\n capability provides you with scalable monitoring of your AWS resources. It brings with it\\ndynamic detection of new resources as they get added to your AWS subscription. Hyperscale Observability supports discovery\\nand monitoring of the AWS services. For more details about the supported AWS services, see \\nSupported AWS services\\n.\\nHyperscale Observability helps to collect CloudWatch Agent Metrics.\\nHyperscale Observability helps to collect metrics for AWS services. You can set thresholds and monitor the metrics for any\\nbreaches. If there is a breach in the threshold, an event gets generated in Operations Bridge Manager (OBM).\\nYou can view the monitored data in OBM using performance dashboards, Performance Troubleshooting dashboards, or the UIF\\ndashboards. You can also view the generated events in the Event Browser in OBM. \\nPerform the following actions and steps in the given order to use the \\nHyperscale Observability\\n capability:\\n1\\n. \\nSet up Hyperscale Observability\\n2\\n. \\nConfigure AWS collectors\\n3\\n. \\nView AWS metrics and events\\nSet up Hyperscale Observability\\nThis section provides information required to set up the \\nHyperscale Observability\\n capability. To be able to use the\\nHyperscale Observability \\ncapability, you must deploy the OBM and the Stakeholder capabilities. \\nFor detailed deployment steps, see \\nDeploy\\n.\\nConfigure AWS collectors\\nThis section provides information required to start monitoring AWS resources. Hyperscale Observability has scalable AWS\\ncollector pods that receive configuration and execute collection tasks at scheduled intervals. During each run, the collector\\npods check for new AWS resources added to the AWS subscription. Metrics for the latest resources are then collected and sent\\nto OPTIC Data Lake.\\nElements of AWS collector configuration\\nThe AWS Collector configuration has the following elements:\\n1\\n. \\nCredential\\nThis is the AWS account access key ID and secret access key that you will use for monitoring.\\nThe AWS account used for monitoring should have a role with the \\nReadOnlyAccess\\n policy assigned to it.\\n2\\n. \\nTarget\\nThis is the AWS region on which the monitored resources reside\\nThe target should reference the credential required for authentication\\n3\\n. \\nThreshold\\nThese are the thresholds that you may apply to the collected metrics.\\nViolations get notified as events on OBM.\\n4\\n. \\nCollector\\nThe collector configuration drives the monitoring and specifies the scope of monitoring within the target.\\nSkip this task if you have already deployed the Hyperscale Observability capability.\\nContainerized Operations Bridge 2022.11\\nPage \\n131\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Monitor AWS using Hyperscale Observability\\nThe \\nHyperscale Observability\\n capability provides you with scalable monitoring of your AWS resources. It brings with it\\ndynamic detection of new resources as they get added to your AWS subscription. Hyperscale Observability supports discovery\\nand monitoring of the AWS services. For more details about the supported AWS services, see \\nSupported AWS services\\n.\\nHyperscale Observability helps to collect CloudWatch Agent Metrics.\\nHyperscale Observability helps to collect metrics for AWS services. You can set thresholds and monitor the metrics for any\\nbreaches. If there is a breach in the threshold, an event gets generated in Operations Bridge Manager (OBM).\\nYou can view the monitored data in OBM using performance dashboards, Performance Troubleshooting dashboards, or the UIF\\ndashboards. You can also view the generated events in the Event Browser in OBM. \\nPerform the following actions and steps in the given order to use the \\nHyperscale Observability\\n capability:\\n1\\n. \\nSet up Hyperscale Observability\\n2\\n. \\nConfigure AWS collectors\\n3\\n. \\nView AWS metrics and events\\nSet up Hyperscale Observability\\nThis section provides information required to set up the \\nHyperscale Observability\\n capability. To be able to use the\\nHyperscale Observability \\ncapability, you must deploy the OBM and the Stakeholder capabilities. \\nFor detailed deployment steps, see \\nDeploy\\n.\\nConfigure AWS collectors\\nThis section provides information required to start monitoring AWS resources. Hyperscale Observability has scalable AWS\\ncollector pods that receive configuration and execute collection tasks at scheduled intervals. During each run, the collector\\npods check for new AWS resources added to the AWS subscription. Metrics for the latest resources are then collected and sent\\nto OPTIC Data Lake.\\nElements of AWS collector configuration\\nThe AWS Collector configuration has the following elements:\\n1\\n. \\nCredential\\nThis is the AWS account access key ID and secret access key that you will use for monitoring.\\nThe AWS account used for monitoring should have a role with the \\nReadOnlyAccess\\n policy assigned to it.\\n2\\n. \\nTarget\\nThis is the AWS region on which the monitored resources reside\\nThe target should reference the credential required for authentication\\n3\\n. \\nThreshold\\nThese are the thresholds that you may apply to the collected metrics.\\nViolations get notified as events on OBM.\\n4\\n. \\nCollector\\nThe collector configuration drives the monitoring and specifies the scope of monitoring within the target.\\nSkip this task if you have already deployed the Hyperscale Observability capability.\\nContainerized Operations Bridge 2022.11\\nPage \\n131\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'af322c18a082a247c939c49da8829371'}>,\n",
              "  <Document: {'content': \"This includes AWS tag based filters and a selection of the metrics that you want to collect for each AWS service.\\nThe collector configuration should reference the target that you want to monitor\\nThe collector configuration can reference one or more thresholds \\nAWS collector CLI\\nUse the \\nops-monitoring-ctl\\n CLI to manage AWS Collector configuration.\\nYou can download the CLI from:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/linux/ops-monitoring-ctl \\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/windows/ops-monitoring-ctl.exe\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort>\\n with the values you provided in the \\nvalues.yaml\\n.\\nRun the following commands to set up the CLI to connect to the containerized Operations Bridge environment:\\n1\\n. \\nRun the following command to set the server details:\\nops-monitoring-ctl config set cs.server https://<externalAccessHost>:<externalAccessPort>\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort> \\nwith the values you provided in the \\nvalues.yaml\\n.\\n2\\n. \\nRun the following command to set the username:\\nops-monitoring-ctl config set cs.user\\nWhen prompted enter \\nadmin\\n. That's, the containerized Operations Bridge admin user.\\n3\\n. \\nRun the following command to set the password:\\nops-monitoring-ctl config set cs.password\\nWhen prompted enter the containerized Operations Bridge admin password. This is the password provided for the\\ninput \\nidm_opsbridge_admin_password\\n when generating secrets.\\nConfigure your first AWS collector\\n1\\n. \\nCopy the following to create a credential configuration file and name it \\nmy-first-aws-cred.yaml\\n:\\napiVersion: core/v1\\ntype: credential\\nmetadata:\\n  name: my-first-aws-cred\\nspec:\\n  subType: aws-access-key\\n  context: \\n    access_key_id: [access_key_id]\\n    secret_access_key: [secret_access_key]\\nReplace [access_key_id] with the right access key id and [secret_access_key] with the right secret access key of your AWS\\nuser\\n2\\n. \\nCopy the following to create a target configuration file and name it \\nmy-first-aws-target.yaml\\n:\\napiVersion: core/v1\\ntype: target\\nmetadata:\\n  name: my-first-aws-target\\nspec:\\n  subType: aws-region\\n  credential: my-first-aws-cred\\n  proxy:\\n    url: http://proxy.company.net:80\\n  endpoint: us-east-1\\nReplace \\nhttp://proxy.company.net:80\\n with the relevant HTTP proxy required by the AWS Collector to access AWS on the\\ninternet.\\n3\\n. \\nCopy the following to create a collector configuration file and name it \\nmy-first-aws-collector.yaml\\n:\\napiVersion: core/v1\\ntype: collector\\nmetadata:\\n  name: my-first-aws-collector\\nspec:\\n  subType: aws\\n  targets:\\n  - my-first-aws-target\\n  context:\\n    filterConfig:\\nContainerized Operations Bridge 2022.11\\nPage \\n132\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.This includes AWS tag based filters and a selection of the metrics that you want to collect for each AWS service.\\nThe collector configuration should reference the target that you want to monitor\\nThe collector configuration can reference one or more thresholds \\nAWS collector CLI\\nUse the \\nops-monitoring-ctl\\n CLI to manage AWS Collector configuration.\\nYou can download the CLI from:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/linux/ops-monitoring-ctl \\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/windows/ops-monitoring-ctl.exe\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort>\\n with the values you provided in the \\nvalues.yaml\\n.\\nRun the following commands to set up the CLI to connect to the containerized Operations Bridge environment:\\n1\\n. \\nRun the following command to set the server details:\\nops-monitoring-ctl config set cs.server https://<externalAccessHost>:<externalAccessPort>\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort> \\nwith the values you provided in the \\nvalues.yaml\\n.\\n2\\n. \\nRun the following command to set the username:\\nops-monitoring-ctl config set cs.user\\nWhen prompted enter \\nadmin\\n. That's, the containerized Operations Bridge admin user.\\n3\\n. \\nRun the following command to set the password:\\nops-monitoring-ctl config set cs.password\\nWhen prompted enter the containerized Operations Bridge admin password. This is the password provided for the\\ninput \\nidm_opsbridge_admin_password\\n when generating secrets.\\nConfigure your first AWS collector\\n1\\n. \\nCopy the following to create a credential configuration file and name it \\nmy-first-aws-cred.yaml\\n:\\napiVersion: core/v1\\ntype: credential\\nmetadata:\\n  name: my-first-aws-cred\\nspec:\\n  subType: aws-access-key\\n  context: \\n    access_key_id: [access_key_id]\\n    secret_access_key: [secret_access_key]\\nReplace [access_key_id] with the right access key id and [secret_access_key] with the right secret access key of your AWS\\nuser\\n2\\n. \\nCopy the following to create a target configuration file and name it \\nmy-first-aws-target.yaml\\n:\\napiVersion: core/v1\\ntype: target\\nmetadata:\\n  name: my-first-aws-target\\nspec:\\n  subType: aws-region\\n  credential: my-first-aws-cred\\n  proxy:\\n    url: http://proxy.company.net:80\\n  endpoint: us-east-1\\nReplace \\nhttp://proxy.company.net:80\\n with the relevant HTTP proxy required by the AWS Collector to access AWS on the\\ninternet.\\n3\\n. \\nCopy the following to create a collector configuration file and name it \\nmy-first-aws-collector.yaml\\n:\\napiVersion: core/v1\\ntype: collector\\nmetadata:\\n  name: my-first-aws-collector\\nspec:\\n  subType: aws\\n  targets:\\n  - my-first-aws-target\\n  context:\\n    filterConfig:\\nContainerized Operations Bridge 2022.11\\nPage \\n132\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c4dd00e5254a676c9b197d312ff62b5d'}>,\n",
              "  <Document: {'content': '      matchRegex: false\\n      targetTags:\\n        app:\\n          - my_aws_app\\n \\nReplace \\napp\\n and \\nmy_aws_app\\n with the right key and value of the AWS tag associated with AWS resources for which you\\nwould like to collect metrics. For example, if you have tagged the AWS resource with \\nenvironment: dev\\n, replace \\napp\\n with \\nen\\nvironment\\n and \\nmy_aws_app\\n with \\ndev\\n.\\n4\\n. \\nRun the following commands to start the collector:\\nops-monitoring-ctl create -f my-first-aws-cred.yaml\\nops-monitoring-ctl create -f my-first-aws-target.yaml\\nops-monitoring-ctl create -f my-first-aws-collector.yaml\\n5\\n. \\nRun the following command to check the monitoring status:\\nops-monitoring-ctl get collector-status\\nIf the \"\\nops-monitoring-ctl get collector-status\\n\" command returns \"\\nNA\\n\" as the status, it indicates that the collector has not run\\nyet.\\nView AWS metrics and events \\nAfter you configure the AWS collectors you can view AWS metrics and events: \\nTo view the monitored data in OBM using Performance Dashboards, see \\nAWS metrics and the Performance Dashboard\\n.\\nTo view performance metrics using Performance Troubleshooting dashboards, see \\nPerformance Troubleshooting\\n.\\nTo view the generated events in the Event Browser in OBM, see \\nAWS events and the Event Browser\\n.\\nRelated topics\\nManage your configurations using any of the following options:\\nUsing OBM policies\\nUsing CLI parameters\\nUsing sample configuration files and CLI\\nKnow more about the discovered AWS topology, see \\nAWS Topology Views\\n.\\nFor details about the metrics collected, see the \\nHyperscale Observability data model\\n.\\nIf you had already installed the suite, but didn\\'t choose Hyperscale Observability and want to install it now, see \\nAdd/Remove capabilities on premises\\nAdd/Remove capabilities on cloud platform\\nIf you upgraded from an earlier version to the current version and want to install Hyperscale Observability capability, see \\nAdd/Remove capabilities on premises\\nAdd/Remove capabilities on cloud platform\\nImportant: \\nAWS collector will start sending events to OBM only if the OBM pod is running. Make sure that the \\nomi-\\n0\\n (and \\nomi-1\\n in HA) pod is running.\\nYou must associate at least one tag with each AWS resource. This will enable Hyperscale Observability\\nto discover and monitor the AWS resource. \\nDuring discovery, only running instances get discovered. Stopped, starting, or terminating instances don\\'t get\\ndiscovered.  However, for the EC2 resources, all instances get discovered irrespective of their state. \\nCloudWatch collects data only once in 24 hours from Amazon S3. For details see, \\nAmazon S3 FAQs\\n.\\nFor a given 5 minute collection period, AWS CloudWatch APIs may sometimes return more than one record.\\nThis would mean that there is more than one record in Vertica for the monitored resource during the time.\\nThis is normal. For example: For the EC2 metric collection during the 5 minute interval from \\n1:05\\n PM to\\n1:10\\n PM, the AWS CloudWatch APIs may return more than one record. One record at \\n1:07\\n PM with CPU\\nrelated metrics, and another record at \\n1:09 \\nPM with the rest of the metrics. You will correspondingly see two\\nrecords in Vertica.\\nTo collect Auto Scaling Groups (ASG) metrics you must enable monitoring on the AWS console. For details,\\nsee \\nMonitoring CloudWatch metrics for your Auto Scaling groups and instances\\n.\\nIf the \"\\nops-monitoring-ctl get collector-status\\n\" command returns \"\\nNA\\n\" as the status, it indicates that the collector\\nhas not run yet.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n133\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.      matchRegex: false\\n      targetTags:\\n        app:\\n          - my_aws_app\\n \\nReplace \\napp\\n and \\nmy_aws_app\\n with the right key and value of the AWS tag associated with AWS resources for which you\\nwould like to collect metrics. For example, if you have tagged the AWS resource with \\nenvironment: dev\\n, replace \\napp\\n with \\nen\\nvironment\\n and \\nmy_aws_app\\n with \\ndev\\n.\\n4\\n. \\nRun the following commands to start the collector:\\nops-monitoring-ctl create -f my-first-aws-cred.yaml\\nops-monitoring-ctl create -f my-first-aws-target.yaml\\nops-monitoring-ctl create -f my-first-aws-collector.yaml\\n5\\n. \\nRun the following command to check the monitoring status:\\nops-monitoring-ctl get collector-status\\nIf the \"\\nops-monitoring-ctl get collector-status\\n\" command returns \"\\nNA\\n\" as the status, it indicates that the collector has not run\\nyet.\\nView AWS metrics and events \\nAfter you configure the AWS collectors you can view AWS metrics and events: \\nTo view the monitored data in OBM using Performance Dashboards, see \\nAWS metrics and the Performance Dashboard\\n.\\nTo view performance metrics using Performance Troubleshooting dashboards, see \\nPerformance Troubleshooting\\n.\\nTo view the generated events in the Event Browser in OBM, see \\nAWS events and the Event Browser\\n.\\nRelated topics\\nManage your configurations using any of the following options:\\nUsing OBM policies\\nUsing CLI parameters\\nUsing sample configuration files and CLI\\nKnow more about the discovered AWS topology, see \\nAWS Topology Views\\n.\\nFor details about the metrics collected, see the \\nHyperscale Observability data model\\n.\\nIf you had already installed the suite, but didn\\'t choose Hyperscale Observability and want to install it now, see \\nAdd/Remove capabilities on premises\\nAdd/Remove capabilities on cloud platform\\nIf you upgraded from an earlier version to the current version and want to install Hyperscale Observability capability, see \\nAdd/Remove capabilities on premises\\nAdd/Remove capabilities on cloud platform\\nImportant: \\nAWS collector will start sending events to OBM only if the OBM pod is running. Make sure that the \\nomi-\\n0\\n (and \\nomi-1\\n in HA) pod is running.\\nYou must associate at least one tag with each AWS resource. This will enable Hyperscale Observability\\nto discover and monitor the AWS resource. \\nDuring discovery, only running instances get discovered. Stopped, starting, or terminating instances don\\'t get\\ndiscovered.  However, for the EC2 resources, all instances get discovered irrespective of their state. \\nCloudWatch collects data only once in 24 hours from Amazon S3. For details see, \\nAmazon S3 FAQs\\n.\\nFor a given 5 minute collection period, AWS CloudWatch APIs may sometimes return more than one record.\\nThis would mean that there is more than one record in Vertica for the monitored resource during the time.\\nThis is normal. For example: For the EC2 metric collection during the 5 minute interval from \\n1:05\\n PM to\\n1:10\\n PM, the AWS CloudWatch APIs may return more than one record. One record at \\n1:07\\n PM with CPU\\nrelated metrics, and another record at \\n1:09 \\nPM with the rest of the metrics. You will correspondingly see two\\nrecords in Vertica.\\nTo collect Auto Scaling Groups (ASG) metrics you must enable monitoring on the AWS console. For details,\\nsee \\nMonitoring CloudWatch metrics for your Auto Scaling groups and instances\\n.\\nIf the \"\\nops-monitoring-ctl get collector-status\\n\" command returns \"\\nNA\\n\" as the status, it indicates that the collector\\nhas not run yet.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n133\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff85c0ea327cc39fa3e450fc4f7910b7'}>,\n",
              "  <Document: {'content': 'Monitor Azure using Hyperscale Observability\\nThe \\nHyperscale Observability\\n capability provides you with scalable monitoring of your Azure resources. It brings with it\\ndynamic detection of new resources as they get added to your Azure subscription. Hyperscale Observability supports discovery\\nand monitoring of the Azure services. For more details about the supported Azure services, see \\nSupported Azure services\\n.\\nHyperscale Observability helps to collect metrics for Azure services. You can set thresholds and monitor the metrics for any\\nbreaches. If there is a breach in the threshold, an event gets generated in Operations Bridge Manager (OBM).\\nYou can view the monitored data in OBM using performance dashboards or on Performance Troubleshooting dashboards. You\\ncan also view the generated events in the Event Browser in OBM. \\nPerform the following actions and steps in the given order to use the \\nHyperscale Observability\\n capability:\\n1\\n. \\nSet up Hyperscale Observability\\n2\\n. \\nConfigure Azure collectors\\n3\\n. \\nView Azure metrics and events\\nSet up Hyperscale Observability\\nSkip this task if you have already deployed the Hyperscale Observability capability.\\nThis section provides information required to set up the \\nHyperscale Observability\\n capability. To be able to use the\\nHyperscale Observability \\ncapability, you must deploy the OBM and the Stakeholder capabilities. \\nFor detailed deployment steps, see \\nDeploy (OpsBridge)\\n.\\nConfigure Azure collectors\\nThis section provides information required to start monitoring Azure resources. Hyperscale Observability has scalable Azure\\ncollector pods that receive configuration and execute collection tasks at scheduled intervals. During each run, the collector\\npods check for new Azure resources added to the Azure subscription. Metrics for the latest resources are then collected and\\nsent to OPTIC Data Lake.\\nElements of Azure collector configuration\\nThe Azure Collector configuration has the following elements:\\n1\\n. \\nCredential\\nThis is the Azure client ID and client secret that you will use for monitoring.\\nThe Azure account used for monitoring should have a role with the \\nReader\\n policy assigned to it. In addition, for\\nAzure Kubernetes Service (AKS) monitoring, the Azure account should have a role with the \\nContributor \\npolicy\\nassigned to it. For more information on assigning policies, see Azure policy documentation on \\ndocs.microsoft.com\\n.\\n2\\n. \\nTarget\\nA target represents an Azure subscription that will be monitored with a given credential. \\nThe target should reference the credential required for authentication\\n3\\n. \\nThreshold\\nThese are the thresholds that you may apply to the collected metrics.\\nViolations get notified as events on OBM.\\n4\\n. \\nCollector\\nThe collector configuration drives the monitoring and specifies the scope of monitoring within the target.\\nThis includes Azure tag based filters and a selection of the metrics that you want to collect for each Azure service.\\nThe collector configuration should reference the target that you want to monitor.\\nThe collector configuration can reference one or more thresholds.\\nAzure collector CLI\\nUse the \\nops-monitoring-ctl\\n CLI to manage Azure Collector configuration.\\nYou can download the CLI from:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/linux/ops-monitoring-ctl \\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/windows/ops-monitoring-ctl.exe\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort>\\n with the values you provided in the \\nvalues.yaml\\n.\\nRun the following commands to set up the CLI to connect to the containerized Operations Bridge environment:\\n1\\n. \\nRun the following command to set the server details:\\nops-monitoring-ctl config set cs.server https://<externalAccessHost>:<externalAccessPort>\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort> \\nwith the values you provided in the \\nvalues.yaml\\n.\\n2\\n. \\nRun the following command to set the username:\\nContainerized Operations Bridge 2022.11\\nPage \\n134\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Monitor Azure using Hyperscale Observability\\nThe \\nHyperscale Observability\\n capability provides you with scalable monitoring of your Azure resources. It brings with it\\ndynamic detection of new resources as they get added to your Azure subscription. Hyperscale Observability supports discovery\\nand monitoring of the Azure services. For more details about the supported Azure services, see \\nSupported Azure services\\n.\\nHyperscale Observability helps to collect metrics for Azure services. You can set thresholds and monitor the metrics for any\\nbreaches. If there is a breach in the threshold, an event gets generated in Operations Bridge Manager (OBM).\\nYou can view the monitored data in OBM using performance dashboards or on Performance Troubleshooting dashboards. You\\ncan also view the generated events in the Event Browser in OBM. \\nPerform the following actions and steps in the given order to use the \\nHyperscale Observability\\n capability:\\n1\\n. \\nSet up Hyperscale Observability\\n2\\n. \\nConfigure Azure collectors\\n3\\n. \\nView Azure metrics and events\\nSet up Hyperscale Observability\\nSkip this task if you have already deployed the Hyperscale Observability capability.\\nThis section provides information required to set up the \\nHyperscale Observability\\n capability. To be able to use the\\nHyperscale Observability \\ncapability, you must deploy the OBM and the Stakeholder capabilities. \\nFor detailed deployment steps, see \\nDeploy (OpsBridge)\\n.\\nConfigure Azure collectors\\nThis section provides information required to start monitoring Azure resources. Hyperscale Observability has scalable Azure\\ncollector pods that receive configuration and execute collection tasks at scheduled intervals. During each run, the collector\\npods check for new Azure resources added to the Azure subscription. Metrics for the latest resources are then collected and\\nsent to OPTIC Data Lake.\\nElements of Azure collector configuration\\nThe Azure Collector configuration has the following elements:\\n1\\n. \\nCredential\\nThis is the Azure client ID and client secret that you will use for monitoring.\\nThe Azure account used for monitoring should have a role with the \\nReader\\n policy assigned to it. In addition, for\\nAzure Kubernetes Service (AKS) monitoring, the Azure account should have a role with the \\nContributor \\npolicy\\nassigned to it. For more information on assigning policies, see Azure policy documentation on \\ndocs.microsoft.com\\n.\\n2\\n. \\nTarget\\nA target represents an Azure subscription that will be monitored with a given credential. \\nThe target should reference the credential required for authentication\\n3\\n. \\nThreshold\\nThese are the thresholds that you may apply to the collected metrics.\\nViolations get notified as events on OBM.\\n4\\n. \\nCollector\\nThe collector configuration drives the monitoring and specifies the scope of monitoring within the target.\\nThis includes Azure tag based filters and a selection of the metrics that you want to collect for each Azure service.\\nThe collector configuration should reference the target that you want to monitor.\\nThe collector configuration can reference one or more thresholds.\\nAzure collector CLI\\nUse the \\nops-monitoring-ctl\\n CLI to manage Azure Collector configuration.\\nYou can download the CLI from:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/linux/ops-monitoring-ctl \\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/windows/ops-monitoring-ctl.exe\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort>\\n with the values you provided in the \\nvalues.yaml\\n.\\nRun the following commands to set up the CLI to connect to the containerized Operations Bridge environment:\\n1\\n. \\nRun the following command to set the server details:\\nops-monitoring-ctl config set cs.server https://<externalAccessHost>:<externalAccessPort>\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort> \\nwith the values you provided in the \\nvalues.yaml\\n.\\n2\\n. \\nRun the following command to set the username:\\nContainerized Operations Bridge 2022.11\\nPage \\n134\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'defc5cd0953825a4d9d0f7610b1523dc'}>,\n",
              "  <Document: {'content': 'Monitor Kubernetes objects using Hyperscale\\nObservability\\nThe \\nHyperscale Observability\\n capability provides you with scalable monitoring of your Kubernetes objects across various\\nKubernetes flavors. It brings with it the dynamic detection of new resources as they get added to your Kubernetes cluster.\\nFor more details about supported Kubernetes flavors, see \\nAdminister Hyperscale Observability for Kubernetes\\n. \\nFor more details about the supported Kubernetes objects, see \\nSupported Kubernetes objects\\n.\\nHyperscale Observability helps to collect metrics for the monitored Kubernetes objects. You can set thresholds and monitor the\\nmetrics for any breaches. If there is a breach in the threshold, an event gets generated in Operations Bridge Manager (OBM).\\nYou can view the monitored data in OBM using Performance Dashboard or on Performance Troubleshooting dashboards. You\\ncan also view the generated events in the Event Browser in OBM. \\nPerform the following actions and steps in the given order to use the \\nHyperscale Observability\\n capability:\\n1\\n. \\nSet up Hyperscale Observability\\n2\\n. \\nConfigure Kubernetes collectors\\n3\\n. \\nView Kubernetes metrics and events\\nSet up Hyperscale Observability\\nSkip this task if you have already deployed the Hyperscale Observability capability.\\nThis section provides information required to set up the \\nHyperscale Observability\\n capability. To be able to use\\nthe \\nHyperscale Observability \\ncapability, you must deploy the OBM and the Stakeholder capabilities. \\nFor detailed deployment steps, see \\nDeploy (OpsBridge)\\n.\\nConfigure Kubernetes collectors\\nThis section provides information required to start monitoring Kubernetes objects. Hyperscale Observability has scalable\\nKubernetes collector pods that receive configuration and execute collection tasks at scheduled intervals. During each run, the\\ncollector pods check for new Kubernetes objects added to the Kubernetes cluster. Metrics for the latest resources are then\\ncollected and sent to OPTIC Data Lake.\\nElements of Kubernetes collector configuration\\nThe Kubernetes Collector configuration has the following elements:\\n1\\n. \\nCredential\\nThis is the Kubernetes service account that you will use for monitoring.\\nThe Kubernetes service account used for monitoring should have a role with the get and list privileges for the objects\\nthat you want to monitor.\\n2\\n. \\nTarget\\nThis is the Kubernetes cluster on which the monitored objects reside.\\nThe target should reference the credential required for authentication.\\n3\\n. \\nThreshold\\nThese are the thresholds that you may apply to the collected metrics.\\nViolations get notified as events on OBM.\\n4\\n. \\nCollector\\nThe collector configuration drives the monitoring and specifies the scope of monitoring within the target.\\nThis includes Kubernetes tag based filters and a selection of the metrics that you want to collect for each Kubernetes\\nobject.\\nThe collector configuration should reference the target that you want to monitor.\\nThe collector configuration can reference one or more thresholds.\\nKubernetes collector CLI\\nUse the \\nops-monitoring-ctl\\n CLI to manage Kubernetes Collector configuration.\\nYou can download the CLI from:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/linux/ops-monitoring-ctl \\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/windows/ops-monitoring-ctl.exe\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort>\\n with the values you provided in the \\nvalues.yaml\\n.\\nRun the following commands to set up the CLI to connect to the containerized Operations Bridge environment:\\n1\\n. \\nRun the following command to set the server details:\\nops-monitoring-ctl config set cs.server https://<externalAccessHost>:<externalAccessPort>\\nContainerized Operations Bridge 2022.11\\nPage \\n136\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Monitor Kubernetes objects using Hyperscale\\nObservability\\nThe \\nHyperscale Observability\\n capability provides you with scalable monitoring of your Kubernetes objects across various\\nKubernetes flavors. It brings with it the dynamic detection of new resources as they get added to your Kubernetes cluster.\\nFor more details about supported Kubernetes flavors, see \\nAdminister Hyperscale Observability for Kubernetes\\n. \\nFor more details about the supported Kubernetes objects, see \\nSupported Kubernetes objects\\n.\\nHyperscale Observability helps to collect metrics for the monitored Kubernetes objects. You can set thresholds and monitor the\\nmetrics for any breaches. If there is a breach in the threshold, an event gets generated in Operations Bridge Manager (OBM).\\nYou can view the monitored data in OBM using Performance Dashboard or on Performance Troubleshooting dashboards. You\\ncan also view the generated events in the Event Browser in OBM. \\nPerform the following actions and steps in the given order to use the \\nHyperscale Observability\\n capability:\\n1\\n. \\nSet up Hyperscale Observability\\n2\\n. \\nConfigure Kubernetes collectors\\n3\\n. \\nView Kubernetes metrics and events\\nSet up Hyperscale Observability\\nSkip this task if you have already deployed the Hyperscale Observability capability.\\nThis section provides information required to set up the \\nHyperscale Observability\\n capability. To be able to use\\nthe \\nHyperscale Observability \\ncapability, you must deploy the OBM and the Stakeholder capabilities. \\nFor detailed deployment steps, see \\nDeploy (OpsBridge)\\n.\\nConfigure Kubernetes collectors\\nThis section provides information required to start monitoring Kubernetes objects. Hyperscale Observability has scalable\\nKubernetes collector pods that receive configuration and execute collection tasks at scheduled intervals. During each run, the\\ncollector pods check for new Kubernetes objects added to the Kubernetes cluster. Metrics for the latest resources are then\\ncollected and sent to OPTIC Data Lake.\\nElements of Kubernetes collector configuration\\nThe Kubernetes Collector configuration has the following elements:\\n1\\n. \\nCredential\\nThis is the Kubernetes service account that you will use for monitoring.\\nThe Kubernetes service account used for monitoring should have a role with the get and list privileges for the objects\\nthat you want to monitor.\\n2\\n. \\nTarget\\nThis is the Kubernetes cluster on which the monitored objects reside.\\nThe target should reference the credential required for authentication.\\n3\\n. \\nThreshold\\nThese are the thresholds that you may apply to the collected metrics.\\nViolations get notified as events on OBM.\\n4\\n. \\nCollector\\nThe collector configuration drives the monitoring and specifies the scope of monitoring within the target.\\nThis includes Kubernetes tag based filters and a selection of the metrics that you want to collect for each Kubernetes\\nobject.\\nThe collector configuration should reference the target that you want to monitor.\\nThe collector configuration can reference one or more thresholds.\\nKubernetes collector CLI\\nUse the \\nops-monitoring-ctl\\n CLI to manage Kubernetes Collector configuration.\\nYou can download the CLI from:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/linux/ops-monitoring-ctl \\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/ops-monitoring-ctl/windows/ops-monitoring-ctl.exe\\nReplace \\n<externalAccessHost>\\n and \\n<externalAccessPort>\\n with the values you provided in the \\nvalues.yaml\\n.\\nRun the following commands to set up the CLI to connect to the containerized Operations Bridge environment:\\n1\\n. \\nRun the following command to set the server details:\\nops-monitoring-ctl config set cs.server https://<externalAccessHost>:<externalAccessPort>\\nContainerized Operations Bridge 2022.11\\nPage \\n136\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e61af3c83d5d7389df01aa22e8fb52c9'}>,\n",
              "  <Document: {'content': 'ops-monitoring-ctl config set cs.user\\nWhen prompted enter \\nadmin\\n. That\\'s, the containerized Operations Bridge admin user.\\n3\\n. \\nRun the following command to set the password:\\nops-monitoring-ctl config set cs.password\\nWhen prompted enter the containerized Operations Bridge admin password. This is the password provided for the\\ninput \\nidm_opsbridge_admin_password\\n when generating secrets.\\nConfigure your first Azure collector\\n1\\n. \\nCreate a credential configuration file and name it \\nmy-first-azure-cred.yaml\\n. For detailed steps, see \\nAzure credential\\nconfiguration\\n. \\n2\\n. \\nCreate a target configuration file and name it \\nmy-first-azure-target.yaml\\n. For detailed steps, see \\nAzure target\\nconfiguration\\n. \\n3\\n. \\nCreate a collector configuration file and name it \\nmy-first-azure-collector.yaml\\n. For detailed steps, see \\nAzure collector\\nconfiguration\\n. \\n4\\n. \\nRun the following commands to start the collector:\\nops-monitoring-ctl create -f my-first-azure-cred.yaml\\nops-monitoring-ctl create -f my-first-azure-target.yaml\\nops-monitoring-ctl create -f my-first-azure-collector.yaml\\n5\\n. \\nRun the following command to check the monitoring status:\\nops-monitoring-ctl get collector-status\\nIf the \"\\nops-monitoring-ctl get collector-status\\n\" command returns \"\\nNA\\n\" as the status, it indicates that the collector hasn\\'t run\\nyet.\\nView Azure metrics and events \\nAfter you configure the Azure collectors you can view Azure metrics and events: \\nTo view the monitored data in OBM using Performance Dashboards, see \\nView Azure metrics in OBM Performance\\nDashboard\\n.\\nTo view performance metrics using Performance Troubleshooting dashboards, see \\nPerformance Troubleshooting for Azure\\nmetrics\\n.\\nTo view the generated events in the Event Browser in OBM, see \\nView Azure events\\n.\\nRelated topics\\nManage your configurations using any of the following options:\\nUsing CLI parameters\\nUsing sample configuration files and CLI\\nKnow more about the discovered Azure topology, see \\nAzure Topology Views\\n.\\nFor details about the metrics collected, see the \\nHyperscale Observability data model for Azure\\n.\\nIf you had already installed the suite, but didn\\'t choose Hyperscale Observability and want to install it now, see \\nAdd/Remove capabilities on premises\\nAdd/Remove capabilities on cloud platform\\nIf you upgraded from an earlier version to the current version and want to install Hyperscale Observability capability, see \\nAdd/Remove capabilities on premises\\nAdd/Remove capabilities on cloud platform\\nImportant\\n: \\nAzure collector will start sending events to OBM only if the OBM pod is running. Make sure that the \\nomi-\\n0\\n (and \\nomi-1\\n in HA) pod is running.\\nYou must associate at least one tag with each Azure resource. This will enable Hyperscale Observability\\nto discover and monitor the Azure resource. \\nIf the \"\\nops-monitoring-ctl get collector-status\\n\" command returns \"\\nNA\\n\" as the status, it indicates that the collector\\nhasn\\'t run yet.\\nContainerized Operations Bridge 2022.11\\nPage \\n135\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.ops-monitoring-ctl config set cs.user\\nWhen prompted enter \\nadmin\\n. That\\'s, the containerized Operations Bridge admin user.\\n3\\n. \\nRun the following command to set the password:\\nops-monitoring-ctl config set cs.password\\nWhen prompted enter the containerized Operations Bridge admin password. This is the password provided for the\\ninput \\nidm_opsbridge_admin_password\\n when generating secrets.\\nConfigure your first Azure collector\\n1\\n. \\nCreate a credential configuration file and name it \\nmy-first-azure-cred.yaml\\n. For detailed steps, see \\nAzure credential\\nconfiguration\\n. \\n2\\n. \\nCreate a target configuration file and name it \\nmy-first-azure-target.yaml\\n. For detailed steps, see \\nAzure target\\nconfiguration\\n. \\n3\\n. \\nCreate a collector configuration file and name it \\nmy-first-azure-collector.yaml\\n. For detailed steps, see \\nAzure collector\\nconfiguration\\n. \\n4\\n. \\nRun the following commands to start the collector:\\nops-monitoring-ctl create -f my-first-azure-cred.yaml\\nops-monitoring-ctl create -f my-first-azure-target.yaml\\nops-monitoring-ctl create -f my-first-azure-collector.yaml\\n5\\n. \\nRun the following command to check the monitoring status:\\nops-monitoring-ctl get collector-status\\nIf the \"\\nops-monitoring-ctl get collector-status\\n\" command returns \"\\nNA\\n\" as the status, it indicates that the collector hasn\\'t run\\nyet.\\nView Azure metrics and events \\nAfter you configure the Azure collectors you can view Azure metrics and events: \\nTo view the monitored data in OBM using Performance Dashboards, see \\nView Azure metrics in OBM Performance\\nDashboard\\n.\\nTo view performance metrics using Performance Troubleshooting dashboards, see \\nPerformance Troubleshooting for Azure\\nmetrics\\n.\\nTo view the generated events in the Event Browser in OBM, see \\nView Azure events\\n.\\nRelated topics\\nManage your configurations using any of the following options:\\nUsing CLI parameters\\nUsing sample configuration files and CLI\\nKnow more about the discovered Azure topology, see \\nAzure Topology Views\\n.\\nFor details about the metrics collected, see the \\nHyperscale Observability data model for Azure\\n.\\nIf you had already installed the suite, but didn\\'t choose Hyperscale Observability and want to install it now, see \\nAdd/Remove capabilities on premises\\nAdd/Remove capabilities on cloud platform\\nIf you upgraded from an earlier version to the current version and want to install Hyperscale Observability capability, see \\nAdd/Remove capabilities on premises\\nAdd/Remove capabilities on cloud platform\\nImportant\\n: \\nAzure collector will start sending events to OBM only if the OBM pod is running. Make sure that the \\nomi-\\n0\\n (and \\nomi-1\\n in HA) pod is running.\\nYou must associate at least one tag with each Azure resource. This will enable Hyperscale Observability\\nto discover and monitor the Azure resource. \\nIf the \"\\nops-monitoring-ctl get collector-status\\n\" command returns \"\\nNA\\n\" as the status, it indicates that the collector\\nhasn\\'t run yet.\\nContainerized Operations Bridge 2022.11\\nPage \\n135\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '72ea8abb240d0650402dcd0f318e01dc'}>,\n",
              "  <Document: {'content': 'Hyperscale Observability\\nThis section includes the following topics:\\nSupported AWS services\\nSupported Azure services\\n \\nSupported Kubernetes objects\\nContainerized Operations Bridge 2022.11\\nPage \\n140\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Hyperscale Observability\\nThis section includes the following topics:\\nSupported AWS services\\nSupported Azure services\\n \\nSupported Kubernetes objects\\nContainerized Operations Bridge 2022.11\\nPage \\n140\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b1a33ee7251890107152b870c23562f9'}>,\n",
              "  <Document: {'content': \"Replace \\n<externalAccessHost>\\n and \\n<externalAccessPort> \\nwith the values you provided in the \\nvalues.yaml\\n.\\n2\\n. \\nRun the following command to set the username:\\nops-monitoring-ctl config set cs.user\\nWhen prompted enter \\nadmin\\n. That's, the containerized Operations Bridge admin user.\\n3\\n. \\nRun the following command to set the password:\\nops-monitoring-ctl config set cs.password\\nWhen prompted enter the containerized Operations Bridge admin password. This is the password provided for the\\ninput \\nidm_opsbridge_admin_password\\n when generating secrets.\\nConfigure your first Kubernetes collector\\n1\\n. \\nCopy the following to create a credential configuration file and name it \\nmy-first-k8s-cred.yaml\\n:\\napiVersion: core/v1\\ntype: credential\\nmetadata:\\n  name: example-k8s-credential\\nspec:\\n  subType: k8s\\n  context:\\n    cacertificate: <cacertificate>\\n    bearer: <bearer_token>\\nFollow the steps described in \\nKubernetes credential configuration\\n to update the credential configuration file. \\n2\\n. \\nCopy the following to create a target configuration file and name it \\nmy-first-k8s-target.yaml\\n:\\napiVersion: core/v1\\ntype: target\\nmetadata:\\n  name: <unique_target_name>\\n  tenant: public\\n  namespace: default\\n  resourceVersion: <resourceversion>\\nspec:\\n  subType: k8s\\n  endpoint: <k8s_cluster>\\n  credential: <credential_name>\\n  proxy:\\n    url: <proxy_url>\\n    credential: <proxy_credential>\\nFollow the steps described in \\nKubernetes target configuration\\n to update the target configuration file.\\n3\\n. \\nCopy the following to create a collector configuration file and name it \\nmy-first-k8s-collector.yaml\\n:\\napiVersion: core/v1\\ntype: collector\\nmetadata:\\n  tenant: public\\n  namespace: default\\n  name: <unique_collector_name>\\nspec:\\n  subType: k8s\\n  enabled: <true or false>\\n  targets:\\n  - <k8s-target_name>\\n  thresholds: []\\n  collectionModes:\\n  - collectionType: pull\\n    frequency: <frequency_value>\\n    backgroundJob: false\\n    dataType: metric\\n  - collectionType: pull\\n    frequency: 60\\n    backgroundJob: false\\n    dataType: discovery\\n  context:\\n    filterConfig:\\n      matchRegex: false\\n      targetTags:\\n        node:\\n          names:\\n          - <name>\\n          tags:\\n          - worker:label\\n    appendConfigTags: true\\nFollow the steps described in \\nKubernetes collector configuration\\n to update the collector configuration file. \\n4\\n. \\nRun the following commands to start the collector:\\nContainerized Operations Bridge 2022.11\\nPage \\n137\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Replace \\n<externalAccessHost>\\n and \\n<externalAccessPort> \\nwith the values you provided in the \\nvalues.yaml\\n.\\n2\\n. \\nRun the following command to set the username:\\nops-monitoring-ctl config set cs.user\\nWhen prompted enter \\nadmin\\n. That's, the containerized Operations Bridge admin user.\\n3\\n. \\nRun the following command to set the password:\\nops-monitoring-ctl config set cs.password\\nWhen prompted enter the containerized Operations Bridge admin password. This is the password provided for the\\ninput \\nidm_opsbridge_admin_password\\n when generating secrets.\\nConfigure your first Kubernetes collector\\n1\\n. \\nCopy the following to create a credential configuration file and name it \\nmy-first-k8s-cred.yaml\\n:\\napiVersion: core/v1\\ntype: credential\\nmetadata:\\n  name: example-k8s-credential\\nspec:\\n  subType: k8s\\n  context:\\n    cacertificate: <cacertificate>\\n    bearer: <bearer_token>\\nFollow the steps described in \\nKubernetes credential configuration\\n to update the credential configuration file. \\n2\\n. \\nCopy the following to create a target configuration file and name it \\nmy-first-k8s-target.yaml\\n:\\napiVersion: core/v1\\ntype: target\\nmetadata:\\n  name: <unique_target_name>\\n  tenant: public\\n  namespace: default\\n  resourceVersion: <resourceversion>\\nspec:\\n  subType: k8s\\n  endpoint: <k8s_cluster>\\n  credential: <credential_name>\\n  proxy:\\n    url: <proxy_url>\\n    credential: <proxy_credential>\\nFollow the steps described in \\nKubernetes target configuration\\n to update the target configuration file.\\n3\\n. \\nCopy the following to create a collector configuration file and name it \\nmy-first-k8s-collector.yaml\\n:\\napiVersion: core/v1\\ntype: collector\\nmetadata:\\n  tenant: public\\n  namespace: default\\n  name: <unique_collector_name>\\nspec:\\n  subType: k8s\\n  enabled: <true or false>\\n  targets:\\n  - <k8s-target_name>\\n  thresholds: []\\n  collectionModes:\\n  - collectionType: pull\\n    frequency: <frequency_value>\\n    backgroundJob: false\\n    dataType: metric\\n  - collectionType: pull\\n    frequency: 60\\n    backgroundJob: false\\n    dataType: discovery\\n  context:\\n    filterConfig:\\n      matchRegex: false\\n      targetTags:\\n        node:\\n          names:\\n          - <name>\\n          tags:\\n          - worker:label\\n    appendConfigTags: true\\nFollow the steps described in \\nKubernetes collector configuration\\n to update the collector configuration file. \\n4\\n. \\nRun the following commands to start the collector:\\nContainerized Operations Bridge 2022.11\\nPage \\n137\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f1a9b7a2adc4d7c78336d20e31016067'}>,\n",
              "  <Document: {'content': 'Reference\\nContainerized Operations Bridge 2022.11\\nPage \\n139\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Reference\\nContainerized Operations Bridge 2022.11\\nPage \\n139\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '189cc3e3ef151444328b9edf44d91421'}>,\n",
              "  <Document: {'content': 'Supported AWS services\\nHyperscale Observability supports the discovery and monitoring of the following AWS services:\\nApplication Load Balancer (ALB)\\nAurora\\nAthena\\nAuto Scaling Group (ASG)\\nBilling\\nCertificate Manager\\nCloudFront\\nDirect Connect\\nDynamoDB service\\nElastic Beanstalk\\nElastic Block Store (EBS)\\nElastiCache\\nElastic Compute Cloud (EC2)\\nElastic Container Registry (ECR)\\nElastic Container Service (ECS)\\nElastic File System (EFS)\\nElastic Kubernetes Service (EKS), but no private EKS cluster.\\nElastic Load Balancing (ELB)\\nGlue\\nKey Management Service (KMS)\\nLambda service\\nQuantum Ledger Database (QLDB)\\nRedshift\\nRelational Database Service (RDS)\\nRoute 53\\nSimple Notification Service (SNS)\\nSimple Queue Service (SQS)\\nSimple Storage Service (S3)\\nVirtual Private Cloud (VPC)\\nVPC NAT Gateway\\nVPC Transit Gateway\\nVPN Tunnel\\nContainerized Operations Bridge 2022.11\\nPage \\n141\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Supported AWS services\\nHyperscale Observability supports the discovery and monitoring of the following AWS services:\\nApplication Load Balancer (ALB)\\nAurora\\nAthena\\nAuto Scaling Group (ASG)\\nBilling\\nCertificate Manager\\nCloudFront\\nDirect Connect\\nDynamoDB service\\nElastic Beanstalk\\nElastic Block Store (EBS)\\nElastiCache\\nElastic Compute Cloud (EC2)\\nElastic Container Registry (ECR)\\nElastic Container Service (ECS)\\nElastic File System (EFS)\\nElastic Kubernetes Service (EKS), but no private EKS cluster.\\nElastic Load Balancing (ELB)\\nGlue\\nKey Management Service (KMS)\\nLambda service\\nQuantum Ledger Database (QLDB)\\nRedshift\\nRelational Database Service (RDS)\\nRoute 53\\nSimple Notification Service (SNS)\\nSimple Queue Service (SQS)\\nSimple Storage Service (S3)\\nVirtual Private Cloud (VPC)\\nVPC NAT Gateway\\nVPC Transit Gateway\\nVPN Tunnel\\nContainerized Operations Bridge 2022.11\\nPage \\n141\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dc855e9b788b63317a20045c8ba2f1a9'}>,\n",
              "  <Document: {'content': 'Supported Azure services\\nHyperscale Observability supports the discovery and monitoring of the following Azure services:\\nApp Service\\nApp Service Plan\\nApplication Gateway\\nAzure Cache for Redis\\nAzure Cosmos DB\\nAzure Front Door\\nAzure Kubernetes Service (AKS)\\nAzure Purview\\nAzure Synapse\\nContainer Registry\\nData Lake Analytics\\nEvent Hubs\\nEvent Hubs Clusters\\nExpressRoute Circuits\\nExpressRoute Direct\\nGuest OS\\nLoad Balancer\\nManaged Disks\\nSQL Database\\nStorage Account\\nTraffic Manager\\nVirtual Machine Scale Sets\\nVirtual Machines\\nVirtual Network Gateway\\nContainerized Operations Bridge 2022.11\\nPage \\n142\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Supported Azure services\\nHyperscale Observability supports the discovery and monitoring of the following Azure services:\\nApp Service\\nApp Service Plan\\nApplication Gateway\\nAzure Cache for Redis\\nAzure Cosmos DB\\nAzure Front Door\\nAzure Kubernetes Service (AKS)\\nAzure Purview\\nAzure Synapse\\nContainer Registry\\nData Lake Analytics\\nEvent Hubs\\nEvent Hubs Clusters\\nExpressRoute Circuits\\nExpressRoute Direct\\nGuest OS\\nLoad Balancer\\nManaged Disks\\nSQL Database\\nStorage Account\\nTraffic Manager\\nVirtual Machine Scale Sets\\nVirtual Machines\\nVirtual Network Gateway\\nContainerized Operations Bridge 2022.11\\nPage \\n142\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1dce17a73b8869d783d63e7b95b3df56'}>,\n",
              "  <Document: {'content': 'ops-monitoring-ctl create -f my-first-k8s-cred.yaml\\nops-monitoring-ctl create -f my-first-k8s-target.yaml\\nops-monitoring-ctl create -f my-first-k8s-collector.yaml\\n5\\n. \\nRun the following command to check the monitoring status:\\nops-monitoring-ctl get collector-status\\nIf the \"ops-monitoring-ctl get collector-status\" command returns \"NA\" as the status, it indicates that the collector has not\\nrun yet.\\nView Kubernetes metrics and events \\nAfter you configure the Kubernetes collectors you can view Kubernetes metrics and events: \\nTo view the monitored data in OBM using Performance Dashboard, see \\nView Kubernetes metrics in OBM Performance\\nDashboard\\n.\\nTo view performance metrics using Performance Troubleshooting dashboards, see \\nPerformance Troubleshooting for\\nKubernetes metrics\\n.\\nTo view the generated events in the Event Browser in OBM, see Kubernetes events and see \\nView Kubernetes events\\n.\\nRelated topics\\nManage your configurations using any of the following options:\\nUsing CLI with parameters\\nUsing CLI with YAML files\\nTo know more about the discovered Kubernetes topology, see \\nKubernetes Topology View\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n138\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.ops-monitoring-ctl create -f my-first-k8s-cred.yaml\\nops-monitoring-ctl create -f my-first-k8s-target.yaml\\nops-monitoring-ctl create -f my-first-k8s-collector.yaml\\n5\\n. \\nRun the following command to check the monitoring status:\\nops-monitoring-ctl get collector-status\\nIf the \"ops-monitoring-ctl get collector-status\" command returns \"NA\" as the status, it indicates that the collector has not\\nrun yet.\\nView Kubernetes metrics and events \\nAfter you configure the Kubernetes collectors you can view Kubernetes metrics and events: \\nTo view the monitored data in OBM using Performance Dashboard, see \\nView Kubernetes metrics in OBM Performance\\nDashboard\\n.\\nTo view performance metrics using Performance Troubleshooting dashboards, see \\nPerformance Troubleshooting for\\nKubernetes metrics\\n.\\nTo view the generated events in the Event Browser in OBM, see Kubernetes events and see \\nView Kubernetes events\\n.\\nRelated topics\\nManage your configurations using any of the following options:\\nUsing CLI with parameters\\nUsing CLI with YAML files\\nTo know more about the discovered Kubernetes topology, see \\nKubernetes Topology View\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n138\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ef2fde88306f8b86fd71f5ab1d0d2572'}>,\n",
              "  <Document: {'content': 'Supported Kubernetes objects\\nHyperscale Observability supports the discovery and monitoring of the following Kubernetes objects:\\nClusters\\nDaemonSets\\nDeployments\\nEndpoints\\nIngresses\\nJobs\\nNamespaces\\nNodes\\nPersistent Volume Claims\\nPersistent Volumes\\nPods\\nReplicaSets\\nServices\\nStatefulSets\\nContainerized Operations Bridge 2022.11\\nPage \\n143\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Supported Kubernetes objects\\nHyperscale Observability supports the discovery and monitoring of the following Kubernetes objects:\\nClusters\\nDaemonSets\\nDeployments\\nEndpoints\\nIngresses\\nJobs\\nNamespaces\\nNodes\\nPersistent Volume Claims\\nPersistent Volumes\\nPods\\nReplicaSets\\nServices\\nStatefulSets\\nContainerized Operations Bridge 2022.11\\nPage \\n143\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6cf493f8ced6e606f2f8729e969accd3'}>,\n",
              "  <Document: {'content': \"Glossary\\nAggregate dataset\\nThe aggregate dataset has data summarized from another dataset.\\nAnnotation\\nKey/Value type metadata that can be specified on Kubernetes object to help identification, management or add-on\\nfunctionality. An example of such annotations are the Kubernetes-vault definitions used to configure authentication tokens and\\ncertificates.\\nAPI server\\nA server application that provides an API to provide information and/or perform a set of tasks. An example is the Kubernetes\\nAPI server that exposes an API to view and change the cluster state.\\nBacklog\\nIn the OPTIC DL Message Bus, the backlog is unacknowledged messages of a subscription.\\nCertificate\\nData file that enables verification of digital identity and ownership and subsequent setup of secure communication.\\nCluster\\nA set of the computer system that's connected electronically so that from the outside they can be viewed as a single system. A\\ncluster is a set of physical or virtual machines and other infrastructure resources used by Kubernetes to run your applications.\\nConfig map\\nA Kubernetes object that holds configuration data.\\nConfiguration files\\nA non-executable file that holds configuration data.\\nConsumer\\nIn the OPTIC DL Message Bus, the consumer reads the messages from one or more topics. The producer writes these\\nmessages to the topics.\\nContainer\\nA way to package applications and just their required libraries and configuration which abstracts them from the underlying\\noperating system and allows them to run anywhere.\\nContent\\nA zip file containing OPTIC Data Lake and BVD artifacts in specified zip format.\\nContainerized Operations Bridge 2022.11\\nPage \\n144\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Glossary\\nAggregate dataset\\nThe aggregate dataset has data summarized from another dataset.\\nAnnotation\\nKey/Value type metadata that can be specified on Kubernetes object to help identification, management or add-on\\nfunctionality. An example of such annotations are the Kubernetes-vault definitions used to configure authentication tokens and\\ncertificates.\\nAPI server\\nA server application that provides an API to provide information and/or perform a set of tasks. An example is the Kubernetes\\nAPI server that exposes an API to view and change the cluster state.\\nBacklog\\nIn the OPTIC DL Message Bus, the backlog is unacknowledged messages of a subscription.\\nCertificate\\nData file that enables verification of digital identity and ownership and subsequent setup of secure communication.\\nCluster\\nA set of the computer system that's connected electronically so that from the outside they can be viewed as a single system. A\\ncluster is a set of physical or virtual machines and other infrastructure resources used by Kubernetes to run your applications.\\nConfig map\\nA Kubernetes object that holds configuration data.\\nConfiguration files\\nA non-executable file that holds configuration data.\\nConsumer\\nIn the OPTIC DL Message Bus, the consumer reads the messages from one or more topics. The producer writes these\\nmessages to the topics.\\nContainer\\nA way to package applications and just their required libraries and configuration which abstracts them from the underlying\\noperating system and allows them to run anywhere.\\nContent\\nA zip file containing OPTIC Data Lake and BVD artifacts in specified zip format.\\nContainerized Operations Bridge 2022.11\\nPage \\n144\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e9c55289aff7e7aeed62c7063fff304c'}>,\n",
              "  <Document: {'content': \"Credential\\nForm of digital authentication, typically rendered as a user ID and an accompanying password.\\nDataset\\nA dataset corresponds to a table in the database. A dataset may contain a collection of metrics, logs, or events. The dataset\\ncan be internal or external. You can query external datasets for creating reports and dashboards. You can't query the internal\\ndatasets.\\nDocker\\nDocker (specifically, Docker Engine) is a software technology providing operating-system-level virtualization also known as\\ncontainers.\\nDaemon sets\\nA Kubernetes controller that schedules/runs/maintains a single instance of a specified container image on every cluster node.\\nDeployment\\nAn installation of an ITOM suite or service.\\nExternal/private Docker registry\\nA repository holding container images.\\nEntity\\nAn entity represents a specific object in a data model. In data warehousing, dimension is an entity. In the configuration\\nmanagement domain, configuration item (CI) is an entity. In the monitoring domain, an entity can be a managed object such\\nas node, network device, application, running software, or business service, and the objects under it.\\nEntity dataset\\nAn entity dataset refers to a collection of entities. The relation between entities is also modeled as an entity.\\nEtcd\\nA consistent and highly-available key-value configuration database that's used as a Kubernetes backing store for all cluster\\ndata.\\nEvaluation deployment\\nA short-term deployment of software meant to prove value or for test or learning purposes. There are typically no\\nrequirements for high availability or high scale.\\nExternal access host\\nA Fully Qualified Domain Name (FQDN) with which you can access the OMT cluster service from locations outside the cluster.\\nThis FQDN must be resolved to a valid network address.\\nContainerized Operations Bridge 2022.11\\nPage \\n145\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Credential\\nForm of digital authentication, typically rendered as a user ID and an accompanying password.\\nDataset\\nA dataset corresponds to a table in the database. A dataset may contain a collection of metrics, logs, or events. The dataset\\ncan be internal or external. You can query external datasets for creating reports and dashboards. You can't query the internal\\ndatasets.\\nDocker\\nDocker (specifically, Docker Engine) is a software technology providing operating-system-level virtualization also known as\\ncontainers.\\nDaemon sets\\nA Kubernetes controller that schedules/runs/maintains a single instance of a specified container image on every cluster node.\\nDeployment\\nAn installation of an ITOM suite or service.\\nExternal/private Docker registry\\nA repository holding container images.\\nEntity\\nAn entity represents a specific object in a data model. In data warehousing, dimension is an entity. In the configuration\\nmanagement domain, configuration item (CI) is an entity. In the monitoring domain, an entity can be a managed object such\\nas node, network device, application, running software, or business service, and the objects under it.\\nEntity dataset\\nAn entity dataset refers to a collection of entities. The relation between entities is also modeled as an entity.\\nEtcd\\nA consistent and highly-available key-value configuration database that's used as a Kubernetes backing store for all cluster\\ndata.\\nEvaluation deployment\\nA short-term deployment of software meant to prove value or for test or learning purposes. There are typically no\\nrequirements for high availability or high scale.\\nExternal access host\\nA Fully Qualified Domain Name (FQDN) with which you can access the OMT cluster service from locations outside the cluster.\\nThis FQDN must be resolved to a valid network address.\\nContainerized Operations Bridge 2022.11\\nPage \\n145\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3e30833992128d116e1b5b686f884d38'}>,\n",
              "  <Document: {'content': \"External access port\\nThe external access port is used along with the external access host to form the URL. This URL is used to access the UI services\\nfor capabilities.\\nExternal image registry\\nCluster-external container image registry. A place where container images are stored so that they can be retrieved by\\ncontainer engines running on cluster nodes.\\nExternal load balancer\\nAn external load balancer balances the outside traffic to all control plane nodes or worker nodes for the OMT services and suite\\nservices.\\nExternal load balancer host\\nA host where the load balancer is installed.\\nExternal Vertica\\nExternal Vertica is Vertica installed and configured in a remote system other than the suite container for production purposes.\\nEvent\\nAn event is a representation of something that happened in the environment as used within monitoring tools. A change in the\\nenvironment or by any other extrinsic event (for example an SNMP trap) triggers an event. An event is typically associated\\nwith severity and some description text. Also, it has a life cycle state such as ‘new, ‘in progress’, or ‘closed’ used in even\\nmanagement processes to process such an event.\\nFirst deployment\\nThe very first deployment that contains a set of cluster-wide services that aren't present in subsequent additional\\ndeployments.\\nFirst control plane node\\nThe control plane node on which you run the install command to install OMT.\\nHA\\nHigh-availability (HA) is a system characteristic to provide higher levels of uptime.\\nHelm-based deployment\\nAn installation of an ITOM suite or service using Helm charts.\\nIdM\\nMicro Focus technology providing authentication and authorization services.\\nContainerized Operations Bridge 2022.11\\nPage \\n146\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.External access port\\nThe external access port is used along with the external access host to form the URL. This URL is used to access the UI services\\nfor capabilities.\\nExternal image registry\\nCluster-external container image registry. A place where container images are stored so that they can be retrieved by\\ncontainer engines running on cluster nodes.\\nExternal load balancer\\nAn external load balancer balances the outside traffic to all control plane nodes or worker nodes for the OMT services and suite\\nservices.\\nExternal load balancer host\\nA host where the load balancer is installed.\\nExternal Vertica\\nExternal Vertica is Vertica installed and configured in a remote system other than the suite container for production purposes.\\nEvent\\nAn event is a representation of something that happened in the environment as used within monitoring tools. A change in the\\nenvironment or by any other extrinsic event (for example an SNMP trap) triggers an event. An event is typically associated\\nwith severity and some description text. Also, it has a life cycle state such as ‘new, ‘in progress’, or ‘closed’ used in even\\nmanagement processes to process such an event.\\nFirst deployment\\nThe very first deployment that contains a set of cluster-wide services that aren't present in subsequent additional\\ndeployments.\\nFirst control plane node\\nThe control plane node on which you run the install command to install OMT.\\nHA\\nHigh-availability (HA) is a system characteristic to provide higher levels of uptime.\\nHelm-based deployment\\nAn installation of an ITOM suite or service using Helm charts.\\nIdM\\nMicro Focus technology providing authentication and authorization services.\\nContainerized Operations Bridge 2022.11\\nPage \\n146\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '45896f7dcf44626a2451e2789aeabc0f'}>,\n",
              "  <Document: {'content': \"Image\\nA stored instance of a container that holds a set of software needed to run an application.\\nITOM\\nIT Operations Management\\nIngress\\nA Kubernetes object that's processed by an Ingress controller which in turn provides inbound access for clients to the\\napplication services running inside the cluster.\\nInstaller node\\nNode on the OpenShift Cluster from which all the suite install related tasks are performed.\\nInternal load balancer\\nThe internal load balancer is used for internal traffic among all control plane and worker nodes. It balances the traffic to all\\ncontrol plane nodes for the internal services which would be accessed by all nodes of the cluster. Usually, these services aren't\\naccessible for users outside of the cluster. So the internal load balancer should also not be accessible for outside users and we\\njust use an internal load balancer for it.\\nJob\\nA Kubernetes object describing a one-time execution of a script or program.\\nJournal\\nThe OPTIC DL Message Bus - BookKeeper holds the journal files. These files consist of the BookKeeper transaction logs.\\nKubectl\\nA command-line tool for communicating with a Kubernetes API server.\\nKubernetes\\nAn open-source system for automating the deployment, scaling, and management of containerized applications.\\nKubernetes API\\nThe application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster.\\nkube-apiserver\\nA component on the control plane node that exposes the Kubernetes API. It's the front-end for the Kubernetes control plane.\\nKubelet\\nThe node agent that's responsible for running the containers scheduled to that node.\\nContainerized Operations Bridge 2022.11\\nPage \\n147\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Image\\nA stored instance of a container that holds a set of software needed to run an application.\\nITOM\\nIT Operations Management\\nIngress\\nA Kubernetes object that's processed by an Ingress controller which in turn provides inbound access for clients to the\\napplication services running inside the cluster.\\nInstaller node\\nNode on the OpenShift Cluster from which all the suite install related tasks are performed.\\nInternal load balancer\\nThe internal load balancer is used for internal traffic among all control plane and worker nodes. It balances the traffic to all\\ncontrol plane nodes for the internal services which would be accessed by all nodes of the cluster. Usually, these services aren't\\naccessible for users outside of the cluster. So the internal load balancer should also not be accessible for outside users and we\\njust use an internal load balancer for it.\\nJob\\nA Kubernetes object describing a one-time execution of a script or program.\\nJournal\\nThe OPTIC DL Message Bus - BookKeeper holds the journal files. These files consist of the BookKeeper transaction logs.\\nKubectl\\nA command-line tool for communicating with a Kubernetes API server.\\nKubernetes\\nAn open-source system for automating the deployment, scaling, and management of containerized applications.\\nKubernetes API\\nThe application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster.\\nkube-apiserver\\nA component on the control plane node that exposes the Kubernetes API. It's the front-end for the Kubernetes control plane.\\nKubelet\\nThe node agent that's responsible for running the containers scheduled to that node.\\nContainerized Operations Bridge 2022.11\\nPage \\n147\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '38fc6bac0fe706b511643f8aa08bf75e'}>,\n",
              "  <Document: {'content': 'An API object that represents a piece of storage in the cluster. Available as a general, pluggable resource that persists beyond\\nthe lifecycle of any individual Pod.\\nPod\\nOne or more containers. This is the unit of scheduling in Kubernetes. A pod represents a set of the running container on your\\ncluster.\\nPod\\nOne or more containers deployed together on a host.\\nPostload processing\\nPostload processing is the type of processing where the data processing happens after loading into the Vertica database.\\nPreload processing\\nPreload processing is the type of processing where the data processing happens before loading into the Vertica database.\\nProduction\\nDeployment of software into a live business operations environment where end-user customers and live business functions\\ngenerate use of IT resources. The requirements around production environments usually entail a higher degree of required\\nuptime and more critical levels of support.\\nProducer\\nIn the OPTIC DL Message Bus, the producer writes the messages to topics.\\nReplication Controller\\nA Kubernetes object that defines and ensures running a specified number of instances of a Pod.\\nReplica Set\\nNext-gen Replication Controller.\\nResource pool\\nVertica resource pools are the allocated subset of system resources to run a queue of queries. For more information about the\\nVertica resource pool, see Vertica documentation.\\nScheduler\\nA control plane node component that schedules Pods to nodes.\\nSchema\\nA schema in the database is a container for database objects like tables, views, etc. You can give privileges at a schema level\\nor at an individual object within the schema.\\nContainerized Operations Bridge 2022.11\\nPage \\n150\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.An API object that represents a piece of storage in the cluster. Available as a general, pluggable resource that persists beyond\\nthe lifecycle of any individual Pod.\\nPod\\nOne or more containers. This is the unit of scheduling in Kubernetes. A pod represents a set of the running container on your\\ncluster.\\nPod\\nOne or more containers deployed together on a host.\\nPostload processing\\nPostload processing is the type of processing where the data processing happens after loading into the Vertica database.\\nPreload processing\\nPreload processing is the type of processing where the data processing happens before loading into the Vertica database.\\nProduction\\nDeployment of software into a live business operations environment where end-user customers and live business functions\\ngenerate use of IT resources. The requirements around production environments usually entail a higher degree of required\\nuptime and more critical levels of support.\\nProducer\\nIn the OPTIC DL Message Bus, the producer writes the messages to topics.\\nReplication Controller\\nA Kubernetes object that defines and ensures running a specified number of instances of a Pod.\\nReplica Set\\nNext-gen Replication Controller.\\nResource pool\\nVertica resource pools are the allocated subset of system resources to run a queue of queries. For more information about the\\nVertica resource pool, see Vertica documentation.\\nScheduler\\nA control plane node component that schedules Pods to nodes.\\nSchema\\nA schema in the database is a container for database objects like tables, views, etc. You can give privileges at a schema level\\nor at an individual object within the schema.\\nContainerized Operations Bridge 2022.11\\nPage \\n150\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '30c3d7b1eebf8b913cf6b30d8dba59a0'}>,\n",
              "  <Document: {'content': 'Vertica container\\nThe suite bundles Vertica container or Internal Vertica. Use the Vertica container for evaluation purposes.\\nVertica Streaming Loader\\nThe Vertica Streaming Loader schedules the data flow from the OPTIC DL Message Bus topics to Vertica tables as micro\\nbatches. These micro batches are equally spread within a time interval for sending the data from the OPTIC DL Message Bus to\\nVertica. (Vertica Streaming Loader also called Scheduler in OPTIC DL Health Insights dashboards).\\nVolume\\nA directory containing data, which containers can access.\\nWorker node\\nSee node. Suites and applications run on the worker nodes.\\nWorkloads\\nWorkloads are objects you use to manage and run your containers on the cluster.\\nContainerized Operations Bridge 2022.11\\nPage \\n152\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Vertica container\\nThe suite bundles Vertica container or Internal Vertica. Use the Vertica container for evaluation purposes.\\nVertica Streaming Loader\\nThe Vertica Streaming Loader schedules the data flow from the OPTIC DL Message Bus topics to Vertica tables as micro\\nbatches. These micro batches are equally spread within a time interval for sending the data from the OPTIC DL Message Bus to\\nVertica. (Vertica Streaming Loader also called Scheduler in OPTIC DL Health Insights dashboards).\\nVolume\\nA directory containing data, which containers can access.\\nWorker node\\nSee node. Suites and applications run on the worker nodes.\\nWorkloads\\nWorkloads are objects you use to manage and run your containers on the cluster.\\nContainerized Operations Bridge 2022.11\\nPage \\n152\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '320ae8e1e9b3560440c9954bf6231810'}>,\n",
              "  <Document: {'content': 'StatefulSet\\nA Kubernetes object that defines and ensures running a specified number of instances of a Pod, with guarantees for order and\\nuniqueness.\\nSecret\\nA Kubernetes object allowing for secure storage of sensitive configuration information.\\nSelector\\nA way to filter resources.\\nService\\nA Kubernetes object that allows defining access to applications.\\nSilent deployment\\nA silent deployment refers to an installation that requires no user interaction.\\nSubscription\\nIn the OPTIC DL Message Bus, a subscription is a configuration rule according to which the consumers receive the messages.\\nsysctl\\nsysctl is a semi-standardized interface for reading or changing the attributes of the running Unix kernel.\\nTasks\\nThe OPTIC DL data processor executes tasks and streams them to Vertica.\\nTaskflow\\nThe taskflow defines one or more tasks and the sequence of execution for data processing.\\nTopic\\nThe topic represents a data stream within OPTIC DL Message Bus. The topic produces or receives messages. The topic holds\\nthe data before streaming to the database. In certain scenarios, the topic stores data temporarily before processing in the data\\nprocessing pipeline.\\nVault\\nHashicorp Vault is the secure storage and certificate generation technology used in ITOM Platform.\\nContainerized Operations Bridge 2022.11\\nPage \\n151\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.StatefulSet\\nA Kubernetes object that defines and ensures running a specified number of instances of a Pod, with guarantees for order and\\nuniqueness.\\nSecret\\nA Kubernetes object allowing for secure storage of sensitive configuration information.\\nSelector\\nA way to filter resources.\\nService\\nA Kubernetes object that allows defining access to applications.\\nSilent deployment\\nA silent deployment refers to an installation that requires no user interaction.\\nSubscription\\nIn the OPTIC DL Message Bus, a subscription is a configuration rule according to which the consumers receive the messages.\\nsysctl\\nsysctl is a semi-standardized interface for reading or changing the attributes of the running Unix kernel.\\nTasks\\nThe OPTIC DL data processor executes tasks and streams them to Vertica.\\nTaskflow\\nThe taskflow defines one or more tasks and the sequence of execution for data processing.\\nTopic\\nThe topic represents a data stream within OPTIC DL Message Bus. The topic produces or receives messages. The topic holds\\nthe data before streaming to the database. In certain scenarios, the topic stores data temporarily before processing in the data\\nprocessing pipeline.\\nVault\\nHashicorp Vault is the secure storage and certificate generation technology used in ITOM Platform.\\nContainerized Operations Bridge 2022.11\\nPage \\n151\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'affa225a4e92d65dc48fe140bdaa6a6d'}>,\n",
              "  <Document: {'content': 'Messages\\nMessages are the units or the method by which the Message Bus carries data. The OPTIC DL Message Bus Broker receives the\\nmessages.\\nMessage bus\\nThe Message Bus helps to transfer and to store the data temporarily. In OPTIC DL , Pulsar is the Message Bus.\\nMicro batch\\nData streams from the OPTIC DL Message Bus into Vertica as micro batches. A micro batch has an associated message bus\\ntopic and an associated Vertica table. The micro batches runs within the Vertica Streaming Loader.\\nMultiple deployments\\nMultiple individual installations of an ITOM suite and/or services or multiple suites and/or services on the same Kubernetes\\ncluster.\\nNamespace\\nThe Kubernetes way to partition a single cluster into multiple virtual clusters,\\nNode\\nA node is a physical or virtual machine running Kubernetes, onto which pods can be scheduled. A Kubernetes cluster consists\\nof two types of resources:\\nControl plane nodes. These nodes host the control plane, which coordinates the cluster.\\nWorker nodes. These nodes run the applications.\\nNon-production\\nSoftware deployment into development, test, staging, or evaluation environment which is not supporting essential business\\nfunctions nor end customers. The requirements for stability, availability, and support are less than for production systems.\\nNFS\\nNetwork File System, a distributed file system protocol.\\nOpsBridge Content Manager\\nOpsBridge Content Manager is a service that enables you to manage OPTIC Data Lake, content, and Agent Metric Collection. \\no\\nps-content-ctl\\n is the command-line interface to OpsBridge Content Manager. It enables you to deploy content required to ingest\\ndata into OPTIC Data Lake, manage Agent Metric Collection, and populate OpsBridge OPTIC Data Lake Reports using Business\\nValue Dashboard (BVD).\\nOpsBridge Content Manager was earlier called Content Administration Service (CAS).\\nPersistent volumes\\nA Kubernetes object providing access to some form of persistent storage such as network file systems or local disks.\\nPersistent volume claim\\nContainerized Operations Bridge 2022.11\\nPage \\n149\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Messages\\nMessages are the units or the method by which the Message Bus carries data. The OPTIC DL Message Bus Broker receives the\\nmessages.\\nMessage bus\\nThe Message Bus helps to transfer and to store the data temporarily. In OPTIC DL , Pulsar is the Message Bus.\\nMicro batch\\nData streams from the OPTIC DL Message Bus into Vertica as micro batches. A micro batch has an associated message bus\\ntopic and an associated Vertica table. The micro batches runs within the Vertica Streaming Loader.\\nMultiple deployments\\nMultiple individual installations of an ITOM suite and/or services or multiple suites and/or services on the same Kubernetes\\ncluster.\\nNamespace\\nThe Kubernetes way to partition a single cluster into multiple virtual clusters,\\nNode\\nA node is a physical or virtual machine running Kubernetes, onto which pods can be scheduled. A Kubernetes cluster consists\\nof two types of resources:\\nControl plane nodes. These nodes host the control plane, which coordinates the cluster.\\nWorker nodes. These nodes run the applications.\\nNon-production\\nSoftware deployment into development, test, staging, or evaluation environment which is not supporting essential business\\nfunctions nor end customers. The requirements for stability, availability, and support are less than for production systems.\\nNFS\\nNetwork File System, a distributed file system protocol.\\nOpsBridge Content Manager\\nOpsBridge Content Manager is a service that enables you to manage OPTIC Data Lake, content, and Agent Metric Collection. \\no\\nps-content-ctl\\n is the command-line interface to OpsBridge Content Manager. It enables you to deploy content required to ingest\\ndata into OPTIC Data Lake, manage Agent Metric Collection, and populate OpsBridge OPTIC Data Lake Reports using Business\\nValue Dashboard (BVD).\\nOpsBridge Content Manager was earlier called Content Administration Service (CAS).\\nPersistent volumes\\nA Kubernetes object providing access to some form of persistent storage such as network file systems or local disks.\\nPersistent volume claim\\nContainerized Operations Bridge 2022.11\\nPage \\n149\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2c366de9137364a2974c6570123a198b'}>,\n",
              "  <Document: {'content': 'Kube Proxy\\nA network proxy that runs on every cluster node.\\nLabel\\nA key/value pair that can be set on most any Kubernetes object.\\nLatency\\nLatency is the time delay for the received message to be written to the Vertica database.\\nLoad balancer\\nDistributes network load across multiple endpoints. Load balancers provide load balancing when the traffic hits one of the\\ncluster nodes and is routed by kube-proxy and/or the in-cluster NGINX instances to the backend pods. The keepalived provides\\nfailover if a control plane node runs into error. A load balancer acts as a reverse proxy and distributes network or application\\ntraffic across the cluster nodes. A load balancer is used to increase capacity (concurrent users) and reliability of applications.\\nLocal registry\\nCluster-internal container image registry. A place where container images are stored so they can be retrieved by container\\nengines running on cluster nodes.\\nLog\\nA log is a message that describes the occurrence of an external event. It represents a single log line of log files written by\\nsystems or applications.\\nLogging\\nLogs are the list of events that are logged by cluster or application.\\nManaged Kubernetes deployment\\nA Managed-Kubernetes deployment can fully manage the services and then realize its capacities on the cloud infrastructure\\nwith effective scaling and minimal efforts in preparing the required hardware and software.\\nMaster node\\nLegacy term.  Master nodes are now referred to as control plane nodes.\\nManagement portal\\nAn application that allows users to manage ITOM Platform cluster configuration and applications.\\nMetric\\nMetrics represent measurements at a point in time. Metrics can be qualitative or quantitative measured over time and stored\\nas time series data.\\nContainerized Operations Bridge 2022.11\\nPage \\n148\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Kube Proxy\\nA network proxy that runs on every cluster node.\\nLabel\\nA key/value pair that can be set on most any Kubernetes object.\\nLatency\\nLatency is the time delay for the received message to be written to the Vertica database.\\nLoad balancer\\nDistributes network load across multiple endpoints. Load balancers provide load balancing when the traffic hits one of the\\ncluster nodes and is routed by kube-proxy and/or the in-cluster NGINX instances to the backend pods. The keepalived provides\\nfailover if a control plane node runs into error. A load balancer acts as a reverse proxy and distributes network or application\\ntraffic across the cluster nodes. A load balancer is used to increase capacity (concurrent users) and reliability of applications.\\nLocal registry\\nCluster-internal container image registry. A place where container images are stored so they can be retrieved by container\\nengines running on cluster nodes.\\nLog\\nA log is a message that describes the occurrence of an external event. It represents a single log line of log files written by\\nsystems or applications.\\nLogging\\nLogs are the list of events that are logged by cluster or application.\\nManaged Kubernetes deployment\\nA Managed-Kubernetes deployment can fully manage the services and then realize its capacities on the cloud infrastructure\\nwith effective scaling and minimal efforts in preparing the required hardware and software.\\nMaster node\\nLegacy term.  Master nodes are now referred to as control plane nodes.\\nManagement portal\\nAn application that allows users to manage ITOM Platform cluster configuration and applications.\\nMetric\\nMetrics represent measurements at a point in time. Metrics can be qualitative or quantitative measured over time and stored\\nas time series data.\\nContainerized Operations Bridge 2022.11\\nPage \\n148\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2921cbf7a180f8e485360f370cd2845e'}>,\n",
              "  <Document: {'content': 'Install\\nThis topic provides a high level overview of tasks that you must complete to install Operations Bridge. \\nStep \\nTask\\nReference\\n1\\nUnderstand the concepts and approach to configuring the deployment prerequisites.\\nDeployment\\nconcepts\\n2\\nDecide the capabilities that you want to deploy based on your business requirements. Also decide\\non the applications that you need to integrate with the containerized deployment as required.\\nPlan the\\ndeployment\\n3\\nUse the sizing calculator to estimate the resources needed. Review the system requirements for\\ninformation on supported software and hardware. Here you obtain the required resources to\\ndeploy the applications.\\nSizing and\\nsystem\\nrequirements\\n4\\nDownload the installation packages.\\nDownload the\\ninstaller\\npackages\\n5\\nPrepare the base infrastructure needed, and also install OPTIC Management Toolkit (OMT).\\nPrepare\\ninfrastructure\\n6\\nComplete the prerequisite tasks before deploying the application. The prerequisites are specific to\\nyour deployment strategy.\\nSet up\\nprerequisites\\n7\\nConfigure and deploy the application.\\nDeploy\\n8\\nVerify that the deployment is successful.\\nVerify the\\ninstall\\n9\\nComplete the post installation tasks to start using your application.\\nPost install\\ntasks\\nContainerized Operations Bridge 2022.11\\nPage \\n153\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Install\\nThis topic provides a high level overview of tasks that you must complete to install Operations Bridge. \\nStep \\nTask\\nReference\\n1\\nUnderstand the concepts and approach to configuring the deployment prerequisites.\\nDeployment\\nconcepts\\n2\\nDecide the capabilities that you want to deploy based on your business requirements. Also decide\\non the applications that you need to integrate with the containerized deployment as required.\\nPlan the\\ndeployment\\n3\\nUse the sizing calculator to estimate the resources needed. Review the system requirements for\\ninformation on supported software and hardware. Here you obtain the required resources to\\ndeploy the applications.\\nSizing and\\nsystem\\nrequirements\\n4\\nDownload the installation packages.\\nDownload the\\ninstaller\\npackages\\n5\\nPrepare the base infrastructure needed, and also install OPTIC Management Toolkit (OMT).\\nPrepare\\ninfrastructure\\n6\\nComplete the prerequisite tasks before deploying the application. The prerequisites are specific to\\nyour deployment strategy.\\nSet up\\nprerequisites\\n7\\nConfigure and deploy the application.\\nDeploy\\n8\\nVerify that the deployment is successful.\\nVerify the\\ninstall\\n9\\nComplete the post installation tasks to start using your application.\\nPost install\\ntasks\\nContainerized Operations Bridge 2022.11\\nPage \\n153\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4169708309ced0aaa0704e2ee49faf9b'}>,\n",
              "  <Document: {'content': 'Deployment concepts\\nThis topic describes the concepts related to deploying containerized Operations Bridge.\\nThe application deployment comprises the following layers:\\nInfrastructure\\nKubernetes Cluster\\nApplications\\nThe following image illustrates different entities that you need to install or configure for the deployment:\\nInfrastructure\\nInfrastructure is the hardware and the software resources needed to ensure computing, storage, and network capabilities\\nrequired to deploy an application. For example, servers, ports, and operating system.\\nPlan the infrastructure that you need for the deployment. Sizing calculator helps you to estimate the resource requirements.\\nThe sizing calculator provides you with a cumulative resource requirement for all the applications that you plan to deploy. \\nKubernetes cluster\\nA Kubernetes cluster is a set of nodes that run containerized applications. To deploy ITOM containerized applications, you can\\nchoose one of the following types of Kubernetes:\\nExternal Kubernetes:\\n A customer-owned Kubernetes cluster based on a third-party Kubernetes distribution. For\\nexample, Amazon EKS or Red Hat OpenShift, either on cloud or on-premises. \\nEmbedded Kubernetes:\\n A Kubernetes distribution provided by as part of Optic Management Toolkit (OMT) that you can\\ninstall on-premises (physical machines or virtual machines) in your datacenter as an alternative to external Kubernetes\\nclusters.\\nPlan the Kubernetes distribution that you want to use. If you are unsure of the choice of Kubernetes, and you want to use the\\ncontainerized capabilities of Operations Bridge, you can use the Embedded Kubernetes.\\nApplications\\nA set of containerized Applications that deliver IT operations management capabilities.  These Applications are shipped as\\nKubernetes-native packages (helm charts) and are deployed into a Kubernetes cluster to host and run the capabilities.  For\\nexample, Operations Bridge (Ops Bridge) and Network Operations Management (NOM).\\nThe containerized application is the ITOM application that you plan to deploy on the Kubernetes cluster. \\nOPTIC Management Toolkit (OMT) \\nOMT is an application provides the necessary tooling for installing and running containerized applications on Kubernetes. OMT\\nalso provides the Kubernetes control plane capability that you can install if required. OMT is the first application that you must\\ndeploy before deploying other ITOM applications on the cluster. When you install OMT you can choose to install only the\\nrequired tools to manage a Kubernetes application, or you can install tools along with the Kubernetes control plane.\\nOMT capabilities include:\\nAppHub Deployment Management\\n, a graphical user interface that simplifies the configuration for installing,\\nmodifying, or upgrading Applications. AppHub provides you the simplicity of a UI-based installer as an alternative to the\\nKubernetes-native installation based on helm.\\nAppHub Monitoring\\n, a graphical user interface based on Prometheus and Grafana for the self-monitoring of\\nApplications.  AppHub Monitoring is optional and recommended if you don’t have their own Prometheus as part of their\\nown Kubernetes cluster.\\nOMT Tools\\n,  a set of Linux command line utilities for administrative activities such as downloading container images\\nfrom a repository, or the starting and stopping of applications. \\nOMT embedded Kubernetes\\n, a Kubernetes control plane as an alternative for customers who do have Kubernetes as\\npart of their IT environment. \\nContainerized Operations Bridge 2022.11\\nPage \\n154\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Deployment concepts\\nThis topic describes the concepts related to deploying containerized Operations Bridge.\\nThe application deployment comprises the following layers:\\nInfrastructure\\nKubernetes Cluster\\nApplications\\nThe following image illustrates different entities that you need to install or configure for the deployment:\\nInfrastructure\\nInfrastructure is the hardware and the software resources needed to ensure computing, storage, and network capabilities\\nrequired to deploy an application. For example, servers, ports, and operating system.\\nPlan the infrastructure that you need for the deployment. Sizing calculator helps you to estimate the resource requirements.\\nThe sizing calculator provides you with a cumulative resource requirement for all the applications that you plan to deploy. \\nKubernetes cluster\\nA Kubernetes cluster is a set of nodes that run containerized applications. To deploy ITOM containerized applications, you can\\nchoose one of the following types of Kubernetes:\\nExternal Kubernetes:\\n A customer-owned Kubernetes cluster based on a third-party Kubernetes distribution. For\\nexample, Amazon EKS or Red Hat OpenShift, either on cloud or on-premises. \\nEmbedded Kubernetes:\\n A Kubernetes distribution provided by as part of Optic Management Toolkit (OMT) that you can\\ninstall on-premises (physical machines or virtual machines) in your datacenter as an alternative to external Kubernetes\\nclusters.\\nPlan the Kubernetes distribution that you want to use. If you are unsure of the choice of Kubernetes, and you want to use the\\ncontainerized capabilities of Operations Bridge, you can use the Embedded Kubernetes.\\nApplications\\nA set of containerized Applications that deliver IT operations management capabilities.  These Applications are shipped as\\nKubernetes-native packages (helm charts) and are deployed into a Kubernetes cluster to host and run the capabilities.  For\\nexample, Operations Bridge (Ops Bridge) and Network Operations Management (NOM).\\nThe containerized application is the ITOM application that you plan to deploy on the Kubernetes cluster. \\nOPTIC Management Toolkit (OMT) \\nOMT is an application provides the necessary tooling for installing and running containerized applications on Kubernetes. OMT\\nalso provides the Kubernetes control plane capability that you can install if required. OMT is the first application that you must\\ndeploy before deploying other ITOM applications on the cluster. When you install OMT you can choose to install only the\\nrequired tools to manage a Kubernetes application, or you can install tools along with the Kubernetes control plane.\\nOMT capabilities include:\\nAppHub Deployment Management\\n, a graphical user interface that simplifies the configuration for installing,\\nmodifying, or upgrading Applications. AppHub provides you the simplicity of a UI-based installer as an alternative to the\\nKubernetes-native installation based on helm.\\nAppHub Monitoring\\n, a graphical user interface based on Prometheus and Grafana for the self-monitoring of\\nApplications.  AppHub Monitoring is optional and recommended if you don’t have their own Prometheus as part of their\\nown Kubernetes cluster.\\nOMT Tools\\n,  a set of Linux command line utilities for administrative activities such as downloading container images\\nfrom a repository, or the starting and stopping of applications. \\nOMT embedded Kubernetes\\n, a Kubernetes control plane as an alternative for customers who do have Kubernetes as\\npart of their IT environment. \\nContainerized Operations Bridge 2022.11\\nPage \\n154\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3e7473a7316d85900d59638e58654b8c'}>,\n",
              "  <Document: {'content': 'Related topics \\nFor more information on OMT system requirements on Embedded Kubernetes or External Kubernetes, see \\nOMT\\ndocumentation\\n.\\nFor more information on estimating the resources and system requirements of Operations Bridge, see \\nSizing and system\\nrequirements\\n. \\nFor more information on preparing the infrastructure, see \\nPrepare  infrastructure\\n.\\nFor more information on setting up application specific prerequisites, see \\nSet up prerequisites\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n157\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Related topics \\nFor more information on OMT system requirements on Embedded Kubernetes or External Kubernetes, see \\nOMT\\ndocumentation\\n.\\nFor more information on estimating the resources and system requirements of Operations Bridge, see \\nSizing and system\\nrequirements\\n. \\nFor more information on preparing the infrastructure, see \\nPrepare  infrastructure\\n.\\nFor more information on setting up application specific prerequisites, see \\nSet up prerequisites\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n157\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3c96cd3ef280a46ba1b2146882036dad'}>,\n",
              "  <Document: {'content': 'Prepare using the guided install script\\nThis method is available only for embedded Kubernetes.\\nThe application installer consists of the guided install script. This script simplifies the application deployment by configuring\\nthe prerequisites required to deploy the application. This reduces the complexity of setting up the database, storage, and OMT\\ndeployment and the time to deploy the application.\\nYou can use this script only in the scenario where you plan to deploy your infrastructure to have OMT with the NFS capability\\nenabled, external PostgreSQL and Vertica that are TLS enabled, and local persistent volumes with a single separate disk\\ndevice for all the LPVs on each worker node. For more information, see \\nGuided installation of OMT and Operations Bridge\\nStep 3: Configure the application specific prerequisites on the infrastructure\\nIn this step, you must configure the application specific prerequisites. The application prerequisites vary based on the\\ncapabilities that you have chosen to deploy and the products that you want to integrate with. \\nFor example, the following diagram illustrates the specific prerequisites that you must complete to deploy Operations Bridge. \\nIn this step, you must complete the application specific requirements by doing certain configurations on the infrastructure you\\nprepared in step 2.\\nFor example, in step 2 you would have installed and configured a database engine on the database host(s) with TLS. In this\\nstep, the application would require that you create certain databases on the database engine based on the capabilities. You\\nwould typically run several SQL queries to create the users and databases. Similarly, another example would be, in step 2, you\\nwould have installed a load balancer software on the load balancer host(s). In step 3, the application would require you to edit\\nthe load balancer configuration file and forward the ports according to the application documentation.\\nTo complete the application specific prerequisites, you must follow the steps described in the Set up prerequisites section.\\nAdding additional capabilities \\nFor adding additional capabilities, you will need to make just the incremental changes. To add additional capability:\\n1\\n. \\nObtain the resources\\n: Obtain only the additional resources required for the application or capability that you want to\\ndeploy based on the estimates provided by the sizing calculator.\\n2\\n. \\nPrepare infrastructure\\n: You can leverage the existing infrastructure. This means that you will need only expand an\\nexisting infrastructure configuration. For example, depending on the capability you want to deploy you may need to add\\nadditional worker nodes in the cluster, increase the storage, or configure additional ports.\\n3\\n. \\nConfigure the application specific prerequisites\\n. After you complete the infrastructure configuration, you will need\\nto complete only the prerequisites required for the application or the specific capability.\\nSome resources such as storage and database servers are common prerequisites for the applications. Consider setting up\\nthese prerequisites upfront or as needed over a period of time. The following illustration provides a conceptual overview of\\ncommon dependencies between different applications. This illustration shows that you can prepare the base infrastructure for\\nall applications by combining the common system requirements. After you prepare the infrastructure, you must configure the\\napplication specific requirements for each application.\\nFor example, considering the requirements of all the applications that you are installing, as the first step you can set up the\\ndatabase server with TLS configuration. After that, you can create the application specific databases for OMT, Ops Bridge, or\\nNOM while setting up the application specific prerequisites.\\nContainerized Operations Bridge 2022.11\\nPage \\n156\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Prepare using the guided install script\\nThis method is available only for embedded Kubernetes.\\nThe application installer consists of the guided install script. This script simplifies the application deployment by configuring\\nthe prerequisites required to deploy the application. This reduces the complexity of setting up the database, storage, and OMT\\ndeployment and the time to deploy the application.\\nYou can use this script only in the scenario where you plan to deploy your infrastructure to have OMT with the NFS capability\\nenabled, external PostgreSQL and Vertica that are TLS enabled, and local persistent volumes with a single separate disk\\ndevice for all the LPVs on each worker node. For more information, see \\nGuided installation of OMT and Operations Bridge\\nStep 3: Configure the application specific prerequisites on the infrastructure\\nIn this step, you must configure the application specific prerequisites. The application prerequisites vary based on the\\ncapabilities that you have chosen to deploy and the products that you want to integrate with. \\nFor example, the following diagram illustrates the specific prerequisites that you must complete to deploy Operations Bridge. \\nIn this step, you must complete the application specific requirements by doing certain configurations on the infrastructure you\\nprepared in step 2.\\nFor example, in step 2 you would have installed and configured a database engine on the database host(s) with TLS. In this\\nstep, the application would require that you create certain databases on the database engine based on the capabilities. You\\nwould typically run several SQL queries to create the users and databases. Similarly, another example would be, in step 2, you\\nwould have installed a load balancer software on the load balancer host(s). In step 3, the application would require you to edit\\nthe load balancer configuration file and forward the ports according to the application documentation.\\nTo complete the application specific prerequisites, you must follow the steps described in the Set up prerequisites section.\\nAdding additional capabilities \\nFor adding additional capabilities, you will need to make just the incremental changes. To add additional capability:\\n1\\n. \\nObtain the resources\\n: Obtain only the additional resources required for the application or capability that you want to\\ndeploy based on the estimates provided by the sizing calculator.\\n2\\n. \\nPrepare infrastructure\\n: You can leverage the existing infrastructure. This means that you will need only expand an\\nexisting infrastructure configuration. For example, depending on the capability you want to deploy you may need to add\\nadditional worker nodes in the cluster, increase the storage, or configure additional ports.\\n3\\n. \\nConfigure the application specific prerequisites\\n. After you complete the infrastructure configuration, you will need\\nto complete only the prerequisites required for the application or the specific capability.\\nSome resources such as storage and database servers are common prerequisites for the applications. Consider setting up\\nthese prerequisites upfront or as needed over a period of time. The following illustration provides a conceptual overview of\\ncommon dependencies between different applications. This illustration shows that you can prepare the base infrastructure for\\nall applications by combining the common system requirements. After you prepare the infrastructure, you must configure the\\napplication specific requirements for each application.\\nFor example, considering the requirements of all the applications that you are installing, as the first step you can set up the\\ndatabase server with TLS configuration. After that, you can create the application specific databases for OMT, Ops Bridge, or\\nNOM while setting up the application specific prerequisites.\\nContainerized Operations Bridge 2022.11\\nPage \\n156\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3fbaa24d70597155a24b3143ffb28b5c'}>,\n",
              "  <Document: {'content': 'Similar to other applications, OMT also has infrastructure prerequisites that you must prepare before deploying the application.\\nFor more information see System requirements for OMT in OMT documentation.\\nApproach to configuring the deployment prerequisites\\nAfter you plan the deployment, you must complete the prerequisites needed to deploy an application.  For a new deployment\\nor if you are adding an additional capability, you must go through the following three steps to complete the prerequisites:\\nStep 1: Obtain the resources\\nThe first step is to estimate and obtain the required infrastructure resources to deploy the application. For example, the\\nfollowing diagram represents different infrastructure resources that you require to deploy Operations Bridge. \\nWhat are the different dependencies\\nYou must obtain and prepare the following  \\nOne or more database servers \\nStorage servers\\nA container image registry\\nNetwork communication between different external servers and the cluster \\nA Kubernetes cluster\\u202fand networking configuration\\nEstimate the resources and understand the system requirements by following the information provided in the section Sizing\\nand system requirements. The system requirements for each of the application that you plan to deploy are available in the\\nspecific application documentation. For example, if your deployment strategy includes OMT and Operations Bridge, you will\\nneed databases for both OMT and Operations Bridge. Your deployment strategy must consider system requirements for both\\nthe applications.\\nStep 2: Prepare the infrastructure\\nThis step involves configuring the infrastructure by installing the required third-party software and configuring them for a\\nstandard production deployment. \\nFor example, the following diagram illustrates the base configurations that you must complete to prepare the infrastructure\\nthat you obtained in Step 1.\\nFor example, you will install a Database Engine on the designated database host(s) and configure TLS for the database\\nengine. Another example would be, if you plan to use a load balancer, you will install the load balancer software on the\\ndesignated load balancer host(s). These installations and configurations are application agnostic and just a standard practice\\nto ready the infrastructure. Later, you must complete certain configurations on the infrastructure as a prerequisite to\\ndeployment. This is the next step explained in step 3.\\nIn addition to preparing the infrastructure you must install the application management utilities by installing the OPTIC\\nManagement Toolkit (OMT).\\nHow to prepare the infrastructure\\nYou can either complete the configurations by following the guidelines provided in the Prepare infrastructure section or\\nPrepare using the guided install script. \\nContainerized Operations Bridge 2022.11\\nPage \\n155\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Similar to other applications, OMT also has infrastructure prerequisites that you must prepare before deploying the application.\\nFor more information see System requirements for OMT in OMT documentation.\\nApproach to configuring the deployment prerequisites\\nAfter you plan the deployment, you must complete the prerequisites needed to deploy an application.  For a new deployment\\nor if you are adding an additional capability, you must go through the following three steps to complete the prerequisites:\\nStep 1: Obtain the resources\\nThe first step is to estimate and obtain the required infrastructure resources to deploy the application. For example, the\\nfollowing diagram represents different infrastructure resources that you require to deploy Operations Bridge. \\nWhat are the different dependencies\\nYou must obtain and prepare the following  \\nOne or more database servers \\nStorage servers\\nA container image registry\\nNetwork communication between different external servers and the cluster \\nA Kubernetes cluster\\u202fand networking configuration\\nEstimate the resources and understand the system requirements by following the information provided in the section Sizing\\nand system requirements. The system requirements for each of the application that you plan to deploy are available in the\\nspecific application documentation. For example, if your deployment strategy includes OMT and Operations Bridge, you will\\nneed databases for both OMT and Operations Bridge. Your deployment strategy must consider system requirements for both\\nthe applications.\\nStep 2: Prepare the infrastructure\\nThis step involves configuring the infrastructure by installing the required third-party software and configuring them for a\\nstandard production deployment. \\nFor example, the following diagram illustrates the base configurations that you must complete to prepare the infrastructure\\nthat you obtained in Step 1.\\nFor example, you will install a Database Engine on the designated database host(s) and configure TLS for the database\\nengine. Another example would be, if you plan to use a load balancer, you will install the load balancer software on the\\ndesignated load balancer host(s). These installations and configurations are application agnostic and just a standard practice\\nto ready the infrastructure. Later, you must complete certain configurations on the infrastructure as a prerequisite to\\ndeployment. This is the next step explained in step 3.\\nIn addition to preparing the infrastructure you must install the application management utilities by installing the OPTIC\\nManagement Toolkit (OMT).\\nHow to prepare the infrastructure\\nYou can either complete the configurations by following the guidelines provided in the Prepare infrastructure section or\\nPrepare using the guided install script. \\nContainerized Operations Bridge 2022.11\\nPage \\n155\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '87cbfcaeaf86daac3304a40eb2ad892'}>,\n",
              "  <Document: {'content': \"Provider application:\\n The application that provides the component. The provider application is always deployed as the\\nfirst application.\\nConsumer application:\\n The application that uses a component that's deployed with the provider application\\nFor example, when you deploy Operations Bridge first and then the NOM application and share the OPTIC Data Lake\\ncomponent, Operations Bridge is the provider application and NOM is the consumer application. NOM can store, process, and\\nshare the data in the common OPTIC Data Lake, without installing the Vertica database in their deployment.\\nYou can configure the shared component during deployment.\\nConnections between applications\\nThere are two types of connection mechanisms between the applications:\\nConnect between two namespaces\\nConnect through the nodeports\\nDecide the deployment strategy based on what applications you want to deploy and the components that you want to share.\\nDecide the storage strategy\\nDepending on the capability that you are installing, following are the recommended storage types.\\nCapability\\nRecommended storage\\ntype\\nEmbedded K8S\\nAWS\\nAzure\\nOpenShift\\nOPTIC Reporting\\nBlock, Shared File\\nSystems\\nLocal storage,\\nNFS\\nEBS,\\nEFS\\nAzure disk, Azure\\nFiles\\nCeph RBD, Ceph\\nFS\\nAutomatic Event\\nCorrelation\\nBlock, Shared File\\nSystems\\nLocal storage,\\nNFS\\nEBS,\\nEFS\\nAzure disk, Azure\\nFiles\\nCeph RBD, Ceph\\nFS\\nStakeholder Dashboards\\nShared File systems\\nNFS\\nEFS\\nAzure Files\\nCeph FS\\nAgentless Monitoring\\nShared File Systems\\nNFS\\nEBS,\\nEFS\\nAzure Files\\nCeph FS\\nHyperscale Observability\\nBlock, Shared File\\nSystems\\nLocal Storage,\\nNFS\\nEBS,\\nEFS\\nAzure disk, Azure\\nFiles\\nCeph RBD, Ceph\\nFS\\nOBM\\nBlock, Shared File\\nSystems\\nNFS\\nEBS,\\nEFS\\nAzure disk, Azure\\nFiles \\nCeph RBD, Ceph\\nFS\\nAt this step you will only decide on the storage type. The next step is to obtain the required storage. You can do the actual\\nsizing of the storage requirements specific to your deployment by using the sizing calculator. \\nContainerized Operations Bridge 2022.11\\nPage \\n159\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Provider application:\\n The application that provides the component. The provider application is always deployed as the\\nfirst application.\\nConsumer application:\\n The application that uses a component that's deployed with the provider application\\nFor example, when you deploy Operations Bridge first and then the NOM application and share the OPTIC Data Lake\\ncomponent, Operations Bridge is the provider application and NOM is the consumer application. NOM can store, process, and\\nshare the data in the common OPTIC Data Lake, without installing the Vertica database in their deployment.\\nYou can configure the shared component during deployment.\\nConnections between applications\\nThere are two types of connection mechanisms between the applications:\\nConnect between two namespaces\\nConnect through the nodeports\\nDecide the deployment strategy based on what applications you want to deploy and the components that you want to share.\\nDecide the storage strategy\\nDepending on the capability that you are installing, following are the recommended storage types.\\nCapability\\nRecommended storage\\ntype\\nEmbedded K8S\\nAWS\\nAzure\\nOpenShift\\nOPTIC Reporting\\nBlock, Shared File\\nSystems\\nLocal storage,\\nNFS\\nEBS,\\nEFS\\nAzure disk, Azure\\nFiles\\nCeph RBD, Ceph\\nFS\\nAutomatic Event\\nCorrelation\\nBlock, Shared File\\nSystems\\nLocal storage,\\nNFS\\nEBS,\\nEFS\\nAzure disk, Azure\\nFiles\\nCeph RBD, Ceph\\nFS\\nStakeholder Dashboards\\nShared File systems\\nNFS\\nEFS\\nAzure Files\\nCeph FS\\nAgentless Monitoring\\nShared File Systems\\nNFS\\nEBS,\\nEFS\\nAzure Files\\nCeph FS\\nHyperscale Observability\\nBlock, Shared File\\nSystems\\nLocal Storage,\\nNFS\\nEBS,\\nEFS\\nAzure disk, Azure\\nFiles\\nCeph RBD, Ceph\\nFS\\nOBM\\nBlock, Shared File\\nSystems\\nNFS\\nEBS,\\nEFS\\nAzure disk, Azure\\nFiles \\nCeph RBD, Ceph\\nFS\\nAt this step you will only decide on the storage type. The next step is to obtain the required storage. You can do the actual\\nsizing of the storage requirements specific to your deployment by using the sizing calculator. \\nContainerized Operations Bridge 2022.11\\nPage \\n159\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c8f9702a5a058c25d9b2e84934a210c3'}>,\n",
              "  <Document: {'content': 'Sizing and system requirements\\nThis topic provides information on the hardware resource requirements and the supported software with their versions to\\ndeploy the application.\\nBefore you begin\\nBefore you move ahead with this topic, make sure that you have read the deployment concepts and planned your deployment.\\nObtain the sizing and system requirements\\nFollow these steps to complete the sizing and system requirements for the application:\\n1\\n. \\nSize your cluster. The \\nSize your deployment\\n topic provides an interactive sizing calculator for considering all the services\\nand dependencies and helps you to arrive at a decision on the number of resources required for your deployment. Also,\\nevaluate the options to reduce the infrastructure requirement calculated based on the deployment strategy and sizing\\ncalculator.\\n2\\n. \\nObtain the required infrastructure. The sizing calculator provides a consolidated number of resources that you need to\\nbuy for each of the applications you plan to deploy. The infrastructure differ based on the environment and the\\ncapabilities that you choose to deploy. Make sure you have obtained the required infrastructure and the required size in\\nthe following areas:\\n1\\n. \\nDatabase server\\n2\\n. \\nStorage servers\\n3\\n. \\nNetwork resources\\n4\\n. \\nImage repository server\\n5\\n. \\nKubernetes resources\\n3\\n. \\nAfter you complete the sizing, see the \\nSystem requirements\\n topic to understand the supported versions of the\\nrequirements to deploy the application. In this section, only the Operations Bridge system requirements are covered.\\nAfter you obtain the required resources, the next step is to prepare the infrastructure.\\nContainerized Operations Bridge 2022.11\\nPage \\n160\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Sizing and system requirements\\nThis topic provides information on the hardware resource requirements and the supported software with their versions to\\ndeploy the application.\\nBefore you begin\\nBefore you move ahead with this topic, make sure that you have read the deployment concepts and planned your deployment.\\nObtain the sizing and system requirements\\nFollow these steps to complete the sizing and system requirements for the application:\\n1\\n. \\nSize your cluster. The \\nSize your deployment\\n topic provides an interactive sizing calculator for considering all the services\\nand dependencies and helps you to arrive at a decision on the number of resources required for your deployment. Also,\\nevaluate the options to reduce the infrastructure requirement calculated based on the deployment strategy and sizing\\ncalculator.\\n2\\n. \\nObtain the required infrastructure. The sizing calculator provides a consolidated number of resources that you need to\\nbuy for each of the applications you plan to deploy. The infrastructure differ based on the environment and the\\ncapabilities that you choose to deploy. Make sure you have obtained the required infrastructure and the required size in\\nthe following areas:\\n1\\n. \\nDatabase server\\n2\\n. \\nStorage servers\\n3\\n. \\nNetwork resources\\n4\\n. \\nImage repository server\\n5\\n. \\nKubernetes resources\\n3\\n. \\nAfter you complete the sizing, see the \\nSystem requirements\\n topic to understand the supported versions of the\\nrequirements to deploy the application. In this section, only the Operations Bridge system requirements are covered.\\nAfter you obtain the required resources, the next step is to prepare the infrastructure.\\nContainerized Operations Bridge 2022.11\\nPage \\n160\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '12b029e8ed6fedf1b2c71fbedf350cb0'}>,\n",
              "  <Document: {'content': 'Plan the deployment\\nTo create a deployment plan you must make a few decisions on the capabilities you want to deploy and how you want to\\ndeploy the application. These decisions are driven by your business goals which are are the key results (specific and\\nmeasurable) that you want to achieve. Use the information in this topic to create a deployment plan.  \\nDecide the capabilities to deploy\\nYour deployment strategy must consider the capabilities you want to deploy and their form factor. Capabilities are available in\\ntwo form factors—as classic products and as containers. Some capabilities are available in both form factors and some are\\navailable only in one. For example, OPTIC Reporting is only available in containers, while OBM or NA are available in both\\nforms. For certain use cases you must integrate containerized capabilities with the classic products.\\nBased on the capabilities that you need, your strategy must consider which containerized capabilities to deploy and which\\nclassic product to install. The following image illustrates the deployment of the entire Operations Bridge portfolio consisting of\\ncontainerized applications integrating with classic products.\\nFor example, your goal might be to replace the older reporting solution with  OPTIC Reporting. For more information on\\ncapabilities see the Get Started section.\\nDeployment strategy\\nYour deployment strategy consists of two main decisions:\\nApplication deployment scenario\\nSharing components between applications\\nApplication deployment scenario\\nYou can share components when deploying multiple applications in the same cluster. For example, when you deploy\\nOperations Bridge and NOM applications in the same cluster, you can share the OPTIC Data Lake component from Operations\\nBridge. You can use the common OPTIC Data Lake to store, process, and share the data among the applications. Sharing\\ncomponents helps optimize infrastructure footprint and hence on infrastructure costs.\\nYou must decide which applications you want to deploy and how you will set up the infrastructure for these applications. You\\ncan deploy in one of the following scenarios:\\nDeploy a single instance of Operations Bridge\\nDeploy multiple instances of the same application\\nDeploying multiple applications and configure to share components - for example sharing OPTIC Data Lake between Ops\\nBridge and NOM\\nWhile deploying a multiple instances of the same application or other applications, you can leverage the existing\\ninfrastructure. \\nSharing components between applications\\nYou can share components when deploying multiple applications in the same cluster. Sharing components helps optimize\\ninfrastructure footprint and therefore reduces infrastructure costs.\\nAn important use case is sharing the OPTIC Data Lake. For example, when you deploy Operations Bridge and NOM applications\\nin the same cluster, you can share the OPTIC Data Lake component from Operations Bridge. You can use the same OPTIC Data\\nLake to store, process, and share the data among the two applications, without installing the Vertica database during NOM\\ndeployment.\\nConcepts related to shared components\\nFollowing are a few concepts and terminologies that you must understand as part of deployments using shared components\\nImportant\\n:\\n Agentless Monitoring is supported only on embedded Kubernetes.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n158\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Plan the deployment\\nTo create a deployment plan you must make a few decisions on the capabilities you want to deploy and how you want to\\ndeploy the application. These decisions are driven by your business goals which are are the key results (specific and\\nmeasurable) that you want to achieve. Use the information in this topic to create a deployment plan.  \\nDecide the capabilities to deploy\\nYour deployment strategy must consider the capabilities you want to deploy and their form factor. Capabilities are available in\\ntwo form factors—as classic products and as containers. Some capabilities are available in both form factors and some are\\navailable only in one. For example, OPTIC Reporting is only available in containers, while OBM or NA are available in both\\nforms. For certain use cases you must integrate containerized capabilities with the classic products.\\nBased on the capabilities that you need, your strategy must consider which containerized capabilities to deploy and which\\nclassic product to install. The following image illustrates the deployment of the entire Operations Bridge portfolio consisting of\\ncontainerized applications integrating with classic products.\\nFor example, your goal might be to replace the older reporting solution with  OPTIC Reporting. For more information on\\ncapabilities see the Get Started section.\\nDeployment strategy\\nYour deployment strategy consists of two main decisions:\\nApplication deployment scenario\\nSharing components between applications\\nApplication deployment scenario\\nYou can share components when deploying multiple applications in the same cluster. For example, when you deploy\\nOperations Bridge and NOM applications in the same cluster, you can share the OPTIC Data Lake component from Operations\\nBridge. You can use the common OPTIC Data Lake to store, process, and share the data among the applications. Sharing\\ncomponents helps optimize infrastructure footprint and hence on infrastructure costs.\\nYou must decide which applications you want to deploy and how you will set up the infrastructure for these applications. You\\ncan deploy in one of the following scenarios:\\nDeploy a single instance of Operations Bridge\\nDeploy multiple instances of the same application\\nDeploying multiple applications and configure to share components - for example sharing OPTIC Data Lake between Ops\\nBridge and NOM\\nWhile deploying a multiple instances of the same application or other applications, you can leverage the existing\\ninfrastructure. \\nSharing components between applications\\nYou can share components when deploying multiple applications in the same cluster. Sharing components helps optimize\\ninfrastructure footprint and therefore reduces infrastructure costs.\\nAn important use case is sharing the OPTIC Data Lake. For example, when you deploy Operations Bridge and NOM applications\\nin the same cluster, you can share the OPTIC Data Lake component from Operations Bridge. You can use the same OPTIC Data\\nLake to store, process, and share the data among the two applications, without installing the Vertica database during NOM\\ndeployment.\\nConcepts related to shared components\\nFollowing are a few concepts and terminologies that you must understand as part of deployments using shared components\\nImportant\\n:\\n Agentless Monitoring is supported only on embedded Kubernetes.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n158\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '956b66b7a23a28c0f59b43742c4b0fc3'}>,\n",
              "  <Document: {'content': \"Low Footprint OpsBridge Reporting Deployment\\nThis topic provides details on the infrastructure configurations, capabilities that can be deployed, and limitations of Low\\nFootprint OpsBridge Reporting Deployment.\\nA Low Footprint OpsBridge Reporting deployment can consist of two machines:\\nThe first machine will run the Vertica database.\\nThe second machine will host the remaining load for Control Plane or Worker node activities, NFS service, and an\\nembedded PostgreSQL database. A single instance containerized PostgreSQL is chosen as the relational database which\\nstores the database files on the NFS volumes.\\nFor Low Footprint OpsBridge Reporting deployment you can use additional systems for an external PostgreSQL, Oracle\\ninstance, external NFS server, or a three node Vertica cluster. \\nIn a low footprint deployment, you can select only the following capabilities:\\nAgentless Monitoring\\nStakeholder Dashboard (Business Value Dashboard)\\nReporting\\nThe low footprint deployment is supported for scenarios, where the OPTIC DL data ingestion rate is less than or equal to 0.5 MB\\nper second (0.5 MB/s). A marginal or occasional increase in ingestion rate is supported, for example, an increase caused by the\\nuse of historic data collection in case of data gaps. \\nYou must use the \\nLow-Footprint-OpsB-Reporting.yaml\\n file to deploy the application for the Low footprint OpsBridge Reporting\\ndeployment. You can use the sizing calculator to plan the provisioning of systems for a containerized Operations Bridge\\ndeployment. \\nExample \\nThis example illustrates a mix of data sources and corresponding quantity for OPTIC Data Lake that has an ingestion rate of\\nless than 0.5 MB/s.  \\nData source\\nQuantity\\nComment\\nOA nodes - SysInfra\\nmetrics through\\nAgent Metric\\nCollector (AMC)\\n2250\\nusing default collection settings\\nOA nodes - SysInfra\\nmetrics through\\nAgent Metric\\nStreaming (AMS)\\n250\\nusing default collection settings\\nSiteScope monitors\\n2000\\nusing default monitor settings\\nTotal number of\\nOBM events or\\nevent updates / one\\nday\\n80000\\nEvery OBM event update is stored as a full copy of the event in the OPTIC Data Lake\\n(Vertica). If there is an average of three event updates per event (For example, assigned\\nto an owner, annotation added, and closing the event) then the 80,000 events are\\nactually 20,000 unique events you can store per day. \\nMSSQL\\nManagement Pack\\n(Number of DB\\ninstances)\\n625\\nUsing default collection settings\\nThe data sources/quantity given previously are an example. You may choose any different type/quantity/interval/granularity\\nfor OpsBridge data sources in the sizing calculator but ensure the ingestion rate is within the defined value (<= ~0.5 MB/s). If\\nthe ingestion rate is higher than the specified value or you choose other capabilities (such as Hyperscale Observability, AEC,\\nContainerized. OBM, DCA Reports, etc), the low footprint deployment is \\nnot supported\\n.\\nImplications of deploying low footprint OpsBridge Reporting\\nA Low footprint deployment with two machines for OpsBridge OPTIC Reporting of Operations Bridge has several restrictions\\nand limitations that should be known before you choose it as the deployment model.  \\nThere is no load balancing, High Availability, or configurable resiliency for the Vertica node and Kubernetes node. The\\nKubernetes node also acts as the master, worker, and optionally hosts the NFS service and embedded PostgreSQL\\ndatabase.  \\nHorizontal scaling out from this deployment model (moving to another deployment model or adding a Vertica node) is\\ncurrently not supported for additional load, load balancing, or HA.\\nThere is a potential data loss risk from databases (PostgreSQL/Oracle and Vertica) during an ungraceful shutdown.\\nIt's recommended to take frequent backups to avoid data loss.\\nThe following table details some potential implications of deploying OpsBridge OPTIC Reporting in a low footprint and possible\\nrecovery options.\\nContainerized Operations Bridge 2022.11\\nPage \\n162\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Low Footprint OpsBridge Reporting Deployment\\nThis topic provides details on the infrastructure configurations, capabilities that can be deployed, and limitations of Low\\nFootprint OpsBridge Reporting Deployment.\\nA Low Footprint OpsBridge Reporting deployment can consist of two machines:\\nThe first machine will run the Vertica database.\\nThe second machine will host the remaining load for Control Plane or Worker node activities, NFS service, and an\\nembedded PostgreSQL database. A single instance containerized PostgreSQL is chosen as the relational database which\\nstores the database files on the NFS volumes.\\nFor Low Footprint OpsBridge Reporting deployment you can use additional systems for an external PostgreSQL, Oracle\\ninstance, external NFS server, or a three node Vertica cluster. \\nIn a low footprint deployment, you can select only the following capabilities:\\nAgentless Monitoring\\nStakeholder Dashboard (Business Value Dashboard)\\nReporting\\nThe low footprint deployment is supported for scenarios, where the OPTIC DL data ingestion rate is less than or equal to 0.5 MB\\nper second (0.5 MB/s). A marginal or occasional increase in ingestion rate is supported, for example, an increase caused by the\\nuse of historic data collection in case of data gaps. \\nYou must use the \\nLow-Footprint-OpsB-Reporting.yaml\\n file to deploy the application for the Low footprint OpsBridge Reporting\\ndeployment. You can use the sizing calculator to plan the provisioning of systems for a containerized Operations Bridge\\ndeployment. \\nExample \\nThis example illustrates a mix of data sources and corresponding quantity for OPTIC Data Lake that has an ingestion rate of\\nless than 0.5 MB/s.  \\nData source\\nQuantity\\nComment\\nOA nodes - SysInfra\\nmetrics through\\nAgent Metric\\nCollector (AMC)\\n2250\\nusing default collection settings\\nOA nodes - SysInfra\\nmetrics through\\nAgent Metric\\nStreaming (AMS)\\n250\\nusing default collection settings\\nSiteScope monitors\\n2000\\nusing default monitor settings\\nTotal number of\\nOBM events or\\nevent updates / one\\nday\\n80000\\nEvery OBM event update is stored as a full copy of the event in the OPTIC Data Lake\\n(Vertica). If there is an average of three event updates per event (For example, assigned\\nto an owner, annotation added, and closing the event) then the 80,000 events are\\nactually 20,000 unique events you can store per day. \\nMSSQL\\nManagement Pack\\n(Number of DB\\ninstances)\\n625\\nUsing default collection settings\\nThe data sources/quantity given previously are an example. You may choose any different type/quantity/interval/granularity\\nfor OpsBridge data sources in the sizing calculator but ensure the ingestion rate is within the defined value (<= ~0.5 MB/s). If\\nthe ingestion rate is higher than the specified value or you choose other capabilities (such as Hyperscale Observability, AEC,\\nContainerized. OBM, DCA Reports, etc), the low footprint deployment is \\nnot supported\\n.\\nImplications of deploying low footprint OpsBridge Reporting\\nA Low footprint deployment with two machines for OpsBridge OPTIC Reporting of Operations Bridge has several restrictions\\nand limitations that should be known before you choose it as the deployment model.  \\nThere is no load balancing, High Availability, or configurable resiliency for the Vertica node and Kubernetes node. The\\nKubernetes node also acts as the master, worker, and optionally hosts the NFS service and embedded PostgreSQL\\ndatabase.  \\nHorizontal scaling out from this deployment model (moving to another deployment model or adding a Vertica node) is\\ncurrently not supported for additional load, load balancing, or HA.\\nThere is a potential data loss risk from databases (PostgreSQL/Oracle and Vertica) during an ungraceful shutdown.\\nIt's recommended to take frequent backups to avoid data loss.\\nThe following table details some potential implications of deploying OpsBridge OPTIC Reporting in a low footprint and possible\\nrecovery options.\\nContainerized Operations Bridge 2022.11\\nPage \\n162\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c099d6faf0974c9ea3c05a7bf01aed33'}>,\n",
              "  <Document: {'content': \"Size your deployment\\nThis topics provides you with the link to download the \\nOperations Bridge Sizing Calculator v1.3\\n and how to use the Operations\\nBridge Sizing Calculator to find the compute and storage requirements for deploying your application.\\nThe Operations Bridge Sizing Calculator can be used to determine the compute resource requirements for a shared OPTIC Data\\nLake between Operations Bridge and NOM deployments.\\nFor information on terms used in the sizing calculator, see \\nGlossary\\n.\\nContainerized Operations Bridge supports \\nEvaluation, \\nLow-Footprint-OpsB-Reporting\\n, Medium, Large, and Extra-\\nLarge\\n deployment sizes. Refer to the sizing calculator to find a suitable deployment size for your environment. \\nPost install you can change your deployment size from:\\nmedium to large or extra-large\\nlarge to medium or extra-large\\nextra-large to medium or large\\nNote\\n:\\n Upgrade from \\nevaluation or \\nLow-Footprint-OpsB-Reporting\\n deployment\\n to \\nmedium, large or\\nextra-large deployment\\n isn't supported.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n161\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Size your deployment\\nThis topics provides you with the link to download the \\nOperations Bridge Sizing Calculator v1.3\\n and how to use the Operations\\nBridge Sizing Calculator to find the compute and storage requirements for deploying your application.\\nThe Operations Bridge Sizing Calculator can be used to determine the compute resource requirements for a shared OPTIC Data\\nLake between Operations Bridge and NOM deployments.\\nFor information on terms used in the sizing calculator, see \\nGlossary\\n.\\nContainerized Operations Bridge supports \\nEvaluation, \\nLow-Footprint-OpsB-Reporting\\n, Medium, Large, and Extra-\\nLarge\\n deployment sizes. Refer to the sizing calculator to find a suitable deployment size for your environment. \\nPost install you can change your deployment size from:\\nmedium to large or extra-large\\nlarge to medium or extra-large\\nextra-large to medium or large\\nNote\\n:\\n Upgrade from \\nevaluation or \\nLow-Footprint-OpsB-Reporting\\n deployment\\n to \\nmedium, large or\\nextra-large deployment\\n isn't supported.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n161\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3cd0ea0eebf21f1e0cbd2fa7552bd003'}>,\n",
              "  <Document: {'content': \"Databases\\nThe following external databases are supported:\\nRelational database\\nPostgreSQL\\n  10x, 11.x, 12.x, 13.x, and 14.x. \\nOracle  \\nOracle Standard and Enterprise Edition (64 bit) \\n19c, \\nOracle RAC Standard and Enterprise Edition (64 bit) \\n19c\\nDatabase for OPTIC Data Lake\\nVertica Enterprise Mode\\n (vertica.com) \\n10.1.1-21\\n or later patch versions and \\n11.1.1-5\\n or later patch versions. \\nStorage\\nConfigure the required storage and additional disks on all worker nodes to persist data across containers when they're started\\nor stopped, and for shared data access between containers. To decide the size for each of the disks, see the sizing calculator.\\nThe application functions rely on the performance of NFS server. It's recommended to use an external NFS server deployed in\\nHA.\\nAWS\\nAmazon Elastic File System (Amazon EFS)\\nAmazon Elastic Block Store (Amazon EBS) - Required if you plan to use OPTIC Reporting or any of the capabilities with\\nOPTIC Data Lake\\nAzure\\nAzure File Share Premium  \\nEmbedded Kubernetes\\nLinux based NFS\\nHPE 3PAR File Persona\\nOther third-party vendors that support configurations similar to HPE 3PAR File Persona. \\nYou will need \\next4\\n or \\nxfs\\n filesystems if you plan to use OPTIC Reporting or any of the capabilities with OPTIC DL.\\nFor more information, see System requirements in \\nOMT documentation\\n.\\nRed Hat OpenShift\\nOpenShift Container Storage (ODF) Ceph File System (\\nCephFS\\n)\\nOpenShift Container Storage (ODF) Ceph RBD if you plan to use OPTIC Reporting or any of the capabilities with OPTIC DL.\\nImportant:\\n  \\nOnly PostgreSQL 11.x is supported on Azure.\\nIf you are planning to integrate containerized Operations Bridge with \\nNOM OPTIC Reporting\\n, then you\\nmust select the\\n PostgreSQL database\\n, as NOM doesn't support Oracle.\\nOn Azure, install \\npostgres-contrib\\n or \\npostgresql<version>-contrib\\n \\npackage on the database server depending on\\nthe PostgreSQL version you are using. \\n\\ue91b\\n\\ue91b\\nNote:\\nOperations Bridge deployment on Azure doesn't support Oracle.\\nOperations Bridge deployment on AWS doesn't support Oracle if you use the ITOM Cloud Deployment toolkit\\nto set up AWS infrastructure. However, for AWS infrastructure that's set up manually, you can deploy\\nOperations Bridge with Oracle database without TLS.\\n\\ue916\\n\\ue916\\nNote: \\nFor Azure, Vertica requires SSL enabled running on CentOS.\\nOperations Bridge deployment on Azure doesn't support Embedded Vertica database, Vertica Analytics\\nPlatform available on Azure Marketplace.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n165\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Databases\\nThe following external databases are supported:\\nRelational database\\nPostgreSQL\\n  10x, 11.x, 12.x, 13.x, and 14.x. \\nOracle  \\nOracle Standard and Enterprise Edition (64 bit) \\n19c, \\nOracle RAC Standard and Enterprise Edition (64 bit) \\n19c\\nDatabase for OPTIC Data Lake\\nVertica Enterprise Mode\\n (vertica.com) \\n10.1.1-21\\n or later patch versions and \\n11.1.1-5\\n or later patch versions. \\nStorage\\nConfigure the required storage and additional disks on all worker nodes to persist data across containers when they're started\\nor stopped, and for shared data access between containers. To decide the size for each of the disks, see the sizing calculator.\\nThe application functions rely on the performance of NFS server. It's recommended to use an external NFS server deployed in\\nHA.\\nAWS\\nAmazon Elastic File System (Amazon EFS)\\nAmazon Elastic Block Store (Amazon EBS) - Required if you plan to use OPTIC Reporting or any of the capabilities with\\nOPTIC Data Lake\\nAzure\\nAzure File Share Premium  \\nEmbedded Kubernetes\\nLinux based NFS\\nHPE 3PAR File Persona\\nOther third-party vendors that support configurations similar to HPE 3PAR File Persona. \\nYou will need \\next4\\n or \\nxfs\\n filesystems if you plan to use OPTIC Reporting or any of the capabilities with OPTIC DL.\\nFor more information, see System requirements in \\nOMT documentation\\n.\\nRed Hat OpenShift\\nOpenShift Container Storage (ODF) Ceph File System (\\nCephFS\\n)\\nOpenShift Container Storage (ODF) Ceph RBD if you plan to use OPTIC Reporting or any of the capabilities with OPTIC DL.\\nImportant:\\n  \\nOnly PostgreSQL 11.x is supported on Azure.\\nIf you are planning to integrate containerized Operations Bridge with \\nNOM OPTIC Reporting\\n, then you\\nmust select the\\n PostgreSQL database\\n, as NOM doesn't support Oracle.\\nOn Azure, install \\npostgres-contrib\\n or \\npostgresql<version>-contrib\\n \\npackage on the database server depending on\\nthe PostgreSQL version you are using. \\n\\ue91b\\n\\ue91b\\nNote:\\nOperations Bridge deployment on Azure doesn't support Oracle.\\nOperations Bridge deployment on AWS doesn't support Oracle if you use the ITOM Cloud Deployment toolkit\\nto set up AWS infrastructure. However, for AWS infrastructure that's set up manually, you can deploy\\nOperations Bridge with Oracle database without TLS.\\n\\ue916\\n\\ue916\\nNote: \\nFor Azure, Vertica requires SSL enabled running on CentOS.\\nOperations Bridge deployment on Azure doesn't support Embedded Vertica database, Vertica Analytics\\nPlatform available on Azure Marketplace.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n165\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27ca0bae5a6e26a38b67304c0f398db3'}>,\n",
              "  <Document: {'content': \"System requirements\\nThis topic provides information about the required infrastructure and software with their supported versions. Review the\\nsystem requirements and create the resources as needed.\\nEnsure that you have understood the install flow and computed resources and storage using the sizing calculator. \\nKubernetes distributions\\nOperations Bridge 2022.11 is supported on the following Kubernetes distributions or managed Kubernetes platforms:\\nAmazon Elastic Kubernetes Engine (EKS) versions:\\n1.21\\n1.22\\nContainerized Operations Bridge deployment on \\nAmazon Web Services China\\n \\nRegions\\n isn't supported.\\nWhile creating the resources for EKS, you must use the same EKS cluster name when creating the VPC, EKS cluster, and\\nworker nodes; otherwise, the application installation will fail.\\nMicrosoft Azure Kubernetes Service (AKS) versions:\\n1.22\\n1.23\\nRedHat OpenShift Container Platform versions:\\n4.8 (uses Kubernetes version 1.21)\\n4.9 (uses Kubernetes version 1.22)\\n4.10 (uses Kubernetes version 1.23)\\n4.11 (uses Kubernetes version 1.24)\\nOpenShift Container Platform deployment on managed cloud platforms is currently not supported.\\nIf you plan to deploy the containerized Operations Bridge Manager (OBM) capability, you must set the kubelet configuration\\nparameter \\npod-max-pids\\n to at least 2048. OBM capability requires a minimum number of 2048 \\nPIDs\\n.\\nFor the steps to configure kubelet, see \\ncommand line reference\\n and \\nKubelet Configuration API\\n. \\nAgentless Monitoring is not supported on RedHat OpenShift.\\nEmbedded Kubernetes packaged with \\nOPTIC Management Toolkit (OMT)\\n version 2022.11\\nOPTIC Management Toolkit (OMT) 2022.11 uses Kubernetes 1.24.\\nOther \\nKubernetes\\n distributions\\nRancher Kubernetes Engine \\n1.21\\n1.22\\n1.23\\nK3S (Lightweight Kubernetes)\\nv1.21\\nv1.22\\nv1.23\\nOnly Agentless Monitoring and Business Value Dashboard capabilities are supported on K3S Kubernetes distribution.\\nSupport of other \\ncertified Kubernetes\\n distributions (as defined in \\nCertified Kubernetes Conformance Program\\n).  External\\naccess like configuring \\ningress controller, Load Balancer \\nrequires manual post deployment steps.\\nThere is no installation assistance available for deploying Kubernetes platforms that are not mentioned in this topic.\\nContainer image registry\\nKubernetes cluster needs an access to a registry where the container images are located. You can set up any image registry\\nthat's Docker Registry \\nHTTP API V2 \\ncompliant. For example, Elastic Container Registry for AWS. \\nMicro Focus publishes the images on Docker Hub.\\nMake sure you have the required access to upload the container images. You require the following information to upload the\\nimages to the registry.\\nRegistry URL\\nCredentials to access the images in the registry. For more information, see \\nImage pull secrets\\n in the Kubernetes\\ndocumentation.\\nNote:\\n If you are using OMT, the registry is available as part of the OMT installation. You also don't need to give\\nthe access requirements.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n164\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.System requirements\\nThis topic provides information about the required infrastructure and software with their supported versions. Review the\\nsystem requirements and create the resources as needed.\\nEnsure that you have understood the install flow and computed resources and storage using the sizing calculator. \\nKubernetes distributions\\nOperations Bridge 2022.11 is supported on the following Kubernetes distributions or managed Kubernetes platforms:\\nAmazon Elastic Kubernetes Engine (EKS) versions:\\n1.21\\n1.22\\nContainerized Operations Bridge deployment on \\nAmazon Web Services China\\n \\nRegions\\n isn't supported.\\nWhile creating the resources for EKS, you must use the same EKS cluster name when creating the VPC, EKS cluster, and\\nworker nodes; otherwise, the application installation will fail.\\nMicrosoft Azure Kubernetes Service (AKS) versions:\\n1.22\\n1.23\\nRedHat OpenShift Container Platform versions:\\n4.8 (uses Kubernetes version 1.21)\\n4.9 (uses Kubernetes version 1.22)\\n4.10 (uses Kubernetes version 1.23)\\n4.11 (uses Kubernetes version 1.24)\\nOpenShift Container Platform deployment on managed cloud platforms is currently not supported.\\nIf you plan to deploy the containerized Operations Bridge Manager (OBM) capability, you must set the kubelet configuration\\nparameter \\npod-max-pids\\n to at least 2048. OBM capability requires a minimum number of 2048 \\nPIDs\\n.\\nFor the steps to configure kubelet, see \\ncommand line reference\\n and \\nKubelet Configuration API\\n. \\nAgentless Monitoring is not supported on RedHat OpenShift.\\nEmbedded Kubernetes packaged with \\nOPTIC Management Toolkit (OMT)\\n version 2022.11\\nOPTIC Management Toolkit (OMT) 2022.11 uses Kubernetes 1.24.\\nOther \\nKubernetes\\n distributions\\nRancher Kubernetes Engine \\n1.21\\n1.22\\n1.23\\nK3S (Lightweight Kubernetes)\\nv1.21\\nv1.22\\nv1.23\\nOnly Agentless Monitoring and Business Value Dashboard capabilities are supported on K3S Kubernetes distribution.\\nSupport of other \\ncertified Kubernetes\\n distributions (as defined in \\nCertified Kubernetes Conformance Program\\n).  External\\naccess like configuring \\ningress controller, Load Balancer \\nrequires manual post deployment steps.\\nThere is no installation assistance available for deploying Kubernetes platforms that are not mentioned in this topic.\\nContainer image registry\\nKubernetes cluster needs an access to a registry where the container images are located. You can set up any image registry\\nthat's Docker Registry \\nHTTP API V2 \\ncompliant. For example, Elastic Container Registry for AWS. \\nMicro Focus publishes the images on Docker Hub.\\nMake sure you have the required access to upload the container images. You require the following information to upload the\\nimages to the registry.\\nRegistry URL\\nCredentials to access the images in the registry. For more information, see \\nImage pull secrets\\n in the Kubernetes\\ndocumentation.\\nNote:\\n If you are using OMT, the registry is available as part of the OMT installation. You also don't need to give\\nthe access requirements.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n164\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5e05af197151ef858e9f2e4eec6e1ff1'}>,\n",
              "  <Document: {'content': 'Server\\nImplication\\nRecovery option\\nKubernetes\\n(without HA)\\nNode down results in Service downtime\\nServices will restart as the node comes back up.\\nNode crash may result in Kubernetes services to\\nnot starting.\\nRestore data from the recent backup.\\nVertica (without\\nHA)\\nNode down results in service downtime and \\nprolonged downtime will lead to data loss\\nDB services will start as the node comes back\\nup.\\nDatabase crashes may cause data corruption on\\nrare occasions causing potential data loss.\\nRestore to the previous checkpoint or recreate\\nthe DB and restore to the recent backup.\\nNFS\\nDisk full can cause service downtime and affect\\nKubernetes startup.\\nClean up the disk and start Kubernetes.\\nEmbedded\\nPostgreSQL\\ndatabase\\nNode down results in service downtime.\\nServices will restart as the node comes back up.\\nDatabase crashes may cause data corruption on\\nrare occasions causing potential data loss.\\nRestore from the recent backup.\\nSet resources of Fluent bit pod\\nNote that the steps in this section are applicable only if you have deployed Operations Bridge in a low footprint environment\\nand used a shared OPTIC Data Lake, OMT offers parameters to set the resources configuration of the Fluent bit pods. You can\\nfind the parameters in the following YAML file for Elasticsearch via HTTP or HTTPS:\\n# We usually recommend not to specify default resources and to leave this as a conscious\\n# choice for the user. This also increases chances charts run on environments with little\\n# resources, such as Minikube. If you do want to specify resources, uncomment the following\\n# lines, adjust them as necessary, and remove the curly braces after \\'resources:\\'.\\nresources:\\n  limits:\\n    cpu: 1000m\\n    memory: 5Gi\\n  requests:\\n    cpu: 100m\\n    memory: 200Mi\\nTo modify the values, perform the following steps:\\n1\\n. \\nRun the following command to locate the location of the chart:\\ncd $CDF_HOME/charts\\n2\\n. \\nRun the following command to set the new values:\\nhelm upgrade apphub <apphub tgz name> -n <namespace> --set fluentd.resources.limits.memory=<values needed> --reuse-values\\nFor examples:\\nhelm upgrade apphub apphub-1.xx.x+202xxxxx.xxxx.tgz -n core --set fluentd.resources.limits.memory=5Gi --reuse-values\\nNote: \\nWhen you deploy Operations Bridge in a low footprint environment and use a shared OPTIC Data Lake,\\nyou may see the \"too many pods\" error. For information to resolve this error, see \\n\"Too many pods\" error when\\ninstalling an application\\n.\\n\\ue916\\n\\ue916\\nNote\\n:\\n Modify the memory values according to your actual situation only when it\\'s necessary. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n163\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Server\\nImplication\\nRecovery option\\nKubernetes\\n(without HA)\\nNode down results in Service downtime\\nServices will restart as the node comes back up.\\nNode crash may result in Kubernetes services to\\nnot starting.\\nRestore data from the recent backup.\\nVertica (without\\nHA)\\nNode down results in service downtime and \\nprolonged downtime will lead to data loss\\nDB services will start as the node comes back\\nup.\\nDatabase crashes may cause data corruption on\\nrare occasions causing potential data loss.\\nRestore to the previous checkpoint or recreate\\nthe DB and restore to the recent backup.\\nNFS\\nDisk full can cause service downtime and affect\\nKubernetes startup.\\nClean up the disk and start Kubernetes.\\nEmbedded\\nPostgreSQL\\ndatabase\\nNode down results in service downtime.\\nServices will restart as the node comes back up.\\nDatabase crashes may cause data corruption on\\nrare occasions causing potential data loss.\\nRestore from the recent backup.\\nSet resources of Fluent bit pod\\nNote that the steps in this section are applicable only if you have deployed Operations Bridge in a low footprint environment\\nand used a shared OPTIC Data Lake, OMT offers parameters to set the resources configuration of the Fluent bit pods. You can\\nfind the parameters in the following YAML file for Elasticsearch via HTTP or HTTPS:\\n# We usually recommend not to specify default resources and to leave this as a conscious\\n# choice for the user. This also increases chances charts run on environments with little\\n# resources, such as Minikube. If you do want to specify resources, uncomment the following\\n# lines, adjust them as necessary, and remove the curly braces after \\'resources:\\'.\\nresources:\\n  limits:\\n    cpu: 1000m\\n    memory: 5Gi\\n  requests:\\n    cpu: 100m\\n    memory: 200Mi\\nTo modify the values, perform the following steps:\\n1\\n. \\nRun the following command to locate the location of the chart:\\ncd $CDF_HOME/charts\\n2\\n. \\nRun the following command to set the new values:\\nhelm upgrade apphub <apphub tgz name> -n <namespace> --set fluentd.resources.limits.memory=<values needed> --reuse-values\\nFor examples:\\nhelm upgrade apphub apphub-1.xx.x+202xxxxx.xxxx.tgz -n core --set fluentd.resources.limits.memory=5Gi --reuse-values\\nNote: \\nWhen you deploy Operations Bridge in a low footprint environment and use a shared OPTIC Data Lake,\\nyou may see the \"too many pods\" error. For information to resolve this error, see \\n\"Too many pods\" error when\\ninstalling an application\\n.\\n\\ue916\\n\\ue916\\nNote\\n:\\n Modify the memory values according to your actual situation only when it\\'s necessary. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n163\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd208e32ebffd78ca2bb96829173d2490'}>,\n",
              "  <Document: {'content': \"1383\\nTCP\\nAgent Metric\\nCollection \\nThis is the default port that external OBM uses to communicate with the Data Broker\\npod (\\nitom-collect-once-data-broker\\n443\\nTCP\\nOperations\\nBridge\\nExternal Access to Operations Bridge Services, that's Autopass, Printing Service, BVD,\\nOPTIC Data Lake Health Insights, etc.\\n30001\\nTCP\\nOPTIC DL\\nHTTP Receiver\\nOPTIC Data Lake data receiver endpoint is used to receive data posted to it from data\\nsources such as Operations Agents.\\n30003\\nTCP\\nOPTIC Data\\nLake Data\\nAccess\\nOPTIC Data Lake data access endpoint is used to query the data via REST API.  Used by\\nPerformance Dashboard.\\n30004\\nTCP\\nOPTIC Data\\nLake\\nAdministration\\nOPTIC Data Lake administration endpoint used to configure OPTIC Data Lake.\\n30012\\nTCP\\nOPTIC Data\\nLake\\nData Ingestion Vertica Database port for external access.\\n5433\\nTCP\\nOPTIC Data\\nLake\\nThe default External Vertica port is 5433 for TLS/non-TLS. The Vertica administrator\\ncan change this default value.\\n31051\\nTCP\\nOPTIC Data\\nLake Message\\nBus\\nUsed for pushing topology from the RTSM to OPTIC Data Lake.\\n31001\\nTCP\\nOPTIC Data\\nLake Message\\nBus Admin\\nThe Pulsar Admin rest endpoint for management of the Message Bus.\\nDefault\\nPort \\nProtocol\\nService\\nDescription\\nNote:\\n The embedded Vertica requires a Vertica database port.\\nImportant:\\n The application requires two-way communication, the port\\nshould be open from Vertica (UDX Plugin) to Pulsar Admin endpoint (cluster)\\nand also from Pulsar Admin endpoint back to the UDX Plugin.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n168\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.1383\\nTCP\\nAgent Metric\\nCollection \\nThis is the default port that external OBM uses to communicate with the Data Broker\\npod (\\nitom-collect-once-data-broker\\n443\\nTCP\\nOperations\\nBridge\\nExternal Access to Operations Bridge Services, that's Autopass, Printing Service, BVD,\\nOPTIC Data Lake Health Insights, etc.\\n30001\\nTCP\\nOPTIC DL\\nHTTP Receiver\\nOPTIC Data Lake data receiver endpoint is used to receive data posted to it from data\\nsources such as Operations Agents.\\n30003\\nTCP\\nOPTIC Data\\nLake Data\\nAccess\\nOPTIC Data Lake data access endpoint is used to query the data via REST API.  Used by\\nPerformance Dashboard.\\n30004\\nTCP\\nOPTIC Data\\nLake\\nAdministration\\nOPTIC Data Lake administration endpoint used to configure OPTIC Data Lake.\\n30012\\nTCP\\nOPTIC Data\\nLake\\nData Ingestion Vertica Database port for external access.\\n5433\\nTCP\\nOPTIC Data\\nLake\\nThe default External Vertica port is 5433 for TLS/non-TLS. The Vertica administrator\\ncan change this default value.\\n31051\\nTCP\\nOPTIC Data\\nLake Message\\nBus\\nUsed for pushing topology from the RTSM to OPTIC Data Lake.\\n31001\\nTCP\\nOPTIC Data\\nLake Message\\nBus Admin\\nThe Pulsar Admin rest endpoint for management of the Message Bus.\\nDefault\\nPort \\nProtocol\\nService\\nDescription\\nNote:\\n The embedded Vertica requires a Vertica database port.\\nImportant:\\n The application requires two-way communication, the port\\nshould be open from Vertica (UDX Plugin) to Pulsar Admin endpoint (cluster)\\nand also from Pulsar Admin endpoint back to the UDX Plugin.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n168\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd449ee6f3e0de3d3a87b5fea81cfc1c0'}>,\n",
              "  <Document: {'content': 'Other Kubernetes distribution storage\\nNFS or any storage supported on an RKE cluster. For more information, see \\nRancher documentation\\n.\\nIf you use dynamic volume provisioning to create the volumes on demand, please make sure that your provisioner is able to\\noffer the storage with access mode \\nReadWriteOnce\\n and \\nReadWriteMany\\n. For more information on the access modes refer to\\nthe \\nKubernetes documentation\\n.\\nUID and GID\\nOparations Bridge requires a specific user and group to be created and associated with the User Identifier (UID) and Group\\nIdentifier (GID) which has a default value of 1999. If 1999 is unavailable, you can set the UID and GID to any value from\\n100000 to 2000000000.\\nFor application installation, the UID and GID must map to the value given during OMT installation.\\nOperating systems\\nEmbedded Kubernetes\\nThe following table details the operating systems supported for embedded Kubernetes.\\nOperating System\\nArchitecture Type\\nVersions\\nRed Hat Enterprise Linux\\nx86_64\\n7.8, 7.9\\n8.4, 8.6\\nCentOS\\nx86_64\\n 7.9\\nOracle Enterprise Linux\\nx86_64\\n7.8, 7.9\\n8.4, 8.5, 8.6\\nMake sure that you have these utilities installed on all the nodes - \\nm4\\n, \\nlsof\\n, \\nchrony\\n, \\nbind-utils\\n, \\nhaveged\\n, and \\nsystem-storage-mana\\nger\\n.\\nExternal Kubernetes\\nFor information on operating system versions supported for external Kubernetes listed in the\\n Kubernetes\\ndistributions\\n section, see the related Kubernetes documentation.\\nBrowsers\\nMozilla Firefox (64 bit) 91.x ESR\\nGoogle Chrome (64 bit) 89.x or later\\nApple Safari (64 bit) 15.x on macOS\\nMicrosoft Edge 89.x or later\\nFonts\\nThe following fonts are required for Stakeholder dashboards:\\nArial\\nMeiryo \\n(for Japanese locales)\\nMalgun Gothic\\n or \\nArial\\n (for Korean locales)\\nSimHei\\n or \\nSimSun\\n (for Simplified Chinese locales)\\nScreen resolution\\n1600 × 900 or higher (recommended)\\n1280 × 1024 (minimum screen resolution required)\\nLanguages\\nOperation Bridge supports the following languages:\\nSupported\\nlanguages\\nStakeholder\\nDashboard\\nAutoPass License\\nManager\\nIDM\\nOPTIC DL Health\\nInsights\\nBVD\\nReports\\nOBM\\nHyperscale\\nObservability\\nEnglish\\n✔\\n✔\\n✔\\n✔\\n✔\\n✔\\n✔\\nEnglish (UK)\\n✔\\nFrench\\n✔\\n✔\\n✔\\nGerman\\n✔\\n✔\\n✔\\nJapanese\\n✔\\n✔\\n✔\\nSpanish\\n✔\\n✔\\n✔\\nContainerized Operations Bridge 2022.11\\nPage \\n166\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Other Kubernetes distribution storage\\nNFS or any storage supported on an RKE cluster. For more information, see \\nRancher documentation\\n.\\nIf you use dynamic volume provisioning to create the volumes on demand, please make sure that your provisioner is able to\\noffer the storage with access mode \\nReadWriteOnce\\n and \\nReadWriteMany\\n. For more information on the access modes refer to\\nthe \\nKubernetes documentation\\n.\\nUID and GID\\nOparations Bridge requires a specific user and group to be created and associated with the User Identifier (UID) and Group\\nIdentifier (GID) which has a default value of 1999. If 1999 is unavailable, you can set the UID and GID to any value from\\n100000 to 2000000000.\\nFor application installation, the UID and GID must map to the value given during OMT installation.\\nOperating systems\\nEmbedded Kubernetes\\nThe following table details the operating systems supported for embedded Kubernetes.\\nOperating System\\nArchitecture Type\\nVersions\\nRed Hat Enterprise Linux\\nx86_64\\n7.8, 7.9\\n8.4, 8.6\\nCentOS\\nx86_64\\n 7.9\\nOracle Enterprise Linux\\nx86_64\\n7.8, 7.9\\n8.4, 8.5, 8.6\\nMake sure that you have these utilities installed on all the nodes - \\nm4\\n, \\nlsof\\n, \\nchrony\\n, \\nbind-utils\\n, \\nhaveged\\n, and \\nsystem-storage-mana\\nger\\n.\\nExternal Kubernetes\\nFor information on operating system versions supported for external Kubernetes listed in the\\n Kubernetes\\ndistributions\\n section, see the related Kubernetes documentation.\\nBrowsers\\nMozilla Firefox (64 bit) 91.x ESR\\nGoogle Chrome (64 bit) 89.x or later\\nApple Safari (64 bit) 15.x on macOS\\nMicrosoft Edge 89.x or later\\nFonts\\nThe following fonts are required for Stakeholder dashboards:\\nArial\\nMeiryo \\n(for Japanese locales)\\nMalgun Gothic\\n or \\nArial\\n (for Korean locales)\\nSimHei\\n or \\nSimSun\\n (for Simplified Chinese locales)\\nScreen resolution\\n1600 × 900 or higher (recommended)\\n1280 × 1024 (minimum screen resolution required)\\nLanguages\\nOperation Bridge supports the following languages:\\nSupported\\nlanguages\\nStakeholder\\nDashboard\\nAutoPass License\\nManager\\nIDM\\nOPTIC DL Health\\nInsights\\nBVD\\nReports\\nOBM\\nHyperscale\\nObservability\\nEnglish\\n✔\\n✔\\n✔\\n✔\\n✔\\n✔\\n✔\\nEnglish (UK)\\n✔\\nFrench\\n✔\\n✔\\n✔\\nGerman\\n✔\\n✔\\n✔\\nJapanese\\n✔\\n✔\\n✔\\nSpanish\\n✔\\n✔\\n✔\\nContainerized Operations Bridge 2022.11\\nPage \\n166\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90b4e7ec72028f7faba295b4ae603351'}>,\n",
              "  <Document: {'content': \"Korean\\n✔\\nRussian\\n✔\\nSimplified\\nChinese\\n✔\\n✔\\nSupported\\nlanguages\\nStakeholder\\nDashboard\\nAutoPass License\\nManager\\nIDM\\nOPTIC DL Health\\nInsights\\nBVD\\nReports\\nOBM\\nHyperscale\\nObservability\\nNetwork requirements\\nThe following table lists the minimum requirement and the recommended network latency for best performance and stability:\\nParameters\\nMinimum\\nrequirement\\nRecommended\\nNetwork round trip time between worker nodes\\nless than 10 ms\\nless than or equal\\nto 1 ms\\nNetwork round trip time between worker nodes and Vertica cluster nodes\\nless than 5 ms\\nless than or equal\\nto 1 ms\\nNetwork round trip time between worker nodes and external PostgreSQL or\\nOracle server\\nless than 5 ms\\nless than or equal\\nto 1 ms\\nNetwork identification\\nContainerized Operations Bridge supports IPv4 only but you can install it on a dual stack enabled (IPv4 and IPv6) server.\\nOpsBridge allows incoming connections using both IPv4/IPv6 address. This will enable data ingestion from operations agents or\\nother data collectors configured with an both IPv4/IPv6 address. \\nIPv4/IPv6 dual stack feature enables the cluster to connect to the external services (such as database, NFS storage server, and\\nimage registry) with both IPv4 and IPv6 protocols. And you can access OMT and application ingress services with both IPv4 and\\nIPv6. All the pods will assign both IPv4 and IPv6 addresses. For Services, set the value\\nof \\n.spec.ipFamilyPolicy\\n to \\nPreferDualStack\\n or \\nRequireDualStack\\n in YAML definition to configure IPv4/IPv6 dual stack.\\nFor OMT Services, only the ingress controller service is configured to assign both IPv4 and IPv6 addresses if dual stack\\nis enabled.\\nThe application doesn't require an HA virtual IP if you have a single master deployment. The FQDN of the master (control\\nplane) node acts as the external access host.\\nTo support dual stack feature on external Kubernetes, make sure the external Kubernetes support dual stack instead of\\npure IPv6. \\nCommunication connection on cloud\\nApplication install uses an external access host for some services. As a prerequisite, you must make sure to allow an L3 Bi\\ndirectional connection between the remote system and AWS or Azure setup to access application services. You can do this\\nusing a VPN connection or Direct Connect. If L3 communication isn't enabled between the remote system and AWS or Azure\\nsetup, you must create an environment in the same VPC or VNet to access application services. You must set up a private\\nDNS zone. \\nYou can either create a private domain or use an existing private domain in \\nRegister domain\\n. For steps to create a private\\ndomain, see the \\nAWS documentation\\n or \\nAzure documentation\\n. You will use this record name for the external access host\\nconfiguration.\\nIn AWS, you can either create a self-signed certificate or use an existing certificate. In the AWS Management Console, search\\nand click \\nCertificate Manager\\n. Import the certificate for the external access host. Once you upload the certificate, it will\\nappear as imported in the AWS Certificate manager tab. You will give this certificate details for\\nthe \\nserverCrt\\n and \\nserverKey\\n parameters when configuring the load balancer for OMT and application.\\nRequired Ports\\nServices can be exposed over a node port or a load balancer.\\nMake sure that these ports are available before you deploy the application in case you are deploying with service type Node\\nPort. These are default ports, each port can be configured to a different value in case one of the listed default port is in use.\\nDefault\\nPort \\nProtocol\\nService\\nDescription\\n383\\nTCP\\nOBM\\nPort used by OBM communications broker (\\novbbccb\\n) for receiving data from operations\\nagents and other OBM servers\\nNote: \\nEKS only supports pure IPv6.  For detailed information,\\nsee \\nhttps://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/\\n. So OMT doesn't support dual\\nstack on EKS for now.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n167\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Korean\\n✔\\nRussian\\n✔\\nSimplified\\nChinese\\n✔\\n✔\\nSupported\\nlanguages\\nStakeholder\\nDashboard\\nAutoPass License\\nManager\\nIDM\\nOPTIC DL Health\\nInsights\\nBVD\\nReports\\nOBM\\nHyperscale\\nObservability\\nNetwork requirements\\nThe following table lists the minimum requirement and the recommended network latency for best performance and stability:\\nParameters\\nMinimum\\nrequirement\\nRecommended\\nNetwork round trip time between worker nodes\\nless than 10 ms\\nless than or equal\\nto 1 ms\\nNetwork round trip time between worker nodes and Vertica cluster nodes\\nless than 5 ms\\nless than or equal\\nto 1 ms\\nNetwork round trip time between worker nodes and external PostgreSQL or\\nOracle server\\nless than 5 ms\\nless than or equal\\nto 1 ms\\nNetwork identification\\nContainerized Operations Bridge supports IPv4 only but you can install it on a dual stack enabled (IPv4 and IPv6) server.\\nOpsBridge allows incoming connections using both IPv4/IPv6 address. This will enable data ingestion from operations agents or\\nother data collectors configured with an both IPv4/IPv6 address. \\nIPv4/IPv6 dual stack feature enables the cluster to connect to the external services (such as database, NFS storage server, and\\nimage registry) with both IPv4 and IPv6 protocols. And you can access OMT and application ingress services with both IPv4 and\\nIPv6. All the pods will assign both IPv4 and IPv6 addresses. For Services, set the value\\nof \\n.spec.ipFamilyPolicy\\n to \\nPreferDualStack\\n or \\nRequireDualStack\\n in YAML definition to configure IPv4/IPv6 dual stack.\\nFor OMT Services, only the ingress controller service is configured to assign both IPv4 and IPv6 addresses if dual stack\\nis enabled.\\nThe application doesn't require an HA virtual IP if you have a single master deployment. The FQDN of the master (control\\nplane) node acts as the external access host.\\nTo support dual stack feature on external Kubernetes, make sure the external Kubernetes support dual stack instead of\\npure IPv6. \\nCommunication connection on cloud\\nApplication install uses an external access host for some services. As a prerequisite, you must make sure to allow an L3 Bi\\ndirectional connection between the remote system and AWS or Azure setup to access application services. You can do this\\nusing a VPN connection or Direct Connect. If L3 communication isn't enabled between the remote system and AWS or Azure\\nsetup, you must create an environment in the same VPC or VNet to access application services. You must set up a private\\nDNS zone. \\nYou can either create a private domain or use an existing private domain in \\nRegister domain\\n. For steps to create a private\\ndomain, see the \\nAWS documentation\\n or \\nAzure documentation\\n. You will use this record name for the external access host\\nconfiguration.\\nIn AWS, you can either create a self-signed certificate or use an existing certificate. In the AWS Management Console, search\\nand click \\nCertificate Manager\\n. Import the certificate for the external access host. Once you upload the certificate, it will\\nappear as imported in the AWS Certificate manager tab. You will give this certificate details for\\nthe \\nserverCrt\\n and \\nserverKey\\n parameters when configuring the load balancer for OMT and application.\\nRequired Ports\\nServices can be exposed over a node port or a load balancer.\\nMake sure that these ports are available before you deploy the application in case you are deploying with service type Node\\nPort. These are default ports, each port can be configured to a different value in case one of the listed default port is in use.\\nDefault\\nPort \\nProtocol\\nService\\nDescription\\n383\\nTCP\\nOBM\\nPort used by OBM communications broker (\\novbbccb\\n) for receiving data from operations\\nagents and other OBM servers\\nNote: \\nEKS only supports pure IPv6.  For detailed information,\\nsee \\nhttps://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/\\n. So OMT doesn't support dual\\nstack on EKS for now.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n167\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f29d575346c43405aa2e2f7dd5267a2c'}>,\n",
              "  <Document: {'content': 'scripts\\nAutoConfigAWS.sh\\nAutoConfigAzure.sh\\nbyok\\ncreate_aws_repositories.py\\nimage-transfer.py\\nissuecert\\nissuecert.sh\\nREADME.md\\nutility\\nstep\\nDBSQLGenerator.sh\\nfind-current-pod-logs.sh\\nguided-install\\nOpsbridgeGuidedInstall\\nresources\\nConfigureFirewallSettingsOmt.sh\\nConfigureFirewallSettings.sh\\nConfigureStorageProvisioner.sh\\nInstallPostgresInstance.sh\\nInstallVerticaInstance.sh\\nomt-db.sql\\npgdg-redhat-repo-latest.noarch.rpm\\nsilent-Config.json\\nSyncTime.sh\\ncustom-input.properties\\nleast-input.properties\\nitom-backup-restore-scripts\\nbackup.sh\\nrestore.sh\\ntools\\namc-benchmark-tool.zip\\nitom-di-pulsarudx-<version>.x86_64.rpm\\nDirectories/files\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n171\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.scripts\\nAutoConfigAWS.sh\\nAutoConfigAzure.sh\\nbyok\\ncreate_aws_repositories.py\\nimage-transfer.py\\nissuecert\\nissuecert.sh\\nREADME.md\\nutility\\nstep\\nDBSQLGenerator.sh\\nfind-current-pod-logs.sh\\nguided-install\\nOpsbridgeGuidedInstall\\nresources\\nConfigureFirewallSettingsOmt.sh\\nConfigureFirewallSettings.sh\\nConfigureStorageProvisioner.sh\\nInstallPostgresInstance.sh\\nInstallVerticaInstance.sh\\nomt-db.sql\\npgdg-redhat-repo-latest.noarch.rpm\\nsilent-Config.json\\nSyncTime.sh\\ncustom-input.properties\\nleast-input.properties\\nitom-backup-restore-scripts\\nbackup.sh\\nrestore.sh\\ntools\\namc-benchmark-tool.zip\\nitom-di-pulsarudx-<version>.x86_64.rpm\\nDirectories/files\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n171\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3f5b4463a78913200be04298a7e67bad'}>,\n",
              "  <Document: {'content': \"It's recommended that you obtain a new copy of the chart before proceeding.\\nDownload and extract the Vertica package\\nYou will need Vertica database if you plan to use the OPTIC Reporting, AEC, and Hyperscale Observability capabilities. If you\\nhaven't installed Vertica, follow these steps to download the installer files.\\n1\\n. \\nDownload the \\nVertica installer\\n \\n.\\nrpm\\n file \\nand \\nlicense file \\nfrom the \\nSoftware Licenses and\\nDownloads\\n website (requires Passport credentials), if not downloaded already. \\nYou will get the Vertica license key when you request a key for the \\nContainerized Operations Bridge\\nPremium/Ultimate.\\n2\\n. \\nCopy the installer and the license to a temporary directory, such as \\n/tmp\\n, on the system where you will install Vertica.\\nDirectory structure of the \\u200b\\u200b\\u200b\\u200b\\u200bapplication installer package \\nThe unzipped file contains the following directories and files under \\nopsbridge-suite-chart\\n:\\nDirectories/files\\nDescription\\ncharts\\nApplication install charts (\\nDO NOT extract\\n):  \\nopsbridge-suite-<version>.tgz\\nopsbridge-suite-<version>.tgz.prov\\ndeployment\\nExtra-Large-Deployment.yaml\\nLarge-Deployment.yaml\\n  \\nMedium-Deployment.yaml\\n  \\nEvaluation-Deployment.yaml\\nLow-Footprint-OpsB-Reporting.yaml\\nsamples\\naws\\nvalues.yaml\\nazure\\nvalues.yaml\\nopenshift\\nvalues.yaml\\ngeneric\\nvalues.yaml\\ngenericPVC.yaml\\ngenericPV.yaml\\nMultiDeploymentSample.yaml\\nvalues.yaml\\nitom-opsb-scc.yaml\\nFilled samples for:\\nExternalOracleSample.yaml\\nExternalPostgresSample.yaml\\nEvaluationSample.yaml\\nContainerized Operations Bridge 2022.11\\nPage \\n170\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.It's recommended that you obtain a new copy of the chart before proceeding.\\nDownload and extract the Vertica package\\nYou will need Vertica database if you plan to use the OPTIC Reporting, AEC, and Hyperscale Observability capabilities. If you\\nhaven't installed Vertica, follow these steps to download the installer files.\\n1\\n. \\nDownload the \\nVertica installer\\n \\n.\\nrpm\\n file \\nand \\nlicense file \\nfrom the \\nSoftware Licenses and\\nDownloads\\n website (requires Passport credentials), if not downloaded already. \\nYou will get the Vertica license key when you request a key for the \\nContainerized Operations Bridge\\nPremium/Ultimate.\\n2\\n. \\nCopy the installer and the license to a temporary directory, such as \\n/tmp\\n, on the system where you will install Vertica.\\nDirectory structure of the \\u200b\\u200b\\u200b\\u200b\\u200bapplication installer package \\nThe unzipped file contains the following directories and files under \\nopsbridge-suite-chart\\n:\\nDirectories/files\\nDescription\\ncharts\\nApplication install charts (\\nDO NOT extract\\n):  \\nopsbridge-suite-<version>.tgz\\nopsbridge-suite-<version>.tgz.prov\\ndeployment\\nExtra-Large-Deployment.yaml\\nLarge-Deployment.yaml\\n  \\nMedium-Deployment.yaml\\n  \\nEvaluation-Deployment.yaml\\nLow-Footprint-OpsB-Reporting.yaml\\nsamples\\naws\\nvalues.yaml\\nazure\\nvalues.yaml\\nopenshift\\nvalues.yaml\\ngeneric\\nvalues.yaml\\ngenericPVC.yaml\\ngenericPV.yaml\\nMultiDeploymentSample.yaml\\nvalues.yaml\\nitom-opsb-scc.yaml\\nFilled samples for:\\nExternalOracleSample.yaml\\nExternalPostgresSample.yaml\\nEvaluationSample.yaml\\nContainerized Operations Bridge 2022.11\\nPage \\n170\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1981358b1da959028240155bd1c1813e'}>,\n",
              "  <Document: {'content': 'Prepare infrastructure\\nThis topic gives you information on infrastructure specifications to prepare the infrastructure for external or embedded\\nKubernetes distributions. To deploy the application, you must install the required infrastructure and create a cluster. \\n Prepare the required infrastructure\\nAfter you obtain the required resources you must configure the infrastructure for deployment of the applications. The following\\nsteps to prepare the infrastructure differs based on whether you are deploying on embedded Kubernetes or external\\nKubernetes:\\nPrepare infrastructure manually and install OMT for embedded Kubernetes\\nPrepare infrastructure and install OMT using guided install for embedded Kubernetes\\nPrepare infrastructure manually and install OMT for external Kubernetes\\nPrepare infrastructure using the toolkit and install OMT for external Kubernetes\\nNote:\\n Skip the Prepare Infrastructure section if you already have your infrastructure, cluster set up, and\\ninstalled OMT. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n172\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Prepare infrastructure\\nThis topic gives you information on infrastructure specifications to prepare the infrastructure for external or embedded\\nKubernetes distributions. To deploy the application, you must install the required infrastructure and create a cluster. \\n Prepare the required infrastructure\\nAfter you obtain the required resources you must configure the infrastructure for deployment of the applications. The following\\nsteps to prepare the infrastructure differs based on whether you are deploying on embedded Kubernetes or external\\nKubernetes:\\nPrepare infrastructure manually and install OMT for embedded Kubernetes\\nPrepare infrastructure and install OMT using guided install for embedded Kubernetes\\nPrepare infrastructure manually and install OMT for external Kubernetes\\nPrepare infrastructure using the toolkit and install OMT for external Kubernetes\\nNote:\\n Skip the Prepare Infrastructure section if you already have your infrastructure, cluster set up, and\\ninstalled OMT. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n172\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '249eb1916ba76046d793a7bb552f4c87'}>,\n",
              "  <Document: {'content': \"Download the installer\\nThis topic provides you with the steps to download and extract the application installer package, Verica database installer rpm,\\nverify the public key for helm chart signing and validate the chart signature.\\nIt also lists the extracted installer package directory structure\\u200b\\u200b\\u200b\\u200b\\u200b.\\nDownload and extract the application installer package\\n1\\n. \\nGo to the \\nSoftware Licenses and Downloads\\n website\\n2\\n. \\nDownload the Operations Bridge Helm chart, with binary name \\nopsbridge-suite-chart-<version>.zip.\\n3\\n. \\nYou must accept the terms and conditions to download the application package from the website. \\n4\\n. \\nRun the following command to unzip the \\nopsbridge-suite-chart-\\n<version>\\n.zip\\n file\\n:\\nunzip opsbridge-suite-chart-<version>.zip\\nVerify application package\\nVerify Operations Bridge application installation and upgrade package. Skip this step if you don't want to verify the package:\\n1\\n. \\nUsing a web browser, visit the \\nMicro Focus Software Licenses and Downloads\\n website. Agree to the terms and conditions,\\nand then download the \\nMF_public_keys.tar.gz\\n package to a local directory.\\n2\\n. \\nRun the following commands to extract the public keys from the package:\\ngunzip MF_public_keys.tar.gz\\ntar -xvf MF_public_keys.tar\\n3\\n. \\nCopy the public key \\nhelm-public-key.asc\\n to a local folder.\\n4\\n. \\nAdd the public key to the \\nGPG\\n keyring:\\ngpg --import helm-public-key.asc\\nOn machines that use \\nkbx\\n format, you may get an error as follows:\\ngpg: no valid OpenPGP data found.\\ngpg: Total number processed: 0\\nIf you get the following error, export the key as \\npubring.gpg\\n:\\ngpg --export > ~/.gnupg/pubring.gpg\\n5\\n. \\nNavigate to the\\n $HOME/opsbridge-suite-chart/charts\\n directory:\\ncd $HOME/opsbridge-suite-chart/charts\\n6\\n. \\nRun the following command to verify the signature of the helm chart files:\\nhelm verify <chart-name>.tgz\\n7\\n. \\nYou will get a message similar to the following message indicating successful verification:\\nSigned by: Micro Focus Group Limited (GPG Key for Helm Chart Signing) xxxxx@microfocus.com\\n  Using Key With Fingerprint: xxxxx\\n  Chart Hash Verified: sha256:xxxxx\\nOPTIC AppHub validates Helm charts to ensure that they're digitally signed and aren't tampered with or corrupted.  If a chart\\nfails signature validation, OPTIC AppHub displays a warning on the application tile. If there is no warning on the tile, the chart's\\nsignature validation is successful and you can deploy it.\\nDigital verification failed\\nIf a chart is digitally signed, but the chart's \\nSHA\\n checksum doesn't match the checksum in the chart’s provenance file,\\nsomeone may have tampered with the chart. In this scenario, OPTIC AppHub displays the \\nDigital verification failed\\n message. This\\nmeans OPTIC AppHub can't guarantee the authenticity of the chart and you won’t be able to deploy the application.  \\nIt's recommended that you obtain a new copy of the chart before proceeding.\\nMissing digital signature\\nIf a chart’s provenance file is missing, OPTIC AppHub can’t validate the digital signature. In this scenario, OPTIC AppHub\\ndisplays the \\nMissing Digital Signature\\n message. This means OPTIC AppHub couldn’t find the provenance file in the chart directory\\nbecause it's not an official chart or it may have been removed to circumvent the authenticity check. You can still deploy an\\nunsigned chart, but you must acknowledge that you accept the risks associated with deploying a chart that isn't verified.\\nContainerized Operations Bridge 2022.11\\nPage \\n169\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Download the installer\\nThis topic provides you with the steps to download and extract the application installer package, Verica database installer rpm,\\nverify the public key for helm chart signing and validate the chart signature.\\nIt also lists the extracted installer package directory structure\\u200b\\u200b\\u200b\\u200b\\u200b.\\nDownload and extract the application installer package\\n1\\n. \\nGo to the \\nSoftware Licenses and Downloads\\n website\\n2\\n. \\nDownload the Operations Bridge Helm chart, with binary name \\nopsbridge-suite-chart-<version>.zip.\\n3\\n. \\nYou must accept the terms and conditions to download the application package from the website. \\n4\\n. \\nRun the following command to unzip the \\nopsbridge-suite-chart-\\n<version>\\n.zip\\n file\\n:\\nunzip opsbridge-suite-chart-<version>.zip\\nVerify application package\\nVerify Operations Bridge application installation and upgrade package. Skip this step if you don't want to verify the package:\\n1\\n. \\nUsing a web browser, visit the \\nMicro Focus Software Licenses and Downloads\\n website. Agree to the terms and conditions,\\nand then download the \\nMF_public_keys.tar.gz\\n package to a local directory.\\n2\\n. \\nRun the following commands to extract the public keys from the package:\\ngunzip MF_public_keys.tar.gz\\ntar -xvf MF_public_keys.tar\\n3\\n. \\nCopy the public key \\nhelm-public-key.asc\\n to a local folder.\\n4\\n. \\nAdd the public key to the \\nGPG\\n keyring:\\ngpg --import helm-public-key.asc\\nOn machines that use \\nkbx\\n format, you may get an error as follows:\\ngpg: no valid OpenPGP data found.\\ngpg: Total number processed: 0\\nIf you get the following error, export the key as \\npubring.gpg\\n:\\ngpg --export > ~/.gnupg/pubring.gpg\\n5\\n. \\nNavigate to the\\n $HOME/opsbridge-suite-chart/charts\\n directory:\\ncd $HOME/opsbridge-suite-chart/charts\\n6\\n. \\nRun the following command to verify the signature of the helm chart files:\\nhelm verify <chart-name>.tgz\\n7\\n. \\nYou will get a message similar to the following message indicating successful verification:\\nSigned by: Micro Focus Group Limited (GPG Key for Helm Chart Signing) xxxxx@microfocus.com\\n  Using Key With Fingerprint: xxxxx\\n  Chart Hash Verified: sha256:xxxxx\\nOPTIC AppHub validates Helm charts to ensure that they're digitally signed and aren't tampered with or corrupted.  If a chart\\nfails signature validation, OPTIC AppHub displays a warning on the application tile. If there is no warning on the tile, the chart's\\nsignature validation is successful and you can deploy it.\\nDigital verification failed\\nIf a chart is digitally signed, but the chart's \\nSHA\\n checksum doesn't match the checksum in the chart’s provenance file,\\nsomeone may have tampered with the chart. In this scenario, OPTIC AppHub displays the \\nDigital verification failed\\n message. This\\nmeans OPTIC AppHub can't guarantee the authenticity of the chart and you won’t be able to deploy the application.  \\nIt's recommended that you obtain a new copy of the chart before proceeding.\\nMissing digital signature\\nIf a chart’s provenance file is missing, OPTIC AppHub can’t validate the digital signature. In this scenario, OPTIC AppHub\\ndisplays the \\nMissing Digital Signature\\n message. This means OPTIC AppHub couldn’t find the provenance file in the chart directory\\nbecause it's not an official chart or it may have been removed to circumvent the authenticity check. You can still deploy an\\nunsigned chart, but you must acknowledge that you accept the risks associated with deploying a chart that isn't verified.\\nContainerized Operations Bridge 2022.11\\nPage \\n169\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd268082cd89c8961de36d1e31df0bd5'}>,\n",
              "  <Document: {'content': \"Prepare infrastructure and install OMT using guided\\ninstall for embedded Kubernetes\\nThis topic provides information about the simplified approach to prepare the infrastructure, install OMT, and configure\\nthe application prerequisites through the guided install script.\\nThis script automates infrastructure preparation and application prerequisites and gives you a quick start on the deployment.\\nThe script is modular and enables you to prepare only the infrastructure that you require. This includes:\\nCreating the Embedded Kubernetes cluster provided by OMT.\\nSetting up the PostgreSQL database server with TLS and creating the required databases for both OPTIC Management\\nToolkit (OMT). The application databases are created during the application deployment based on the capabilities that you\\nchoose.\\nSetting up the NFS server. The OMT NFS provisioner is used as the storage provisioner to create the volumes.\\nCreating Local Persistent Volumes (LPV) for OPTIC Data Lake. You must use a single separate disk device for all the LPVs\\non each worker node to create the Local persistent volumes\\nInstalling and configuring the Vertica database with ENFORCED TLS enabled. For information about the application\\nsupported Vertica versions, see \\nSystem requirements\\n.\\nAfter preparing the infrastructure using the guided install, you can complete the additional prerequisites and deploy the\\napplication using AppHub UI.\\nWhile the guided install script uses the default values for most of the parameters, you can customize the following deployment\\nparameters:\\nDeployment size: You can change the deployment size based on your requirement.\\nCertificates: You can use the existing certificates or certificates created by the guided install script. \\nPostgreSQL instance: You can use an existing PostgreSQL instance. The guided install script creates the OMT databases. \\nApplication capabilities: You can select the application capabilities according to your requirements.\\nBefore you begin\\n1\\n. \\nReview the Operations Bridge capabilities and decide which capabilities you want to deploy.\\n2\\n. \\nUse the sizing calculator to estimate the resource requirement. Based on the estimates obtain the resources.\\n3\\n. \\nCreate and configure a single raw disk. The disk size must be the sum of individual LPVs (as provided by the sizing\\ncalculator) and an additional 10% (minimum) space. For example, if the size of LPV recommended for the ledger is 100\\nGB, BookKeeper and ZooKeeper is 20 GB each, add a disk of size 160 GB. \\nMake sure the new disk isn't mounted and\\ndoesn't have a partition created.\\nPrerequisites for the guided install script\\nPerform these prerequisites before you run the guided install script:\\nMake sure additional disk is available\\nRun the \\nlsblk\\n command to list the new empty disk. The result will appear similar to the following. In this example the \\nsdb\\n disk\\nis the additional disk and isn't mounted:\\n1. # lsblk\\n2. NAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\\n3. sda            8:0    0  200G  0 disk\\n4. ├─sda1         8:1    0    2G  0 part /boot\\n5. └─sda2         8:2    0  198G  0 part\\n6.   ├─vgs-root 253:0    0  185G  0 lvm  /\\n7.   ├─vgs-swap 253:1    0    8G  0 lvm\\n8.   └─vgs-home 253:2    0    5G  0 lvm  /home\\n9. sdb            8:16   0  210G  0 disk\\n10. sr0           11:0    1 1024M  0 rom\\nIf the disk isn't listed, obtain a disk according to the sizing calculator.\\nCheck the connection and authentication settings\\n1\\n. \\nObtain the FQDN of the nodes used in the deployment\\n2\\n. \\nCheck access to all the repositories configured in \\nyum\\n. The dependent libraries of the guided install script are downloaded\\nusing \\nyum\\n.\\n3\\n. \\nMake sure you have access to the time server to perform time synchronization for all the nodes.\\n4\\n. \\nCheck if you are able to connect to the control plane, worker, Postgres, NFS, and Vertica nodes using the same\\ncredentials using \\nssh\\n from the node where you will run the \\nOpsbridgeGuidedInstall\\n script. Otherwise, the installer won't be\\nable to connect to any nodes. \\n5\\n. \\nCheck if you are able to run commands as the root user and all permission for the commands are added to \\n/etc/sudoers\\n file\\nNote:\\n \\nThe guided installer exposes network ports mentioned in \\nCheck that the required ports are open\\n(embedded K8s)\\n, \\nfirewall settings\\n for OMT, and \\nService ports required for Operations Bridge\\n during the application\\ndeployment. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n174\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Prepare infrastructure and install OMT using guided\\ninstall for embedded Kubernetes\\nThis topic provides information about the simplified approach to prepare the infrastructure, install OMT, and configure\\nthe application prerequisites through the guided install script.\\nThis script automates infrastructure preparation and application prerequisites and gives you a quick start on the deployment.\\nThe script is modular and enables you to prepare only the infrastructure that you require. This includes:\\nCreating the Embedded Kubernetes cluster provided by OMT.\\nSetting up the PostgreSQL database server with TLS and creating the required databases for both OPTIC Management\\nToolkit (OMT). The application databases are created during the application deployment based on the capabilities that you\\nchoose.\\nSetting up the NFS server. The OMT NFS provisioner is used as the storage provisioner to create the volumes.\\nCreating Local Persistent Volumes (LPV) for OPTIC Data Lake. You must use a single separate disk device for all the LPVs\\non each worker node to create the Local persistent volumes\\nInstalling and configuring the Vertica database with ENFORCED TLS enabled. For information about the application\\nsupported Vertica versions, see \\nSystem requirements\\n.\\nAfter preparing the infrastructure using the guided install, you can complete the additional prerequisites and deploy the\\napplication using AppHub UI.\\nWhile the guided install script uses the default values for most of the parameters, you can customize the following deployment\\nparameters:\\nDeployment size: You can change the deployment size based on your requirement.\\nCertificates: You can use the existing certificates or certificates created by the guided install script. \\nPostgreSQL instance: You can use an existing PostgreSQL instance. The guided install script creates the OMT databases. \\nApplication capabilities: You can select the application capabilities according to your requirements.\\nBefore you begin\\n1\\n. \\nReview the Operations Bridge capabilities and decide which capabilities you want to deploy.\\n2\\n. \\nUse the sizing calculator to estimate the resource requirement. Based on the estimates obtain the resources.\\n3\\n. \\nCreate and configure a single raw disk. The disk size must be the sum of individual LPVs (as provided by the sizing\\ncalculator) and an additional 10% (minimum) space. For example, if the size of LPV recommended for the ledger is 100\\nGB, BookKeeper and ZooKeeper is 20 GB each, add a disk of size 160 GB. \\nMake sure the new disk isn't mounted and\\ndoesn't have a partition created.\\nPrerequisites for the guided install script\\nPerform these prerequisites before you run the guided install script:\\nMake sure additional disk is available\\nRun the \\nlsblk\\n command to list the new empty disk. The result will appear similar to the following. In this example the \\nsdb\\n disk\\nis the additional disk and isn't mounted:\\n1. # lsblk\\n2. NAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\\n3. sda            8:0    0  200G  0 disk\\n4. ├─sda1         8:1    0    2G  0 part /boot\\n5. └─sda2         8:2    0  198G  0 part\\n6.   ├─vgs-root 253:0    0  185G  0 lvm  /\\n7.   ├─vgs-swap 253:1    0    8G  0 lvm\\n8.   └─vgs-home 253:2    0    5G  0 lvm  /home\\n9. sdb            8:16   0  210G  0 disk\\n10. sr0           11:0    1 1024M  0 rom\\nIf the disk isn't listed, obtain a disk according to the sizing calculator.\\nCheck the connection and authentication settings\\n1\\n. \\nObtain the FQDN of the nodes used in the deployment\\n2\\n. \\nCheck access to all the repositories configured in \\nyum\\n. The dependent libraries of the guided install script are downloaded\\nusing \\nyum\\n.\\n3\\n. \\nMake sure you have access to the time server to perform time synchronization for all the nodes.\\n4\\n. \\nCheck if you are able to connect to the control plane, worker, Postgres, NFS, and Vertica nodes using the same\\ncredentials using \\nssh\\n from the node where you will run the \\nOpsbridgeGuidedInstall\\n script. Otherwise, the installer won't be\\nable to connect to any nodes. \\n5\\n. \\nCheck if you are able to run commands as the root user and all permission for the commands are added to \\n/etc/sudoers\\n file\\nNote:\\n \\nThe guided installer exposes network ports mentioned in \\nCheck that the required ports are open\\n(embedded K8s)\\n, \\nfirewall settings\\n for OMT, and \\nService ports required for Operations Bridge\\n during the application\\ndeployment. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n174\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'af431c4e41050c16bac226e99859cdf7'}>,\n",
              "  <Document: {'content': \"Prepare infrastructure and install OMT with embedded\\nKubernetes\\nThis topic provides you with the information on infrastructure specifications to prepare the infrastructure for embedded\\nKubernetes distributions manually.\\nTo deploy your application on-premises and use the embedded Kubernetes provided by OMT, you must install OMT and create\\nthe Kubernetes cluster. For more information, see \\nInstall OMT\\n. This link takes you to the OMT documentation. In the OMT\\ndocumentation, follow the steps required only for embedded Kubernetes.\\nAfter you install OMT, complete the following application specific prerequisites.\\nDatabase related infrastructure\\nPostgreSQL\\nFor a shared OPTIC Data Lake deployment, it's recommended to use the same external PostgreSQL server as the provider\\napplication to create the additional database for Operations Bridge.\\nFor a multi application deployment, it's recommended to use a separate external PostgreSQL server for the databases\\nrequired for the second application.\\nVertica\\nFor capabilities that use OPTIC Data Lake, you must install and configure the Vertica database. You can install and configure\\nVertica later as part of setting up OPerations Bridge prerequisites.\\nIn a shared OPTIC Data Lake deployment, it's recommended to use the same external Vertica server set up by the provider\\napplication. \\nIn a multi suite deployment, it's recommended to use a separate external Vertica server for the databases required for the\\nsecond application.\\nStorage related infrastructure\\nObtain local disks to mount persistent volumes\\nObtain local disks to mount persistent volumes locally for capabilities that use OPTIC Data Lake. Decide on your local storage\\nstrategy based one the following guidelines:\\nConfigure separate disk devices for each LPV on every worker node\\n: Using three separate disk devices per\\nworker node, one for each LPV shown by the sizing calculator, is recommended for best performance. You will add empty\\ndisk devices to all your worker systems, each sized so that enough free space will result when a filesystem is installed to\\naccommodate the output from the sizing calculator for the \\nledger, journal, zookeeper\\n LPVs according to your expected scale.\\nUse a single separate disk device for all LPVs on every worker node\\n: One separate disk device common for all\\nLPVs on each worker node is also supported. You will add one empty disk device on your worker systems which should be\\nsized for filesystem free space more than the sum of the sizes for the\\n ledger, journal, zookeeper\\n LPVs, based on the output\\nfrom the sizing calculator.\\nUse the space under one of your existing worker disks\\n: Using an existing disk device to mount LPVs is supported\\nfor non-production deployments and Low footprint OPTIC Reporting deployment only. Note that using an existing\\nfilesystem may have lower performance, and space utilization of the LPVs could impact the system or applications after\\nsome time.\\nNetwork related infrastructure\\nConfigure network ports for the application\\nNote: Skip this task, if you have already completed configuring the network ports for the application.\\nServices can be exposed over a node port or a load balancer. Make sure that these ports are available before you deploy the\\napplication in case you are deploying with service type Node Port. For more information about the required ports, see \\nNetwork\\nrequirement\\n section in \\nSystem requirements\\n. \\nContainerized Operations Bridge 2022.11\\nPage \\n173\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Prepare infrastructure and install OMT with embedded\\nKubernetes\\nThis topic provides you with the information on infrastructure specifications to prepare the infrastructure for embedded\\nKubernetes distributions manually.\\nTo deploy your application on-premises and use the embedded Kubernetes provided by OMT, you must install OMT and create\\nthe Kubernetes cluster. For more information, see \\nInstall OMT\\n. This link takes you to the OMT documentation. In the OMT\\ndocumentation, follow the steps required only for embedded Kubernetes.\\nAfter you install OMT, complete the following application specific prerequisites.\\nDatabase related infrastructure\\nPostgreSQL\\nFor a shared OPTIC Data Lake deployment, it's recommended to use the same external PostgreSQL server as the provider\\napplication to create the additional database for Operations Bridge.\\nFor a multi application deployment, it's recommended to use a separate external PostgreSQL server for the databases\\nrequired for the second application.\\nVertica\\nFor capabilities that use OPTIC Data Lake, you must install and configure the Vertica database. You can install and configure\\nVertica later as part of setting up OPerations Bridge prerequisites.\\nIn a shared OPTIC Data Lake deployment, it's recommended to use the same external Vertica server set up by the provider\\napplication. \\nIn a multi suite deployment, it's recommended to use a separate external Vertica server for the databases required for the\\nsecond application.\\nStorage related infrastructure\\nObtain local disks to mount persistent volumes\\nObtain local disks to mount persistent volumes locally for capabilities that use OPTIC Data Lake. Decide on your local storage\\nstrategy based one the following guidelines:\\nConfigure separate disk devices for each LPV on every worker node\\n: Using three separate disk devices per\\nworker node, one for each LPV shown by the sizing calculator, is recommended for best performance. You will add empty\\ndisk devices to all your worker systems, each sized so that enough free space will result when a filesystem is installed to\\naccommodate the output from the sizing calculator for the \\nledger, journal, zookeeper\\n LPVs according to your expected scale.\\nUse a single separate disk device for all LPVs on every worker node\\n: One separate disk device common for all\\nLPVs on each worker node is also supported. You will add one empty disk device on your worker systems which should be\\nsized for filesystem free space more than the sum of the sizes for the\\n ledger, journal, zookeeper\\n LPVs, based on the output\\nfrom the sizing calculator.\\nUse the space under one of your existing worker disks\\n: Using an existing disk device to mount LPVs is supported\\nfor non-production deployments and Low footprint OPTIC Reporting deployment only. Note that using an existing\\nfilesystem may have lower performance, and space utilization of the LPVs could impact the system or applications after\\nsome time.\\nNetwork related infrastructure\\nConfigure network ports for the application\\nNote: Skip this task, if you have already completed configuring the network ports for the application.\\nServices can be exposed over a node port or a load balancer. Make sure that these ports are available before you deploy the\\napplication in case you are deploying with service type Node Port. For more information about the required ports, see \\nNetwork\\nrequirement\\n section in \\nSystem requirements\\n. \\nContainerized Operations Bridge 2022.11\\nPage \\n173\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6084485025ec39837ec547143785ea1e'}>,\n",
              "  <Document: {'content': 'Download the installer packages\\n1\\n. \\nDownload the \\ndownloaded the installer\\n. Unzip the application installer package. The guided install script is available at \\n<e\\nxtracted application installer packages>/<application-chart>/scripts/guided-install\\n directory.\\n2\\n. \\nDownload the Vertica RPM and place it in the \\n<extracted application installer packages>/<application-chart>/scripts/guided-install/res\\nources\\n folder.\\n3\\n. \\nDownload the \\nOMT installer\\n. \\n(Optional)\\n If any of your control plane or worker node servers are in different subnets. Unzip\\nthe OMT installer package. Open the \\ninstall.properties\\n file in a text editor. The file is in the first level of the installation\\npackage. Set the FLANNEL_BACKEND_TYPE parameter to \\nvxlan\\n. Save the file in the same location.\\nGather required certificates\\nThe guided installer creates all the certificates required by the deployment. \\nIf you don’t want to use the certificates created by the guided installer, you can use the certificates generated by a Certificate\\nAuthority. Make sure you are using certificates generated by one of the methods and don’t mix both the certificates\\nConfigure Vertica servers\\nSkip this task if you plan to deploy Vertica on a single node cluster.\\nFor Vertica deployment on a multi node cluster, complete these prerequisites on all the Vertica nodes to enable passwordless\\naccess:\\n1\\n. \\nCreate \\ndbadmin\\n user and add it to \\nverticadba\\n group.\\n2\\n. \\nOpen the \\n/etc/sudoers\\n file, update the following as the last line: \\ndbadmin ALL=(ALL) NOPASSWD: ALL\\n3\\n. \\nRun the command \\nsudo passwd dbadmin\\n to update the password for \\ndbadmin\\n user, make sure the password is the same as\\nthe password given for sudo/root user.\\n4\\n. \\nEnable passwordless SSH for the \\ndbadmin\\n user.\\n5\\n. \\nIdentify the node from where you installed Vertica. This node must be the first node in the comma separated Vertica\\nnodes in the \\nguided-install\\n directory properties file.\\nFor example: \\nvertica: main,othernode1,othernode2\\n6\\n. \\nRun the following commands on the main node:\\nsu dbadmin\\nssh-keygen\\n \\nKeep the Passphrase empty and if you are generating the keys at any location other than the default, update the path in\\nthe next step.\\nssh-copy-id -i /home/dbadmin/.ssh/id_rsa.pub dbadmin@<other_vertica_nodes>\\nFollow the instruction given on the console.\\n7\\n. \\nConnect to each node and check that you are able to connect to these servers without providing a password.\\nEdit the properties file \\nThis task is optional.\\nYou can give inputs to the guided install script in the properties file before you run the script or when prompted during\\nruntime.\\nThere are two properties files available at  <\\nextracted application installer packages>/<application-chart>/scripts/guided-install\\n directory.\\nThe \\nOpsbridgeGuidedInstall\\n script derives the values to deploy from either the console or the properties file. You can type these\\nvalues on the prompt while you run the script or edit the \\nproperties\\n file in the location <\\nextracted application installer packages>/<ap\\nplication-chart>/scripts/guided-install\\n. You can edit the file in the following two ways based on your deployment requirement:\\nleast-input.properties\\n: Properties file requiring minimal inputs. If you choose this file, Kubernetes, Postgres, Vertica, with\\ncertificates generated by the guided installer, LPV are set up by the guided installer. You will use the self-signed\\ncertificates, and use the PostgreSQL server set up by the script.\\ncustom-input.properties\\n: Properties file which contains the complete list of parameters that you need to configure. If you\\nchoose this file, you can use the CA signed certificates, an existing PostgreSQL instance, and choose the required\\nmodules.\\nLeast or default input installation\\nYou must type the values in the \\n<application-chart>/scripts/guided-install/\\nleast-input.properties\\n file.\\nThe following table lists the parameters and the description for the \\nleast-input.properties\\n file:\\ntypicalInstallUserConfig\\nParameters\\nDescription\\nomtAdminUser\\nPassword\\n OMT administrator password in form of plain text. The OMT services like AppHub, \\ncdfctl\\n are accessed\\nusing this password. \\nNote\\n:\\nThe guided installer exposes Vertica ports mentioned in the \\nVertica documentation\\n for the application\\ndeployment. \\n The guided installer script does not apply license for Vertica, you must apply license after completing the script\\nexecution.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n175\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Download the installer packages\\n1\\n. \\nDownload the \\ndownloaded the installer\\n. Unzip the application installer package. The guided install script is available at \\n<e\\nxtracted application installer packages>/<application-chart>/scripts/guided-install\\n directory.\\n2\\n. \\nDownload the Vertica RPM and place it in the \\n<extracted application installer packages>/<application-chart>/scripts/guided-install/res\\nources\\n folder.\\n3\\n. \\nDownload the \\nOMT installer\\n. \\n(Optional)\\n If any of your control plane or worker node servers are in different subnets. Unzip\\nthe OMT installer package. Open the \\ninstall.properties\\n file in a text editor. The file is in the first level of the installation\\npackage. Set the FLANNEL_BACKEND_TYPE parameter to \\nvxlan\\n. Save the file in the same location.\\nGather required certificates\\nThe guided installer creates all the certificates required by the deployment. \\nIf you don’t want to use the certificates created by the guided installer, you can use the certificates generated by a Certificate\\nAuthority. Make sure you are using certificates generated by one of the methods and don’t mix both the certificates\\nConfigure Vertica servers\\nSkip this task if you plan to deploy Vertica on a single node cluster.\\nFor Vertica deployment on a multi node cluster, complete these prerequisites on all the Vertica nodes to enable passwordless\\naccess:\\n1\\n. \\nCreate \\ndbadmin\\n user and add it to \\nverticadba\\n group.\\n2\\n. \\nOpen the \\n/etc/sudoers\\n file, update the following as the last line: \\ndbadmin ALL=(ALL) NOPASSWD: ALL\\n3\\n. \\nRun the command \\nsudo passwd dbadmin\\n to update the password for \\ndbadmin\\n user, make sure the password is the same as\\nthe password given for sudo/root user.\\n4\\n. \\nEnable passwordless SSH for the \\ndbadmin\\n user.\\n5\\n. \\nIdentify the node from where you installed Vertica. This node must be the first node in the comma separated Vertica\\nnodes in the \\nguided-install\\n directory properties file.\\nFor example: \\nvertica: main,othernode1,othernode2\\n6\\n. \\nRun the following commands on the main node:\\nsu dbadmin\\nssh-keygen\\n \\nKeep the Passphrase empty and if you are generating the keys at any location other than the default, update the path in\\nthe next step.\\nssh-copy-id -i /home/dbadmin/.ssh/id_rsa.pub dbadmin@<other_vertica_nodes>\\nFollow the instruction given on the console.\\n7\\n. \\nConnect to each node and check that you are able to connect to these servers without providing a password.\\nEdit the properties file \\nThis task is optional.\\nYou can give inputs to the guided install script in the properties file before you run the script or when prompted during\\nruntime.\\nThere are two properties files available at  <\\nextracted application installer packages>/<application-chart>/scripts/guided-install\\n directory.\\nThe \\nOpsbridgeGuidedInstall\\n script derives the values to deploy from either the console or the properties file. You can type these\\nvalues on the prompt while you run the script or edit the \\nproperties\\n file in the location <\\nextracted application installer packages>/<ap\\nplication-chart>/scripts/guided-install\\n. You can edit the file in the following two ways based on your deployment requirement:\\nleast-input.properties\\n: Properties file requiring minimal inputs. If you choose this file, Kubernetes, Postgres, Vertica, with\\ncertificates generated by the guided installer, LPV are set up by the guided installer. You will use the self-signed\\ncertificates, and use the PostgreSQL server set up by the script.\\ncustom-input.properties\\n: Properties file which contains the complete list of parameters that you need to configure. If you\\nchoose this file, you can use the CA signed certificates, an existing PostgreSQL instance, and choose the required\\nmodules.\\nLeast or default input installation\\nYou must type the values in the \\n<application-chart>/scripts/guided-install/\\nleast-input.properties\\n file.\\nThe following table lists the parameters and the description for the \\nleast-input.properties\\n file:\\ntypicalInstallUserConfig\\nParameters\\nDescription\\nomtAdminUser\\nPassword\\n OMT administrator password in form of plain text. The OMT services like AppHub, \\ncdfctl\\n are accessed\\nusing this password. \\nNote\\n:\\nThe guided installer exposes Vertica ports mentioned in the \\nVertica documentation\\n for the application\\ndeployment. \\n The guided installer script does not apply license for Vertica, you must apply license after completing the script\\nexecution.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n175\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3aba0326806138f073f148ef3d1399e'}>,\n",
              "  <Document: {'content': \"externalAccess\\nHost\\n FQDN of the host used to access the OMT AppHub UI. \\napplicationInst\\nallerLocation\\nThe complete path of the location where you have downloaded the application chart. The supported\\nformat for the downloadable - \\n.tgz\\n file.\\nomtInstallerLoc\\nation\\nComplete path of the location where you have downloaded the OMT installer file.\\nIf any of your control plane or worker node servers are in different subnets, type the location where you\\nhave downloaded and unzipped the OMT installer file.\\nallowWorkerOn\\nMaster\\nChange this to \\ntrue\\n if you want to deploy the workload on the master node.\\nntpServer\\nThe server used to synchronize time for all the nodes used for application deployment. You must have a\\nNetwork Time Protocol server available if you aren't connected to the internet.\\nParameters\\nDescription\\nnodes\\nParameters\\nDescription\\ncontrolPlane\\nFQDN of all Control plane nodes. The multiple nodes must be comma separated. For example: \\nControlPlane1.\\nNode.Server,ControlPlane2.Node.Server\\nworker\\nFQDN of all Worker plane nodes. The multiple nodes must be comma separated. For example: \\nWorker1.Node\\n.Server,Worker2.Node.Server\\nnfsServer\\nFQDN of the NFS server.\\nvertica\\nFQDN of the Vertica server. The multiple nodes must be comma separated. For example: \\nVertica1.Node.serve\\nr,Vertica2.node.server\\n.\\nIf you don't plan to use OPTIC Data Lake, you won't require Vertica. You can leave this parameter empty.\\nrdbmsServe\\nr\\nFQDN of the RDBMS server.\\nserverCredential\\nParameters\\nDescription\\nusername\\nUsername that's used across all the nodes and has permission to execute all the commands.\\npassword\\nPassword in plain text for the user.\\nCustomized Installation\\nYou must type the values in the \\n<application-chart>/scripts/guided-install/\\ncustom-input.properties\\n file. \\nThe following table lists the parameters and the description for the \\ncustom-input.properties\\n file:\\ntypicalInstallUserConfig\\nParameters\\nDescription\\nomtAdminUs\\nerPassword\\nOMT administrator password in form of plain text. The OMT services like AppHub, \\ncdfctl\\n are accessed\\nusing this password. \\nexternalAcce\\nssHost\\nFQDN of the host used to access the OMT AppHub UI. \\napplicationIn\\nstallerLocatio\\nn\\nComplete path of the location where you have downloaded the application chart. The supported format for\\nthe downloadable - \\n.tgz\\n file. For example: \\n$HOME/opsbridge-suite-chart/charts/opsbridge-suite-<version>.tgz\\nomtInstallerL\\nocation\\nComplete path of the location where you have downloaded and unzipped the OMT installer file. \\nallowWorker\\nOnMaster\\nChange this to \\ntrue\\n if you want to deploy the workload on the master node.\\nntpServer\\nThe Server used to synchronize time for all the nodes used for application deployment. You must have\\nNetwork Time Protocol server available if you aren't connected to the internet.\\nnodes\\nParameters\\nDescription\\ncontrolPlane\\n FQDN of all Control plane nodes. The multiple nodes must be comma separated. For example: \\nControlPlane1\\n.Node.Server,ControlPlane2.Node.Server\\nworker\\nFQDN of all Worker plane nodes. The multiple nodes must be comma separated. For example: \\nWorker1.Node\\n.Server,Worker2.Node.Server\\nnfsServer\\nFQDN of the NFS server.\\nContainerized Operations Bridge 2022.11\\nPage \\n176\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.externalAccess\\nHost\\n FQDN of the host used to access the OMT AppHub UI. \\napplicationInst\\nallerLocation\\nThe complete path of the location where you have downloaded the application chart. The supported\\nformat for the downloadable - \\n.tgz\\n file.\\nomtInstallerLoc\\nation\\nComplete path of the location where you have downloaded the OMT installer file.\\nIf any of your control plane or worker node servers are in different subnets, type the location where you\\nhave downloaded and unzipped the OMT installer file.\\nallowWorkerOn\\nMaster\\nChange this to \\ntrue\\n if you want to deploy the workload on the master node.\\nntpServer\\nThe server used to synchronize time for all the nodes used for application deployment. You must have a\\nNetwork Time Protocol server available if you aren't connected to the internet.\\nParameters\\nDescription\\nnodes\\nParameters\\nDescription\\ncontrolPlane\\nFQDN of all Control plane nodes. The multiple nodes must be comma separated. For example: \\nControlPlane1.\\nNode.Server,ControlPlane2.Node.Server\\nworker\\nFQDN of all Worker plane nodes. The multiple nodes must be comma separated. For example: \\nWorker1.Node\\n.Server,Worker2.Node.Server\\nnfsServer\\nFQDN of the NFS server.\\nvertica\\nFQDN of the Vertica server. The multiple nodes must be comma separated. For example: \\nVertica1.Node.serve\\nr,Vertica2.node.server\\n.\\nIf you don't plan to use OPTIC Data Lake, you won't require Vertica. You can leave this parameter empty.\\nrdbmsServe\\nr\\nFQDN of the RDBMS server.\\nserverCredential\\nParameters\\nDescription\\nusername\\nUsername that's used across all the nodes and has permission to execute all the commands.\\npassword\\nPassword in plain text for the user.\\nCustomized Installation\\nYou must type the values in the \\n<application-chart>/scripts/guided-install/\\ncustom-input.properties\\n file. \\nThe following table lists the parameters and the description for the \\ncustom-input.properties\\n file:\\ntypicalInstallUserConfig\\nParameters\\nDescription\\nomtAdminUs\\nerPassword\\nOMT administrator password in form of plain text. The OMT services like AppHub, \\ncdfctl\\n are accessed\\nusing this password. \\nexternalAcce\\nssHost\\nFQDN of the host used to access the OMT AppHub UI. \\napplicationIn\\nstallerLocatio\\nn\\nComplete path of the location where you have downloaded the application chart. The supported format for\\nthe downloadable - \\n.tgz\\n file. For example: \\n$HOME/opsbridge-suite-chart/charts/opsbridge-suite-<version>.tgz\\nomtInstallerL\\nocation\\nComplete path of the location where you have downloaded and unzipped the OMT installer file. \\nallowWorker\\nOnMaster\\nChange this to \\ntrue\\n if you want to deploy the workload on the master node.\\nntpServer\\nThe Server used to synchronize time for all the nodes used for application deployment. You must have\\nNetwork Time Protocol server available if you aren't connected to the internet.\\nnodes\\nParameters\\nDescription\\ncontrolPlane\\n FQDN of all Control plane nodes. The multiple nodes must be comma separated. For example: \\nControlPlane1\\n.Node.Server,ControlPlane2.Node.Server\\nworker\\nFQDN of all Worker plane nodes. The multiple nodes must be comma separated. For example: \\nWorker1.Node\\n.Server,Worker2.Node.Server\\nnfsServer\\nFQDN of the NFS server.\\nContainerized Operations Bridge 2022.11\\nPage \\n176\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e264fd7e98745af7381944848a5e3922'}>,\n",
              "  <Document: {'content': 'vertica\\nFQDN of the Vertica server. The multiple nodes must be comma separated. Make sure to type the FQDN of\\nthe node where you installed Vertica first and then type the other Vertica nodes FQDN.\\nFor example: \\nVertica1.Node.server,Vertica2.node.server\\n.\\nIf you don\\'t plan to use OPTIC Data Lake you won\\'t require Vertica. You can leave this parameter empty.\\nrdbmsServe\\nr\\nFQDN of the RDBMS server.\\nParameters\\nDescription\\nrelationalDatabase\\nParameters\\nDescription\\nuseExisting\\nOption to choose the PostgreSQL instance. The default value is \\nfalse\\n. If you have an existing PostgreSQL\\ninstance, type \\ntrue\\n and give the values for the parameters in the \\nexistingDatabaseDetails\\n section.\\nexistingData\\nbaseDetails\\nDetails of the database server.  You must give the values for the parameters in this section if you have an\\nexisting instance of the database and prefer to use it. Make sure you have set \\nrelationalDatabase.useExisting: t\\nrue\\n.\\ndatabaseAd\\nminUser\\nDatabase administrator username that you will use to create the databases required for the application\\ndeployment. \\nadminUserP\\nassword\\nDatabase administrator password.\\nserverCredential\\nParameters\\nDescription\\nusername\\nUsername that\\'s used across all the nodes and has permission to execute all the commands.\\npassword\\nPassword in plain text for the user.\\ncertificate\\nParameters\\nDescription\\nuseExistingC\\nertificate\\nThe default value is \\nfalse\\n. If you have all the required certificates (For example: OMT Ingress, PostgreSQL,\\nand Vertica), type \\ntrue\\n and give the values for the parameters in the \\nexistingCertificateLocation\\nsection.  \\nexistingCerti\\nficateLocatio\\nn\\nCertificate details of OMT Ingress, PostgreSQL, and Vertica. You must give the values for the parameters in\\nthis section if you have all the required certificates and prefer to use them. Make sure you have set \\ncertifica\\nte.useExistingCertificate: true\\n.\\ningressCertif\\nicate\\nComplete path of the OMT ingress root certificate, server certificate, and server key, separated by a\\ncomma. For example: \\nroot_ca.crt,server_crt.crt,server_key.key\\npostgresCert\\nificate\\nComplete path of the PostgreSQL root certificate, server certificate, and server key, separated by a\\ncomma. For example: \\nroot_ca.crt,server_crt.crt,server_key.key\\nverticaCertifi\\ncate\\nComplete path of the Vertica root certificate, server certificate, and server key, separated by a comma. For\\nexample: \\nroot_ca.crt,server_crt.crt,server_key.key\\nmodules\\nParameters\\nDescription\\ndeploy\\nYou must include the modules that you want to deploy using the guided installer as follows:\\nMaster,Worker,NFS\\n - This module completes all the prerequisites for the control plane and worker nodes.\\nThe NFS module is for NFS folder creation.\\nOMTDatabase,OMTPreRequisite,OMTDeployment \\n- This module completes all the prerequisites to create OMT\\ndatabase and OMT deployment.\\nVertica \\n- This module creates Vertica server with TLS ENFORCED.\\nrelationalDatabase \\n- This module creates PostgreSQL 14 server with TLS enabled.\\nLPVDeployment \\n- This module completes all the prerequisites to create Local persistent volumes.\\nApplicationPrerequisite \\n- This module completes the chart upload to AppHub.\\nType these modules comma separated according to your requirement - \\n\"Master,Worker,NFS,relationalDatabase,V\\nertica,OMTDatabase,OMTPreRequisite,OMTDeployment,LPVDeployment,ApplicationPrerequisite\"\\nRun the script\\nAfter you complete the prerequisites, follow these steps to run the script:\\n1\\n. \\nGo to \\n<application-chart>/scripts/guided-install\\n directory. Run the following command:\\nTo use the console to perform the installation: \\n./OpsbridgeGuidedInstall\\nTo use the \\nleast-input.properties\\n file: \\n./OpsbridgeGuidedInstall least-input.properties\\nContainerized Operations Bridge 2022.11\\nPage \\n177\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.vertica\\nFQDN of the Vertica server. The multiple nodes must be comma separated. Make sure to type the FQDN of\\nthe node where you installed Vertica first and then type the other Vertica nodes FQDN.\\nFor example: \\nVertica1.Node.server,Vertica2.node.server\\n.\\nIf you don\\'t plan to use OPTIC Data Lake you won\\'t require Vertica. You can leave this parameter empty.\\nrdbmsServe\\nr\\nFQDN of the RDBMS server.\\nParameters\\nDescription\\nrelationalDatabase\\nParameters\\nDescription\\nuseExisting\\nOption to choose the PostgreSQL instance. The default value is \\nfalse\\n. If you have an existing PostgreSQL\\ninstance, type \\ntrue\\n and give the values for the parameters in the \\nexistingDatabaseDetails\\n section.\\nexistingData\\nbaseDetails\\nDetails of the database server.  You must give the values for the parameters in this section if you have an\\nexisting instance of the database and prefer to use it. Make sure you have set \\nrelationalDatabase.useExisting: t\\nrue\\n.\\ndatabaseAd\\nminUser\\nDatabase administrator username that you will use to create the databases required for the application\\ndeployment. \\nadminUserP\\nassword\\nDatabase administrator password.\\nserverCredential\\nParameters\\nDescription\\nusername\\nUsername that\\'s used across all the nodes and has permission to execute all the commands.\\npassword\\nPassword in plain text for the user.\\ncertificate\\nParameters\\nDescription\\nuseExistingC\\nertificate\\nThe default value is \\nfalse\\n. If you have all the required certificates (For example: OMT Ingress, PostgreSQL,\\nand Vertica), type \\ntrue\\n and give the values for the parameters in the \\nexistingCertificateLocation\\nsection.  \\nexistingCerti\\nficateLocatio\\nn\\nCertificate details of OMT Ingress, PostgreSQL, and Vertica. You must give the values for the parameters in\\nthis section if you have all the required certificates and prefer to use them. Make sure you have set \\ncertifica\\nte.useExistingCertificate: true\\n.\\ningressCertif\\nicate\\nComplete path of the OMT ingress root certificate, server certificate, and server key, separated by a\\ncomma. For example: \\nroot_ca.crt,server_crt.crt,server_key.key\\npostgresCert\\nificate\\nComplete path of the PostgreSQL root certificate, server certificate, and server key, separated by a\\ncomma. For example: \\nroot_ca.crt,server_crt.crt,server_key.key\\nverticaCertifi\\ncate\\nComplete path of the Vertica root certificate, server certificate, and server key, separated by a comma. For\\nexample: \\nroot_ca.crt,server_crt.crt,server_key.key\\nmodules\\nParameters\\nDescription\\ndeploy\\nYou must include the modules that you want to deploy using the guided installer as follows:\\nMaster,Worker,NFS\\n - This module completes all the prerequisites for the control plane and worker nodes.\\nThe NFS module is for NFS folder creation.\\nOMTDatabase,OMTPreRequisite,OMTDeployment \\n- This module completes all the prerequisites to create OMT\\ndatabase and OMT deployment.\\nVertica \\n- This module creates Vertica server with TLS ENFORCED.\\nrelationalDatabase \\n- This module creates PostgreSQL 14 server with TLS enabled.\\nLPVDeployment \\n- This module completes all the prerequisites to create Local persistent volumes.\\nApplicationPrerequisite \\n- This module completes the chart upload to AppHub.\\nType these modules comma separated according to your requirement - \\n\"Master,Worker,NFS,relationalDatabase,V\\nertica,OMTDatabase,OMTPreRequisite,OMTDeployment,LPVDeployment,ApplicationPrerequisite\"\\nRun the script\\nAfter you complete the prerequisites, follow these steps to run the script:\\n1\\n. \\nGo to \\n<application-chart>/scripts/guided-install\\n directory. Run the following command:\\nTo use the console to perform the installation: \\n./OpsbridgeGuidedInstall\\nTo use the \\nleast-input.properties\\n file: \\n./OpsbridgeGuidedInstall least-input.properties\\nContainerized Operations Bridge 2022.11\\nPage \\n177\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dfedf347539ede373c72961e2e94823e'}>,\n",
              "  <Document: {'content': 'To use the \\ncustom-input.properties\\n file: \\n./OpsbridgeGuidedInstall custom-input.properties\\n2\\n. \\nIf your inputs are valid, the installation starts The status along with the  PostgreSQL, Vertica, OMT database user names\\nand passwords, and the certificate locations for PostgreSQL, Vertica, and OMT Ingress appears on the console. Check the\\nlogs in \"\\nguided_Install.log\\n\" in the same directory. In case of a failure, you can use the log file to troubleshoot issues.\\n3\\n. \\nComplete the remaining prerequisites to deploy the application and deploy using the AppHub UI.\\nComplete the prerequisites to deploy the application\\nDatabases\\nTask\\nRole\\nWhere to perform\\nCapability\\nTune PostgreSQL parameters\\nDatabase administrator\\nExternal PostgreSQL server\\nAll\\nVertica\\n \\nTask\\nRole\\nWhere to\\nperform\\nScenario\\nTuning parameters\\nfor Vertica\\nDatabase\\nadministrator\\nExternal\\nVertica server\\nOPTIC Reporting, shared OPTIC Data Lake with Operations Bridge as\\nthe provider, Hyperscale Observability\\nOther prerequisites\\nTask\\nRole\\nWhere to perform\\nCapability\\nObtain OBM CA certificate\\nApplication\\nowner\\nWorker node\\nHyperscale Observability \\nCreate an Agent Metric Collector\\nintegration user\\nApplication\\nowner\\nBastion node or\\nControl plane\\nOPTIC Reporting capability using the\\nclassic OBM.\\nUpdate Security context constraints for\\napplication\\nApplication\\nowner\\nBastion node or\\nControl plane\\nAll capabilities on OpenShift\\nDeploy the application\\nFollow these steps to deploy the application:\\n1\\n. \\n(Optional)\\n AppHub UI creates the application namespace during deployment. If you want to export the configured\\napplication and install it manually at a later stage or if you want to install using CLI, you must perform this task. To create\\nthe namespace manually, see \\nCreate application namespace\\n.\\n2\\n. \\nDeploy the application using AppHub\\n. While configuring the parameters, you must enable the \\nAutomatically create\\nrequired databases\\n option in the AppHub UI to create the databases required by the application.\\n3\\n. \\nAfter you complete the installation, you can \\nverify\\n and perform the required \\npost install configurations\\n.\\nNote\\n:\\nRun \\nkubectl\\n \\nget\\n \\npv\\n to verify the PVs you have created. The allocated capacity for all the PVs will be shown as\\nequivalent to the full capacity of the disk. However, the actual allocation will remain as per your\\nconfiguration.\\nIf you run the guided installer again because of a failure or reinstall, the installer resumes from the last\\ncompleted stage depending on the user input. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n178\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.To use the \\ncustom-input.properties\\n file: \\n./OpsbridgeGuidedInstall custom-input.properties\\n2\\n. \\nIf your inputs are valid, the installation starts The status along with the  PostgreSQL, Vertica, OMT database user names\\nand passwords, and the certificate locations for PostgreSQL, Vertica, and OMT Ingress appears on the console. Check the\\nlogs in \"\\nguided_Install.log\\n\" in the same directory. In case of a failure, you can use the log file to troubleshoot issues.\\n3\\n. \\nComplete the remaining prerequisites to deploy the application and deploy using the AppHub UI.\\nComplete the prerequisites to deploy the application\\nDatabases\\nTask\\nRole\\nWhere to perform\\nCapability\\nTune PostgreSQL parameters\\nDatabase administrator\\nExternal PostgreSQL server\\nAll\\nVertica\\n \\nTask\\nRole\\nWhere to\\nperform\\nScenario\\nTuning parameters\\nfor Vertica\\nDatabase\\nadministrator\\nExternal\\nVertica server\\nOPTIC Reporting, shared OPTIC Data Lake with Operations Bridge as\\nthe provider, Hyperscale Observability\\nOther prerequisites\\nTask\\nRole\\nWhere to perform\\nCapability\\nObtain OBM CA certificate\\nApplication\\nowner\\nWorker node\\nHyperscale Observability \\nCreate an Agent Metric Collector\\nintegration user\\nApplication\\nowner\\nBastion node or\\nControl plane\\nOPTIC Reporting capability using the\\nclassic OBM.\\nUpdate Security context constraints for\\napplication\\nApplication\\nowner\\nBastion node or\\nControl plane\\nAll capabilities on OpenShift\\nDeploy the application\\nFollow these steps to deploy the application:\\n1\\n. \\n(Optional)\\n AppHub UI creates the application namespace during deployment. If you want to export the configured\\napplication and install it manually at a later stage or if you want to install using CLI, you must perform this task. To create\\nthe namespace manually, see \\nCreate application namespace\\n.\\n2\\n. \\nDeploy the application using AppHub\\n. While configuring the parameters, you must enable the \\nAutomatically create\\nrequired databases\\n option in the AppHub UI to create the databases required by the application.\\n3\\n. \\nAfter you complete the installation, you can \\nverify\\n and perform the required \\npost install configurations\\n.\\nNote\\n:\\nRun \\nkubectl\\n \\nget\\n \\npv\\n to verify the PVs you have created. The allocated capacity for all the PVs will be shown as\\nequivalent to the full capacity of the disk. However, the actual allocation will remain as per your\\nconfiguration.\\nIf you run the guided installer again because of a failure or reinstall, the installer resumes from the last\\ncompleted stage depending on the user input. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n178\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27c2fba4832a02d3b45279aa6b45cd47'}>,\n",
              "  <Document: {'content': 'Prepare infrastructure and install OMT on external\\nKubernetes\\nThis topic gives you information on infrastructure specifications to prepare the infrastructure for external Kubernetes\\ndistributions manually. While preparing the infrastructure make sure you follow the specifications provided in the following\\nsubtopics:\\nDatabase related infrastructure\\nStorage related infrastructure\\nNetwork related infrastructure\\nImage registry\\nKubernetes related infrastructure\\nInstall OMT for external Kubernetes\\nContainerized Operations Bridge 2022.11\\nPage \\n179\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Prepare infrastructure and install OMT on external\\nKubernetes\\nThis topic gives you information on infrastructure specifications to prepare the infrastructure for external Kubernetes\\ndistributions manually. While preparing the infrastructure make sure you follow the specifications provided in the following\\nsubtopics:\\nDatabase related infrastructure\\nStorage related infrastructure\\nNetwork related infrastructure\\nImage registry\\nKubernetes related infrastructure\\nInstall OMT for external Kubernetes\\nContainerized Operations Bridge 2022.11\\nPage \\n179\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '321cd4266f813650f3abebda9a8105c3'}>,\n",
              "  <Document: {'content': \"Database related infrastructure\\nThis topic provides you with the required specifications to configure the database server requirements.\\nContainerized Operations Bridge supports the following databases:\\nPostgreSQL: This relational database server is required to store the application related data.\\nOracle: This relational database server is required to store the application-related data.\\nVertica: This database server is optional and is required only if you choose the capabilities that use OPTIC Data Lake like\\nOPTIC Reporting.\\nPostgreSQL\\nOperations Bridge supports PostgreSQL database. You can use PostgreSQL database as an embedded database or an external\\ndatabase.\\nEmbedded PostgreSQL is installed and configured in a separate container during deployment, and database files are stored on\\nthe NFS server. Embedded PostgreSQL is supported only for Low Footprint OpsBridge Reporting Deployment.\\nExternal PostgreSQL is installed on the system that's separate from your application master (control plane) and worker\\nnodes. For details to install PostgreSQL, see the \\nPostgreSQL documentation\\n.\\nOn Microsoft Azure, Operations Bridge supports only PostgreSQL version 11.x. You must install \\npostgres-contrib\\n or \\npostgresql<ver\\nsion>-contrib\\n \\npackage on the database server depending on the PostgreSQL version you are using. \\nIn a shared OPTIC Data Lake deployment, use the external PostgreSQL server set up by the provider application to create the\\nadditional database for Operations Bridge.\\nIf you are planning to integrate containerized Operations Bridge with \\nNOM OPTIC Reporting\\n, then you must select\\nthe\\n PostgreSQL database\\n, as NOM doesn't support Oracle.\\nIn a multi application deployment where you will install Operations Bridge as the second product, you can't use the existing\\nPostgreSQL instance. You must create a separate PostgreSQL instance.\\nSet up trust for PostgreSQL database\\nIt's recommended to enable TLS for databases to provide privacy and data integrity between a client and a server.\\nSet up trust for PostgreSQL database for AWS\\nYou can download the RDS certificate from the \\nAWS location\\n. Scroll down the page and download the SSL certificate\\n- \\nhttps://s3.amazonaws.com/rds-downloads/rds-ca-2019-root.pem\\n. You must save the certificate in \\n.crt\\n file.\\nSet up trust for PostgreSQL database for Azure \\nYou can find the certificate to connect to an Azure Database for PostgreSQL server at \\nBaltimoreCyberTrustRoot.crt\\n. Download\\nthe certificate file and save it as \\npostgres.crt\\n in the \\n$HOME/nom\\n \\ndirectory in Azure Bastion node. You need this certificate to\\nconnect to the PostgreSQL database securely.\\nOracle\\nOperations Bridge supports Oracle. \\nExternal Oracle database is installed on the system that's separate from your application master (control plane) and worker\\nnodes.  For details, see the \\nOracle documentation\\n. \\nIf you are planning to integrate containerized Operations Bridge with \\nNOM OPTIC Reporting\\n, then you must select\\nthe\\n PostgreSQL database\\n, as NOM doesn't support Oracle.\\nOperations Bridge deployment on Azure doesn't support Oracle.\\nOperations Bridge deployment on AWS doesn't support Oracle if you use the ITOM Cloud Deployment toolkit to set up AWS\\ninfrastructure. However, for AWS infrastructure that's set up manually, you can deploy Operations Bridge with Oracle database\\nwithout TLS.\\nVertica\\nNote:\\n \\nYou can choose either PostgreSQL or Oracle as relational database for storing application\\ndata.  For AWS and Azure, there is no support for Oracle. \\n\\ue916\\n\\ue916\\nImportant:\\n In a production environment, use an external PostgreSQL or Oracle database. Embedded\\nPostgreSQL is supported in non-production and low footprint environments only. \\n\\ue91b\\n\\ue91b\\nNote:\\n \\nWhen you create entities like database, schema, users, etc in PostgreSQL, specify them in lowercase\\nonly.  \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n180\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Database related infrastructure\\nThis topic provides you with the required specifications to configure the database server requirements.\\nContainerized Operations Bridge supports the following databases:\\nPostgreSQL: This relational database server is required to store the application related data.\\nOracle: This relational database server is required to store the application-related data.\\nVertica: This database server is optional and is required only if you choose the capabilities that use OPTIC Data Lake like\\nOPTIC Reporting.\\nPostgreSQL\\nOperations Bridge supports PostgreSQL database. You can use PostgreSQL database as an embedded database or an external\\ndatabase.\\nEmbedded PostgreSQL is installed and configured in a separate container during deployment, and database files are stored on\\nthe NFS server. Embedded PostgreSQL is supported only for Low Footprint OpsBridge Reporting Deployment.\\nExternal PostgreSQL is installed on the system that's separate from your application master (control plane) and worker\\nnodes. For details to install PostgreSQL, see the \\nPostgreSQL documentation\\n.\\nOn Microsoft Azure, Operations Bridge supports only PostgreSQL version 11.x. You must install \\npostgres-contrib\\n or \\npostgresql<ver\\nsion>-contrib\\n \\npackage on the database server depending on the PostgreSQL version you are using. \\nIn a shared OPTIC Data Lake deployment, use the external PostgreSQL server set up by the provider application to create the\\nadditional database for Operations Bridge.\\nIf you are planning to integrate containerized Operations Bridge with \\nNOM OPTIC Reporting\\n, then you must select\\nthe\\n PostgreSQL database\\n, as NOM doesn't support Oracle.\\nIn a multi application deployment where you will install Operations Bridge as the second product, you can't use the existing\\nPostgreSQL instance. You must create a separate PostgreSQL instance.\\nSet up trust for PostgreSQL database\\nIt's recommended to enable TLS for databases to provide privacy and data integrity between a client and a server.\\nSet up trust for PostgreSQL database for AWS\\nYou can download the RDS certificate from the \\nAWS location\\n. Scroll down the page and download the SSL certificate\\n- \\nhttps://s3.amazonaws.com/rds-downloads/rds-ca-2019-root.pem\\n. You must save the certificate in \\n.crt\\n file.\\nSet up trust for PostgreSQL database for Azure \\nYou can find the certificate to connect to an Azure Database for PostgreSQL server at \\nBaltimoreCyberTrustRoot.crt\\n. Download\\nthe certificate file and save it as \\npostgres.crt\\n in the \\n$HOME/nom\\n \\ndirectory in Azure Bastion node. You need this certificate to\\nconnect to the PostgreSQL database securely.\\nOracle\\nOperations Bridge supports Oracle. \\nExternal Oracle database is installed on the system that's separate from your application master (control plane) and worker\\nnodes.  For details, see the \\nOracle documentation\\n. \\nIf you are planning to integrate containerized Operations Bridge with \\nNOM OPTIC Reporting\\n, then you must select\\nthe\\n PostgreSQL database\\n, as NOM doesn't support Oracle.\\nOperations Bridge deployment on Azure doesn't support Oracle.\\nOperations Bridge deployment on AWS doesn't support Oracle if you use the ITOM Cloud Deployment toolkit to set up AWS\\ninfrastructure. However, for AWS infrastructure that's set up manually, you can deploy Operations Bridge with Oracle database\\nwithout TLS.\\nVertica\\nNote:\\n \\nYou can choose either PostgreSQL or Oracle as relational database for storing application\\ndata.  For AWS and Azure, there is no support for Oracle. \\n\\ue916\\n\\ue916\\nImportant:\\n In a production environment, use an external PostgreSQL or Oracle database. Embedded\\nPostgreSQL is supported in non-production and low footprint environments only. \\n\\ue91b\\n\\ue91b\\nNote:\\n \\nWhen you create entities like database, schema, users, etc in PostgreSQL, specify them in lowercase\\nonly.  \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n180\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e55546d511359e22807e48255348661'}>,\n",
              "  <Document: {'content': \"Storage related infrastructure\\nThis topic provides you the required specifications to configure the storage server requirements. \\nIf a container stops or restarts, all changes made inside the container are lost. To save information such as configuration files\\nand databases, the information must be stored external to the container in an entity called Persistent Volume (PV) within the\\ncluster. \\nTo decide the size for each of the disks, refer to the sizing calculator. \\nThe cluster components (say a Pod) access a storage volume in the following flow:\\nPod -> PVC -> PV-> Storage ( for example, NFS)\\nA PV is a storage object in a cluster that maps to a physical storage for example, an NFS server volume. When you create a PV,\\nthe mapped physical storage volume becomes available in the Kubernetes cluster. The cluster components use this storage for\\npersistent data storage. However, they don't use a PV for their storage directly. They in turn use an object called Persistent\\nVolume Claim (PVC). A PVC is an intermediary between the application requesting a particular storage of a specific type and\\nthe PV representing a physical volume in the cluster.\\nPVCs are owned by applications and are automatically created when you deploy the application. However, a PVC must be\\nbound to a PV and this binding is done by Kubernetes. To enable kubernetes to bind a PV to a PVC the PV’s either have to be\\ncreated manually upfront, or through a storage provisioner. If you choose to use a storage provisioner, Kubernetes uses the\\nprovisioner to create PV’s on demand based on PVCs that are created while deploying the application. The following two\\nsections describe two ways of creating PVs.\\nStatic provisioning without storage provisioner\\nIn this scenario, the PVC is directly bound to the persistent volumes. You must create the PVs before deploying the application.\\nThe following diagram illustrates the relationship between the physical storage and the Kubernetes storage objects.\\nDynamic provisioning with storage provisioner\\nYou can also set up dynamic volume provisioning, which creates storage volumes (the PVs) on demand. For more information\\nabout dynamic volume provisioning, see \\nKubernetes documentation\\n.  If you use dynamic volume provisioning to create the\\nvolumes on demand, make sure that the provisioner is able to offer the storage with\\nthe \\nReadWriteOnce\\n and \\nReadWriteMany\\n access modes. For more information on the access modes,  see \\nKubernetes\\ndocumentation\\n.\\nYou can also have multiple storage provisioners, for example, one storage provisioner which is able to create PVs with access\\nmode \\nReadWriteMany\\n and another one with access mode \\nReadWriteMany\\n. Refer to the documentation of your storage provisioner\\nto see what type of access modes it supports. For more information on the access modes,  see \\nKubernetes documentation\\n.\\nWhen you use a storage provisioner, the PVC is configured to request PVs of a specific storage class. The dynamic storage\\nprovisioner responsible for a specific storage class will be triggered to create a new volume. As a result a new PV is created\\npointing to the dynamically created volume. The PV will be marked with the same storageclass as the PVC which requested the\\nPV. The advantage is that you can define a storage class that maps to different characteristics. The following diagram\\nillustrates how the PVC accesses the persistent volumes through a storage class.\\nImportant: \\nSkip this section if you have already set up the required storage infrastructure, either by using\\nthe ITOM Cloud Deployment Toolkit or by following the system requirements topic.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n182\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Storage related infrastructure\\nThis topic provides you the required specifications to configure the storage server requirements. \\nIf a container stops or restarts, all changes made inside the container are lost. To save information such as configuration files\\nand databases, the information must be stored external to the container in an entity called Persistent Volume (PV) within the\\ncluster. \\nTo decide the size for each of the disks, refer to the sizing calculator. \\nThe cluster components (say a Pod) access a storage volume in the following flow:\\nPod -> PVC -> PV-> Storage ( for example, NFS)\\nA PV is a storage object in a cluster that maps to a physical storage for example, an NFS server volume. When you create a PV,\\nthe mapped physical storage volume becomes available in the Kubernetes cluster. The cluster components use this storage for\\npersistent data storage. However, they don't use a PV for their storage directly. They in turn use an object called Persistent\\nVolume Claim (PVC). A PVC is an intermediary between the application requesting a particular storage of a specific type and\\nthe PV representing a physical volume in the cluster.\\nPVCs are owned by applications and are automatically created when you deploy the application. However, a PVC must be\\nbound to a PV and this binding is done by Kubernetes. To enable kubernetes to bind a PV to a PVC the PV’s either have to be\\ncreated manually upfront, or through a storage provisioner. If you choose to use a storage provisioner, Kubernetes uses the\\nprovisioner to create PV’s on demand based on PVCs that are created while deploying the application. The following two\\nsections describe two ways of creating PVs.\\nStatic provisioning without storage provisioner\\nIn this scenario, the PVC is directly bound to the persistent volumes. You must create the PVs before deploying the application.\\nThe following diagram illustrates the relationship between the physical storage and the Kubernetes storage objects.\\nDynamic provisioning with storage provisioner\\nYou can also set up dynamic volume provisioning, which creates storage volumes (the PVs) on demand. For more information\\nabout dynamic volume provisioning, see \\nKubernetes documentation\\n.  If you use dynamic volume provisioning to create the\\nvolumes on demand, make sure that the provisioner is able to offer the storage with\\nthe \\nReadWriteOnce\\n and \\nReadWriteMany\\n access modes. For more information on the access modes,  see \\nKubernetes\\ndocumentation\\n.\\nYou can also have multiple storage provisioners, for example, one storage provisioner which is able to create PVs with access\\nmode \\nReadWriteMany\\n and another one with access mode \\nReadWriteMany\\n. Refer to the documentation of your storage provisioner\\nto see what type of access modes it supports. For more information on the access modes,  see \\nKubernetes documentation\\n.\\nWhen you use a storage provisioner, the PVC is configured to request PVs of a specific storage class. The dynamic storage\\nprovisioner responsible for a specific storage class will be triggered to create a new volume. As a result a new PV is created\\npointing to the dynamically created volume. The PV will be marked with the same storageclass as the PVC which requested the\\nPV. The advantage is that you can define a storage class that maps to different characteristics. The following diagram\\nillustrates how the PVC accesses the persistent volumes through a storage class.\\nImportant: \\nSkip this section if you have already set up the required storage infrastructure, either by using\\nthe ITOM Cloud Deployment Toolkit or by following the system requirements topic.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n182\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c3539f20a8bf29438d602a855f7ed0ea'}>,\n",
              "  <Document: {'content': \"For capabilities that use OPTIC Data Lake, you must install and configure the Vertica database. You can install and configure\\nVertica as an external or embedded database later as part of setting up Operations Bridge prerequisites.\\nEmbedded Vertica is installed and configured in a separate container during deployment. The embedded Vertica database isn't\\nsupported for use in the production environment. It's for Evaluation and low footprint environments only.\\nExternal Vertica is installed on the system that's separate from your application master (control plane) and worker nodes.\\nIn a shared OPTIC Data Lake deployment, use the external Vertica set up by the provider application. \\nIn a multi suite deployment where you will install Operations Bridge as the second product, you can't use the existing Vertica\\ninstance. You must create a separate Vertica instance.\\nContainerized Operations Bridge 2022.11\\nPage \\n181\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.For capabilities that use OPTIC Data Lake, you must install and configure the Vertica database. You can install and configure\\nVertica as an external or embedded database later as part of setting up Operations Bridge prerequisites.\\nEmbedded Vertica is installed and configured in a separate container during deployment. The embedded Vertica database isn't\\nsupported for use in the production environment. It's for Evaluation and low footprint environments only.\\nExternal Vertica is installed on the system that's separate from your application master (control plane) and worker nodes.\\nIn a shared OPTIC Data Lake deployment, use the external Vertica set up by the provider application. \\nIn a multi suite deployment where you will install Operations Bridge as the second product, you can't use the existing Vertica\\ninstance. You must create a separate Vertica instance.\\nContainerized Operations Bridge 2022.11\\nPage \\n181\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8907d8abc7692bae38b97447f103e279'}>,\n",
              "  <Document: {'content': \"Image registry\\nThis topic provides you the required specifications to configure the image registry requirements. \\nImages are downloaded not only during install and upgrade, but anytime the cluster goes down or a pod goes down, and if any\\nimage is missing in the cluster, there will be a call to the image repository server to download the images again. If images are\\nfetched from Docker Hub each time, they will use plenty of network resources each time images are pulled. To prevent this,\\nit's recommended to use a private image repository so that, such network traffic stays within the customer network. A better\\nchoice would be to download images once to the cluster and let the cluster handle any image requests.\\nYou may need to do one of the following to download and upload installation images: \\nDownload OMT and application images from Docker Hub to a private image repository server in a customer network.  \\nYou can use this private image repository server to download images during OMT and application installation as\\nrequired and thus serves as a “cache” for all users in that network. \\nDownload OMT and application images from Docker Hub to a private machine in the customer network\\nDownload OMT and application images directly from Docker Hub to a local machine using the download images script\\nprovided by OMT.\\nUpload these images to a \\nMicro Focus\\n embedded Kubernetes cluster using the upload images script provided by\\nOMT.\\nDownload OMT and application images from Docker Hub to a  local machine (\\nlocalhost:5000\\n) using scripts provided by\\nOMT\\nThe images are stored within the cluster.\\nImportant: \\nSkip this section if you have already set up the required storage infrastructure, either by using\\nthe ITOM Cloud Deployment Toolkit or by following the system requirements topic.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n186\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Image registry\\nThis topic provides you the required specifications to configure the image registry requirements. \\nImages are downloaded not only during install and upgrade, but anytime the cluster goes down or a pod goes down, and if any\\nimage is missing in the cluster, there will be a call to the image repository server to download the images again. If images are\\nfetched from Docker Hub each time, they will use plenty of network resources each time images are pulled. To prevent this,\\nit's recommended to use a private image repository so that, such network traffic stays within the customer network. A better\\nchoice would be to download images once to the cluster and let the cluster handle any image requests.\\nYou may need to do one of the following to download and upload installation images: \\nDownload OMT and application images from Docker Hub to a private image repository server in a customer network.  \\nYou can use this private image repository server to download images during OMT and application installation as\\nrequired and thus serves as a “cache” for all users in that network. \\nDownload OMT and application images from Docker Hub to a private machine in the customer network\\nDownload OMT and application images directly from Docker Hub to a local machine using the download images script\\nprovided by OMT.\\nUpload these images to a \\nMicro Focus\\n embedded Kubernetes cluster using the upload images script provided by\\nOMT.\\nDownload OMT and application images from Docker Hub to a  local machine (\\nlocalhost:5000\\n) using scripts provided by\\nOMT\\nThe images are stored within the cluster.\\nImportant: \\nSkip this section if you have already set up the required storage infrastructure, either by using\\nthe ITOM Cloud Deployment Toolkit or by following the system requirements topic.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n186\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '73f7f967b5de7089a78916a6d834fb6d'}>,\n",
              "  <Document: {'content': \"Azure\\nStorage type\\nNotes\\nAzure File\\nShare\\nPremium Azure File Share with \\n<share_quota>\\n greater than 0, and less than or equal to \\n5\\nTB\\n (\\n5120\\n). \\nAzure disks\\nYou will require Azure disks if you plan to use OPTIC Reporting, Hyperscale Observability, AEC, and\\nOBM.\\nFor more information on creating Azure storage see \\nAzure documentation\\n.\\nRed Hat OpenShift\\nStorage\\ntype\\nNotes\\nCeph File\\nSystem\\n(\\nCephFS\\n)\\nTo set up OpenShift Data Foundation on an OpenShift Container Platform with worker nodes on a Red Hat\\nEnterprise Linux, you must install Local Operator and OpenShift Data Foundation Operator. See \\nProduct\\nDocumentation for Red Hat OpenShift Container Storage\\n for details.\\nConfigure additional \\ncephfs\\n disks all worker nodes to persist data across containers started and stopped,\\nand for shared data access between containers. To decide the size for each of the disks, refer to the sizing\\ncalculator.\\nYou can set up the OpenShift Data Foundation (ODF) either internally, within the OpenShift cluster, or\\nexternally. \\nWhen using OpenShift Data Foundation as storage, there is no need to create persistent volumes. During\\ndeployment, OpenShift Data Foundation will automatically create the required persistent volumes, based on\\nthe storage class provided as input.\\nYou must make sure that the \\nStorageClasses\\n are available using the below \\nProvisioners \\nor volume\\nplugins:\\n1\\n. \\nopenshift-storage.rbd.csi.ceph.com\\n2\\n. \\nopenshift-storage.cephfs.csi.ceph.com\\nCeph RBD\\nIf you plan to deploy OPTIC Reporting, it's mandatory to configure OpenShift Data Foundation as some\\nOPTIC Reporting components require persistent volumes from the \\nocs-storagecluster-ceph-rbd\\n storage class.\\nIf you plan to use shared OPTIC Data Lake from a provider application, then there is no need to configure\\nOpenShift Data Foundation for the consumer application\\nIf you use dynamic volume provisioning to create the volumes on demand, please make sure that your provisioner is able to\\noffer the storage with access mode ReadWriteOnce and ReadWriteMany. For more information on the access modes refer to\\nthe \\nKubernetes documentation\\n.\\nOther Kubernetes distribution storage\\nIf you are deploying on a Rancher Kubernetes Engine (RKE), you can use NFS or any storage supported on an RKE cluster. If\\nyou are deploying a capability that requires OPTIC Data Lake on a Rancher Kubernetes Engine, and auto-provisioning volumes\\non a locally mounted storage, make sure that you enable extra binds for the cluster. For more information, see \\nRancher\\ndocumentation\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n184\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Azure\\nStorage type\\nNotes\\nAzure File\\nShare\\nPremium Azure File Share with \\n<share_quota>\\n greater than 0, and less than or equal to \\n5\\nTB\\n (\\n5120\\n). \\nAzure disks\\nYou will require Azure disks if you plan to use OPTIC Reporting, Hyperscale Observability, AEC, and\\nOBM.\\nFor more information on creating Azure storage see \\nAzure documentation\\n.\\nRed Hat OpenShift\\nStorage\\ntype\\nNotes\\nCeph File\\nSystem\\n(\\nCephFS\\n)\\nTo set up OpenShift Data Foundation on an OpenShift Container Platform with worker nodes on a Red Hat\\nEnterprise Linux, you must install Local Operator and OpenShift Data Foundation Operator. See \\nProduct\\nDocumentation for Red Hat OpenShift Container Storage\\n for details.\\nConfigure additional \\ncephfs\\n disks all worker nodes to persist data across containers started and stopped,\\nand for shared data access between containers. To decide the size for each of the disks, refer to the sizing\\ncalculator.\\nYou can set up the OpenShift Data Foundation (ODF) either internally, within the OpenShift cluster, or\\nexternally. \\nWhen using OpenShift Data Foundation as storage, there is no need to create persistent volumes. During\\ndeployment, OpenShift Data Foundation will automatically create the required persistent volumes, based on\\nthe storage class provided as input.\\nYou must make sure that the \\nStorageClasses\\n are available using the below \\nProvisioners \\nor volume\\nplugins:\\n1\\n. \\nopenshift-storage.rbd.csi.ceph.com\\n2\\n. \\nopenshift-storage.cephfs.csi.ceph.com\\nCeph RBD\\nIf you plan to deploy OPTIC Reporting, it's mandatory to configure OpenShift Data Foundation as some\\nOPTIC Reporting components require persistent volumes from the \\nocs-storagecluster-ceph-rbd\\n storage class.\\nIf you plan to use shared OPTIC Data Lake from a provider application, then there is no need to configure\\nOpenShift Data Foundation for the consumer application\\nIf you use dynamic volume provisioning to create the volumes on demand, please make sure that your provisioner is able to\\noffer the storage with access mode ReadWriteOnce and ReadWriteMany. For more information on the access modes refer to\\nthe \\nKubernetes documentation\\n.\\nOther Kubernetes distribution storage\\nIf you are deploying on a Rancher Kubernetes Engine (RKE), you can use NFS or any storage supported on an RKE cluster. If\\nyou are deploying a capability that requires OPTIC Data Lake on a Rancher Kubernetes Engine, and auto-provisioning volumes\\non a locally mounted storage, make sure that you enable extra binds for the cluster. For more information, see \\nRancher\\ndocumentation\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n184\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '43f6212c0aabe2f0e61d78d81523b914'}>,\n",
              "  <Document: {'content': \"Kubernetes related infrastructure\\nThis topic provides you the required specifications to configure the Kubernetes requirements. \\nWhile setting up any of the Kubernetes clusters, make sure that you adhere to the networking model described in\\nthe \\nKubernetes documentation\\n.\\nAmazon Elastic Kubernetes (EKS)\\nTo set up an EKS cluster (v 1.21 or 1.22), see \\nAmazon Elastic Kubernetes Service (EKS)\\n. \\nPoints to note while creating an EKS\\nWhile creating the resources for EKS, you must use the same EKS cluster name when creating the VPC, EKS cluster, and\\nworker nodes; otherwise, the application installation will fail.\\nOperations Bridge supports single and multi availability zone deployment for the Amazon Web Services EKS cluster\\nworker nodes. You can choose to deploy either on a single AZ or multiple AZ according to your requirement. If you install\\nthis release with a single AZ deployment, you can't change it to a multi AZ deployment later. AWS doesn't\\nsupport converting a single AZ deployment to multi AZ deployment. You can't upgrade your previous single AZ\\ndeployment to a multi AZ deployment.\\nMicrosoft Azure Kubernetes Service (AKS)\\nTo set up an AKS cluster (v 1.22 and 1.23). See \\nMicrosoft Azure Kubernetes Service (AKS)\\n.\\nPoints to note while creating an AKS\\nYou must place all the created resources in the same \\nVNet\\n to prevent performance issues caused by network latency,\\nincluding resource group, AKS cluster, bastion, Azure file share, Azure NetApp Files, PostgreSQL, etc. \\nDon't route the internal traffic inside this \\nVNet\\n out to another \\nVNet\\n (for example, a firewall \\nVNet\\n). Make sure all the traffic\\nbetween the created resources is within the same \\nVNet\\n.\\nAKS isn't compatible with \\nAzure Security Center\\n. Therefore, if you enable \\nAzure Security Center\\n on a subscription,\\nthere will be disk \\nIOPS\\n spikes on all worker nodes of all AKS clusters in the subscription\\nRed Hat OpenShift Container Platform\\nTo set up a Red Hat OpenShift Container Platform cluster (v4.8, v4,9, and v4.10), see \\nRed Hat OpenShift documentation\\n.\\nOther Kubernetes distributions\\nTo set up an RKE cluster (v1.21, v122, and v1.23), see \\nRancher documentation\\n. \\nNote: \\nOperations Bridge deployment on \\nAmazon Web Services China\\n \\nRegions\\n isn't supported.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n187\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Kubernetes related infrastructure\\nThis topic provides you the required specifications to configure the Kubernetes requirements. \\nWhile setting up any of the Kubernetes clusters, make sure that you adhere to the networking model described in\\nthe \\nKubernetes documentation\\n.\\nAmazon Elastic Kubernetes (EKS)\\nTo set up an EKS cluster (v 1.21 or 1.22), see \\nAmazon Elastic Kubernetes Service (EKS)\\n. \\nPoints to note while creating an EKS\\nWhile creating the resources for EKS, you must use the same EKS cluster name when creating the VPC, EKS cluster, and\\nworker nodes; otherwise, the application installation will fail.\\nOperations Bridge supports single and multi availability zone deployment for the Amazon Web Services EKS cluster\\nworker nodes. You can choose to deploy either on a single AZ or multiple AZ according to your requirement. If you install\\nthis release with a single AZ deployment, you can't change it to a multi AZ deployment later. AWS doesn't\\nsupport converting a single AZ deployment to multi AZ deployment. You can't upgrade your previous single AZ\\ndeployment to a multi AZ deployment.\\nMicrosoft Azure Kubernetes Service (AKS)\\nTo set up an AKS cluster (v 1.22 and 1.23). See \\nMicrosoft Azure Kubernetes Service (AKS)\\n.\\nPoints to note while creating an AKS\\nYou must place all the created resources in the same \\nVNet\\n to prevent performance issues caused by network latency,\\nincluding resource group, AKS cluster, bastion, Azure file share, Azure NetApp Files, PostgreSQL, etc. \\nDon't route the internal traffic inside this \\nVNet\\n out to another \\nVNet\\n (for example, a firewall \\nVNet\\n). Make sure all the traffic\\nbetween the created resources is within the same \\nVNet\\n.\\nAKS isn't compatible with \\nAzure Security Center\\n. Therefore, if you enable \\nAzure Security Center\\n on a subscription,\\nthere will be disk \\nIOPS\\n spikes on all worker nodes of all AKS clusters in the subscription\\nRed Hat OpenShift Container Platform\\nTo set up a Red Hat OpenShift Container Platform cluster (v4.8, v4,9, and v4.10), see \\nRed Hat OpenShift documentation\\n.\\nOther Kubernetes distributions\\nTo set up an RKE cluster (v1.21, v122, and v1.23), see \\nRancher documentation\\n. \\nNote: \\nOperations Bridge deployment on \\nAmazon Web Services China\\n \\nRegions\\n isn't supported.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n187\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd294054cf2a0bb305a53de716b71191b'}>,\n",
              "  <Document: {'content': \"For detailed information on storage concepts in Kubernetes see \\nStorage\\n.\\nStorage configuration guidelines for the application\\nThe following table provides a mapping of supported storage types based on the storage selector\\nStorage selector\\nAccess Mode\\nPerformance characteristics\\nQualified storage types\\nDefault RWO\\nReadWriteOnce\\nNFS, Ceph RBD, EBS, Azure Disk\\nDefault RWX\\nReadWriteMany\\nNFS, EBS, Azure Disk, Cephfs\\nFast RWO\\nReadWriteOnce\\nNFS, EBS, Azure Disk\\n The following tables provides a mapping of capabilities and storage selectors\\nPVC\\nStorage class selector helm parameter\\nCapabilities\\nDefault\\nvalue\\nconfigvolumeclaim\\nglobal.persistence.storageClasses.default-rwx\\nall\\ncapabilities\\ndatavolumeclaim\\nglobal.persistence.storageClasses.default-rwx\\nall\\ncapabilities\\ndbvolumeclaim\\nglobal.persistence.storageClasses.default-rwx\\nall\\ncapabilities\\nlogvolumeclaim\\nglobal.persistence.storageClasses.default-rwx\\nall\\ncapabilities\\nomi-artemis-pvc\\nglobal.persistence.storageClasses.default-rwo\\nOnly if\\nOBM is\\nselected\\npvc-omi-0, pvc-omi-1*\\nglobal.persistence.storageClasses.default-rwo\\nOnly if\\nOBM is\\nselected\\nitomdipulsar-bookkeeper-journal-\\nitomdipulsar-bookkeeper-n\\nitomdipulsar.bookkeeper.volumes.journal.storageClassName\\nOnly if AEC\\nor\\nReporting\\nis selected\\nfast-\\ndisks\\nitomdipulsar-bookkeeper-ledgers-\\nitomdipulsar-bookkeeper-n\\nitomdipulsar.bookkeeper.volumes.ledgers.storageClassName\\nOnly if AEC\\nor\\nReporting\\nis selected\\nfast-\\ndisks\\nitomdipulsar-zookeeper-zookeeper-\\ndata-itomdipulsar-zookeeper-n\\nitomdipulsar.zookeeper.volumes.data.storageClassName\\nOnly if AEC\\nor\\nReporting\\nis selected\\nfast-\\ndisks\\nBased on the above requirements, please make sure that you have the required storage set up.\\nAWS\\nStorage type\\nNotes\\nAmazon\\nElastic File\\nSystem\\n(Amazon EFS)\\nMake sure you configure Amazon EFS with two private subnets, Amazon EFS \\nperformance\\nmode \\nas General Purpose and \\nthroughput mode\\n as Bursting Throughput.\\nAmazon\\nElastic Block\\nStore\\n(Amazon EBS)\\nYou will require EBS if you plan to use OPTIC Reporting or any of the capabilities with OPTIC DL. You\\nmust deploy the Amazon EBS Container Storage Interface (CSI)  \\nwithout\\n \\nthe tags.\\nThe OPTIC DL Message Bus uses Amazon Web Services Elastic Block Store (AWS EBS) as the storage\\nwith dynamic provisioning. It's recommended to use \\nio2\\n and \\ngp3\\n EBS volume types. \\nio2\\n volumes are\\nsupported in the specific regions in AWS. See \\nAWS documentation\\n for a list of supported regions.\\nIf you plan to use an existing OPTIC Data Lake from another deployment, then there is no need to deploy\\nEBS again for OPTIC Data Lake.\\nContainerized Operations Bridge 2022.11\\nPage \\n183\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.For detailed information on storage concepts in Kubernetes see \\nStorage\\n.\\nStorage configuration guidelines for the application\\nThe following table provides a mapping of supported storage types based on the storage selector\\nStorage selector\\nAccess Mode\\nPerformance characteristics\\nQualified storage types\\nDefault RWO\\nReadWriteOnce\\nNFS, Ceph RBD, EBS, Azure Disk\\nDefault RWX\\nReadWriteMany\\nNFS, EBS, Azure Disk, Cephfs\\nFast RWO\\nReadWriteOnce\\nNFS, EBS, Azure Disk\\n The following tables provides a mapping of capabilities and storage selectors\\nPVC\\nStorage class selector helm parameter\\nCapabilities\\nDefault\\nvalue\\nconfigvolumeclaim\\nglobal.persistence.storageClasses.default-rwx\\nall\\ncapabilities\\ndatavolumeclaim\\nglobal.persistence.storageClasses.default-rwx\\nall\\ncapabilities\\ndbvolumeclaim\\nglobal.persistence.storageClasses.default-rwx\\nall\\ncapabilities\\nlogvolumeclaim\\nglobal.persistence.storageClasses.default-rwx\\nall\\ncapabilities\\nomi-artemis-pvc\\nglobal.persistence.storageClasses.default-rwo\\nOnly if\\nOBM is\\nselected\\npvc-omi-0, pvc-omi-1*\\nglobal.persistence.storageClasses.default-rwo\\nOnly if\\nOBM is\\nselected\\nitomdipulsar-bookkeeper-journal-\\nitomdipulsar-bookkeeper-n\\nitomdipulsar.bookkeeper.volumes.journal.storageClassName\\nOnly if AEC\\nor\\nReporting\\nis selected\\nfast-\\ndisks\\nitomdipulsar-bookkeeper-ledgers-\\nitomdipulsar-bookkeeper-n\\nitomdipulsar.bookkeeper.volumes.ledgers.storageClassName\\nOnly if AEC\\nor\\nReporting\\nis selected\\nfast-\\ndisks\\nitomdipulsar-zookeeper-zookeeper-\\ndata-itomdipulsar-zookeeper-n\\nitomdipulsar.zookeeper.volumes.data.storageClassName\\nOnly if AEC\\nor\\nReporting\\nis selected\\nfast-\\ndisks\\nBased on the above requirements, please make sure that you have the required storage set up.\\nAWS\\nStorage type\\nNotes\\nAmazon\\nElastic File\\nSystem\\n(Amazon EFS)\\nMake sure you configure Amazon EFS with two private subnets, Amazon EFS \\nperformance\\nmode \\nas General Purpose and \\nthroughput mode\\n as Bursting Throughput.\\nAmazon\\nElastic Block\\nStore\\n(Amazon EBS)\\nYou will require EBS if you plan to use OPTIC Reporting or any of the capabilities with OPTIC DL. You\\nmust deploy the Amazon EBS Container Storage Interface (CSI)  \\nwithout\\n \\nthe tags.\\nThe OPTIC DL Message Bus uses Amazon Web Services Elastic Block Store (AWS EBS) as the storage\\nwith dynamic provisioning. It's recommended to use \\nio2\\n and \\ngp3\\n EBS volume types. \\nio2\\n volumes are\\nsupported in the specific regions in AWS. See \\nAWS documentation\\n for a list of supported regions.\\nIf you plan to use an existing OPTIC Data Lake from another deployment, then there is no need to deploy\\nEBS again for OPTIC Data Lake.\\nContainerized Operations Bridge 2022.11\\nPage \\n183\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '618e7ed10bb092c82d22783caf1ccb82'}>,\n",
              "  <Document: {'content': \"Network related infrastructure\\nThis topic provides you with the required specifications to configure the network requirements.\\nCommunication connection on cloud\\nApplication install uses an external access host for some services. As a prerequisite, you must make sure to allow an L3 Bi\\ndirectional connection between the remote system and AWS or Azure setup to access application services. You can do this\\nusing a VPN connection or Direct Connect. If L3 communication isn't enabled between the remote system and AWS or Azure\\nsetup, you must create an environment in the same VPC or VNet to access application services. You must set up a private\\nDNS zone. \\nYou can either create a private domain or use an existing private domain in \\nRegister domain\\n. For steps to create a private\\ndomain, see the \\nAWS documentation\\n or \\nAzure documentation\\n. You will use this record name for the external access host\\nconfiguration.\\nIn AWS, you can either create a self-signed certificate or use an existing certificate. In the AWS Management Console, search\\nand click \\nCertificate Manager\\n. Import the certificate for the external access host. Once you upload the certificate, it will\\nappear as imported in the AWS Certificate manager tab. You will give this certificate details for\\nthe \\nserverCrt\\n and \\nserverKey\\n parameters when configuring the load balancer for OMT and application.\\nRequired Ports\\nServices can be exposed over a node port or a load balancer. Make sure that these ports are available before you deploy the\\napplication in case you are deploying with service type Node Port. See the \\nNetwork requirement\\n section in \\nSystem\\nrequirements\\n for a consolidated list of default ports used for application deployment.  These are default ports, each port can\\nbe configured to a different value in case one of the listed default port is in use.\\nContainerized Operations Bridge 2022.11\\nPage \\n185\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Network related infrastructure\\nThis topic provides you with the required specifications to configure the network requirements.\\nCommunication connection on cloud\\nApplication install uses an external access host for some services. As a prerequisite, you must make sure to allow an L3 Bi\\ndirectional connection between the remote system and AWS or Azure setup to access application services. You can do this\\nusing a VPN connection or Direct Connect. If L3 communication isn't enabled between the remote system and AWS or Azure\\nsetup, you must create an environment in the same VPC or VNet to access application services. You must set up a private\\nDNS zone. \\nYou can either create a private domain or use an existing private domain in \\nRegister domain\\n. For steps to create a private\\ndomain, see the \\nAWS documentation\\n or \\nAzure documentation\\n. You will use this record name for the external access host\\nconfiguration.\\nIn AWS, you can either create a self-signed certificate or use an existing certificate. In the AWS Management Console, search\\nand click \\nCertificate Manager\\n. Import the certificate for the external access host. Once you upload the certificate, it will\\nappear as imported in the AWS Certificate manager tab. You will give this certificate details for\\nthe \\nserverCrt\\n and \\nserverKey\\n parameters when configuring the load balancer for OMT and application.\\nRequired Ports\\nServices can be exposed over a node port or a load balancer. Make sure that these ports are available before you deploy the\\napplication in case you are deploying with service type Node Port. See the \\nNetwork requirement\\n section in \\nSystem\\nrequirements\\n for a consolidated list of default ports used for application deployment.  These are default ports, each port can\\nbe configured to a different value in case one of the listed default port is in use.\\nContainerized Operations Bridge 2022.11\\nPage \\n185\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5ca020fdd7efa3f4b541b3b17f1c83c9'}>,\n",
              "  <Document: {'content': 'Install OMT for external Kubernetes\\nInstall OMT on external Kubernetes\\nAfter you set up the infrastructure and the cluster, deploy OMT as the first application. Before you proceed with installing OMT,\\nmake sure you have completed preparing the required infrastructure for OMT and Operations Bridge. You can now complete\\nthe OMT prerequisites and deploy OMT by following the OMT documentation starting with \\nSet up prerequisites for OMT\\n. To\\ndeploy OMT tools and utilities, see \\nInstall OMT\\n.  This link takes you to the OMT documentation. In the OMT documentation,\\nfollow the steps required only for external Kubernetes. \\nContainerized Operations Bridge 2022.11\\nPage \\n188\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Install OMT for external Kubernetes\\nInstall OMT on external Kubernetes\\nAfter you set up the infrastructure and the cluster, deploy OMT as the first application. Before you proceed with installing OMT,\\nmake sure you have completed preparing the required infrastructure for OMT and Operations Bridge. You can now complete\\nthe OMT prerequisites and deploy OMT by following the OMT documentation starting with \\nSet up prerequisites for OMT\\n. To\\ndeploy OMT tools and utilities, see \\nInstall OMT\\n.  This link takes you to the OMT documentation. In the OMT documentation,\\nfollow the steps required only for external Kubernetes. \\nContainerized Operations Bridge 2022.11\\nPage \\n188\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e0183aea832f1a450e0b6fac1caacbfd'}>,\n",
              "  <Document: {'content': \"Prepare infrastructure using toolkit and install OMT on\\nexternal Kubernetes\\nThis topic helps you to prepare the external Kubernetes infrastructure using the ITOM Cloud Deployment Toolkit and then\\ninstall OMT and the application.\\nITOM Cloud Deployment Toolkit\\nThe ITOM Cloud Deployment Toolkit provides an automated alternative to setting up cloud infrastructure to deploy the\\napplication. The automation provided through the toolkit is developed over Terraform. With minimal configuration, the toolkit\\nsimplifies the setup of the entire infrastructure in a few steps. As part of the infrastructure, the toolkit creates the following:\\nFor AWS, ITOM Cloud Deployment Toolkit provisions the following appropriate security measures:\\nProvisioning Virtual Private Cloud provisioning (VPC)\\nInternet gateway\\nTwo public subnets each across a different availability zones (AZ)\\nNetwork Address Translation (NAT) gateway attached to the first public subnet\\nTwo private subnets each across a different AZ for EKS\\nTwo private subnets each across a different AZ for RDS\\nSingle private subnet each across a different AZ for Vertica\\nRoute53 public and private hosted zones\\n[Optional] Generate a certificate with a wild card for the public hosted zone\\nRegister a domain over Route53 cloud DNS\\nKubernetes cluster with EKS worker node group on a single AZ\\nSingle Postgres RDS instances\\nElastic File System (EFS) for persistent volumes. EFS folders are created automatically for OMT. The script is available\\nfor the creation of\\nadditional folders for product specific requirements.\\nBastion setup contains clients for Kubernetes, Helm, and Postgres\\nElastic Block Store (EBS) Container Storage Interface (CSI) driver for dynamic EBS volume provisioning\\nExternal DNS to synchronize Kubernetes services and ingresses with Route 53\\nVertica with the following:\\nVertica management console\\nVertica cluster and database\\nOPTIC DL Vertica plugin\\nVertica license\\nFor Azure, ITOM Cloud Deployment Toolkit provisions the following appropriate security measures:\\nAzure virtual network (VNet)\\nResource group\\nAzure container registry\\nAzure Kubernetes Service (AKS) cluster\\nPostgres database\\nAzure premium file share\\nDNS zones\\nBastion host setup with Kubernetes, Docker, Helm, Postgres, and Azure CLI\\nVertica with the following:\\nVertica management console\\nVertica cluster and database\\nOPTIC DL Vertica plugin\\nVertica license\\nYou can upload any ITOM software binaries into the bastion host via Terraform. Additionally, the toolkit also creates core\\nnamespace in Kubernetes along with a secret named 'azure-secret' that allows access to mount Azure file share as a\\npersistent volume.\\nUsing the ITOM Cloud Deployment Toolkit\\n1\\n. \\nDownload the ITOM Cloud Deployment Toolkit package  (\\nitom-cloud-toolkit-202x.xx.0x.zip\\n) that contains the scripts required\\nto configure cloud infrastructure from the \\nMicro Focus ITOM Marketplace\\n website. The \\ntoolkit documentation\\n is also\\navailable online.\\n2\\n. \\nFollow the documentation and complete the infrastructure setup.\\n3\\n. \\nDeploy OMT and Operations Bridge on AWS or Azure.\\n4\\n. \\nEnvironment\\nProcedure to deploy the OMT and application\\nAWS\\nAfter you complete the infrastructure setup using the toolkit, deploy OMT and Operations Bridge. For\\nsteps, see \\nDeploy on AWS\\n.\\nAzure\\nAfter you complete the infrastructure setup using the toolkit, deploy OMT and Operations Bridge. For\\nsteps, see \\nDeploy on Azure\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n189\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Prepare infrastructure using toolkit and install OMT on\\nexternal Kubernetes\\nThis topic helps you to prepare the external Kubernetes infrastructure using the ITOM Cloud Deployment Toolkit and then\\ninstall OMT and the application.\\nITOM Cloud Deployment Toolkit\\nThe ITOM Cloud Deployment Toolkit provides an automated alternative to setting up cloud infrastructure to deploy the\\napplication. The automation provided through the toolkit is developed over Terraform. With minimal configuration, the toolkit\\nsimplifies the setup of the entire infrastructure in a few steps. As part of the infrastructure, the toolkit creates the following:\\nFor AWS, ITOM Cloud Deployment Toolkit provisions the following appropriate security measures:\\nProvisioning Virtual Private Cloud provisioning (VPC)\\nInternet gateway\\nTwo public subnets each across a different availability zones (AZ)\\nNetwork Address Translation (NAT) gateway attached to the first public subnet\\nTwo private subnets each across a different AZ for EKS\\nTwo private subnets each across a different AZ for RDS\\nSingle private subnet each across a different AZ for Vertica\\nRoute53 public and private hosted zones\\n[Optional] Generate a certificate with a wild card for the public hosted zone\\nRegister a domain over Route53 cloud DNS\\nKubernetes cluster with EKS worker node group on a single AZ\\nSingle Postgres RDS instances\\nElastic File System (EFS) for persistent volumes. EFS folders are created automatically for OMT. The script is available\\nfor the creation of\\nadditional folders for product specific requirements.\\nBastion setup contains clients for Kubernetes, Helm, and Postgres\\nElastic Block Store (EBS) Container Storage Interface (CSI) driver for dynamic EBS volume provisioning\\nExternal DNS to synchronize Kubernetes services and ingresses with Route 53\\nVertica with the following:\\nVertica management console\\nVertica cluster and database\\nOPTIC DL Vertica plugin\\nVertica license\\nFor Azure, ITOM Cloud Deployment Toolkit provisions the following appropriate security measures:\\nAzure virtual network (VNet)\\nResource group\\nAzure container registry\\nAzure Kubernetes Service (AKS) cluster\\nPostgres database\\nAzure premium file share\\nDNS zones\\nBastion host setup with Kubernetes, Docker, Helm, Postgres, and Azure CLI\\nVertica with the following:\\nVertica management console\\nVertica cluster and database\\nOPTIC DL Vertica plugin\\nVertica license\\nYou can upload any ITOM software binaries into the bastion host via Terraform. Additionally, the toolkit also creates core\\nnamespace in Kubernetes along with a secret named 'azure-secret' that allows access to mount Azure file share as a\\npersistent volume.\\nUsing the ITOM Cloud Deployment Toolkit\\n1\\n. \\nDownload the ITOM Cloud Deployment Toolkit package  (\\nitom-cloud-toolkit-202x.xx.0x.zip\\n) that contains the scripts required\\nto configure cloud infrastructure from the \\nMicro Focus ITOM Marketplace\\n website. The \\ntoolkit documentation\\n is also\\navailable online.\\n2\\n. \\nFollow the documentation and complete the infrastructure setup.\\n3\\n. \\nDeploy OMT and Operations Bridge on AWS or Azure.\\n4\\n. \\nEnvironment\\nProcedure to deploy the OMT and application\\nAWS\\nAfter you complete the infrastructure setup using the toolkit, deploy OMT and Operations Bridge. For\\nsteps, see \\nDeploy on AWS\\n.\\nAzure\\nAfter you complete the infrastructure setup using the toolkit, deploy OMT and Operations Bridge. For\\nsteps, see \\nDeploy on Azure\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n189\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c9e0b38dc5f17577c06ff14ab5641156'}>,\n",
              "  <Document: {'content': 'Set up prerequisites\\nThis topic lists the tasks that you must complete before deploying Operations Bridge.\\nDatabases\\nTask\\nRole\\nWhere to perform\\nCapability\\nEnvironment\\nCreate the database using DBSQLGenerator script\\nDatabase\\nadministrator\\nExternal\\nPostgreSQL or\\nOracle server\\nAll\\nEmbedded and\\nExternal\\nKubernetes\\nPostgreSQL parameter settings\\n or \\nOracle parameter\\nsettings\\n depending on your database choice.\\nDatabase\\nadministrator\\nExternal\\nPostgreSQL or\\nOracle server\\nContainerized\\nOBM\\nEmbedded and\\nExternal\\nKubernetes\\nUnderstand certificate requirements for TLS\\nconnection\\nDatabase\\nadministrator\\nExternal\\nPostgreSQL or\\nOracle server\\nAll\\nEmbedded and\\nExternal\\nKubernetes\\nIssue certificate utility\\nDatabase\\nadministrator\\nExternal\\nPostgreSQL or\\nOracle server\\nAll\\nEmbedded and\\nExternal\\nKubernetes\\nVertica\\n \\nTask\\nRole\\nWhere to\\nperform\\nCapability\\nEnvironment\\nInstall Vertica\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nInstall the Vertica OPTIC DL plugin\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nCreate the variables file\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nCreate certificates to enable TLS using \\nIssue certificate\\nutility\\n or \\nManually create certificates to enable TLS for\\nVertica\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nConfigure the Vertica database and Enable TLS\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nVerify that TLS is enabled\\n \\n(Optional)\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nTuning parameters for Vertica\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nStorage\\nTask\\nRole\\nWhere to\\nperform\\nCapability\\nEnvironment\\nCreate persistent volumes\\nStorage\\nadministrator\\nNFS server\\nAll\\nEmbedded and External\\nKubernetes\\nCreate local persistent volumes\\non worker nodes\\nStorage\\nadministrator\\nWorker\\nnodes\\nOPTIC Reporting, AEC,\\nHyperscale Observability\\nEmbedded Kubernetes\\nInstall storage provisioner chart\\nApplication\\nowner\\nControl\\nplane node\\nOPTIC Reporting, AEC,\\nHyperscale Observability\\nEmbedded Kubernetes\\nNetwork\\nTask\\nRole\\nWhere to perform\\nCapability\\nEnvironment\\nConfigure service to allow external access to the\\ncluster\\nNetwork\\nadministrator\\nExternal Network Load\\nbalancer\\nAll\\nAws and\\nAzure\\nUpdate the load balancer configuration after OMT\\ninstallation\\nNetwork\\nadministrator\\nExternal Network Load\\nbalancer\\nAll\\nRed Hat\\nOpenShift\\nNote: \\nEach task is associated with the capability and the environment in which you are deploying the\\napplication. Review each task and complete the ones that apply to your scenario.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n190\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Set up prerequisites\\nThis topic lists the tasks that you must complete before deploying Operations Bridge.\\nDatabases\\nTask\\nRole\\nWhere to perform\\nCapability\\nEnvironment\\nCreate the database using DBSQLGenerator script\\nDatabase\\nadministrator\\nExternal\\nPostgreSQL or\\nOracle server\\nAll\\nEmbedded and\\nExternal\\nKubernetes\\nPostgreSQL parameter settings\\n or \\nOracle parameter\\nsettings\\n depending on your database choice.\\nDatabase\\nadministrator\\nExternal\\nPostgreSQL or\\nOracle server\\nContainerized\\nOBM\\nEmbedded and\\nExternal\\nKubernetes\\nUnderstand certificate requirements for TLS\\nconnection\\nDatabase\\nadministrator\\nExternal\\nPostgreSQL or\\nOracle server\\nAll\\nEmbedded and\\nExternal\\nKubernetes\\nIssue certificate utility\\nDatabase\\nadministrator\\nExternal\\nPostgreSQL or\\nOracle server\\nAll\\nEmbedded and\\nExternal\\nKubernetes\\nVertica\\n \\nTask\\nRole\\nWhere to\\nperform\\nCapability\\nEnvironment\\nInstall Vertica\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nInstall the Vertica OPTIC DL plugin\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nCreate the variables file\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nCreate certificates to enable TLS using \\nIssue certificate\\nutility\\n or \\nManually create certificates to enable TLS for\\nVertica\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nConfigure the Vertica database and Enable TLS\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nVerify that TLS is enabled\\n \\n(Optional)\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nTuning parameters for Vertica\\nDatabase\\nadministrator\\nExternal\\nVertica\\nserver\\nOPTIC Reporting,\\nAEC, Hyperscale\\nObservability\\nEmbedded and\\nExternal\\nKubernetes\\nStorage\\nTask\\nRole\\nWhere to\\nperform\\nCapability\\nEnvironment\\nCreate persistent volumes\\nStorage\\nadministrator\\nNFS server\\nAll\\nEmbedded and External\\nKubernetes\\nCreate local persistent volumes\\non worker nodes\\nStorage\\nadministrator\\nWorker\\nnodes\\nOPTIC Reporting, AEC,\\nHyperscale Observability\\nEmbedded Kubernetes\\nInstall storage provisioner chart\\nApplication\\nowner\\nControl\\nplane node\\nOPTIC Reporting, AEC,\\nHyperscale Observability\\nEmbedded Kubernetes\\nNetwork\\nTask\\nRole\\nWhere to perform\\nCapability\\nEnvironment\\nConfigure service to allow external access to the\\ncluster\\nNetwork\\nadministrator\\nExternal Network Load\\nbalancer\\nAll\\nAws and\\nAzure\\nUpdate the load balancer configuration after OMT\\ninstallation\\nNetwork\\nadministrator\\nExternal Network Load\\nbalancer\\nAll\\nRed Hat\\nOpenShift\\nNote: \\nEach task is associated with the capability and the environment in which you are deploying the\\napplication. Review each task and complete the ones that apply to your scenario.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n190\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '212cadf5d5dbb5cdc658af87d0c5058c'}>,\n",
              "  <Document: {'content': 'Other prerequisites\\nTask\\nRole\\nWhere to perform\\nCapability\\nEnvironment\\nImport UCMDB server certificate\\nApplication\\nowner\\n OBM server\\nHyperscale Observability\\ncapability with OBM\\nEmbedded and\\nExternal Kubernetes\\nCreate an Agent Metric Collector\\nintegration user\\nApplication\\nowner\\nOBM server\\nOPTIC Reporting with\\nexternal OBM\\nEmbedded and\\nExternal Kubernetes\\n \\nUpdate Security context\\nconstraints (SCCs) for application\\nNetwork\\nadministrator\\nExternal Network\\nLoad balancer\\nAll\\nRed Hat OpenShift\\nContainerized Operations Bridge 2022.11\\nPage \\n191\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Other prerequisites\\nTask\\nRole\\nWhere to perform\\nCapability\\nEnvironment\\nImport UCMDB server certificate\\nApplication\\nowner\\n OBM server\\nHyperscale Observability\\ncapability with OBM\\nEmbedded and\\nExternal Kubernetes\\nCreate an Agent Metric Collector\\nintegration user\\nApplication\\nowner\\nOBM server\\nOPTIC Reporting with\\nexternal OBM\\nEmbedded and\\nExternal Kubernetes\\n \\nUpdate Security context\\nconstraints (SCCs) for application\\nNetwork\\nadministrator\\nExternal Network\\nLoad balancer\\nAll\\nRed Hat OpenShift\\nContainerized Operations Bridge 2022.11\\nPage \\n191\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b0dcd66db7efc8a222d1ffb942efc658'}>,\n",
              "  <Document: {'content': 'Create the database using DBSQLGenerator script\\nThe \\nDBSQLGenerator.sh\\n enables you to generate a SQL script that you can execute to create the relational databases. The\\napplication installer zip contains the \\nDBSQLGenerator.sh\\n file under \\nscripts\\n directory on the control plane or bastion or\\nInstaller node\\n.\\n \\nDatabase\\nCapability/Component that\\nrequires the database\\nDescription\\nDefault\\nuser name \\ncdfapiserv\\nerdb\\nCDF\\nAPI server database for \\ncdf-apiserver\\n service\\ncdfapiuser\\ncdfidmdb\\nCDF\\ncdfidmdb\\n is the IdM database used by CDF\\ncdfidmuser\\nidm\\nAll capabilities\\nRequired for managing application user/authentication. You must\\ncreate a separate database for the application. This is different\\nfrom the one created during the OMT installation\\nidm\\nautopass\\nAll capabilities\\nRequired\\n by autopass\\n for managing licenses.\\nautopass\\nbvd\\nOPTIC Reporting,\\nStakeholder\\nDashboards,Cloud\\nMonitoring, Automatic\\nEvent Correlation\\nRequired for OPTIC Reporting or Stakeholder Dashboard\\ncapabilities.\\nbvd\\nobm_mgm\\nt\\nOperations Bridge Manager\\nOBM requires a management database for storing system wide and\\nmanagement related metadata of the OBM environment. \\nobm_mgmt\\nobm_even\\nt\\nOperations Bridge Manager\\nRequired for storing events and related data, such as annotations,\\nconfiguration data, and event correlation rules.\\nobm_event\\nrtsm\\nOperations Bridge Manager\\nThe run time service model database is required for storing\\nconfiguration information gathered from the various OBM and third\\nparty applications and tools. The application uses this\\ninformation when building OBM views.\\nrtsm\\nmonitorin\\ngadmindb\\nCloud Monitoring, OPTIC\\nReporting\\nCloud Monitoring uses this database for storing Cloud\\nMonitoring specific configurations.\\nmonitoringa\\ndminuser\\nmonitorin\\ngsnfdb\\nCloud Monitoring, OPTIC\\nReporting\\nCloud Monitoring uses this database for storing data temporarily.\\nmonitorings\\nnfuser\\ncredential\\nmanager\\nCloud Monitoring, OPTIC\\nReporting\\nCredential manager service uses this database in Cloud Monitoring.\\ncredentialm\\nanageruser\\nbtcd\\nNOM OPTIC Reporting\\n Required for nom-metrics-transform service\\n btcd\\n \\nCreate the relational databases using the script\\n1\\n. \\nOn the control plane or bastion or Installer node, go to the \\n<extracted application installation packages>/scripts\\n directory and\\nexecute the \\nDBSQLGenerator.sh\\n:\\n./DBSQLGenerator.sh \\nThe script allows you to use default values and common passwords for all the SQL query creations. You can also choose to\\ngive the database name, username, and passwords of your choice using the script.  The script will prompt you to give the\\nrequired values for all the queries. For all the queries in the script,  if you press the enter key without entering any value,\\nthe script uses the default value for the parameter (as displayed within the square brackets \"[]\").\\nThis script generates \\nCreateSQL.sql\\n and \\nRemoveSQL.sql\\n, in the current working directory.\\n2\\n. \\nCopy the \\nCreateSQL.sql\\n to the system where you have the PostgreSQL (\\npsql\\n) client (or Oracle) installed.\\n3\\n. \\nExecute \\nCreateSQL.sql\\n with database admin privileges: Run the following commands on the database server to create the\\nusers and databases:\\nTip: \\nIf you plan to use the \\nAppHub UI\\n to deploy OpsBridge and use an \\nexternal PostgreSQL database\\n,\\nyou can create the OpsBridge databases and its users by enabling the \\nAutomatically create required\\ndatabases\\n toggle. However, you will need to give the database administrator username and password to create\\nthe databases. In such a case, make sure you create the databases for OMT services (\\ncdfidm\\n and \\ncdfapiserver\\ndatabases) and you can skip this topic.\\n\\ue917\\n\\ue917\\nNote: \\nThe DB script is provided for Linux operating system. If any other operating system (for example, on\\nWindows) hosts your database then, work with your \\nDBA\\n to adjust the required steps as required.\\nMake sure that you have checked the databases support based on the Kubernetes platforms from the \\nSystem\\nrequirements\\n page.\\nMake a note of the inputs you have provided while running the script as these database names, user\\nnames, and passwords are required while configuring the capabilities. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n193\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Create the database using DBSQLGenerator script\\nThe \\nDBSQLGenerator.sh\\n enables you to generate a SQL script that you can execute to create the relational databases. The\\napplication installer zip contains the \\nDBSQLGenerator.sh\\n file under \\nscripts\\n directory on the control plane or bastion or\\nInstaller node\\n.\\n \\nDatabase\\nCapability/Component that\\nrequires the database\\nDescription\\nDefault\\nuser name \\ncdfapiserv\\nerdb\\nCDF\\nAPI server database for \\ncdf-apiserver\\n service\\ncdfapiuser\\ncdfidmdb\\nCDF\\ncdfidmdb\\n is the IdM database used by CDF\\ncdfidmuser\\nidm\\nAll capabilities\\nRequired for managing application user/authentication. You must\\ncreate a separate database for the application. This is different\\nfrom the one created during the OMT installation\\nidm\\nautopass\\nAll capabilities\\nRequired\\n by autopass\\n for managing licenses.\\nautopass\\nbvd\\nOPTIC Reporting,\\nStakeholder\\nDashboards,Cloud\\nMonitoring, Automatic\\nEvent Correlation\\nRequired for OPTIC Reporting or Stakeholder Dashboard\\ncapabilities.\\nbvd\\nobm_mgm\\nt\\nOperations Bridge Manager\\nOBM requires a management database for storing system wide and\\nmanagement related metadata of the OBM environment. \\nobm_mgmt\\nobm_even\\nt\\nOperations Bridge Manager\\nRequired for storing events and related data, such as annotations,\\nconfiguration data, and event correlation rules.\\nobm_event\\nrtsm\\nOperations Bridge Manager\\nThe run time service model database is required for storing\\nconfiguration information gathered from the various OBM and third\\nparty applications and tools. The application uses this\\ninformation when building OBM views.\\nrtsm\\nmonitorin\\ngadmindb\\nCloud Monitoring, OPTIC\\nReporting\\nCloud Monitoring uses this database for storing Cloud\\nMonitoring specific configurations.\\nmonitoringa\\ndminuser\\nmonitorin\\ngsnfdb\\nCloud Monitoring, OPTIC\\nReporting\\nCloud Monitoring uses this database for storing data temporarily.\\nmonitorings\\nnfuser\\ncredential\\nmanager\\nCloud Monitoring, OPTIC\\nReporting\\nCredential manager service uses this database in Cloud Monitoring.\\ncredentialm\\nanageruser\\nbtcd\\nNOM OPTIC Reporting\\n Required for nom-metrics-transform service\\n btcd\\n \\nCreate the relational databases using the script\\n1\\n. \\nOn the control plane or bastion or Installer node, go to the \\n<extracted application installation packages>/scripts\\n directory and\\nexecute the \\nDBSQLGenerator.sh\\n:\\n./DBSQLGenerator.sh \\nThe script allows you to use default values and common passwords for all the SQL query creations. You can also choose to\\ngive the database name, username, and passwords of your choice using the script.  The script will prompt you to give the\\nrequired values for all the queries. For all the queries in the script,  if you press the enter key without entering any value,\\nthe script uses the default value for the parameter (as displayed within the square brackets \"[]\").\\nThis script generates \\nCreateSQL.sql\\n and \\nRemoveSQL.sql\\n, in the current working directory.\\n2\\n. \\nCopy the \\nCreateSQL.sql\\n to the system where you have the PostgreSQL (\\npsql\\n) client (or Oracle) installed.\\n3\\n. \\nExecute \\nCreateSQL.sql\\n with database admin privileges: Run the following commands on the database server to create the\\nusers and databases:\\nTip: \\nIf you plan to use the \\nAppHub UI\\n to deploy OpsBridge and use an \\nexternal PostgreSQL database\\n,\\nyou can create the OpsBridge databases and its users by enabling the \\nAutomatically create required\\ndatabases\\n toggle. However, you will need to give the database administrator username and password to create\\nthe databases. In such a case, make sure you create the databases for OMT services (\\ncdfidm\\n and \\ncdfapiserver\\ndatabases) and you can skip this topic.\\n\\ue917\\n\\ue917\\nNote: \\nThe DB script is provided for Linux operating system. If any other operating system (for example, on\\nWindows) hosts your database then, work with your \\nDBA\\n to adjust the required steps as required.\\nMake sure that you have checked the databases support based on the Kubernetes platforms from the \\nSystem\\nrequirements\\n page.\\nMake a note of the inputs you have provided while running the script as these database names, user\\nnames, and passwords are required while configuring the capabilities. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n193\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9efff495534ecc1ae4ea9a85bd8b5704'}>,\n",
              "  <Document: {'content': 'Database related application prerequisites\\nThis topic provides information on tasks that you must complete to configure the databases required for deployment.  This\\nsection contains the following topics:\\nCreate the database using DBSQLGenerator script\\nPostgreSQL parameter settings\\nOracle parameter settings\\nContainerized Operations Bridge 2022.11\\nPage \\n192\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Database related application prerequisites\\nThis topic provides information on tasks that you must complete to configure the databases required for deployment.  This\\nsection contains the following topics:\\nCreate the database using DBSQLGenerator script\\nPostgreSQL parameter settings\\nOracle parameter settings\\nContainerized Operations Bridge 2022.11\\nPage \\n192\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '193bb341077209994430bc04731967fc'}>,\n",
              "  <Document: {'content': 'For PostgreSQL database on Embedded Kubernetes or Red Hat OpenShift or generic:\\nsu - postgres\\npsql -f <Path where script is copied>/CreateSQL.sql\\nExample:\\npsql -f /root/script/CreateSQL.sql\\nFor Oracle database on Embedded Kubernetes or Red Hat OpenShift:\\necho exit | sqlplus username/password as SYSDBA @<Path where script is copied>/CreateSQL.sql\\nExample:\\necho exit | sqlplus sys/syspassword as SYSDBA @/tmp/script/CreateSQL.sql\\n \\nFor PostgreSQL database on Azure:\\npsql -h <DB_HOSTNAME> -U <db admin>@<DB_HOSTNAME> -d <database name> -f CreateSQL.sql\\nwhere, \\n<DB_HOSTNAME>\\n is the \\ndb-address\\n. \\n<db admin>\\n is the admin user that was provided to\\nthe \\nDBSQLGenerator.sh\\n script. \\nExample:\\npsql -h itom-toolkit-8bxxx34x-db-postgres.postgres.database.azure.com -U dbadmin@itom-toolkit-8bxxx34x-db-postgres -d postgres -f CreateSQL.sql\\nFor PostgreSQL database on AWS:\\npsql -h <DB_HOSTNAME> -U <db admin> -d <database name> -f CreateSQL.sql\\nwhere, \\n<DB_HOSTNAME>\\n is the \\ndb-address\\n. \\n<db admin>\\n is the admin user that was provided to\\nthe \\nDBSQLGenerator.sh\\n script. \\nExample:\\npsql -h itom-toolkit-8bxxx34x-db-postgres.postgres.database.aws.com -U dbadmin -d postgres -f CreateSQL.sql\\nImportant\\n: \\nCreateSQL.sql\\n contains the specified password in plain text, based on your security policies you\\ncan delete this query file after execution or create the databases. manually.   \\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n194\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.For PostgreSQL database on Embedded Kubernetes or Red Hat OpenShift or generic:\\nsu - postgres\\npsql -f <Path where script is copied>/CreateSQL.sql\\nExample:\\npsql -f /root/script/CreateSQL.sql\\nFor Oracle database on Embedded Kubernetes or Red Hat OpenShift:\\necho exit | sqlplus username/password as SYSDBA @<Path where script is copied>/CreateSQL.sql\\nExample:\\necho exit | sqlplus sys/syspassword as SYSDBA @/tmp/script/CreateSQL.sql\\n \\nFor PostgreSQL database on Azure:\\npsql -h <DB_HOSTNAME> -U <db admin>@<DB_HOSTNAME> -d <database name> -f CreateSQL.sql\\nwhere, \\n<DB_HOSTNAME>\\n is the \\ndb-address\\n. \\n<db admin>\\n is the admin user that was provided to\\nthe \\nDBSQLGenerator.sh\\n script. \\nExample:\\npsql -h itom-toolkit-8bxxx34x-db-postgres.postgres.database.azure.com -U dbadmin@itom-toolkit-8bxxx34x-db-postgres -d postgres -f CreateSQL.sql\\nFor PostgreSQL database on AWS:\\npsql -h <DB_HOSTNAME> -U <db admin> -d <database name> -f CreateSQL.sql\\nwhere, \\n<DB_HOSTNAME>\\n is the \\ndb-address\\n. \\n<db admin>\\n is the admin user that was provided to\\nthe \\nDBSQLGenerator.sh\\n script. \\nExample:\\npsql -h itom-toolkit-8bxxx34x-db-postgres.postgres.database.aws.com -U dbadmin -d postgres -f CreateSQL.sql\\nImportant\\n: \\nCreateSQL.sql\\n contains the specified password in plain text, based on your security policies you\\ncan delete this query file after execution or create the databases. manually.   \\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n194\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '68e307fb7f1cfa249d9d3104309d98df'}>,\n",
              "  <Document: {'content': \"UNDO_ MA\\nNAGEMENT\\nAuto\\nAuto\\nAuto\\nUNDO_ RET\\nENTION\\nOracle d\\nefault val\\nue\\nOracle\\ndefault\\nvalue\\nOracle\\ndefault\\nvalue\\nAutomatic tuning is performed.\\nRECYCLEBI\\nN\\nOff\\nOff\\nOff\\nNLS_LENGT\\nH_SEMANTI\\nCS\\nBYTE\\nBYTE\\nBYTE\\nThis parameter controls the length definition of \\nvarchar\\n columns.\\nWORKAREA\\n_SIZE_ POLI\\nCY\\nAUTO\\nAUTO\\nAUTO\\nPGA_AGGR\\nEGATE_ TAR\\nGET\\n400 MB\\n600 M\\nB\\nMinim\\num 1\\nGB\\nSTATISTICS\\n_LEVEL\\nTYPICAL\\nTYPIC\\nAL\\nTYPIC\\nAL\\nEnables tuning if required.\\nOPTIMIZER_\\nCAPTURE_S\\nQL_PLAN_B\\nASELINES\\nFALSE\\nFALSE\\nFALSE\\nControls Automatic Plan Capture as part of Oracle SQL Management Base (SMB).\\nAUDIT_TRAI\\nL\\nNONE\\nNONE\\nNONE\\nOut-of-the-box database auditing is written to the \\nSYS.AUD$\\n audit trail table. It's\\nadvisable to change this value to none to avoid the growth of the system tablespace.\\nBLANK_ TRI\\nMMING\\nFalse\\nFalse\\nFalse\\nFIXED_DAT\\nE\\nDo not\\nset this\\nvalue.\\nDo\\nnot\\nset\\nthis\\nvalue.\\nDo\\nnot\\nset\\nthis\\nvalue.\\nOBM uses the \\nSYSDATE\\n function for generating system time as part of the application\\nprocess.\\nEvaluation\\nMedium\\nLarge\\nContainerized Operations Bridge 2022.11\\nPage \\n197\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.UNDO_ MA\\nNAGEMENT\\nAuto\\nAuto\\nAuto\\nUNDO_ RET\\nENTION\\nOracle d\\nefault val\\nue\\nOracle\\ndefault\\nvalue\\nOracle\\ndefault\\nvalue\\nAutomatic tuning is performed.\\nRECYCLEBI\\nN\\nOff\\nOff\\nOff\\nNLS_LENGT\\nH_SEMANTI\\nCS\\nBYTE\\nBYTE\\nBYTE\\nThis parameter controls the length definition of \\nvarchar\\n columns.\\nWORKAREA\\n_SIZE_ POLI\\nCY\\nAUTO\\nAUTO\\nAUTO\\nPGA_AGGR\\nEGATE_ TAR\\nGET\\n400 MB\\n600 M\\nB\\nMinim\\num 1\\nGB\\nSTATISTICS\\n_LEVEL\\nTYPICAL\\nTYPIC\\nAL\\nTYPIC\\nAL\\nEnables tuning if required.\\nOPTIMIZER_\\nCAPTURE_S\\nQL_PLAN_B\\nASELINES\\nFALSE\\nFALSE\\nFALSE\\nControls Automatic Plan Capture as part of Oracle SQL Management Base (SMB).\\nAUDIT_TRAI\\nL\\nNONE\\nNONE\\nNONE\\nOut-of-the-box database auditing is written to the \\nSYS.AUD$\\n audit trail table. It's\\nadvisable to change this value to none to avoid the growth of the system tablespace.\\nBLANK_ TRI\\nMMING\\nFalse\\nFalse\\nFalse\\nFIXED_DAT\\nE\\nDo not\\nset this\\nvalue.\\nDo\\nnot\\nset\\nthis\\nvalue.\\nDo\\nnot\\nset\\nthis\\nvalue.\\nOBM uses the \\nSYSDATE\\n function for generating system time as part of the application\\nprocess.\\nEvaluation\\nMedium\\nLarge\\nContainerized Operations Bridge 2022.11\\nPage \\n197\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdbf35aed68400aa8675a0dbf4f0fc1e'}>,\n",
              "  <Document: {'content': \"PostgreSQL parameter settings\\nYou can skip this topic if you are using an embedded PostgreSQL database.\\nThe table below describes the recommended values for a number of PostgreSQL database initialization parameters. However,\\ndepending on how large your OBM deployment is, you might need to increase some values. For example, when using several\\ngateway servers, you might also need to increase the maximum number of client connections allowed.\\nFor the steps to configure these parameters on cloud setup, see the \\nAWS\\n and \\nAzure\\n documentation.\\nParameter name\\nEvaluation\\nMedium\\nLarge\\nshared_buffers\\n512 MB\\n1024 MB\\n4096 MB\\nwork_mem\\n25 MB\\n50 MB\\n50 MB\\nmaintenance_work_mem\\n128 MB\\n256 MB\\n340 MB\\neffective_cache_size\\n1024 MB\\n4096 MB\\n8192 MB\\nmax_wal_size\\n1536 MB\\n2048 MB\\n3072 MB\\nmax_connections\\n475\\n475\\n475\\ncheckpoint_timeout\\n10min\\n or '\\n10 min\\n'\\n20min\\n or '\\n20 min\\n'\\n30min \\nor '\\n30 min\\n'\\nIf there is a space between the value and the unit, you should include them in single\\nquotes.\\ncheckpoint_completion_target\\n0.9\\n0.9\\n0.9\\nautovacuum_vacuum_threshold\\n5000\\n5000\\n5000\\nautovacuum_analyze_threshold\\n5000\\n5000\\n5000\\nautovacuum_analyze_scale_facto\\nr\\n0.1\\n0.2\\n0.2\\nlog_min_duration_statement\\n3000\\n3000\\n3000\\ncommit_delay\\n500\\n500\\n500\\nmax_locks_per_transaction\\n512\\n512\\n512\\nContainerized Operations Bridge 2022.11\\nPage \\n195\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.PostgreSQL parameter settings\\nYou can skip this topic if you are using an embedded PostgreSQL database.\\nThe table below describes the recommended values for a number of PostgreSQL database initialization parameters. However,\\ndepending on how large your OBM deployment is, you might need to increase some values. For example, when using several\\ngateway servers, you might also need to increase the maximum number of client connections allowed.\\nFor the steps to configure these parameters on cloud setup, see the \\nAWS\\n and \\nAzure\\n documentation.\\nParameter name\\nEvaluation\\nMedium\\nLarge\\nshared_buffers\\n512 MB\\n1024 MB\\n4096 MB\\nwork_mem\\n25 MB\\n50 MB\\n50 MB\\nmaintenance_work_mem\\n128 MB\\n256 MB\\n340 MB\\neffective_cache_size\\n1024 MB\\n4096 MB\\n8192 MB\\nmax_wal_size\\n1536 MB\\n2048 MB\\n3072 MB\\nmax_connections\\n475\\n475\\n475\\ncheckpoint_timeout\\n10min\\n or '\\n10 min\\n'\\n20min\\n or '\\n20 min\\n'\\n30min \\nor '\\n30 min\\n'\\nIf there is a space between the value and the unit, you should include them in single\\nquotes.\\ncheckpoint_completion_target\\n0.9\\n0.9\\n0.9\\nautovacuum_vacuum_threshold\\n5000\\n5000\\n5000\\nautovacuum_analyze_threshold\\n5000\\n5000\\n5000\\nautovacuum_analyze_scale_facto\\nr\\n0.1\\n0.2\\n0.2\\nlog_min_duration_statement\\n3000\\n3000\\n3000\\ncommit_delay\\n500\\n500\\n500\\nmax_locks_per_transaction\\n512\\n512\\n512\\nContainerized Operations Bridge 2022.11\\nPage \\n195\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '12f8efbd64535af47f7a5e9924cdb76'}>,\n",
              "  <Document: {'content': \"Oracle parameter settings\\nThe table below lists the recommended values for a number of Oracle database initialization parameters when working with\\nthe OBM database server.\\nTo configure these parameters on AWS setup, see the \\nAWS\\n documentation.\\nParameter\\nname\\nOBM deployment\\nRemarks\\nEvaluation\\nMedium\\nLarge\\nDB_BLOCK_\\nSIZE\\n8K\\n8K\\n8K-16\\nK\\nShould be a multiple of the operating system block size.\\nDB_CACHE_\\nADVICE\\nON\\nON\\nON\\nFor gathering statistics when tuning is required.\\nSGA_TARG\\nET\\n1 GB\\n3 GB\\nMinim\\num 4\\nGB\\nSetting this parameter configures Oracle to automatically determine the size of the\\nbuffer cache (\\ndb_cache_size\\n), shared pool (\\nshared_pool_size\\n), large pool\\n(\\nlarge_pool_size\\n), java pool (\\njava_pool_size\\n), and streams pool\\n(\\nstreams_pool_size\\n).\\nThe value configured for \\nSGA_TARGET\\n sets the total size of the System Global Area\\ncomponents.\\nWhen \\nSGA_TARGET\\n is set (its value isn't \\n0\\n), and one of the above pools is also set to a\\nvalue other than zero, the pool value is used as the minimum value for that pool.\\nMEMORY_T\\nARGET\\n2 GB\\n4 GB\\nMinim\\num 5\\nGB\\nAutomatic Memory Management enables the entire instance memory to be\\nautomatically managed and tuned by the instance. The instance memory contains\\nthe System Global Area (SGA) and the Program Global Area (PGA). \\nMEMORY_TARGET\\n is\\nthe only required memory parameter to set. However, it's recommended to set \\nSGA_T\\nARGET\\n or \\nPGA_AGGREGATE_TARGET\\n as well to avoid frequent resizing of the SGA and\\nPGA components. The values entered\\nfor \\nSGA_TARGET\\n and \\nPGA_AGGREGATE_TARGET\\n serve as minimum values.\\nLOG_BUFFE\\nR\\n1 MB\\n3 MB\\n5 MB\\nDB_FILE_M\\nULTIBLOCK_\\nREAD_COU\\nNT\\nOracle d\\nefault val\\nue\\nOracle\\ndefault\\nvalue\\nOracle\\ndefault\\nvalue\\nPROCESSE\\nS\\n500\\n700\\n1000\\nSESSIONS\\n550\\n775\\n1110\\n(1.1 * PROCESSES) + 5\\nOPTIMIZER_\\nINDEX_COS\\nT_ ADJ\\n100\\n100\\n100\\nWhen set to 100, the optimizer evaluates the index access path at the regular cost.\\nWhen set to any other value (for example, 50), the access path is evaluated at that\\npercentage of the regular cost.\\nTIMED_ STA\\nTISTICS\\nTrue\\nTrue\\nTrue\\nLOG_ CHEC\\nKPOINT_ INT\\nERVAL\\n0\\n0\\n0\\nLOG_ CHEC\\nKPOINT_ TI\\nMEOUT\\n0 or ≥ 1\\n800 (gre\\nater than\\nor equal \\nto 1800)\\n0 or ≥\\n1800 (\\ngreate\\nr than \\nor equ\\nal to 1\\n800)\\n0 or ≥\\n1800 (\\ngreate\\nr than \\nor equ\\nal to 1\\n800)\\nOPTIMIZER_\\nMODE\\nALL_RO\\nWS\\nALL_R\\nOWS\\nALL_R\\nOWS\\nCURSOR_ S\\nHARING\\nExact\\nExact\\nExact\\nOPEN_CUR\\nSORS\\n2000\\n2000\\n2000\\nImportant: \\nSkip this topic if you are using an embedded PostgreSQL database.\\n\\ue91b\\n\\ue91b\\nNote\\n: If any of the database initialization parameters listed below are deprecated in your Oracle version, you can\\nignore them.\\nContainerized Operations Bridge 2022.11\\nPage \\n196\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Oracle parameter settings\\nThe table below lists the recommended values for a number of Oracle database initialization parameters when working with\\nthe OBM database server.\\nTo configure these parameters on AWS setup, see the \\nAWS\\n documentation.\\nParameter\\nname\\nOBM deployment\\nRemarks\\nEvaluation\\nMedium\\nLarge\\nDB_BLOCK_\\nSIZE\\n8K\\n8K\\n8K-16\\nK\\nShould be a multiple of the operating system block size.\\nDB_CACHE_\\nADVICE\\nON\\nON\\nON\\nFor gathering statistics when tuning is required.\\nSGA_TARG\\nET\\n1 GB\\n3 GB\\nMinim\\num 4\\nGB\\nSetting this parameter configures Oracle to automatically determine the size of the\\nbuffer cache (\\ndb_cache_size\\n), shared pool (\\nshared_pool_size\\n), large pool\\n(\\nlarge_pool_size\\n), java pool (\\njava_pool_size\\n), and streams pool\\n(\\nstreams_pool_size\\n).\\nThe value configured for \\nSGA_TARGET\\n sets the total size of the System Global Area\\ncomponents.\\nWhen \\nSGA_TARGET\\n is set (its value isn't \\n0\\n), and one of the above pools is also set to a\\nvalue other than zero, the pool value is used as the minimum value for that pool.\\nMEMORY_T\\nARGET\\n2 GB\\n4 GB\\nMinim\\num 5\\nGB\\nAutomatic Memory Management enables the entire instance memory to be\\nautomatically managed and tuned by the instance. The instance memory contains\\nthe System Global Area (SGA) and the Program Global Area (PGA). \\nMEMORY_TARGET\\n is\\nthe only required memory parameter to set. However, it's recommended to set \\nSGA_T\\nARGET\\n or \\nPGA_AGGREGATE_TARGET\\n as well to avoid frequent resizing of the SGA and\\nPGA components. The values entered\\nfor \\nSGA_TARGET\\n and \\nPGA_AGGREGATE_TARGET\\n serve as minimum values.\\nLOG_BUFFE\\nR\\n1 MB\\n3 MB\\n5 MB\\nDB_FILE_M\\nULTIBLOCK_\\nREAD_COU\\nNT\\nOracle d\\nefault val\\nue\\nOracle\\ndefault\\nvalue\\nOracle\\ndefault\\nvalue\\nPROCESSE\\nS\\n500\\n700\\n1000\\nSESSIONS\\n550\\n775\\n1110\\n(1.1 * PROCESSES) + 5\\nOPTIMIZER_\\nINDEX_COS\\nT_ ADJ\\n100\\n100\\n100\\nWhen set to 100, the optimizer evaluates the index access path at the regular cost.\\nWhen set to any other value (for example, 50), the access path is evaluated at that\\npercentage of the regular cost.\\nTIMED_ STA\\nTISTICS\\nTrue\\nTrue\\nTrue\\nLOG_ CHEC\\nKPOINT_ INT\\nERVAL\\n0\\n0\\n0\\nLOG_ CHEC\\nKPOINT_ TI\\nMEOUT\\n0 or ≥ 1\\n800 (gre\\nater than\\nor equal \\nto 1800)\\n0 or ≥\\n1800 (\\ngreate\\nr than \\nor equ\\nal to 1\\n800)\\n0 or ≥\\n1800 (\\ngreate\\nr than \\nor equ\\nal to 1\\n800)\\nOPTIMIZER_\\nMODE\\nALL_RO\\nWS\\nALL_R\\nOWS\\nALL_R\\nOWS\\nCURSOR_ S\\nHARING\\nExact\\nExact\\nExact\\nOPEN_CUR\\nSORS\\n2000\\n2000\\n2000\\nImportant: \\nSkip this topic if you are using an embedded PostgreSQL database.\\n\\ue91b\\n\\ue91b\\nNote\\n: If any of the database initialization parameters listed below are deprecated in your Oracle version, you can\\nignore them.\\nContainerized Operations Bridge 2022.11\\nPage \\n196\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f8d4a086dc65ab3ab4153905a68c55e6'}>,\n",
              "  <Document: {'content': 'Vertica\\nThis section provides information on tasks that you must complete to install and configure Vertica database required for\\ndeployment. This section contains the following topics:\\n1\\n. \\nInstall Vertica\\n2\\n. \\nTuning parameters for Vertica\\n3\\n. \\nCreate certificates to enable TLS in Vertica\\n4\\n. \\nInstall the Vertica OPTIC DL plugin\\n5\\n. \\nCreate the variables file\\n6\\n. \\nConfigure the Vertica database\\nContainerized Operations Bridge 2022.11\\nPage \\n198\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Vertica\\nThis section provides information on tasks that you must complete to install and configure Vertica database required for\\ndeployment. This section contains the following topics:\\n1\\n. \\nInstall Vertica\\n2\\n. \\nTuning parameters for Vertica\\n3\\n. \\nCreate certificates to enable TLS in Vertica\\n4\\n. \\nInstall the Vertica OPTIC DL plugin\\n5\\n. \\nCreate the variables file\\n6\\n. \\nConfigure the Vertica database\\nContainerized Operations Bridge 2022.11\\nPage \\n198\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '94f322b625cbd43c74b0ef4558f41aea'}>,\n",
              "  <Document: {'content': \"Install Vertica\\nIf you plan to enable OPTIC Reporting capability, or external OPTIC Data Lake scenario for a provider application, these use\\nOPTIC Data Lake as a central datastore. OPTIC Data Lake uses Vertica to store data. To populate data into the OPTIC Data Lake\\nyou must connect to the Vertica database.\\nIn this documentation, you will install Vertica for OPTIC Data Lake on systems or VMs that are separate from your\\napplication master (control plane) and worker nodes refers to as External Vertica. It's recommended to use External\\nVertica as the store for all production purposes.\\n Skip this topic if you are using the Vertica container. Embedded\\nVertica is supported during evaluation only.\\nFor all production deployments, it's recommended to use external Vertica deployments with a minimum of three\\ndedicated nodes for High Availability, load balancing, and fault tolerance. \\nIf you are planning to deploy the application in a VMware environment, it's recommended that the virtual machines have\\npolicies applied that end the VM suspension or the VM migration (for example VMware vMotion) while \\nMicro Focus\\nsoftware is running on it. A slow vMotion on a Vertica or an OPTIC Management Toolkit (OMT) node VM may cause\\nnetworking timeouts between management VMs that lead to problems with the OMT cluster or \\nMicro Focus\\n software\\nrunning on it. For more details about Vertica on VMware, see \\nVertica in a Virtualized Environment\\n.\\nTo install and set up a new Vertica database perform the following:\\n1\\n. \\nMake sure that you have computed resources and storage using the Sizing calculator.\\n2\\n. \\nYou can tune the TCP/IP stack for network performance on the system where you plan to install Vertica for the production\\nenvironment, useful in scaled out networks.\\na\\n. \\nAs the root user, edit the \\nsysctl.conf\\n file in the \\n/etc/\\n directory and set the following parameters:\\n#Sets the receive socket buffer maximum size in bytes.\\nnet.core.rmem_max = 16777216\\n# Increase number of incoming connections\\nnet.core.somaxconn = 1024\\n#Sets the receive socket buffer default size in bytes.\\nnet.core.wmem_default = 212992\\n#Sets the send socket buffer maximum size in bytes.\\nnet.core.wmem_max = 16777216\\n#Sets the maximum number of packets allowed to queue when a particular interface receives packets faster than the kernel can process them.\\n# increase the length of the processor input queue\\nnet.core.netdev_max_backlog = 100000\\nnet.ipv4.tcp_mem = 16777216 16777216 16777216\\nnet.ipv4.tcp_rmem = 8192 262144 8388608\\nnet.ipv4.tcp_wmem = 8192 262144 8388608\\nnet.ipv4.udp_mem = 16777216 16777216 16777216 \\nnet.ipv4.udp_rmem_min = 16384\\nnet.ipv4.udp_wmem_min = 16384\\nvm.dirty_ratio = 5\\nvm.swappiness = 1\\nb\\n. \\nThe parameters updated in step 1 take effect after restart. However, if you want to avoid restarting, run the following\\ncommand to load the file:\\nsysctl -p /etc/sysctl.conf\\nThis will instantly update the parameters.\\nFor information on recommended network tuning parameters for the Vertica cluster, see \\nTuning the TCP/IP Stack\\nin the \\nVertica Hardware Guide\\n.\\n(\\nOptional\\n) You may choose to run the following command to verify the updated values of the parameters:\\nsysctl net.core.netdev_max_backlog net.core.rmem_max net.core.somaxconn net.core.wmem_default net.core.wmem_max net.ipv4.tcp_mem net.ipv4.tcp_rmem net.ipv4.tcp_wmem net.ipv4.udp_mem net.ipv4.udp_rmem_min net.ipv4.udp_wmem_min vm.dirty_ratio vm.swappiness\\n3\\n. \\nEnsure that you have obtained the information about the supported Vertica version and downloaded the Vertica package\\nfrom the \\nSoftware Licenses and Downloads\\n website.\\n4\\n. \\nSee the following Vertica documentation sections before you install Vertica:\\nVertica documentation link\\nDescription\\nInstalling Vertica on cloud\\n or\\nInstall manually\\nContains information to install Vertica based on your platform.\\nRecommendations for Sizing\\nVertica Nodes and Cluster\\nContains recommendations for hardware for your Vertica nodes. It also provides\\ninformation that helps you appropriately size the hardware components to meet\\nthe needs of your environment.\\nSupported Operating Systems\\nand File System Requirements\\nfor Vertica Server\\nContains information about the operating systems and file systems required for\\nthe installation of the Vertica server.\\nIt's recommended to use \\next4\\n or \\nXFS\\n file systems for \\nVertica 10.x\\n. \\nVertica in a Virtualized\\nEnvironment\\nContains information about installing Vertica in a virtualized environment.\\nRequired packages\\nContains information about the packages that require installation on all nodes in\\nyour cluster before you install the database platform.\\nInstallation Overview and\\nChecklist\\nContains an overview of installation tasks.\\nBefore You Install Vertica\\nProvides the required steps to ensure the node on which you install Vertica\\nmeets all the prerequisites.\\n5\\n. \\nLog in as the root user on one of the nodes of the Vertica cluster. Run the following command:\\nContainerized Operations Bridge 2022.11\\nPage \\n199\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Install Vertica\\nIf you plan to enable OPTIC Reporting capability, or external OPTIC Data Lake scenario for a provider application, these use\\nOPTIC Data Lake as a central datastore. OPTIC Data Lake uses Vertica to store data. To populate data into the OPTIC Data Lake\\nyou must connect to the Vertica database.\\nIn this documentation, you will install Vertica for OPTIC Data Lake on systems or VMs that are separate from your\\napplication master (control plane) and worker nodes refers to as External Vertica. It's recommended to use External\\nVertica as the store for all production purposes.\\n Skip this topic if you are using the Vertica container. Embedded\\nVertica is supported during evaluation only.\\nFor all production deployments, it's recommended to use external Vertica deployments with a minimum of three\\ndedicated nodes for High Availability, load balancing, and fault tolerance. \\nIf you are planning to deploy the application in a VMware environment, it's recommended that the virtual machines have\\npolicies applied that end the VM suspension or the VM migration (for example VMware vMotion) while \\nMicro Focus\\nsoftware is running on it. A slow vMotion on a Vertica or an OPTIC Management Toolkit (OMT) node VM may cause\\nnetworking timeouts between management VMs that lead to problems with the OMT cluster or \\nMicro Focus\\n software\\nrunning on it. For more details about Vertica on VMware, see \\nVertica in a Virtualized Environment\\n.\\nTo install and set up a new Vertica database perform the following:\\n1\\n. \\nMake sure that you have computed resources and storage using the Sizing calculator.\\n2\\n. \\nYou can tune the TCP/IP stack for network performance on the system where you plan to install Vertica for the production\\nenvironment, useful in scaled out networks.\\na\\n. \\nAs the root user, edit the \\nsysctl.conf\\n file in the \\n/etc/\\n directory and set the following parameters:\\n#Sets the receive socket buffer maximum size in bytes.\\nnet.core.rmem_max = 16777216\\n# Increase number of incoming connections\\nnet.core.somaxconn = 1024\\n#Sets the receive socket buffer default size in bytes.\\nnet.core.wmem_default = 212992\\n#Sets the send socket buffer maximum size in bytes.\\nnet.core.wmem_max = 16777216\\n#Sets the maximum number of packets allowed to queue when a particular interface receives packets faster than the kernel can process them.\\n# increase the length of the processor input queue\\nnet.core.netdev_max_backlog = 100000\\nnet.ipv4.tcp_mem = 16777216 16777216 16777216\\nnet.ipv4.tcp_rmem = 8192 262144 8388608\\nnet.ipv4.tcp_wmem = 8192 262144 8388608\\nnet.ipv4.udp_mem = 16777216 16777216 16777216 \\nnet.ipv4.udp_rmem_min = 16384\\nnet.ipv4.udp_wmem_min = 16384\\nvm.dirty_ratio = 5\\nvm.swappiness = 1\\nb\\n. \\nThe parameters updated in step 1 take effect after restart. However, if you want to avoid restarting, run the following\\ncommand to load the file:\\nsysctl -p /etc/sysctl.conf\\nThis will instantly update the parameters.\\nFor information on recommended network tuning parameters for the Vertica cluster, see \\nTuning the TCP/IP Stack\\nin the \\nVertica Hardware Guide\\n.\\n(\\nOptional\\n) You may choose to run the following command to verify the updated values of the parameters:\\nsysctl net.core.netdev_max_backlog net.core.rmem_max net.core.somaxconn net.core.wmem_default net.core.wmem_max net.ipv4.tcp_mem net.ipv4.tcp_rmem net.ipv4.tcp_wmem net.ipv4.udp_mem net.ipv4.udp_rmem_min net.ipv4.udp_wmem_min vm.dirty_ratio vm.swappiness\\n3\\n. \\nEnsure that you have obtained the information about the supported Vertica version and downloaded the Vertica package\\nfrom the \\nSoftware Licenses and Downloads\\n website.\\n4\\n. \\nSee the following Vertica documentation sections before you install Vertica:\\nVertica documentation link\\nDescription\\nInstalling Vertica on cloud\\n or\\nInstall manually\\nContains information to install Vertica based on your platform.\\nRecommendations for Sizing\\nVertica Nodes and Cluster\\nContains recommendations for hardware for your Vertica nodes. It also provides\\ninformation that helps you appropriately size the hardware components to meet\\nthe needs of your environment.\\nSupported Operating Systems\\nand File System Requirements\\nfor Vertica Server\\nContains information about the operating systems and file systems required for\\nthe installation of the Vertica server.\\nIt's recommended to use \\next4\\n or \\nXFS\\n file systems for \\nVertica 10.x\\n. \\nVertica in a Virtualized\\nEnvironment\\nContains information about installing Vertica in a virtualized environment.\\nRequired packages\\nContains information about the packages that require installation on all nodes in\\nyour cluster before you install the database platform.\\nInstallation Overview and\\nChecklist\\nContains an overview of installation tasks.\\nBefore You Install Vertica\\nProvides the required steps to ensure the node on which you install Vertica\\nmeets all the prerequisites.\\n5\\n. \\nLog in as the root user on one of the nodes of the Vertica cluster. Run the following command:\\nContainerized Operations Bridge 2022.11\\nPage \\n199\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ccfc4a98cfcbd143a49d8741f2e9d54c'}>,\n",
              "  <Document: {'content': \"rpm -ivh vertica-<version>.x86_64.RHEL6.rpm\\n6\\n. \\nRun the validation scripts as mentioned in the \\nVertica documentation\\n.\\n7\\n. \\nRun the following \\ninstall_vertica\\n command as the root user. In a Vertica cluster, run the command on one of your nodes\\nonly and it installs and configures Vertica on all your intended cluster nodes.\\n/opt/vertica/sbin/install_vertica --hosts <FQDN_of_Node1,FQDN_of_Node2,FQDN_of_Node3> --rpm /tmp/vertica-<version>.x86_64.RHEL6.rpm --dba-user <db-admin-username> [ --data-dir <database data files path> ] [ --point-to-point ]\\nWhere\\n<FQDN_of_Node1,FQDN_of_Node2,FQDN_of_Node3>\\n is a comma separated list of FQDNs of the servers on which you are\\ninstalling Vertica.  For a single node Vertica, give the Vertica node hostname or IP address.\\n<db-admin-username>\\n is the name of the database superuser system account to create, if it doesn't exist.  If you don't\\nspecify the \\n--dba-user\\n parameter, the default account name is \\ndbadmin\\n.\\n<database data files path>\\n is the data directory where the database data and catalog files are stored. It can be overridden in\\nthe create a Vertica database step if you wish to specify separate directories for the data and catalog directories. If you\\ndon't specify the \\n--data-dir\\n parameter, the default location is \\n/home/dbadmin\\n. Please follow your company's best practices\\nwhen deciding whether you should change the default behavior. If your data files path is different from \\n/opt/vertica\\n, make\\nsure you change the ownership to \\n<db-admin-username>\\n, else installation will fail.\\nFor example, the highest throughput is usually obtained when placing the \\ncatalog \\nand \\ndata files\\n on different filesystems\\nthat are mounted on different underlying storage devices.\\nSpecify the \\n--point-to-point\\n parameter if the Vertica nodes in your cluster are running on virtual servers or aren't on the\\nsame subnet.\\nFor information on the \\ninstall_vertica\\n script, refer to \\nInstalling Vertica with the Installation Script\\n and \\ninstall_vertica Options\\n.\\nIf you see any errors while you run the \\ninstall_vertica\\n script, follow the Vertica documentation links that appear on the\\ncommand prompt to resolve them.\\nExample:\\n/opt/vertica/sbin/install_vertica --hosts VerticaNode1.example.net,VerticaNode2.example.net,VerticaNode3.example.net --rpm /tmp/vertica-<version>.x86_64.RHEL6.rpm --dba-user dbadmin --data-dir /var/opt/vertica\\n8\\n. \\nOn the Vertica host, as the \\n<db-admin-username>\\n, execute the \\n/opt/vertica/bin/adminTools\\n program. The program prompts you\\nto enter a license file. Enter the path to the license key. Select \\nAccept\\n for the End User License Agreement and exit the\\nprogram. For more information to apply the license, see the \\nVertica documentation\\n.\\n9\\n. \\nYou can tune the memory used by adjusting the MALLOC_ARENA_MAX environment variable. This parameter enables you\\nto control the memory growth in Vertica. If you don't tune this parameter, over a period of time, the Vertica process may\\ncrash due to memory issues. Follow these steps: \\na\\n. \\nLog in as the database administrator user. Or, run the following command to switch to the database administrator\\nuser:\\nsu <db-admin-username>\\nb\\n. \\nEdit the  \\n~/.bashrc\\n file on all Vertica nodes add the following line:\\nexport MALLOC_ARENA_MAX=4\\nc\\n. \\nLog out as \\n<db-admin-username>\\n, log back in as \\n<db-admin-username>\\n.\\nd\\n. \\nRun the following command to check if the environment variables are set correctly:\\n$ set | grep ARENA\\nYou must get the following output:\\nMALLOC_ARENA_MAX=4\\n10\\n. \\nSet the user process and the number of file limits. From the\\n/etc/security/\\n directory, edit the \\nlimits.conf\\n file and set the\\nfollowing recommended limits:\\ndbadmin - nproc 257652\\ndbadmin - nofile 257652\\n11\\n. \\nAs the \\n<db-admin-username>\\n, run the following command to create a Vertica database:\\n/opt/vertica/bin/adminTools -t create_db -d <db-name> -p <db-password> --hosts=<FQDN_of_Node1>,<FQDN_of_Node2>,<FQDN_of_Node3> [ --data_path=<path of data directory> --catalog_path=<path of catalog directory> ]\\nWhere: \\ndb-name\\n: Vertica database instance name.\\ndb-password\\n: is the password for the Vertica database instance.\\nReplace \\n<FQDN_of_Node1>,<FQDN_of_Node2>,<FQDN_of_Node3>\\n with the host names of Vertica cluster.\\n<path of data directory>\\n Replace with the path of the database data files. If you don't specify the \\n--data_path\\n parameter, the \\n--data-dir\\n uses the command configured in the \\ninstall_vertica\\n.\\n<path of catalog directory>\\n Replace with the path of the catalog files. It's your decision on whether to specify non-default\\npaths to obtain higher performance. Vertica product documentation discusses best practices for preparing disk storage\\nlocations.\\nExample:\\n/opt/vertica/bin/adminTools -t create_db -d itomdb -p 'MyPassword' --hosts=VerticaNode1.example.net,VerticaNode2.example.net,VerticaNode3.example.net --data_path=/var/opt/vertica --catalog_path=/var/opt/vertica\\nThis example uses \\n'MyPassword'\\n as the password for the \\nitomdb\\n database instance. For more information to create the\\nVertica database, see \\nVertica documentation\\n.\\nOnce you complete installing Vertica and creating the database, complete the configurations - \\nInstall OPTIC DL Vertica\\nPlugin\\n, \\nCreate the variables file\\n, and \\nConfigure Vertica database\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n200\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.rpm -ivh vertica-<version>.x86_64.RHEL6.rpm\\n6\\n. \\nRun the validation scripts as mentioned in the \\nVertica documentation\\n.\\n7\\n. \\nRun the following \\ninstall_vertica\\n command as the root user. In a Vertica cluster, run the command on one of your nodes\\nonly and it installs and configures Vertica on all your intended cluster nodes.\\n/opt/vertica/sbin/install_vertica --hosts <FQDN_of_Node1,FQDN_of_Node2,FQDN_of_Node3> --rpm /tmp/vertica-<version>.x86_64.RHEL6.rpm --dba-user <db-admin-username> [ --data-dir <database data files path> ] [ --point-to-point ]\\nWhere\\n<FQDN_of_Node1,FQDN_of_Node2,FQDN_of_Node3>\\n is a comma separated list of FQDNs of the servers on which you are\\ninstalling Vertica.  For a single node Vertica, give the Vertica node hostname or IP address.\\n<db-admin-username>\\n is the name of the database superuser system account to create, if it doesn't exist.  If you don't\\nspecify the \\n--dba-user\\n parameter, the default account name is \\ndbadmin\\n.\\n<database data files path>\\n is the data directory where the database data and catalog files are stored. It can be overridden in\\nthe create a Vertica database step if you wish to specify separate directories for the data and catalog directories. If you\\ndon't specify the \\n--data-dir\\n parameter, the default location is \\n/home/dbadmin\\n. Please follow your company's best practices\\nwhen deciding whether you should change the default behavior. If your data files path is different from \\n/opt/vertica\\n, make\\nsure you change the ownership to \\n<db-admin-username>\\n, else installation will fail.\\nFor example, the highest throughput is usually obtained when placing the \\ncatalog \\nand \\ndata files\\n on different filesystems\\nthat are mounted on different underlying storage devices.\\nSpecify the \\n--point-to-point\\n parameter if the Vertica nodes in your cluster are running on virtual servers or aren't on the\\nsame subnet.\\nFor information on the \\ninstall_vertica\\n script, refer to \\nInstalling Vertica with the Installation Script\\n and \\ninstall_vertica Options\\n.\\nIf you see any errors while you run the \\ninstall_vertica\\n script, follow the Vertica documentation links that appear on the\\ncommand prompt to resolve them.\\nExample:\\n/opt/vertica/sbin/install_vertica --hosts VerticaNode1.example.net,VerticaNode2.example.net,VerticaNode3.example.net --rpm /tmp/vertica-<version>.x86_64.RHEL6.rpm --dba-user dbadmin --data-dir /var/opt/vertica\\n8\\n. \\nOn the Vertica host, as the \\n<db-admin-username>\\n, execute the \\n/opt/vertica/bin/adminTools\\n program. The program prompts you\\nto enter a license file. Enter the path to the license key. Select \\nAccept\\n for the End User License Agreement and exit the\\nprogram. For more information to apply the license, see the \\nVertica documentation\\n.\\n9\\n. \\nYou can tune the memory used by adjusting the MALLOC_ARENA_MAX environment variable. This parameter enables you\\nto control the memory growth in Vertica. If you don't tune this parameter, over a period of time, the Vertica process may\\ncrash due to memory issues. Follow these steps: \\na\\n. \\nLog in as the database administrator user. Or, run the following command to switch to the database administrator\\nuser:\\nsu <db-admin-username>\\nb\\n. \\nEdit the  \\n~/.bashrc\\n file on all Vertica nodes add the following line:\\nexport MALLOC_ARENA_MAX=4\\nc\\n. \\nLog out as \\n<db-admin-username>\\n, log back in as \\n<db-admin-username>\\n.\\nd\\n. \\nRun the following command to check if the environment variables are set correctly:\\n$ set | grep ARENA\\nYou must get the following output:\\nMALLOC_ARENA_MAX=4\\n10\\n. \\nSet the user process and the number of file limits. From the\\n/etc/security/\\n directory, edit the \\nlimits.conf\\n file and set the\\nfollowing recommended limits:\\ndbadmin - nproc 257652\\ndbadmin - nofile 257652\\n11\\n. \\nAs the \\n<db-admin-username>\\n, run the following command to create a Vertica database:\\n/opt/vertica/bin/adminTools -t create_db -d <db-name> -p <db-password> --hosts=<FQDN_of_Node1>,<FQDN_of_Node2>,<FQDN_of_Node3> [ --data_path=<path of data directory> --catalog_path=<path of catalog directory> ]\\nWhere: \\ndb-name\\n: Vertica database instance name.\\ndb-password\\n: is the password for the Vertica database instance.\\nReplace \\n<FQDN_of_Node1>,<FQDN_of_Node2>,<FQDN_of_Node3>\\n with the host names of Vertica cluster.\\n<path of data directory>\\n Replace with the path of the database data files. If you don't specify the \\n--data_path\\n parameter, the \\n--data-dir\\n uses the command configured in the \\ninstall_vertica\\n.\\n<path of catalog directory>\\n Replace with the path of the catalog files. It's your decision on whether to specify non-default\\npaths to obtain higher performance. Vertica product documentation discusses best practices for preparing disk storage\\nlocations.\\nExample:\\n/opt/vertica/bin/adminTools -t create_db -d itomdb -p 'MyPassword' --hosts=VerticaNode1.example.net,VerticaNode2.example.net,VerticaNode3.example.net --data_path=/var/opt/vertica --catalog_path=/var/opt/vertica\\nThis example uses \\n'MyPassword'\\n as the password for the \\nitomdb\\n database instance. For more information to create the\\nVertica database, see \\nVertica documentation\\n.\\nOnce you complete installing Vertica and creating the database, complete the configurations - \\nInstall OPTIC DL Vertica\\nPlugin\\n, \\nCreate the variables file\\n, and \\nConfigure Vertica database\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n200\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4bbc27b8d27912e732597d44129982db'}>,\n",
              "  <Document: {'content': 'Prepare Vertica database\\nAfter you install Vertica, you must install the package that contains OPTIC DL Vertica Plugin on any one Vertica node. The\\nplugin library becomes a part of the database and accessible to all nodes in the cluster. The OPTIC DL Vertica Plugin RPM\\nincludes an installation script that will initialize Vertica for OPTIC DL. You must ensure to use the same values during the OPTIC\\nDL Vertica Plugin install and application deployment.\\nDownload the plugin\\nAfter installing external Vertica and setting up the database, follow these steps on any one of the Vertica nodes:\\n1\\n. \\nCopy the \\nitom-di-pulsarudx-<version>.x86_64.rpm\\n  from the location where you have extracted the application installation\\npackage into a Vertica node.\\n2\\n. \\nRun the following command to switch as the \\nroot\\n user:\\nsu - root\\n3\\n. \\nRun the following command to install the rpm:\\nrpm -ivh <rpm download location>/itom-di-pulsarudx-<version>.x86_64.rpm\\nThe \\ndbinit.sh\\n script is in the location\\n - /usr/local/itom-di-pulsarudx/bin\\nTip\\n:\\n See \\nDownload the installer packages\\n topic for the details on certified \\nitom-di-pulsarudx\\n version and the\\nfile location.\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n201\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Prepare Vertica database\\nAfter you install Vertica, you must install the package that contains OPTIC DL Vertica Plugin on any one Vertica node. The\\nplugin library becomes a part of the database and accessible to all nodes in the cluster. The OPTIC DL Vertica Plugin RPM\\nincludes an installation script that will initialize Vertica for OPTIC DL. You must ensure to use the same values during the OPTIC\\nDL Vertica Plugin install and application deployment.\\nDownload the plugin\\nAfter installing external Vertica and setting up the database, follow these steps on any one of the Vertica nodes:\\n1\\n. \\nCopy the \\nitom-di-pulsarudx-<version>.x86_64.rpm\\n  from the location where you have extracted the application installation\\npackage into a Vertica node.\\n2\\n. \\nRun the following command to switch as the \\nroot\\n user:\\nsu - root\\n3\\n. \\nRun the following command to install the rpm:\\nrpm -ivh <rpm download location>/itom-di-pulsarudx-<version>.x86_64.rpm\\nThe \\ndbinit.sh\\n script is in the location\\n - /usr/local/itom-di-pulsarudx/bin\\nTip\\n:\\n See \\nDownload the installer packages\\n topic for the details on certified \\nitom-di-pulsarudx\\n version and the\\nfile location.\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n201\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3edebb0c577e7829b6f1968171b610e9'}>,\n",
              "  <Document: {'content': \"Configure the variables\\nThe \\ndbinit.sh\\n \\ngenconfig\\n option allows you to configure the variables to create objects within Vertica while executing the\\nscript. You must run this command for a new deployment scenario where the \\ndbinit_conf.yaml\\n file isn't available in the location \\n/\\nusr/local/itom-di-pulsarudx/conf\\n. If you have already run the script earlier and the file exists, you can skip running the command.\\nThe command generates a file in \\n/usr/local/itom-di-pulsarudx/conf/dbinit_conf.yaml\\n. If you plan to run the \\ndbinit.sh\\n script later or on\\nthe next runs, you needn't set the variables again. The script uses the \\ndbinit_conf.yaml\\n file. Note that this step uses default\\nvalues unless the required variables are set. You can edit the file to change the default values before you run the \\ndbinit.sh\\n to\\ninitialize or update the database.\\nPerform the following steps on the Vertica node where you have installed the RPM: \\n1\\n. \\nLog on to the Vertica node as the \\nroot\\n user. \\n2\\n. \\nGo to the location \\n/usr/local/itom-di-pulsarudx/bin.\\n3\\n. \\nRun the following command to create the configuration file for the variables:\\n./dbinit.sh genconfig <options>\\nOptions usage:\\n \\n./dbinit.sh genconfig [-h] [-f|--filepath] [-y|--yes] [-p false| --postload false]\\nOption\\nDescription\\n-h\\nDisplay the script usage or help.\\nUsage\\n: \\n./dbinit.sh genconfig -h\\n-y\\n, \\n--yes\\nForce overwrite. The script prompts you to overwrite an existing configuration file.\\nThis option will cause the file to be overwritten.\\nUsage\\n: \\n./dbinit.sh genconfig -y \\nOR\\nUsage\\n: \\n./dbinit.sh genconfig --yes\\n-f\\n, \\n--filepath\\nThe default configuration file location - \\n/usr/local/itom-di-pulsar-\\nudx/conf/dbinit_conf.yaml\\n. However, if you want to generate a configuration file in a\\ndifferent location you can use this option.\\nUsage\\n: \\n./dbinit.sh genconfig -f /tmp/my_conf.yaml \\nOR\\nUsage\\n: \\n./dbinit.sh genconfig --filepath /tmp/my_conf.yaml\\n-p false\\n, \\n--postload false\\nThe configuration file will have a Postload resource pool configured by default. If you\\ndon't require this resource pool, you can use this option.\\nUsage\\n: \\n./dbinit.sh genconfig -p false \\nOR\\nUsage\\n: \\n./dbinit.sh genconfig --postload false\\nA \\ndbinit_conf_sample.yaml\\n file is available in the location \\n/usr/local/itom-di-pulsarudx/conf\\n. The following tables explain the\\nenvironment variables and the default values:\\nglobal\\nParameter\\nDescription\\nverticaAdminUsers.admin\\nThis is the database administrator username.\\nDefault value\\n - \\ndbadmin\\nverticaDatabase\\nThis is the Vertica database name used by OPTIC DL.\\nDefault value\\n - \\nitomdb\\nverticaHome\\nThis is the Vertica install location.\\nDefault value\\n - \\n/opt/vertica\\ntenants\\nParameter\\nDescription\\nNote\\n: For a fresh deployment, you must give the variable settings using \\ngenconfig\\n option. If you use the settings\\nsupported in earlier versions, you can give the input through environment variables. However, to edit the settings\\nprovided with this version you must create a \\ndbinit_conf.yaml\\n file and edit the settings. It's recommended to use \\nge\\nnconfig\\n as it supports all the settings that you can configure through \\ndbinit.sh\\n script.\\nImportant\\n: \\nYou can edit the \\ndbinit_conf.yaml\\n and edit the parameters and values as required by the\\napplication.\\nRemoving resource pool configuration from the configuration file won't cause \\ndbinit.sh\\n to remove the resource\\npool from Vertica. You must manually remove the pool from Vertica or you can set all the values to zero to disable\\nthe resource pool.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n202\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Configure the variables\\nThe \\ndbinit.sh\\n \\ngenconfig\\n option allows you to configure the variables to create objects within Vertica while executing the\\nscript. You must run this command for a new deployment scenario where the \\ndbinit_conf.yaml\\n file isn't available in the location \\n/\\nusr/local/itom-di-pulsarudx/conf\\n. If you have already run the script earlier and the file exists, you can skip running the command.\\nThe command generates a file in \\n/usr/local/itom-di-pulsarudx/conf/dbinit_conf.yaml\\n. If you plan to run the \\ndbinit.sh\\n script later or on\\nthe next runs, you needn't set the variables again. The script uses the \\ndbinit_conf.yaml\\n file. Note that this step uses default\\nvalues unless the required variables are set. You can edit the file to change the default values before you run the \\ndbinit.sh\\n to\\ninitialize or update the database.\\nPerform the following steps on the Vertica node where you have installed the RPM: \\n1\\n. \\nLog on to the Vertica node as the \\nroot\\n user. \\n2\\n. \\nGo to the location \\n/usr/local/itom-di-pulsarudx/bin.\\n3\\n. \\nRun the following command to create the configuration file for the variables:\\n./dbinit.sh genconfig <options>\\nOptions usage:\\n \\n./dbinit.sh genconfig [-h] [-f|--filepath] [-y|--yes] [-p false| --postload false]\\nOption\\nDescription\\n-h\\nDisplay the script usage or help.\\nUsage\\n: \\n./dbinit.sh genconfig -h\\n-y\\n, \\n--yes\\nForce overwrite. The script prompts you to overwrite an existing configuration file.\\nThis option will cause the file to be overwritten.\\nUsage\\n: \\n./dbinit.sh genconfig -y \\nOR\\nUsage\\n: \\n./dbinit.sh genconfig --yes\\n-f\\n, \\n--filepath\\nThe default configuration file location - \\n/usr/local/itom-di-pulsar-\\nudx/conf/dbinit_conf.yaml\\n. However, if you want to generate a configuration file in a\\ndifferent location you can use this option.\\nUsage\\n: \\n./dbinit.sh genconfig -f /tmp/my_conf.yaml \\nOR\\nUsage\\n: \\n./dbinit.sh genconfig --filepath /tmp/my_conf.yaml\\n-p false\\n, \\n--postload false\\nThe configuration file will have a Postload resource pool configured by default. If you\\ndon't require this resource pool, you can use this option.\\nUsage\\n: \\n./dbinit.sh genconfig -p false \\nOR\\nUsage\\n: \\n./dbinit.sh genconfig --postload false\\nA \\ndbinit_conf_sample.yaml\\n file is available in the location \\n/usr/local/itom-di-pulsarudx/conf\\n. The following tables explain the\\nenvironment variables and the default values:\\nglobal\\nParameter\\nDescription\\nverticaAdminUsers.admin\\nThis is the database administrator username.\\nDefault value\\n - \\ndbadmin\\nverticaDatabase\\nThis is the Vertica database name used by OPTIC DL.\\nDefault value\\n - \\nitomdb\\nverticaHome\\nThis is the Vertica install location.\\nDefault value\\n - \\n/opt/vertica\\ntenants\\nParameter\\nDescription\\nNote\\n: For a fresh deployment, you must give the variable settings using \\ngenconfig\\n option. If you use the settings\\nsupported in earlier versions, you can give the input through environment variables. However, to edit the settings\\nprovided with this version you must create a \\ndbinit_conf.yaml\\n file and edit the settings. It's recommended to use \\nge\\nnconfig\\n as it supports all the settings that you can configure through \\ndbinit.sh\\n script.\\nImportant\\n: \\nYou can edit the \\ndbinit_conf.yaml\\n and edit the parameters and values as required by the\\napplication.\\nRemoving resource pool configuration from the configuration file won't cause \\ndbinit.sh\\n to remove the resource\\npool from Vertica. You must manually remove the pool from Vertica or you can set all the values to zero to disable\\nthe resource pool.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n202\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c4cc82148352d0f119629caf5bff6b25'}>,\n",
              "  <Document: {'content': 'name\\nThis is the name of the tenant.\\nDefault value\\n - \\nprovider\\ndeployments\\nThis is the name of the deployment.\\nDefault value\\n - \\ndefault\\nParameter\\nDescription\\nResourcePools - MonitoringResourcePool\\nParameter\\nDescription\\nvalue\\nThis is the Monitoring resource pool name. \\nDefault value\\n - \\nitom_di_monitoring_respool\\nconcurrency\\nThis is the number of queries to execute concurrently in the Monitoring resource pool.\\nDefault value\\n - \\n\"\"\\nmaxMemorySize\\nThis is the maximum memory you must reserve for the Monitoring resource pool. \\nDefault value\\n - \\n500M\\nmemorySize\\nThis is the percentage of Vertica Node Memory you must reserve for the Monitoring\\nresource pool. \\nDefault value\\n - \\n250M\\nparallelism\\nThis is the number of threads that can run in parallel for a query in the Monitoring\\nresource pool.\\nDefault value\\n - \\n\"\"\\ntimeout\\nThis is the timeout for the request in the Monitoring resource pool.\\nDefault value\\n - \\n\"\"\\nResourcePools - PostloadResourcePool\\nParameter\\nDescription\\nvalue\\nThis is the Postload resource pool name. The configuration file will have a Postload\\nresource pool configured by default. If you don\\'t require this resource pool, you can run\\nthe script as follows:\\n./dbinit.sh genconfig --postload false\\nDefault value\\n - \\nitom_di_postload_respool\\nconcurrency\\nThis is the number of queries to execute concurrently in the Postload resource pool.\\nDefault value\\n - \\n\"\"\\nmaxMemorySize\\nThis is the maximum memory percentage you must reserve for the Postload resource\\npool. \\nDefault value\\n - \\n25%\\nmemorySize\\nThis is the percentage of Vertica Node Memory you must reserve for the Postload\\nresource pool. \\nDefault value\\n - \\n10%\\nparallelism\\nThis is the number of threads that can run in parallel for a query in the Postload resource\\npool.\\nDefault value\\n - \\n\"\"\\ntimeout\\nThis is the timeout for the request in the Postload resource pool.\\nDefault value\\n - \\n\"\"\\nResourcePools - ROUserPostloadResourcePool\\nParameter\\nDescription\\nvalue\\nThis is the RO User resource pool name. \\nDefault value\\n - \\nitom_di_rouser_respool\\nconcurrency\\nThis is the number of queries to execute concurrently in the RO User resource pool.\\nDefault value\\n - \\n\"\"\\nmaxMemorySize\\nThis is the maximum memory percentage you must reserve for the RO User resource\\npool. \\nDefault value\\n - \\n\"\"\\nmemorySize\\nThis is the percentage of Vertica Node Memory you must reserve for the RO User\\nresource pool. \\nDefault value\\n - \\n10%\\nparallelism\\nThis is the number of threads that can run in parallel for a query in the RO User resource\\npool.\\nDefault value\\n - \\n\"\"\\ntimeout\\nThis is the timeout for the request in the RO User resource pool.\\nDefault value\\n - \\n\"\"\\nResourcePools - StreamResourcePool\\nContainerized Operations Bridge 2022.11\\nPage \\n203\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.name\\nThis is the name of the tenant.\\nDefault value\\n - \\nprovider\\ndeployments\\nThis is the name of the deployment.\\nDefault value\\n - \\ndefault\\nParameter\\nDescription\\nResourcePools - MonitoringResourcePool\\nParameter\\nDescription\\nvalue\\nThis is the Monitoring resource pool name. \\nDefault value\\n - \\nitom_di_monitoring_respool\\nconcurrency\\nThis is the number of queries to execute concurrently in the Monitoring resource pool.\\nDefault value\\n - \\n\"\"\\nmaxMemorySize\\nThis is the maximum memory you must reserve for the Monitoring resource pool. \\nDefault value\\n - \\n500M\\nmemorySize\\nThis is the percentage of Vertica Node Memory you must reserve for the Monitoring\\nresource pool. \\nDefault value\\n - \\n250M\\nparallelism\\nThis is the number of threads that can run in parallel for a query in the Monitoring\\nresource pool.\\nDefault value\\n - \\n\"\"\\ntimeout\\nThis is the timeout for the request in the Monitoring resource pool.\\nDefault value\\n - \\n\"\"\\nResourcePools - PostloadResourcePool\\nParameter\\nDescription\\nvalue\\nThis is the Postload resource pool name. The configuration file will have a Postload\\nresource pool configured by default. If you don\\'t require this resource pool, you can run\\nthe script as follows:\\n./dbinit.sh genconfig --postload false\\nDefault value\\n - \\nitom_di_postload_respool\\nconcurrency\\nThis is the number of queries to execute concurrently in the Postload resource pool.\\nDefault value\\n - \\n\"\"\\nmaxMemorySize\\nThis is the maximum memory percentage you must reserve for the Postload resource\\npool. \\nDefault value\\n - \\n25%\\nmemorySize\\nThis is the percentage of Vertica Node Memory you must reserve for the Postload\\nresource pool. \\nDefault value\\n - \\n10%\\nparallelism\\nThis is the number of threads that can run in parallel for a query in the Postload resource\\npool.\\nDefault value\\n - \\n\"\"\\ntimeout\\nThis is the timeout for the request in the Postload resource pool.\\nDefault value\\n - \\n\"\"\\nResourcePools - ROUserPostloadResourcePool\\nParameter\\nDescription\\nvalue\\nThis is the RO User resource pool name. \\nDefault value\\n - \\nitom_di_rouser_respool\\nconcurrency\\nThis is the number of queries to execute concurrently in the RO User resource pool.\\nDefault value\\n - \\n\"\"\\nmaxMemorySize\\nThis is the maximum memory percentage you must reserve for the RO User resource\\npool. \\nDefault value\\n - \\n\"\"\\nmemorySize\\nThis is the percentage of Vertica Node Memory you must reserve for the RO User\\nresource pool. \\nDefault value\\n - \\n10%\\nparallelism\\nThis is the number of threads that can run in parallel for a query in the RO User resource\\npool.\\nDefault value\\n - \\n\"\"\\ntimeout\\nThis is the timeout for the request in the RO User resource pool.\\nDefault value\\n - \\n\"\"\\nResourcePools - StreamResourcePool\\nContainerized Operations Bridge 2022.11\\nPage \\n203\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '243cd9e36492fa0b5d781c0f82a0e6d4'}>,\n",
              "  <Document: {'content': 'Parameter\\nDescription\\nvalue\\nThis is the Stream resource pool name. \\nDefault value\\n - \\nitom_di_stream_respool\\nconcurrency\\nThis is the number of queries to execute concurrently in the Stream resource pool.\\nDefault value\\n - \\n\"4\"\\nmaxMemorySize\\nThis is the maximum memory percentage you must reserve for the Stream resource\\npool. \\nDefault value\\n - \\n\"\"\\nmemorySize\\nThis is the percentage of Vertica Node Memory you must reserve for the Stream resource\\npool. \\nDefault value\\n - \\n10%\\nparallelism\\nThis is the number of threads that can run in parallel for a query in the Stream resource\\npool.\\nDefault value\\n - \\n\"\"\\ntimeout\\nThis is the timeout for the request in the Stream resource pool.\\nDefault value\\n - \\n\"\"\\nVerticaUsers\\nParameter\\nDescription\\nreadOnlyUser\\nThis is the read-only user used for monitoring.\\nDefault value\\n - \\nvertica_rouser\\nreadWriteUser\\nThis is the Vertica read/write user to write data into Vertica.\\nDefault value\\n - \\nvertica_rwuser\\nExample \\ndbinit_conf.yaml\\nglobal:\\n    verticaAdminUser:\\n        admin:\\n            value: dbadmin\\n            comment: The database administrator\\n    verticaDatabase:\\n        value: itomdb\\n        comment: The Vertica database used by OPTIC DL.\\n    verticaHome:\\n        value: /opt/vertica\\n        comment: Path where Vertica was installed\\ntenants:\\n    - name: provider\\n      deployments:\\n        - name: default\\n          vars:\\n            ResourcePools:\\n                value: \"\"\\n                comment: The Resource Pool Group\\n                entries:\\n                    MonitoringResourcePool:\\n                        value: itom_di_monitoring_respool\\n                        comment: Monitoring resource pool\\n                        entries:\\n                            concurrency:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            maxMemorySize:\\n                                value: 500M\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            memorySize:\\n                                value: 250M\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            parallelism:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            timeout:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                    PostloadResourcePool:\\n                        value: itom_di_postload_respool\\n                        comment: Post load resource pool\\n                        entries:\\n                            concurrency:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            maxMemorySize:\\n                                value: 25%\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            memorySize:\\n                                value: 10%\\nContainerized Operations Bridge 2022.11\\nPage \\n204\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Parameter\\nDescription\\nvalue\\nThis is the Stream resource pool name. \\nDefault value\\n - \\nitom_di_stream_respool\\nconcurrency\\nThis is the number of queries to execute concurrently in the Stream resource pool.\\nDefault value\\n - \\n\"4\"\\nmaxMemorySize\\nThis is the maximum memory percentage you must reserve for the Stream resource\\npool. \\nDefault value\\n - \\n\"\"\\nmemorySize\\nThis is the percentage of Vertica Node Memory you must reserve for the Stream resource\\npool. \\nDefault value\\n - \\n10%\\nparallelism\\nThis is the number of threads that can run in parallel for a query in the Stream resource\\npool.\\nDefault value\\n - \\n\"\"\\ntimeout\\nThis is the timeout for the request in the Stream resource pool.\\nDefault value\\n - \\n\"\"\\nVerticaUsers\\nParameter\\nDescription\\nreadOnlyUser\\nThis is the read-only user used for monitoring.\\nDefault value\\n - \\nvertica_rouser\\nreadWriteUser\\nThis is the Vertica read/write user to write data into Vertica.\\nDefault value\\n - \\nvertica_rwuser\\nExample \\ndbinit_conf.yaml\\nglobal:\\n    verticaAdminUser:\\n        admin:\\n            value: dbadmin\\n            comment: The database administrator\\n    verticaDatabase:\\n        value: itomdb\\n        comment: The Vertica database used by OPTIC DL.\\n    verticaHome:\\n        value: /opt/vertica\\n        comment: Path where Vertica was installed\\ntenants:\\n    - name: provider\\n      deployments:\\n        - name: default\\n          vars:\\n            ResourcePools:\\n                value: \"\"\\n                comment: The Resource Pool Group\\n                entries:\\n                    MonitoringResourcePool:\\n                        value: itom_di_monitoring_respool\\n                        comment: Monitoring resource pool\\n                        entries:\\n                            concurrency:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            maxMemorySize:\\n                                value: 500M\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            memorySize:\\n                                value: 250M\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            parallelism:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            timeout:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                    PostloadResourcePool:\\n                        value: itom_di_postload_respool\\n                        comment: Post load resource pool\\n                        entries:\\n                            concurrency:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            maxMemorySize:\\n                                value: 25%\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            memorySize:\\n                                value: 10%\\nContainerized Operations Bridge 2022.11\\nPage \\n204\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '69254efa2c8892b48e2f0334319311b5'}>,\n",
              "  <Document: {'content': '                                comment: Performance configuration. Please review with support prior to making changes\\n                            parallelism:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            timeout:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                    ROUserResourcePool:\\n                        value: itom_di_rouser_respool\\n                        comment: RO User pool\\n                        entries:\\n                            concurrency:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            maxMemorySize:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            memorySize:\\n                                value: 10%\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            parallelism:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            timeout:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                    StreamResourcePool:\\n                        value: itom_di_stream_respool\\n                        comment: Stream resource pool and its properties\\n                        entries:\\n                            concurrency:\\n                                value: \"4\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            maxMemorySize:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            memorySize:\\n                                value: 10%\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            parallelism:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            timeout:\\n                                value: \"0\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n            VerticaUsers:\\n                value: \"\"\\n                comment: The Vertica users needed for OPTIC DL\\n                entries:\\n                    readOnlyUser:\\n                        value: vertica_rouser\\n                        comment: The Vertica user with Read Only access to the database.\\n                    readWriteUser:\\n                        value: vertica_rwuser\\n                        comment: The user with Read and Write access to the database.\\nContainerized Operations Bridge 2022.11\\nPage \\n205\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.                                comment: Performance configuration. Please review with support prior to making changes\\n                            parallelism:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            timeout:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                    ROUserResourcePool:\\n                        value: itom_di_rouser_respool\\n                        comment: RO User pool\\n                        entries:\\n                            concurrency:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            maxMemorySize:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            memorySize:\\n                                value: 10%\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            parallelism:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            timeout:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                    StreamResourcePool:\\n                        value: itom_di_stream_respool\\n                        comment: Stream resource pool and its properties\\n                        entries:\\n                            concurrency:\\n                                value: \"4\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            maxMemorySize:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            memorySize:\\n                                value: 10%\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            parallelism:\\n                                value: \"\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n                            timeout:\\n                                value: \"0\"\\n                                comment: Performance configuration. Please review with support prior to making changes\\n            VerticaUsers:\\n                value: \"\"\\n                comment: The Vertica users needed for OPTIC DL\\n                entries:\\n                    readOnlyUser:\\n                        value: vertica_rouser\\n                        comment: The Vertica user with Read Only access to the database.\\n                    readWriteUser:\\n                        value: vertica_rwuser\\n                        comment: The user with Read and Write access to the database.\\nContainerized Operations Bridge 2022.11\\nPage \\n205\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1a484f7d78840501ff936e3264556b9c'}>,\n",
              "  <Document: {'content': \"Configure the Vertica database\\nFollow these steps in a single session on the Vertica node where you have installed the RPM: \\n1\\n. \\nLog on to the Vertica node as \\nroot\\n user.\\n2\\n. \\nGo to the location \\n/usr/local/itom-di-pulsarudx/conf\\n and ensure the \\ndbinit_conf.yaml\\n file is available in the location. If the file\\nisn't available, you must run the script \\n./dbinit.sh genconfig <options>\\n. You may edit the \\ndbinit_conf.yaml\\n file, change the\\nvalues and make sure that the values are as required for your deployment.  For more information, see \\nCreate the\\nvariables file\\n.\\n3\\n. \\nGo to the location \\n/usr/local/itom-di-pulsarudx/bin.\\n4\\n. \\nRun the following command to perform the user creation, grants, install, and create the resource pool:\\n./dbinit.sh <option>\\nYou can retain the same values as prompted or change the values and press \\nenter\\n. You must type the database\\nadministrator password when prompted. For the \\nreadWriteUser\\n and \\nreadOnlyUser\\n, \\ndbinit.sh\\n prompts for passwords unless\\nyou use the \\n--silent\\n option.\\nOptions usage\\n: \\ndbinit.sh [-h|-?|--help] [-c|--tlscrt] [-k|--tlskey] [-e|--tlsenforce] [-t|--tlsonly] [-s|--silent|--suppress] [-v|--verbose] [-n|--nochan\\nge]\\nOption\\nDescription\\n-h\\n, \\n-?\\n, \\n--h\\nelp\\nDisplay the script usage or help.\\n-s\\n, \\n--\\nsilent\\n, \\n--su\\nppress\\nOption to run the script without prompting for variable input. The passwords gets pulled from\\nenvironment variables: \\nVERTICA_DBA_PASS, VERTICA_RW_PASSWD, VERTICA_RO_PASSWD\\n.\\n-c\\n, \\n--tlscrt\\nYou must type the path of the file that contains the base64 encoded server certificate along with the\\ncertificate file name. If you don't give this option, the Vertica TLS remains as configured earlier.\\nYou must use this option with \\n--tlskey\\n, to enable TLS communication with Vertica.\\n-k\\n, \\n--\\ntlskey\\nYou must type the path of the file that contains the base64 encoded server key certificate along with\\nthe key file name. \\n-e\\n, \\n--tlsenf\\norce\\nOption to enforce TLS. The default is \\ntrue\\n. When you set \\ntlsenforce\\n to \\ntrue\\n, Vertica gets configured to\\nreject non TLS communication. When set to \\nfalse\\n, then Vertica will accept non TLS communication. \\nYou can use this option without the \\n--tlscrt\\n and \\n--tlskey\\n options to disable or enable TLS enforcement.\\n-t\\n, \\n--tlsonl\\ny\\nOption to apply only the TLS changes to Vertica. You must use \\n--tlscrt\\n and \\n--tlskey \\noptions or \\n--tlsenforce\\noption with this option.\\nThe script performs only the TLS configurations if you use the \\ntlsonly\\n option and the other settings in\\nVertica remain the same. \\n-v,--verbos\\ne\\nOption to view the detailed output on the console that includes all the SQL queries invoked.\\n-n,--nochan\\nge\\nOption to view the SQL queries that are invoked when the script runs. This option doesn't change\\nVertica. It displays only the queries. \\nEnable TLS using \\ndbinit.sh\\nYou can use the \\ndbinit.sh\\n script to enable TLS as mentioned in this section.\\nFor example:\\nTo configure the server key and certificate for TLS communication along with other configurations:\\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce true\\nNote that the \\n--tlsonly \\noption isn't required in the command while you perform other configurations using the script.\\nTo configure the server key and certificate for TLS communication and to disable non TLS communication with Vertica: \\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce true --tlsonly\\nTo configure the server key and certificate for TLS communication and to allow non TLS communication with Vertica: \\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce false --tlsonly\\nIf you have a Vertica system already configured for TLS communication, to disable non TLS connections to Vertica: \\n./dbinit.sh --tlsenforce true --tlsonly\\nTo apply the TLS settings, make sure to restart the Vertica database.\\nView the configuration file history\\nThe database configuration tool stores its history in the database. You can use the \\ndbinit history\\n command to retrieve the\\nhistory or save the content of the last \\ndbinit_conf.yaml\\n file used.\\nNote\\n: It isn't recommended to set the  \\ntlsenforce\\n to \\nfalse\\n.\\nImportant\\n: If you want to use a new CA signed certificate, you must regenerate the certificate and key files and\\nupdate the configuration in Vertica. To update the configuration, run the \\ndbinit\\n script again with the new\\ncertificate and key file using the \\n--tlscert\\n and \\n--tlskey\\n options.\\nContainerized Operations Bridge 2022.11\\nPage \\n206\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Configure the Vertica database\\nFollow these steps in a single session on the Vertica node where you have installed the RPM: \\n1\\n. \\nLog on to the Vertica node as \\nroot\\n user.\\n2\\n. \\nGo to the location \\n/usr/local/itom-di-pulsarudx/conf\\n and ensure the \\ndbinit_conf.yaml\\n file is available in the location. If the file\\nisn't available, you must run the script \\n./dbinit.sh genconfig <options>\\n. You may edit the \\ndbinit_conf.yaml\\n file, change the\\nvalues and make sure that the values are as required for your deployment.  For more information, see \\nCreate the\\nvariables file\\n.\\n3\\n. \\nGo to the location \\n/usr/local/itom-di-pulsarudx/bin.\\n4\\n. \\nRun the following command to perform the user creation, grants, install, and create the resource pool:\\n./dbinit.sh <option>\\nYou can retain the same values as prompted or change the values and press \\nenter\\n. You must type the database\\nadministrator password when prompted. For the \\nreadWriteUser\\n and \\nreadOnlyUser\\n, \\ndbinit.sh\\n prompts for passwords unless\\nyou use the \\n--silent\\n option.\\nOptions usage\\n: \\ndbinit.sh [-h|-?|--help] [-c|--tlscrt] [-k|--tlskey] [-e|--tlsenforce] [-t|--tlsonly] [-s|--silent|--suppress] [-v|--verbose] [-n|--nochan\\nge]\\nOption\\nDescription\\n-h\\n, \\n-?\\n, \\n--h\\nelp\\nDisplay the script usage or help.\\n-s\\n, \\n--\\nsilent\\n, \\n--su\\nppress\\nOption to run the script without prompting for variable input. The passwords gets pulled from\\nenvironment variables: \\nVERTICA_DBA_PASS, VERTICA_RW_PASSWD, VERTICA_RO_PASSWD\\n.\\n-c\\n, \\n--tlscrt\\nYou must type the path of the file that contains the base64 encoded server certificate along with the\\ncertificate file name. If you don't give this option, the Vertica TLS remains as configured earlier.\\nYou must use this option with \\n--tlskey\\n, to enable TLS communication with Vertica.\\n-k\\n, \\n--\\ntlskey\\nYou must type the path of the file that contains the base64 encoded server key certificate along with\\nthe key file name. \\n-e\\n, \\n--tlsenf\\norce\\nOption to enforce TLS. The default is \\ntrue\\n. When you set \\ntlsenforce\\n to \\ntrue\\n, Vertica gets configured to\\nreject non TLS communication. When set to \\nfalse\\n, then Vertica will accept non TLS communication. \\nYou can use this option without the \\n--tlscrt\\n and \\n--tlskey\\n options to disable or enable TLS enforcement.\\n-t\\n, \\n--tlsonl\\ny\\nOption to apply only the TLS changes to Vertica. You must use \\n--tlscrt\\n and \\n--tlskey \\noptions or \\n--tlsenforce\\noption with this option.\\nThe script performs only the TLS configurations if you use the \\ntlsonly\\n option and the other settings in\\nVertica remain the same. \\n-v,--verbos\\ne\\nOption to view the detailed output on the console that includes all the SQL queries invoked.\\n-n,--nochan\\nge\\nOption to view the SQL queries that are invoked when the script runs. This option doesn't change\\nVertica. It displays only the queries. \\nEnable TLS using \\ndbinit.sh\\nYou can use the \\ndbinit.sh\\n script to enable TLS as mentioned in this section.\\nFor example:\\nTo configure the server key and certificate for TLS communication along with other configurations:\\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce true\\nNote that the \\n--tlsonly \\noption isn't required in the command while you perform other configurations using the script.\\nTo configure the server key and certificate for TLS communication and to disable non TLS communication with Vertica: \\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce true --tlsonly\\nTo configure the server key and certificate for TLS communication and to allow non TLS communication with Vertica: \\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce false --tlsonly\\nIf you have a Vertica system already configured for TLS communication, to disable non TLS connections to Vertica: \\n./dbinit.sh --tlsenforce true --tlsonly\\nTo apply the TLS settings, make sure to restart the Vertica database.\\nView the configuration file history\\nThe database configuration tool stores its history in the database. You can use the \\ndbinit history\\n command to retrieve the\\nhistory or save the content of the last \\ndbinit_conf.yaml\\n file used.\\nNote\\n: It isn't recommended to set the  \\ntlsenforce\\n to \\nfalse\\n.\\nImportant\\n: If you want to use a new CA signed certificate, you must regenerate the certificate and key files and\\nupdate the configuration in Vertica. To update the configuration, run the \\ndbinit\\n script again with the new\\ncertificate and key file using the \\n--tlscert\\n and \\n--tlskey\\n options.\\nContainerized Operations Bridge 2022.11\\nPage \\n206\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '36615ee9059b5a1b708bdacb1c894a0e'}>,\n",
              "  <Document: {'content': 'Create certificates to enable TLS in Vertica\\nThis section describes the steps to create a CA signed server certificate and server private key for enabling TLS\\ncommunication with the Vertica cluster. If your organization has a centrally managed CA server, you can make use of that for\\nCA signing the server certificate. Optionally, you can set up a CA server on one of the Vertica servers to sign the server\\ncertificate.\\nYou can choose to \\nConfigure the Vertica database and Enable TLS\\n using \\ndbinit.sh\\n script or manually following Vertica\\ndocumentation.\\nOnce you complete these steps, you would have 3 files that are required to enable various services with TLS communication:\\nca.pem\\n – A file that contains the root CA certificate and any intermediate CA certificate chains used for signing the server\\ncertificate. The \\nca.pem\\n will be used as \\n<Vertica certificate file>\\n during the application deployment\\nserver_signed.crt\\n – A file that contains the signed server certificate\\nserver.key\\n – the private key on the Vertica server\\nThe filenames mentioned above are only examples and are used in all use cases in the steps below.\\nCreate a configuration file with Vertica cluster information\\nConfiguration file for \\n3 node\\n Vertica cluster \\nCreate 2 files \\nreq_ca.conf \\nand \\nreq_serv.conf \\nwith the following sample content\\nReplace all values in <> with appropriate values from your organization\\nreq_ca.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_ca\\nprompt = no\\n[itomvertica]\\nC = <Country> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = <OU>\\nCN = <some string other than vertica FQDN>   # Use different CN in req_ca.conf file than what is specified for req_serv.conf\\n[v3_req_ca]\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = <FQDN of Node1>\\nDNS.2 = <FQDN of Node2>\\nDNS.3 = <FQDN of Node3>\\nreq_serv.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_serv\\nprompt = no\\n[itomvertica]\\nC = <Country> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = <OU>\\nCN = <FQDN of Node1>\\n[v3_req_serv]\\nNote: \\nYou may skip this topic if you don\\'t require TLS or if you are referring to \\nVertica documentation\\n to\\nenable Vertica TLS/SSL authentication.\\n\\ue917\\n\\ue917\\nImportant: \\nIf you are installing the application using AppHub UI, rename your Vertica certificate as \"\\nvertica-\\nca.pem\\n \\nor \\nvertica-ca.crt\\n\" before you upload the certificates.\\nOnly X509 certificates with PEM\\n(Privacy Enhanced Mail) encoding (Base64 ASCII) are supported.\\n\\ue91b\\n\\ue91b\\nImportant\\n: \\n If your Vertica server acts as one of the Certificate Authority, you need to create\\nseparate configuration files, \\nreq_ca.conf\\n and \\nreq_serv.conf.\\nVertica 10.x or Vertica 11.x\\n: It is suggested to have different CN in \\nreq_ca.conf\\n and\\n req_serv.conf\\n files (like\\nyour email address) for CA cert & server certificate.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n208\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Create certificates to enable TLS in Vertica\\nThis section describes the steps to create a CA signed server certificate and server private key for enabling TLS\\ncommunication with the Vertica cluster. If your organization has a centrally managed CA server, you can make use of that for\\nCA signing the server certificate. Optionally, you can set up a CA server on one of the Vertica servers to sign the server\\ncertificate.\\nYou can choose to \\nConfigure the Vertica database and Enable TLS\\n using \\ndbinit.sh\\n script or manually following Vertica\\ndocumentation.\\nOnce you complete these steps, you would have 3 files that are required to enable various services with TLS communication:\\nca.pem\\n – A file that contains the root CA certificate and any intermediate CA certificate chains used for signing the server\\ncertificate. The \\nca.pem\\n will be used as \\n<Vertica certificate file>\\n during the application deployment\\nserver_signed.crt\\n – A file that contains the signed server certificate\\nserver.key\\n – the private key on the Vertica server\\nThe filenames mentioned above are only examples and are used in all use cases in the steps below.\\nCreate a configuration file with Vertica cluster information\\nConfiguration file for \\n3 node\\n Vertica cluster \\nCreate 2 files \\nreq_ca.conf \\nand \\nreq_serv.conf \\nwith the following sample content\\nReplace all values in <> with appropriate values from your organization\\nreq_ca.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_ca\\nprompt = no\\n[itomvertica]\\nC = <Country> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = <OU>\\nCN = <some string other than vertica FQDN>   # Use different CN in req_ca.conf file than what is specified for req_serv.conf\\n[v3_req_ca]\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = <FQDN of Node1>\\nDNS.2 = <FQDN of Node2>\\nDNS.3 = <FQDN of Node3>\\nreq_serv.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_serv\\nprompt = no\\n[itomvertica]\\nC = <Country> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = <OU>\\nCN = <FQDN of Node1>\\n[v3_req_serv]\\nNote: \\nYou may skip this topic if you don\\'t require TLS or if you are referring to \\nVertica documentation\\n to\\nenable Vertica TLS/SSL authentication.\\n\\ue917\\n\\ue917\\nImportant: \\nIf you are installing the application using AppHub UI, rename your Vertica certificate as \"\\nvertica-\\nca.pem\\n \\nor \\nvertica-ca.crt\\n\" before you upload the certificates.\\nOnly X509 certificates with PEM\\n(Privacy Enhanced Mail) encoding (Base64 ASCII) are supported.\\n\\ue91b\\n\\ue91b\\nImportant\\n: \\n If your Vertica server acts as one of the Certificate Authority, you need to create\\nseparate configuration files, \\nreq_ca.conf\\n and \\nreq_serv.conf.\\nVertica 10.x or Vertica 11.x\\n: It is suggested to have different CN in \\nreq_ca.conf\\n and\\n req_serv.conf\\n files (like\\nyour email address) for CA cert & server certificate.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n208\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a19591faac927bad15414309c6d91ef1'}>,\n",
              "  <Document: {'content': './dbinit.sh history <options>\\nOptions usage\\n: \\ndbinit.sh history [-h|-?|--help] [-c|--config] [--p <path to prefix>|--prefix=<path to prefix>] [-s|--sql] [-n <number>|--number <n\\number>] [-z|--zip]\\nOption\\nDescription\\n-h\\n, \\n-?\\n, \\n--help\\nDisplay the script usage or help.\\n-c\\n, \\n--config\\nDisplay the last configuration used on the screen.\\nUsage:\\n \\ndbinit.sh history --config > /tmp/my_conf.yaml\\n-p <path to prefix>\\n,\\n--prefix=<path to\\nprefix>\\nOption to specify the path to save the history. The history file(s) will use that prefix\\nwith\\n _<n>.yaml\\n appended to it. \\nUsage:\\n \\ndbinit.sh history -p /tmp/my_history\\ndbinit.sh history --prefix=/tmp/my_history\\n-s\\n, \\n--sql\\nOption to include the SQL commands in the dump. The \\ndbinit\\n script saves all the SQL commands\\nused to modify the database. If you want to include these SQL commands to the history dumps, you\\ncan use this option.\\nUsage:\\n \\ndbinit.sh history -s\\ndbinit.sh history --sql\\n-n <number>\\n, \\n--nu\\nmber <number>\\nOption to specify how many history items you want to save. By default, the last script run gets\\nupdated in the history. You can use this option to include more history.\\nUsage:\\n to save the history for the last three runs of \\ndbinit.sh\\n: \\ndbinit.sh history -n 3\\nto save the entire history: \\ndbinit.sh history -n 0\\n-z\\n, \\n--zip\\nOption to zip all the history files into one file. The \\ndbinit.sh\\n script saves history items into separate\\nfiles. This option will zip all the files into a file \\n<prefix>.zip\\n.\\nUsage:\\n \\ndbinit.sh history -n 2 -p /tmp/my_history -z\\nThis will save the last two histories into \\n/tmp/my_history_1.zip\\n and \\n/tmp/my_history_2.zip\\n. These files\\ngets zipped into \\n/tmp/my_history.zip\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n207\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n../dbinit.sh history <options>\\nOptions usage\\n: \\ndbinit.sh history [-h|-?|--help] [-c|--config] [--p <path to prefix>|--prefix=<path to prefix>] [-s|--sql] [-n <number>|--number <n\\number>] [-z|--zip]\\nOption\\nDescription\\n-h\\n, \\n-?\\n, \\n--help\\nDisplay the script usage or help.\\n-c\\n, \\n--config\\nDisplay the last configuration used on the screen.\\nUsage:\\n \\ndbinit.sh history --config > /tmp/my_conf.yaml\\n-p <path to prefix>\\n,\\n--prefix=<path to\\nprefix>\\nOption to specify the path to save the history. The history file(s) will use that prefix\\nwith\\n _<n>.yaml\\n appended to it. \\nUsage:\\n \\ndbinit.sh history -p /tmp/my_history\\ndbinit.sh history --prefix=/tmp/my_history\\n-s\\n, \\n--sql\\nOption to include the SQL commands in the dump. The \\ndbinit\\n script saves all the SQL commands\\nused to modify the database. If you want to include these SQL commands to the history dumps, you\\ncan use this option.\\nUsage:\\n \\ndbinit.sh history -s\\ndbinit.sh history --sql\\n-n <number>\\n, \\n--nu\\nmber <number>\\nOption to specify how many history items you want to save. By default, the last script run gets\\nupdated in the history. You can use this option to include more history.\\nUsage:\\n to save the history for the last three runs of \\ndbinit.sh\\n: \\ndbinit.sh history -n 3\\nto save the entire history: \\ndbinit.sh history -n 0\\n-z\\n, \\n--zip\\nOption to zip all the history files into one file. The \\ndbinit.sh\\n script saves history items into separate\\nfiles. This option will zip all the files into a file \\n<prefix>.zip\\n.\\nUsage:\\n \\ndbinit.sh history -n 2 -p /tmp/my_history -z\\nThis will save the last two histories into \\n/tmp/my_history_1.zip\\n and \\n/tmp/my_history_2.zip\\n. These files\\ngets zipped into \\n/tmp/my_history.zip\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n207\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7581a186edfd7d5f17561a32e374846b'}>,\n",
              "  <Document: {'content': 'keyUsage = keyEncipherment, dataEncipherment\\nextendedKeyUsage = serverAuth\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = <FQDN of Node1>\\nDNS.2 = <FQDN of Node2>\\nDNS.3 = <FQDN of Node3>\\nExample: \\n#cat req_ca.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_ca\\nprompt = no\\n[itomvertica]\\nC = IN\\nST = KA\\nL = Bangalore\\nO = MF\\nOU = OpsB\\nCN = somestring\\n[v3_req_ca]\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = verticaNode01.lab.net\\nDNS.2 = verticaNode03.lab.net\\nDNS.3 = verticaNode05.lab.net\\n#cat req_serv.conf\\ndistinguished_name = itomvertica\\nreq_extensions =  v3_req_serv\\nprompt = no\\n[itomvertica]\\nC = IN\\nST = KA\\nL = Bangalore\\nO = MF\\nOU = OpsB2\\nCN = verticaNode01.lab.net\\n[v3_req_serv]\\nkeyUsage = keyEncipherment, dataEncipherment\\nextendedKeyUsage = serverAuth\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = verticaNode01.lab.net\\nDNS.2 = verticaNode03.lab.net\\nDNS.3 = verticaNode05.lab.net\\n Configuration file for \\nsingle node \\nVertica server\\nCreate 2 files named req_ca.conf and req_serv.conf\\nReplace all values in <> with appropriate values from your organization.\\nreq_ca.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_ca\\nprompt = no\\n[itomvertica]\\nC = <Country_name> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = <OU>\\nCN = <some string other than vertica FQDN>   # Use different CN in req_ca.conf file than what is specified for req_serv.conf\\n[v3_req_ca]\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = <Vertica FQDN>\\n \\nreq_serv.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_serv\\nprompt = no\\n[itomvertica]\\n Note:\\n You must enter the Fully Qualified Domain Name (FQDN) of the Vertica server for the CN\\n(Common Name) parameter and in [alt_names] section (DNS.x).\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n209\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.keyUsage = keyEncipherment, dataEncipherment\\nextendedKeyUsage = serverAuth\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = <FQDN of Node1>\\nDNS.2 = <FQDN of Node2>\\nDNS.3 = <FQDN of Node3>\\nExample: \\n#cat req_ca.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_ca\\nprompt = no\\n[itomvertica]\\nC = IN\\nST = KA\\nL = Bangalore\\nO = MF\\nOU = OpsB\\nCN = somestring\\n[v3_req_ca]\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = verticaNode01.lab.net\\nDNS.2 = verticaNode03.lab.net\\nDNS.3 = verticaNode05.lab.net\\n#cat req_serv.conf\\ndistinguished_name = itomvertica\\nreq_extensions =  v3_req_serv\\nprompt = no\\n[itomvertica]\\nC = IN\\nST = KA\\nL = Bangalore\\nO = MF\\nOU = OpsB2\\nCN = verticaNode01.lab.net\\n[v3_req_serv]\\nkeyUsage = keyEncipherment, dataEncipherment\\nextendedKeyUsage = serverAuth\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = verticaNode01.lab.net\\nDNS.2 = verticaNode03.lab.net\\nDNS.3 = verticaNode05.lab.net\\n Configuration file for \\nsingle node \\nVertica server\\nCreate 2 files named req_ca.conf and req_serv.conf\\nReplace all values in <> with appropriate values from your organization.\\nreq_ca.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_ca\\nprompt = no\\n[itomvertica]\\nC = <Country_name> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = <OU>\\nCN = <some string other than vertica FQDN>   # Use different CN in req_ca.conf file than what is specified for req_serv.conf\\n[v3_req_ca]\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = <Vertica FQDN>\\n \\nreq_serv.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_serv\\nprompt = no\\n[itomvertica]\\n Note:\\n You must enter the Fully Qualified Domain Name (FQDN) of the Vertica server for the CN\\n(Common Name) parameter and in [alt_names] section (DNS.x).\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n209\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6909144d710dc158b6e279a8bdf5a136'}>,\n",
              "  <Document: {'content': \"C = <Country_name> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = <OU>\\nCN = <FQDN of Node1>\\n[v3_req_serv]\\nkeyUsage = keyEncipherment, dataEncipherment\\nextendedKeyUsage = serverAuth\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = <Vertica FQDN>\\nExample:\\n \\n#cat req_ca.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_ca\\nprompt = no\\n[itomvertica]\\nC = IN\\nST = KA\\nL = Bangalore\\nO = MF\\nOU = OpsB\\nCN = somestring\\n[v3_req_ca]\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = verticaNode01.lab.net\\n#cat req_serv.conf\\ndistinguished_name = itomvertica\\nreq_extensions =  v3_req_serv\\nprompt = no\\n[itomvertica]\\nC = IN\\nST = KA\\nL = Bangalore\\nO = MF\\nOU = OpsB2\\nCN = verticaNode01.lab.net\\n[v3_req_serv]\\nkeyUsage = keyEncipherment, dataEncipherment\\nextendedKeyUsage = serverAuth\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = verticaNode01.lab.net\\nOptional.\\n Create Certificate Authority \\nThe steps below configure one of the Vertica servers as a Certificate Authority (CA) for signing the server certificate. If your\\norganization has a centrally managed CA server, you can get the server certificate signed by that CA and the steps below\\naren't required to perform.\\nThe steps below generate the CA files for the server: \\nca.key, ca.csr\\n and \\nca.pem\\n.\\n1\\n. \\nGenerate RSA key for the CA\\nopenssl genrsa -out ca.key 2048\\n \\n \\n2\\n. \\nGenerate a certificate signing request for the CA with the specified extensions mentioned in the \\nreq_ca.conf\\n file\\nA file named \\nca.csr\\n will be generated as specified in the command below: \\nopenssl req -new -key ca.key -out ca.csr -config req_ca.conf\\n -extensions 'v3_req_ca'\\n3\\n. \\nGenerate a Certificate Authority \\npem\\n file with sufficient validity.  For example,\\n \\nopenssl x509 -sha512 -signkey ca.key -in ca.csr -req -days 3650 -out ca.pem\\nThe above command generates a \\nca.pem\\n file that is valid for 10 years.\\n Sample output:\\nNote: \\nYou must enter the Fully Qualified Domain Name (FQDN) of the Vertica server for the CN (Common\\nName) parameter.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n210\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.C = <Country_name> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = <OU>\\nCN = <FQDN of Node1>\\n[v3_req_serv]\\nkeyUsage = keyEncipherment, dataEncipherment\\nextendedKeyUsage = serverAuth\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = <Vertica FQDN>\\nExample:\\n \\n#cat req_ca.conf\\ndistinguished_name = itomvertica\\nreq_extensions = v3_req_ca\\nprompt = no\\n[itomvertica]\\nC = IN\\nST = KA\\nL = Bangalore\\nO = MF\\nOU = OpsB\\nCN = somestring\\n[v3_req_ca]\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = verticaNode01.lab.net\\n#cat req_serv.conf\\ndistinguished_name = itomvertica\\nreq_extensions =  v3_req_serv\\nprompt = no\\n[itomvertica]\\nC = IN\\nST = KA\\nL = Bangalore\\nO = MF\\nOU = OpsB2\\nCN = verticaNode01.lab.net\\n[v3_req_serv]\\nkeyUsage = keyEncipherment, dataEncipherment\\nextendedKeyUsage = serverAuth\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = verticaNode01.lab.net\\nOptional.\\n Create Certificate Authority \\nThe steps below configure one of the Vertica servers as a Certificate Authority (CA) for signing the server certificate. If your\\norganization has a centrally managed CA server, you can get the server certificate signed by that CA and the steps below\\naren't required to perform.\\nThe steps below generate the CA files for the server: \\nca.key, ca.csr\\n and \\nca.pem\\n.\\n1\\n. \\nGenerate RSA key for the CA\\nopenssl genrsa -out ca.key 2048\\n \\n \\n2\\n. \\nGenerate a certificate signing request for the CA with the specified extensions mentioned in the \\nreq_ca.conf\\n file\\nA file named \\nca.csr\\n will be generated as specified in the command below: \\nopenssl req -new -key ca.key -out ca.csr -config req_ca.conf\\n -extensions 'v3_req_ca'\\n3\\n. \\nGenerate a Certificate Authority \\npem\\n file with sufficient validity.  For example,\\n \\nopenssl x509 -sha512 -signkey ca.key -in ca.csr -req -days 3650 -out ca.pem\\nThe above command generates a \\nca.pem\\n file that is valid for 10 years.\\n Sample output:\\nNote: \\nYou must enter the Fully Qualified Domain Name (FQDN) of the Vertica server for the CN (Common\\nName) parameter.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n210\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2868e4ddd53b25ccc1274e4dd1e9873d'}>,\n",
              "  <Document: {'content': \"Signature ok\\nsubject=/C=IN/ST=KA/L=Bangalore/O=MF/OU=Opsb/CN=cluster1-vdb1.swinfra.net\\nGetting Private key\\nCreate server private key and signing request \\nIn this step, you will create the server private key (server.key) and a certificate signing request (\\nserver.csr\\n).\\n1\\n. \\nGenerate a private key for the Vertica server. \\nThis will generate a \\nserver.key\\n certificate which will be used as \\nSSLPrivateKey\\n value while setting SSL in Vertica. \\nopenssl genrsa -out server.key 2048\\n2\\n. \\nGenerate a certificate signing request for the Vertica server with the specified extensions mentioned in the \\nreq_serv.conf\\nfile, this will generate a \\nserver.csr\\n file.  \\nopenssl req -nodes -new -sha512 -key server.key -out server.csr -config req_serv.conf -extensions 'v3_req_serv'\\nGenerate signed server certificate\\nUsing central Certificate Authority\\nIf you are using a central Certificate Authority for signing the server certificate, send the CSR (\\nserver.csr\\n) to your CA\\nsigning authority. They should give you a signed server certificate (for example: \\nserver_signed.crt\\n) along with CA certificate\\n(for example: ca.pem) that contains the root certificate and any intermediate CA certificates.\\nVertica server acts as one of the Certificate Authority\\nIf you have followed the steps in \\nOptional.\\n Create Certificate Authority\\n to make the Vertica server as a CA server,\\nfollow the steps below to generate a signed server certificate.\\nThis step uses \\nca.pem\\n and \\nca.key\\n that were created in \\nOptional.\\n Create Certificate Authority\\n along\\nwith \\nserver.csr\\n file to create \\nserver_signed.crt\\n certificate. \\nopenssl x509 -req -in server.csr -days 3650 -sha512 -CAcreateserial -CA ca.pem -CAkey ca.key -out server_signed.crt -extensions 'v3_req_ca' -\\nextfile req_ca.conf\\nSample output:\\n\\u200b# openssl x509 -req -in server.csr -days 3650 -sha512 -CAcreateserial -CA ca.pem -CAkey ca.key -out server_signed.crt -extensions 'v3_req' -extfile req_ca.conf\\nSignature ok\\nsubject=/C=IN/ST=KA/L=Bangalore/O=MF/OU=Opsb/CN=cluster1-vdb1.swinfra.net\\nGetting CA Private Key\\n \\n  \\nOptional.\\n Verification of server_signed certificate \\nThis section describes steps to verify the generated server_signed.crt against Certificate Authority. \\nImportant: \\nOnly X509 certificates with Privacy Enhanced Mail\\n \\n(PEM) encoding (Base64 ASCII) are\\nsupported.\\n\\ue91b\\n\\ue91b\\nImportant:\\n If the Vertica server certificate was issued by an intermediate CA then create a certificate\\nchain file that contains all intermediate certificates, starting with the CA that issued the server certificate and\\nending with the root CA certificate. Use this file (for example ca.pem) during the application deployment.\\n\\ue91b\\n\\ue91b\\nNote\\n:\\n1\\n. \\nIf the certificate is trusted in the server and is signed by multiple intermediate CA’s, you can concatenate\\nall the CA’s as a single crt file in the secret to trust the complete CA chain.\\n2\\n. \\nIf the user has multiple CA’s that aren't linked to each other or they don't form a chain or don't act as\\nintermediary CA’s, then these different CA’s should be trusted individually in the secret and should not\\nbe concatenated as a single certificate.\\n3\\n. \\nOnly X509 certificates with Privacy Enhanced Mail\\n \\n(PEM) encoding (Base64 ASCII) are\\nsupported.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n211\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Signature ok\\nsubject=/C=IN/ST=KA/L=Bangalore/O=MF/OU=Opsb/CN=cluster1-vdb1.swinfra.net\\nGetting Private key\\nCreate server private key and signing request \\nIn this step, you will create the server private key (server.key) and a certificate signing request (\\nserver.csr\\n).\\n1\\n. \\nGenerate a private key for the Vertica server. \\nThis will generate a \\nserver.key\\n certificate which will be used as \\nSSLPrivateKey\\n value while setting SSL in Vertica. \\nopenssl genrsa -out server.key 2048\\n2\\n. \\nGenerate a certificate signing request for the Vertica server with the specified extensions mentioned in the \\nreq_serv.conf\\nfile, this will generate a \\nserver.csr\\n file.  \\nopenssl req -nodes -new -sha512 -key server.key -out server.csr -config req_serv.conf -extensions 'v3_req_serv'\\nGenerate signed server certificate\\nUsing central Certificate Authority\\nIf you are using a central Certificate Authority for signing the server certificate, send the CSR (\\nserver.csr\\n) to your CA\\nsigning authority. They should give you a signed server certificate (for example: \\nserver_signed.crt\\n) along with CA certificate\\n(for example: ca.pem) that contains the root certificate and any intermediate CA certificates.\\nVertica server acts as one of the Certificate Authority\\nIf you have followed the steps in \\nOptional.\\n Create Certificate Authority\\n to make the Vertica server as a CA server,\\nfollow the steps below to generate a signed server certificate.\\nThis step uses \\nca.pem\\n and \\nca.key\\n that were created in \\nOptional.\\n Create Certificate Authority\\n along\\nwith \\nserver.csr\\n file to create \\nserver_signed.crt\\n certificate. \\nopenssl x509 -req -in server.csr -days 3650 -sha512 -CAcreateserial -CA ca.pem -CAkey ca.key -out server_signed.crt -extensions 'v3_req_ca' -\\nextfile req_ca.conf\\nSample output:\\n\\u200b# openssl x509 -req -in server.csr -days 3650 -sha512 -CAcreateserial -CA ca.pem -CAkey ca.key -out server_signed.crt -extensions 'v3_req' -extfile req_ca.conf\\nSignature ok\\nsubject=/C=IN/ST=KA/L=Bangalore/O=MF/OU=Opsb/CN=cluster1-vdb1.swinfra.net\\nGetting CA Private Key\\n \\n  \\nOptional.\\n Verification of server_signed certificate \\nThis section describes steps to verify the generated server_signed.crt against Certificate Authority. \\nImportant: \\nOnly X509 certificates with Privacy Enhanced Mail\\n \\n(PEM) encoding (Base64 ASCII) are\\nsupported.\\n\\ue91b\\n\\ue91b\\nImportant:\\n If the Vertica server certificate was issued by an intermediate CA then create a certificate\\nchain file that contains all intermediate certificates, starting with the CA that issued the server certificate and\\nending with the root CA certificate. Use this file (for example ca.pem) during the application deployment.\\n\\ue91b\\n\\ue91b\\nNote\\n:\\n1\\n. \\nIf the certificate is trusted in the server and is signed by multiple intermediate CA’s, you can concatenate\\nall the CA’s as a single crt file in the secret to trust the complete CA chain.\\n2\\n. \\nIf the user has multiple CA’s that aren't linked to each other or they don't form a chain or don't act as\\nintermediary CA’s, then these different CA’s should be trusted individually in the secret and should not\\nbe concatenated as a single certificate.\\n3\\n. \\nOnly X509 certificates with Privacy Enhanced Mail\\n \\n(PEM) encoding (Base64 ASCII) are\\nsupported.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n211\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd3aed4dd5528f580a74cbc7fd2c53973'}>,\n",
              "  <Document: {'content': 'Verify the server_signed certificate against Certificate Authority\\nExecute the following command:\\nopenssl verify -verbose -CAfile ca.pem server_signed.crt\\nSample output :\\nserver_signed.crt: OK\\nView the generated server_signed.crt \\nExecute the following command:\\nopenssl x509 -in server_signed.crt -text -noout\\nExample output:\\nCertificate:\\n    Data:\\n        Version: 3 (0x2)\\n        Serial Number:\\n            cc:31:fe:53:54:32:98:c6\\n    Signature Algorithm: sha512WithRSAEncryption\\n        Issuer: C=<Country>, ST=<State>, L=<Locality>, O=<Organisation>, OU=<Organisation Unit>, CN=<FQDN of Node1>\\n        Validity\\n            Not Before: Jul 17 12:35:27 2020 GMT\\n            Not After : Jul 15 12:35:27 2030 GMT\\n        Subject: C=<Country>, ST=<State>, L=<Locality>, O=<Organisation>, OU=<Organisation Unit>, CN=<FQDN of Node1>\\n        Subject Public Key Info:\\n            Public Key Algorithm: rsaEncryption\\n                Public-Key: (2048 bit)\\n                Modulus:\\n                    00:fc:51:00:d1:9d:09:32:8f:a7:9f:9a:a9:f1:e9:\\n                    01:ac:1b:29:65:76:87:2d:26:19:fc:7c:5f:53:d5:\\n                    16:ee:42:c7:d6:62:c9:60:d5:08:41:4e:a3:44:de:\\n                    39:b5:2c:53:89:72:11:e0:6c:a7:ee:34:7b:d5:9b:\\n                    e5:a0:2c:08:7b:4d:bf:07:12:fe:3e:43:1f:59:5e:\\n                    13:11:1f:b2:ce:11:3f:a6:75:2b:91:16:17:e2:da:\\n                    1c:46:5a:ca:77:a0:e4:53:8c:c1:3d:40:31:a8:a1:\\n                    56:b6:ba:6d:62:36:15:20:db:df:8f:70:92:b3:70:\\n                    55:8a:08:05:df:18:7f:90:79:e7:d7:79:65:02:50:\\n                    6c:54:a7:cd:88:3d:14:69:6e:93:f9:23:56:d3:44:\\n                    57:ee:f7:54:7b:44:17:d6:45:fb:93:cd:29:cb:65:\\n                    29:d2:18:1e:8e:ef:b3:7e:df:f1:18:b1:e8:43:f6:\\n                    3a:be:3d:dd:7b:c1:8b:24:38:7f:9e:1c:0f:ee:c7:\\n                    de:e0:8a:c9:f2:69:88:bc:6c:de:a7:03:03:86:37:\\n                    59:55:e2:45:a1:9b:ed:19:ba:ac:df:7d:fe:f6:63:\\n                    ec:aa:5b:fa:71:56:6f:db:54:88:51:cb:70:04:c9:\\n                    84:ce:20:10:38:74:03:d2:5d:bc:62:10:59:1f:ca:\\n                    6e:75\\n                Exponent: 65537 (0x10001)\\n        X509v3 extensions:\\n            X509v3 Key Usage:\\n                Key Encipherment, Data Encipherment\\n            X509v3 Extended Key Usage:\\n                TLS Web Server Authentication\\n            X509v3 Subject Alternative Name:\\n                DNS:<FQDN of Node1>, DNS:<FQDN of Node2>, DNS:<FQDN of Node3>\\n    Signature Algorithm: sha512WithRSAEncryption\\n         38:09:dd:ed:74:e6:c8:d3:5b:f3:e7:4c:c9:a0:bf:fe:87:9d:\\n         8d:7c:e2:76:ff:ba:bb:74:84:15:bc:fe:f7:b0:6a:32:bc:e6:\\n         dc:71:b9:ad:c5:40:31:84:e9:38:51:6c:00:26:10:4d:b9:6b:\\n         2a:c3:0f:74:ac:3b:c1:de:64:4a:fe:c5:c6:5f:0d:50:d3:77:\\n         69:6f:89:f9:28:52:45:15:5f:23:87:82:79:0b:5a:6b:6d:1c:\\n         29:2c:81:6c:b8:17:27:cc:8f:cf:32:8c:eb:a5:6c:92:c2:1f:\\n         d0:ba:9c:e3:06:88:91:a1:8c:6a:a4:8d:18:bc:cd:3b:4e:c6:\\n         62:0f:6a:7d:70:b7:a7:3f:e6:81:5b:29:38:7e:af:40:f6:95:\\n         30:de:32:5e:52:4e:dc:45:8c:97:fc:bf:b7:91:83:1c:44:86:\\n         44:bd:bd:76:ce:77:58:a2:9a:83:d4:44:ee:da:95:f7:5a:ad:\\n         10:16:0e:57:0b:ff:6a:a8:13:c8:dd:89:b8:e9:4f:19:97:f2:\\n         e5:0c:1a:ee:a2:ee:99:20:e6:ca:8f:69:58:2c:ce:77:b8:b4:\\n         11:28:db:40:d8:4f:cb:8c:40:fa:5c:5e:51:07:82:95:87:bb:\\n         1e:7a:ad:82:ee:17:a0:37:7a:a1:0c:c1:f2:72:d9:62:c4:bb:\\n         4c:69:db:f7\\n \\nEnable TLS \\nTo enable TLS you can use the \\ndbinit.sh\\n script as mentioned in  \\nConfigure the Vertica database\\n.\\nVerify that TLS is enabled\\nContainerized Operations Bridge 2022.11\\nPage \\n212\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Verify the server_signed certificate against Certificate Authority\\nExecute the following command:\\nopenssl verify -verbose -CAfile ca.pem server_signed.crt\\nSample output :\\nserver_signed.crt: OK\\nView the generated server_signed.crt \\nExecute the following command:\\nopenssl x509 -in server_signed.crt -text -noout\\nExample output:\\nCertificate:\\n    Data:\\n        Version: 3 (0x2)\\n        Serial Number:\\n            cc:31:fe:53:54:32:98:c6\\n    Signature Algorithm: sha512WithRSAEncryption\\n        Issuer: C=<Country>, ST=<State>, L=<Locality>, O=<Organisation>, OU=<Organisation Unit>, CN=<FQDN of Node1>\\n        Validity\\n            Not Before: Jul 17 12:35:27 2020 GMT\\n            Not After : Jul 15 12:35:27 2030 GMT\\n        Subject: C=<Country>, ST=<State>, L=<Locality>, O=<Organisation>, OU=<Organisation Unit>, CN=<FQDN of Node1>\\n        Subject Public Key Info:\\n            Public Key Algorithm: rsaEncryption\\n                Public-Key: (2048 bit)\\n                Modulus:\\n                    00:fc:51:00:d1:9d:09:32:8f:a7:9f:9a:a9:f1:e9:\\n                    01:ac:1b:29:65:76:87:2d:26:19:fc:7c:5f:53:d5:\\n                    16:ee:42:c7:d6:62:c9:60:d5:08:41:4e:a3:44:de:\\n                    39:b5:2c:53:89:72:11:e0:6c:a7:ee:34:7b:d5:9b:\\n                    e5:a0:2c:08:7b:4d:bf:07:12:fe:3e:43:1f:59:5e:\\n                    13:11:1f:b2:ce:11:3f:a6:75:2b:91:16:17:e2:da:\\n                    1c:46:5a:ca:77:a0:e4:53:8c:c1:3d:40:31:a8:a1:\\n                    56:b6:ba:6d:62:36:15:20:db:df:8f:70:92:b3:70:\\n                    55:8a:08:05:df:18:7f:90:79:e7:d7:79:65:02:50:\\n                    6c:54:a7:cd:88:3d:14:69:6e:93:f9:23:56:d3:44:\\n                    57:ee:f7:54:7b:44:17:d6:45:fb:93:cd:29:cb:65:\\n                    29:d2:18:1e:8e:ef:b3:7e:df:f1:18:b1:e8:43:f6:\\n                    3a:be:3d:dd:7b:c1:8b:24:38:7f:9e:1c:0f:ee:c7:\\n                    de:e0:8a:c9:f2:69:88:bc:6c:de:a7:03:03:86:37:\\n                    59:55:e2:45:a1:9b:ed:19:ba:ac:df:7d:fe:f6:63:\\n                    ec:aa:5b:fa:71:56:6f:db:54:88:51:cb:70:04:c9:\\n                    84:ce:20:10:38:74:03:d2:5d:bc:62:10:59:1f:ca:\\n                    6e:75\\n                Exponent: 65537 (0x10001)\\n        X509v3 extensions:\\n            X509v3 Key Usage:\\n                Key Encipherment, Data Encipherment\\n            X509v3 Extended Key Usage:\\n                TLS Web Server Authentication\\n            X509v3 Subject Alternative Name:\\n                DNS:<FQDN of Node1>, DNS:<FQDN of Node2>, DNS:<FQDN of Node3>\\n    Signature Algorithm: sha512WithRSAEncryption\\n         38:09:dd:ed:74:e6:c8:d3:5b:f3:e7:4c:c9:a0:bf:fe:87:9d:\\n         8d:7c:e2:76:ff:ba:bb:74:84:15:bc:fe:f7:b0:6a:32:bc:e6:\\n         dc:71:b9:ad:c5:40:31:84:e9:38:51:6c:00:26:10:4d:b9:6b:\\n         2a:c3:0f:74:ac:3b:c1:de:64:4a:fe:c5:c6:5f:0d:50:d3:77:\\n         69:6f:89:f9:28:52:45:15:5f:23:87:82:79:0b:5a:6b:6d:1c:\\n         29:2c:81:6c:b8:17:27:cc:8f:cf:32:8c:eb:a5:6c:92:c2:1f:\\n         d0:ba:9c:e3:06:88:91:a1:8c:6a:a4:8d:18:bc:cd:3b:4e:c6:\\n         62:0f:6a:7d:70:b7:a7:3f:e6:81:5b:29:38:7e:af:40:f6:95:\\n         30:de:32:5e:52:4e:dc:45:8c:97:fc:bf:b7:91:83:1c:44:86:\\n         44:bd:bd:76:ce:77:58:a2:9a:83:d4:44:ee:da:95:f7:5a:ad:\\n         10:16:0e:57:0b:ff:6a:a8:13:c8:dd:89:b8:e9:4f:19:97:f2:\\n         e5:0c:1a:ee:a2:ee:99:20:e6:ca:8f:69:58:2c:ce:77:b8:b4:\\n         11:28:db:40:d8:4f:cb:8c:40:fa:5c:5e:51:07:82:95:87:bb:\\n         1e:7a:ad:82:ee:17:a0:37:7a:a1:0c:c1:f2:72:d9:62:c4:bb:\\n         4c:69:db:f7\\n \\nEnable TLS \\nTo enable TLS you can use the \\ndbinit.sh\\n script as mentioned in  \\nConfigure the Vertica database\\n.\\nVerify that TLS is enabled\\nContainerized Operations Bridge 2022.11\\nPage \\n212\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f2784ae1730d2208c39bae1e72cd7a8d'}>,\n",
              "  <Document: {'content': \"Tuning parameters for Vertica\\nThis topic provides you with steps to tune the TM Resource Pool parameters for Vertica. Perform these steps if you set up\\nshared OPTIC DL with NOM and classic NNMi.\\n1\\n. \\nSet this parameter only if you deploy NNMi SPI performance for Traffic deployed beyond small tier. The \\nMEMORYSIZE\\nparameter defines the initial memory assigned to the pool. Increase the memory size of TM resource pool to 14 GB.  \\nALTER RESOURCE POOL tm  MEMORYSIZE '14G'\\n2\\n. \\nSet this parameter only if you deploy NOM in \\nvery large\\n tier or later. The \\nEXECUTIONPARALLELISM\\n parameter limits\\nthe number of threads used to process any single query issued in the resource pool. Set this to 8.\\nALTER RESOURCE POOL itom_di_stream_respool_provider_default  EXECUTIONPARALLELISM  8 ;\\n3\\n. \\nRun \\n\\\\q\\n to exit \\nvsql\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n214\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6d0322fdfb4def01d8eb19ae01295569'}>,\n",
              "  <Document: {'content': 'su\\n - \\n<dbauser>\\nLog on to \\nvsql\\n.\\nvsql -h <Host name> -U <User name> -p <Port> -d <Database Name>\\nExample:\\nvsql -h Myverticahost.net -U verticadba -p 5433 -d itomdb\\nitomdb=> select * from tls_configurations;\\n     name     |  owner  |  certificate  | ca_certificate | cipher_suites |  mode\\n--------------+---------+---------------+----------------+---------------+---------\\nserver       | dbadmin | server_signed |                |               | ENABLE\\nLDAPLink     | dbadmin |               |                |               | DISABLE\\nLDAPAuth     | dbadmin |               |                |               | DISABLE\\ndata_channel | dbadmin |               |                |               | DISABLE\\n(4 rows)\\nitomdb=>\\nContainerized Operations Bridge 2022.11\\nPage \\n213\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '823e2f26df417234b6cde157bef06c13'}>,\n",
              "  <Document: {'content': 'Storage related application prerequisites\\nThis topic provides information on tasks that you must complete to configure the storage required for deployment. This section\\ncontains the following topics:\\nCreate PV manually\\nCreate local persistent volumes on worker nodes\\nInstall storage provisioner chart\\nContainerized Operations Bridge 2022.11\\nPage \\n215\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'befab304be741ef68244aa3294108603'}>,\n",
              "  <Document: {'content': \"Create PV manually\\nTo install the Operations Bridge in the Helm mode, you need to create Persistent Volumes (PVs).\\nYou can consider PVs as a mapping to a storage volume, for example, an NFS server volume in this case. When you create a\\npersistent volume, the storage volume becomes available in Kubernetes. The cluster components use this storage. The cluster\\ncomponents directly don't use a PV for their storage. They use something called a PVC (Persistent Volume Claim). You can\\nthink of PVC as an intermediary between the cluster components and the actual storage volume. The PVC will then bind to the\\nPV, or in simpler words, use the PV. The cluster components then use the PVC instead.\\nThis makes the cluster components (say a Pod) access a storage volume in the following flow:\\nPod -> PVC -> PV -> Storage Volume (ex. NFS)\\nFor embedded K8s\\nTo create Persistent volumes, perform the following tasks:\\nThe application zip (\\nopsbridge-suite-chart-2022.05.x.zip\\n) contains the \\ngenericPV.yaml\\n file under \\nsamples\\n folder\\n.\\n You can\\nedit the same file as applicable. \\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol1\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol1\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol2\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol2\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol3\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol3\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol4\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol4\\n    server: yournfsserver.example.net\\nImportant\\n:  Give the FQDN of the NFS server and the NFS directory path that you used while configuring the NFS\\nserver.\\nDo not edit any names or labels or change any indentation in the yaml file.\\nYou can update the required values and retain the yaml syntax.\\nContainerized Operations Bridge 2022.11\\nPage \\n216\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1f5d345efc76affb66c969050c7e8b5b'}>,\n",
              "  <Document: {'content': '  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol5\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol5\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol6\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol6\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol7\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol7\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\nTo create persistent volumes, run the following command:\\nkubectl create -f genericPV.yaml\\nTo verify volume creation, run the following command:\\nkubectl get pv\\nFor AWS\\nFollow these steps to create the PVs manually:\\nOn AWS, the \\nEFS Service\\n provides NFS in specific regions. See the \\nAWS documentation\\n and adjust the mount path if you use a\\ndifferent one.\\nTo create the PVs required for the application, connect to the bastion machine within your VPC with SSH.\\nRun the following commands to create volumes for the application by replacing the NFS DNS with the EFS DNS name:\\nsudo su\\nNFS_FQDN=<EFS_NFS_DNS>\\nfor i in {1..4}; do\\n    mkdir \"/mnt/efs/var/vols/itom/opsbvol$i\"\\n    chown 1999:1999 \"/mnt/efs/var/vols/itom/opsbvol$i\"\\n    chmod g+w+s \"/mnt/efs/var/vols/itom/opsbvol$i\"\\n    kubectl apply -f - <<<\"\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol$i\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  capacity:\\nContainerized Operations Bridge 2022.11\\nPage \\n217\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdcac033bbc617b9fc104d8cb46abecb'}>,\n",
              "  <Document: {'content': '    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol$i\\n    server: $NFS_FQDN\\n  persistentVolumeReclaimPolicy: Retain\\n  storageClassName: opsb-default\\n  volumeMode: Filesystem\\n\"\\ndone\\nfor i in 5; do\\n    mkdir \"/mnt/efs/var/vols/itom/opsbvol$i\"\\n    chown 1999:1999 \"/mnt/efs/var/vols/itom/opsbvol$i\"\\n    chmod g+w+s \"/mnt/efs/var/vols/itom/opsbvol$i\"\\n    kubectl apply -f - <<<\"\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol$i\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol$i\\n    server: $NFS_FQDN\\n  persistentVolumeReclaimPolicy: Retain\\n  storageClassName: opsb-default\\n  volumeMode: Filesystem\\n\"\\ndone\\nYou can then exit the root prompt with \\nCtrl+D\\n. \\nTo verify Persistent Volume creation, run the command: \\nkubectl get pv\\nIf you plan to use the capabilities that require OPTIC DL (For example, OPTIC Reporting, AEC, Hyperscale Observability), you\\nmust create storage classes for the dynamic provisioning of volumes. Run the following commands to create required storage\\nclasses:\\nkubectl apply -f - <<<\\'\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: gp3\\n  annotations:\\n    storageclass.kubernetes.io/is-default-class: \"false\"\\nprovisioner: ebs.csi.aws.com\\nparameters:\\n  type: gp3\\n  fsType: ext4\\nvolumeBindingMode: WaitForFirstConsumer\\n---\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: io2\\n  annotations:\\n    storageclass.kubernetes.io/is-default-class: \"false\"\\nprovisioner: ebs.csi.aws.com\\nparameters:\\n  type: io2\\n  fsType: ext4\\n  iopsPerGB: \"50\"\\nvolumeBindingMode: WaitForFirstConsumer\\n\\'\\nAfter Operations Bridge is installed, these volumes will be visible in your chosen region in the \\nEC2 Service\\n under \\nElastic\\nBlock Storage\\n > \\nVolumes\\n.\\nFor Azure\\nOnce the Azure file shares are ready, log on to the bastion and set up volumes to store data of the application by following\\nthese steps:\\n1\\n. \\nInstall the NFS client on the bastion:\\nsudo yum install -y nfs-utils cifs-utils\\nImportant\\n: the value \\n1999:1999\\n represents the \\nuser:group\\n ids that containers use to run as part of the\\napplication. If you change them, ensure that you update the values when you configure your YAML file or the\\nAppHub UI.\\nContainerized Operations Bridge 2022.11\\nPage \\n218\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '25c986074f2b00659a346047cbeee0b8'}>,\n",
              "  <Document: {'content': '2\\n. \\nCopy and save the following script as \\nmountazurefile.sh\\n:\\n#!/bin/bash\\nMOUNTPOINT=/mnt/nfs/$3\\nsudo mkdir -p $MOUNTPOINT\\nif [ ! -d \"/etc/smbcredentials\" ]; then\\nsudo mkdir /etc/smbcredentials\\nfi\\nif [ ! -f \"/etc/smbcredentials/$1.cred\" ]; then\\n    sudo bash -c \"echo username=$1 >> /etc/smbcredentials/$1.cred\"\\n    sudo bash -c \"echo password=$2 >> /etc/smbcredentials/$1.cred\"\\nfi\\nsudo chmod 600 /etc/smbcredentials/$1.cred\\nsudo bash -c \"echo //$1.file.core.windows.net/$3 $MOUNTPOINT cifs nofail,vers=3.0,credentials=/etc/smbcredentials/$1.cred,dir_mode=0775,file_mode=0775,serverino,mfsymlinks,uid=$4,gid=$5 >> /etc/fstab\"\\nsudo mount -t cifs //$1.file.core.windows.net/$3 $MOUNTPOINT -o vers=3.0,credentials=/etc/smbcredentials/$1.cred,dir_mode=0775,file_mode=0775,serverino,mfsymlinks,uid=$4,gid=$5\\n3\\n. \\nRun the following command to mount the Azure file share to bastion:\\nchmod 755 mountazurefile.sh\\nsudo ./mountazurefile.sh  <storage_account_name> <storage_account_key> <fileshare_name> <SYSTEM_USER_ID> <SYSTEM_GROUP_ID>\\nIn these commands,\\n<storage_account_name>\\n is the name of the created \\nFileStorage\\n account. \\n<storage_account_key>\\n is the key you got when creating Azure file shares. \\n<fileshare_name>\\n is the name of the Azure file share. \\n<SYSTEM_USER_ID>\\n and \\n<SYSTEM_GROUP_ID>\\n: specify an operating system user ID and group ID that are used to run\\nthe process in the container, respectively. Both values must be \\n1999\\n or a number\\nbetween \\n100000\\n and \\n2000000000\\n (for example, \\nUID=100001\\n and \\nGID=100002\\n).\\nFor example:\\nchmod 755 mountazurefile.sh \\nsudo ./mountazurefile.sh myopsbstorage ${storageAccountKey} generic \\n1999\\n \\n1999\\nFollow these steps to create the PVs manually:\\n1\\n. \\nLog on to the bastion node.\\n2\\n. \\nRun the following commands to create the PV directories:\\ncd /mnt/nfs/<fileshare name>\\nsudo mkdir -p var/vols/itom/opsbvol1\\nsudo mkdir -p var/vols/itom/opsbvol2\\nsudo mkdir -p var/vols/itom/opsbvol3\\nsudo mkdir -p var/vols/itom/opsbvol4\\nsudo mkdir -p var/vols/itom/opsbvol5\\nsudo chown -R <user id>:<group id> /mnt/nfs/<fileshare name>\\nsudo chmod g+w+s var/vols/itom/opsbvol*\\nRun the following commands by replacing the File Share with the Azure File Share name:\\nsudo su\\nFILE_SHARE=<FILE_SHARE_NAME>\\nfor i in {1..4}; do\\n    mkdir -p \"/mnt/nfs/$FILE_SHARE/var/vols/itom/opsbvol$i\"\\n    chown 1999:1999 \"/mnt/nfs/$FILE_SHARE/var/vols/itom/opsbvol$i\"\\n    chmod g+w+s \"/mnt/nfs/$FILE_SHARE/var/vols/itom/opsbvol$i\"\\n    kubectl apply -f - <<<\"\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol$i\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  azureFile:\\n    secretName: azure-secret\\n    secretNamespace: core\\n    shareName: $FILE_SHARE/var/vols/itom/opsbvol$i\\n  capacity:\\n    storage: 10Gi\\n  mountOptions:\\n    - dir_mode=0775\\n    - uid=1999\\n    - gid=1999\\n    - mfsymlinks\\n    - nobrl\\n    - noserverino\\n  persistentVolumeReclaimPolicy: Retain\\nNote\\n: The default value for \\n<user id>:<group id>\\n is \\n1999:1999\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n219\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ebbd5e8067f8785024e3fdafe4d4772e'}>,\n",
              "  <Document: {'content': '  storageClassName: opsb-default\\n  volumeMode: Filesystem\\n\"\\ndone\\nfor i in 5; do\\n    mkdir \"/mnt/nfs/$FILE_SHARE/var/vols/itom/opsbvol$i\"\\n    chown 1999:1999 \"/mnt/nfs/$FILE_SHARE/var/vols/itom/opsbvol$i\"\\n    chmod g+w+s \"/mnt/nfs/$FILE_SHARE/var/vols/itom/opsbvol$i\"\\n    kubectl apply -f - <<<\"\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol$i\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  azureFile:\\n    secretName: azure-secret\\n    secretNamespace: core\\n    shareName: $FILE_SHARE/var/vols/itom/opsbvol$i\\n  capacity:\\n    storage: 10Gi\\n  mountOptions:\\n    - dir_mode=0775\\n    - uid=1999\\n    - gid=1999\\n    - mfsymlinks\\n    - nobrl\\n    - noserverino\\n  persistentVolumeReclaimPolicy: Retain\\n  storageClassName: opsb-default\\n  volumeMode: Filesystem\\n\"\\ndone\\nYou can then exit the root prompt with \\nCtrl+D\\n. \\nTo verify Persistent Volume creation, run the command: \\nkubectl get pv\\n \\nImportant\\n: the value \\n1999:1999\\n represents the \\nuser:group\\n ids used by the containers that run as part of the\\napplication. If you change them, ensure that you update the values when you configure your YAML file or the\\nAppHub UI.\\nContainerized Operations Bridge 2022.11\\nPage \\n220\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ed1408b5965eced78614d17459ba2e16'}>,\n",
              "  <Document: {'content': 'Create local persistent volumes on worker nodes\\nIf you are going to deploy the OPTIC Reporting capability using embedded kubernetes, then you will be installing OPTIC Data\\nLake. The OPTIC DL Message Bus requires Local Persistent Volumes (LPVs) for runtime data persistence. This storage gets\\nallocated from filesystems on your worker nodes. There are three LPVs required on each worker node (\\nledger, bookkeeper\\n, and \\nz\\nookeeper\\n LPV). \\nThe exact number of worker nodes required for any deployment that\\'s not an evaluation is given in the sizing calculator or the\\nDeployment Architecture\\n topic in this documentation. If you require High Availability (HA), then you must allocate the LPVs to\\nat least three worker nodes and also allocate an extra worker node if one of the three workers is lost out of the cluster\\npermanently for some reason. A single worker node is supported for use in Evaluation and Low footprint OPTIC Reporting\\ndeployment only. \\nDuring this step, you will be configuring the LPVs on each worker node. The options are set up in slightly different ways:\\nConfigure separate disk devices for each LPV on every worker node\\n: Using three separate disk devices per\\nworker node, one for each LPV shown by the sizing calculator, is recommended for best performance. You will add empty\\ndisk devices to all your worker systems, each sized so that enough free space will result when a filesystem is installed to\\naccommodate the output from the sizing calculator for the \\nledger, journal, zookeeper\\n LPVs according to your expected scale.\\nUse a single separate disk device for all LPVs on every worker node\\n: One separate disk device common for all\\nLPVs on each worker node is also supported. You will add one empty disk device on your worker systems which should be\\nsized for filesystem free space more than the sum of the sizes for the\\n ledger, journal, zookeeper\\n LPVs, based on the output\\nfrom the sizing calculator.\\nUse the space under one of your existing worker disks\\n: Using an existing disk device to mount LPVs is supported\\nfor non-production deployments and Low footprint OPTIC Reporting deployment only. Note that using an existing\\nfilesystem may have lower performance, and space utilization of the LPVs could impact the system or applications after\\nsome time.\\nDepending on your choice, follow one of the below sections to mount the filesystems so they will be ready for the software\\ninstallation.\\nIn all cases, note that:\\nThe default OPTIC Data Lake path to mount the LPV filesystems under is\\n /mnt/disks.\\n You can choose a non-default OPTIC\\nData Lake path but it must be identical on all the workers where you create LPVs. The OPTIC Data Lake path should be\\nowned by root and have \\n0755\\n permissions. When deploying OPTIC Data Lake for the first time, make sure these\\ndirectories are empty.\\nThe three directories under the OPTIC Data Lake path you create below for LPVs will be owned by \\nSYSTEM_USER_ID\\n \\nand\\n \\nSY\\nSTEM_GROUP_ID.\\n For the ID values, see the application \\ninstall.properties\\n file. The default values are 1999 and 1999\\nrespectively. The LPV directory permissions will be set below to 755.\\nThe filesystems hosting the LPVs can be either \\next4 or xfs.\\nEach of the three directories you create under the OPTIC Data Lake path must be separate mount points, with\\ncorresponding lines in your\\n /etc/fstab\\n. This is to make sure all three directories are seen in the output of the \"mount\"\\ncommand even after reboots.\\nThe parent LPV mount directory mustn\\'t be directly mounted in \\n/\\n. You must create a directory in \\n/\\n and use that as the\\nparent LPV mount directory. For example, create a directory \\nmntdisk\\n in \\n/\\n and then create the \\nnewdisk\\n as the LPV\\nmount directory.\\nThe first section below describes how to configure separate disk devices on each node to dedicate to each of your LPVs. The\\nsecond section describes how to configure one single separate disk device on each node to use for LPVs. The third section\\ndiscusses using an existing mounted filesystem that has ample space, for non-production and Low footprint OPTIC Reporting\\ndeployment use only.\\nConfigure separate disk devices for each LPV on worker nodes\\nThis section covers the recommended procedure for production use where you have added three additional disk devices, one\\nfor each LPV (on ESX they would be new “hard disks”) on all your worker nodes.\\n1\\n. \\nLog on to a worker node as root or \\nSUDO\\n as root.\\n2\\n. \\nRun the following command to create a new directory as the OPTIC Data Lake path, under which you will mount\\nfilesystems:\\nmkdir -p <mount path>\\nFor example:\\nmkdir -p /mnt/disks\\nNote:\\n If you deploy OPTIC Data more than once then it\\'s recommended to create LPVs for each OPTIC Data Lake\\ndeployment on each worker node as OPTIC Data Lake instances run on different worker nodes.\\nImportant:\\n Add disks to the worker nodes with a minimum 10% extra space on top of the size recommended for\\nLPV disks by the sizing calculator. \\nFor example, if the size of LPV recommended for \\nledger\\n is 100 GB and 20 GB each for \\nbookeeper\\n and \\nzookeeper\\n,\\nadd disks with size of 110 GB, 22 GB, and 22 GB respectively.\\nContainerized Operations Bridge 2022.11\\nPage \\n221\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2169912ca12c92586446943d69b2db20'}>,\n",
              "  <Document: {'content': \"3\\n. \\nRun the \\nlsblk\\n command to list the new empty disks that your sysadmin has created or you create now using vCenter or\\nother tools. The result may appear similar to the following:\\n# lsblk\\nNAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\\nsda            8:0    0  200G  0 disk\\n├─sda1         8:1    0    2G  0 part /boot\\n└─sda2         8:2    0  198G  0 part\\n  ├─vgs-root 253:0    0  185G  0 lvm  /\\n  ├─vgs-swap 253:1    0    8G  0 lvm\\n  └─vgs-home 253:2    0    5G  0 lvm  /home\\nsdb            8:16   0  110G  0 disk\\nsdc            8:32   0   22G  0 disk\\nsdd            8:48   0   22G  0 disk\\n4\\n. \\nRun either \\nmkfs.ext4\\n or \\nmkfs.xfs\\n to format the new disks or partitions to then mount as a filesystem, being careful to\\nspecify the unused disks or partitions you intend to use.\\nFor example, to create 3 LPVs:\\nmkfs.ext4 /dev/sdb\\nmkfs.ext4 /dev/sdc\\nmkfs.ext4 /dev/sdd\\n5\\n. \\nUse standard conventions for your Linux systems to add entries for the new filesystems in \\n/etc/fstab\\n which will mount\\nthem under your chosen OPTIC Data Lake path.\\nFor example:\\nmkdir -p /mnt/disks/lpv1\\nmkdir -p /mnt/disks/lpv2\\nmkdir -p /mnt/disks/lpv3\\nmount -t ext4 /dev/sdb /mnt/disks/lpv1\\nmount -t ext4 /dev/sdc /mnt/disks/lpv2\\nmount -t ext4 /dev/sdd /mnt/disks/lpv3\\necho '/dev/sdb /mnt/disks/lpv1 ext4 defaults 0 0' >> /etc/fstab\\necho '/dev/sdc /mnt/disks/lpv2 ext4 defaults 0 0' >> /etc/fstab\\necho '/dev/sdd /mnt/disks/lpv3 ext4 defaults 0 0' >> /etc/fstab\\nThe resulting lines in \\n/etc/fstab\\n may appear similar to the following:\\n/dev/sdb /mnt/disks/lpv1 ext4 defaults 0 0\\n/dev/sdc /mnt/disks/lpv2 ext4 defaults 0 0\\n/dev/sdd /mnt/disks/lpv3 ext4 defaults 0 0\\n6\\n. \\nYou can run \\ndf -h\\n command to ensure that the newly created file systems are mounted correctly and the free space\\navailable for each filesystem meets the minimum size recommended by the sizing calculator. The result may appear\\nsimilar to the following:\\n# df -h |grep -E 'Filesystem|lpv'\\nFilesystem           Size  Used Avail Use% Mounted on\\n/dev/sdb             109G   61M  103G   1% /mnt/disks/lpv1\\n/dev/sdc              22G   45M   21G   1% /mnt/disks/lpv2\\n/dev/sdd              22G   45M   21G   1% /mnt/disks/lpv3\\n7\\n. \\nRun commands similar to the following to change the ownership of the three directories under the OPTIC Data Lake path.\\nThe user and system group ID were defined in your \\ninstall.properties\\n file. The defaults are 1999 for both. The parent\\ndirectory needs read and execute permissions for all:\\ncd <OPTIC DL path>; chown -R <SYSTEM_USER_ID>:<SYSTEM_GROUP_ID> *; chmod -R 755 <OPTIC DL path>; ls -la\\nFor example: if the system user id is 1999 and the system group id is 1999:\\ncd /mnt/disks; chown -R 1999:1999 *; chmod -R 755 /mnt/disks; ls -la   \\nThe result should appear similar to the following:\\n# cd /mnt/disks; chown -R 1999:1999 *; chmod -R 755 /mnt/disks; ls -la\\ntotal 12\\ndrwxr-xr-x  5 root root   42 Apr  5 16:16 .\\ndrwxr-xr-x. 3 root root   19 Apr  5 16:15 ..\\ndrwxr-xr-x  3 1999 1999 4096 Apr  5 16:13 lpv1\\nImportant:\\n The default OPTIC Data Lake path is \\n/mnt/disks\\n. You can use a different OPTIC Data Lake path to\\nmount LPV filesystems. If you use a non-default mount path, you must create a YAML file and provide the\\npath during the helm install. \\nContainerized Operations Bridge 2022.11\\nPage \\n222\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '46566ffc5f31d6dd823a2ce842395cef'}>,\n",
              "  <Document: {'content': 'drwxr-xr-x  3 1999 1999 4096 Apr  5 16:13 lpv2\\ndrwxr-xr-x  3 1999 1999 4096 Apr  5 16:13 lpv3\\n8\\n. \\nRepeat steps 1 to 7, to mount new disks on all the worker nodes you will be using for OPTIC Data Lake LPVs.\\nConfigure a single separate disk device for all LPVs on every worker node\\nThis section covers the case where you have added one additional separate disk device on each worker node to share for all\\nthe LPVs (on ESX they would be new “hard disks”). Make sure that the size of the new disk is at least the size of the sum of all\\nthe intended LPVs from your sizing calculator.\\n1\\n. \\nLog on to a worker node as root or SUDO as root.\\n2\\n. \\nRun the following command to create a new directory as the OPTIC Data Lake path, under which you will mount\\nfilesystems:\\nmkdir -p <mount path>\\nFor example:\\nmkdir -p /mnt/disks\\n3\\n. \\nRun the \\nlsblk\\n command to list the new empty disk that your sysadmin has created or that you create now using vCenter\\nor other tools. The result will appear similar to the following:\\n# lsblk\\nNAME         MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\\nsda            8:0    0  200G  0 disk\\n├─sda1         8:1    0    2G  0 part /boot\\n└─sda2         8:2    0  198G  0 part\\n  ├─vgs-root 253:0    0  185G  0 lvm  /\\n  ├─vgs-swap 253:1    0    8G  0 lvm\\n  └─vgs-home 253:2    0    5G  0 lvm  /home\\nsdb            8:16   0  210G  0 disk\\nsr0           11:0    1 1024M  0 rom\\n4\\n. \\nRun either \\nmkfs.ext4\\n or \\nmkfs.xfs\\n to format the new disk or partition to then mount as a filesystem, being careful to specify\\nthe unused disks or partitions you intend to use.\\nFor example:\\nmkfs.ext4 /dev/sdb\\n5\\n. \\nUse standard conventions for your Linux systems to add an entry for the new filesystem in \\n/etc/fstab\\n which will mount it\\nunder your chosen OPTIC Data Lake path.\\nFor example:\\nmount -t ext4 /dev/sdb /mnt/disks\\necho \\'/dev/sdb /mnt/disks ext4 defaults 0 0\\' >> /etc/fstab\\n6\\n. \\nYou can then use the\\n mount -a\\n command, followed by the mount command, to ensure it\\'s mounted correctly. For example,\\nthe new filesystems may be shown by the mount command as follows:\\n/dev/sdb on /mnt/disks type ext4 (rw,relatime)\\n7\\n. \\nYou now need to create three directories under it, one each for the LPVs as indicated by your sizing calculator output.\\nNote there should only ever be one directory per LPV under the OPTIC Data Lake path. Because the directories must also\\nbe mount points, you need to add \\n\"bind\"\\n entries in this case to\\n /etc/fstab.\\nFor example:\\ncd /mnt/disks\\nfor lpv in lpv1 lpv2 lpv3 \\ndo\\n    mkdir /mnt/disks/$lpv\\n    echo \"/mnt/disks/$lpv /mnt/disks/$lpv none bind\" >> /etc/fstab\\ndone\\nmount -a\\nmount | grep lpv\\nFor example, the new filesystems may be shown by the above command as follows:\\nImportant:\\n Add disks to the worker nodes with a minimum 10% extra space on top of the size recommended for\\nLPV disks by the sizing calculator. \\nFor example, if the size of LPV recommended for \\nledger\\n is 100 GB and 20 GB each for \\nbookeeper\\n and \\nzookeeper\\n,\\nadd a disk with size of 160 GB.\\nImportant:\\n The default OPTIC Data Lake path is \\n/mnt/disks\\n. You can use a different OPTIC Data Lake path to\\nmount LPV filesystems. If you use a non-default mount path, you must create a YAML file and provide the path\\nduring the helm install. \\nContainerized Operations Bridge 2022.11\\nPage \\n223\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b9dcbd060b1f569cd8cb1641d86204e8'}>,\n",
              "  <Document: {'content': \"# mount | grep lpv\\n/dev/sdb on /mnt/disks/lpv1 type ext4 (rw,relatime)\\n/dev/sdb on /mnt/disks/lpv2 type ext4 (rw,relatime)\\n/dev/sdb on /mnt/disks/lpv3 type ext4 (rw,relatime)\\n8\\n. \\nRun the following command to change the ownership of the new directories under the OPTIC Data Lake path. The user\\nand system group ID were defined in your \\ninstall.properties\\n file and the defaults are 1999 for both. The parent directory\\nneeds read and execute permissions for all: \\ncd <OPTIC DL path>; chown -R <SYSTEM_USER_ID>:<SYSTEM_GROUP_ID> *; chmod -R 755 <OPTIC DLSO path>; ls -l\\nFor example: if the system user id is 1999 and the system group id is 1999,\\ncd /mnt/disks; chown -R 1999:1999 *; chmod -R 755 /mnt/disks; ls -l    \\nThe result should appear similar to the following:\\n# cd /mnt/disks; chown -R 1999:1999 *; chmod -R 755 /mnt/disks; ls -la\\ntotal 36\\ndrwxr-xr-x  7 root root  4096 Apr  5 19:30 .\\ndrwxr-xr-x. 3 root root    19 Apr  5 19:29 ..\\ndrwxr-xr-x  2 1999 1999 16384 Apr  5 19:28 lost+found\\ndrwxr-xr-x  2 1999 1999  4096 Apr  5 19:30 lpv1\\ndrwxr-xr-x  2 1999 1999  4096 Apr  5 19:30 lpv2\\ndrwxr-xr-x  2 1999 1999  4096 Apr  5 19:30 lpv3\\n9\\n. \\nRepeat steps 1 to 8, to mount the new disks on all the worker nodes you will be using for OPTIC Data Lake LPVs.\\nUse existing disk devices on worker nodes\\nYou can use existing filesystems to create local persistent volumes, this method is supported for non-production and Low\\nfootprint OPTIC Reporting deployment use only. You may choose to allocate space from any existing filesystem that has ample\\nfree space, well in excess of the sum of all LPV sizes from the sizing calculator. In the following example, your Sizing\\nCalculator has determined the minimum recommended LPV size for \\nledger\\n, \\njournal\\n and \\nzookeeper\\n as 100 GB, 20 GB, and 20 GB\\nrespectively. You will now create files under\\n /disks\\n that are larger than those values: 110 GB, 22 GB, and 22 GB. This is\\nrequired to ensure that the free space available in the final file system for LPV should match or exceed the size requirement as\\nrecommended by the sizing calculator.\\nFollow these steps:\\n1\\n. \\nLog on to a worker node as root or \\nSUDO\\n.\\n2\\n. \\nRun the following commands, one at a time preferably, to create large files to meet your sizing needs and then mount\\nthem as loop filesystems bound to mount points on the existing root filesystem or an alternate filesystem that you will\\nspecify as the OPTIC Data Lake path. Make sure you have ample free space under the filesystem you choose for the files.\\nIn this example, you will use the \\n/disks\\n directory and creating 3 files of size 110 GB, 22 GB, and 22 GB. The /disks\\ndirectory should have at least 160 GB of free space to create these 3 files. \\nThe \\ndd\\n command may take a while to complete: \\ndd if=/dev/zero of=/var/opt/vol1 bs=1G count=110\\ndd if=/dev/zero of=/var/opt/vol2 bs=1G count=22\\ndd if=/dev/zero of=/var/opt/vol3 bs=1G count=22\\nmkfs.ext4 -F /var/opt/vol1\\nmkfs.ext4 -F /var/opt/vol2\\nmkfs.ext4 -F /var/opt/vol3\\nmkdir -p /mnt/disks/lpv1\\nmkdir -p /mnt/disks/lpv2\\nmkdir -p /mnt/disks/lpv3\\nmount /var/opt/vol1 /mnt/disks/lpv1\\nmount /var/opt/vol2 /mnt/disks/lpv2\\nmount /var/opt/vol3 /mnt/disks/lpv3\\n3\\n. \\nEdit the \\n/etc/fstab\\n and add entries for the new file based filesystems\\nExample entries in \\n/etc/fstab:\\n/var/opt/vol1 /mnt/disks/lpv1 ext4 defaults\\n/var/opt/vol2 /mnt/disks/lpv2 ext4 defaults\\n/var/opt/vol3 /mnt/disks/lpv3 ext4 defaults\\n4\\n. \\nYou can run \\ndf\\n \\n-h\\n command to ensure that the newly created file systems are mounted correctly and the free space\\navailable for each filesystem meets the minimum size recommended by the sizing calculator. The result may appear\\nsimilar to the following:\\n# df -h |grep -E 'Filesystem|lpv'\\nFilesystem                         Size  Used Avail Use% Mounted on\\n/dev/loop0                         109G   61M  103G   1% /mnt/disks/lpv1\\n/dev/loop1                          22G   45M   21G   1% /mnt/disks/lpv2\\n/dev/loop2                          22G   45M   21G   1% /mnt/disks/lpv3\\nContainerized Operations Bridge 2022.11\\nPage \\n224\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '10d9fe6dc8441c2e3dd03bb97a72a1d2'}>,\n",
              "  <Document: {'content': 'As you can see, the resulting free space (Avail) shown is somewhat less than the files created. This is because of\\nfilesystem overhead. When you later edit the sizes in your \\nvalues.yaml\\n file for the LPVs, you would set \\n100\\nGi\\n for ledger and \\n20 Gi\\n for journal and zookeeper sizes. The resulting LPV free space should be greater than the minimum\\nrequirement from your sizing calculator.\\n5\\n. \\nRun the following commands to change the ownership of \\n/mnt/disks\\n or your chosen OPTIC Data Lake path and three sub\\ndirectories to have read and execute permissions for all:\\nchown -R 1999:1999 /mnt/disks/*; chmod -R 755 /mnt/disks; ls -la /mnt/disks\\nThe parent directory requires read and execute permissions for all.\\nExample result:\\n# chown -R 1999:1999 /mnt/disks/*; chmod -R 755 /mnt/disks; ls -la /mnt/disks\\ntotal 20\\ndrwxr-xr-x  5 root root 4096 Sep  6 11:48 .\\ndrwxr-xr-x. 3 root root 4096 Sep  6 10:43 ..\\ndrwxr-xr-x  3 1999 1999 4096 Sep  6 10:43 lpv1\\ndrwxr-xr-x  3 1999 1999 4096 Sep  6 11:41 lpv2\\ndrwxr-xr-x  3 1999 1999 4096 Sep  6 11:48 lpv3\\n6\\n. \\nRepeat steps 1 to 5 on the other worker nodes you will use for OPTIC Data Lake, if there will be more than one worker\\nnode.\\nContainerized Operations Bridge 2022.11\\nPage \\n225\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a5a42cd12628c668af67b5e067637c6c'}>,\n",
              "  <Document: {'content': 'Verify the installation of storage provisioner chart\\nWait about 2 minutes for the provisioning process, then verify the creation of provisioner pods.\\nkubectl get pod -n core | grep local-volume\\nExample:\\n#kubectl get pod -n core | grep local-volume                                         \\nlocal-volume-provisioner-gn8jk    1/1      Running         0                37m\\nlocal-volume-provisioner-kcxrn    1/1      Running         0                37m\\nlocal-volume-provisioner-ltmdz    1/1      Running         0                37m\\nVerify that the application creates the local persistent volumes, typically nine total, and that their size meets or exceeds the\\nsizes output from the sizing calculator:\\n# kubectl get pv \\n# kubectl get pv | grep local\\nNAME                CAPACITY   ACCESS   MODES    RECLAIM POLICY  STATUS      CLAIM                                  STORAGECLASS      REASON     AGE\\nlocal-pv-103b7ed2   97Gi      RWO                Delete          Available                                          fast-disks                   37m\\nlocal-pv-27af0bf4   97Gi      RWO                Delete          Available                                          fast-disks                   37m\\nlocal-pv-2a35e15b   97Gi      RWO                Delete          Available                                          fast-disks                   37m\\nlocal-pv-79250ee7   97Gi      RWO                Delete          Available                                          fast-disks                   37m\\nlocal-pv-7d27e148   97Gi      RWO                Delete          Available                                          fast-disks                   37m\\nlocal-pv-93f9f3cd   97Gi      RWO                Delete          Available                                          fast-disks                   37m\\nlocal-pv-958e562d   97Gi      RWO                Delete          Available                                          fast-disks                   37m\\nlocal-pv-be24c2a6   97Gi      RWO                Delete          Available                                          fast-disks                   37m\\nlocal-pv-e68eb868   97Gi      RWO                Delete          Available                                          fast-disks                   37m\\nIf the \\nget\\n \\npv\\n command doesn\\'t show at least nine \\nAvailable\\n \\nlocal-pv\\n volumes (three for Evaluation and Low footprint OPTIC\\nReporting deployment), then don\\'t proceed to the application deployment step until you have determined why the LPV\\nprovisioning didn\\'t succeed as expected. In all cases, when you deploy the application, you need to edit your \\nvalues.yaml\\n size\\nfields for the LPVs to be equal to or smaller than the \"CAPACITY\" field shown by this command.\\nContainerized Operations Bridge 2022.11\\nPage \\n227\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eda50a11e8a054accf9890aa4a9608e7'}>,\n",
              "  <Document: {'content': \"Install storage provisioner chart\\nThe helm install of the storage provisioner chart converts the filesystem mount points you created on the worker nodes (see\\nCreate Local Persistent Volumes on worker nodes) into Kubernetes persistent volumes. This gets claimed via the application's\\nhelm installation step according to the \\nvalues.yaml\\n file and deployment YAML specifications. \\nAs mentioned in the Create Local Persistent Volumes on worker nodes topic, you should have at least three workers for a\\nproduction implementation of OPTIC Data Lake, each provisioned with filesystems mounted under the OPTIC Data Lake path.\\nYou can deploy Evaluation or Low footprint OPTIC Reporting deployment with a single worker node with 3 LPV directories\\nmounted under the OPTIC Data Lake path, or with three separate disk devices allocated to support your LPVs.\\nThe OPTIC Data Lake path for LPVs is \\n/mnt/disks\\n, or you may have chosen an alternative. The available free space needs to be\\ngreater than the amount you obtained from the sizing calculator for the \\nledger, bookkeeper\\n and \\nzookeeper\\n volumes, which will\\nget attached to the three directories under the OPTIC Data Lake path. If you chose not to use \\n/mnt/disks\\n as the OPTIC Data Lake\\npath, then in this step you must create a \\nlpv.yaml\\n file that specifies your chosen OPTIC Data Lake path as the \\nhostDir\\n. If you\\nare unsure, you should be able to use the \\nmount\\n command on your workers to verify the directory name.  The workers you will\\nuse for OPTIC Data Lake (default three workers) need to have the same structure under the OPTIC Data Lake path.\\nFor example, from a worker in an environment using the defaults where there is no need for \\nlpv.yaml\\n file:\\n# mount | grep disks\\n/dev/sdc on /mnt/disks/lpv1 type ext4 (rw,relatime)\\n/dev/sdb on /mnt/disks/lpv2 type ext4 (rw,relatime)\\n/dev/sdd on /mnt/disks/lpv3 type ext4 (rw,relatime)\\nFor example, on a worker in an environment using non-default OPTIC Data Lake path where you will need to create a \\nlpv.yaml\\nfile:\\n# mount | grep coso\\n/dev/sdb1 on /<cosodir>/lpv1 type xfs (rw,relatime,attr2,inode64,noquota)\\n/dev/sdb2 on /<cosodir>/lpv2 type xfs (rw,relatime,attr2,inode64,noquota)\\n/dev/sdb3 on /<cosodir>/lpv3 type xfs (rw,relatime,attr2,inode64,noquota)\\nUse this format to create the \\nlpv.yaml \\nfile if needed on your master (control plane) node. Preserve the exact indentation as\\nshown:\\nclasses:\\n  - name: fast-disks\\n    hostDir: <Path of the parent directory of the local disks>\\n    fsType: <File system type>\\n    storageClass:\\n      reclaimPolicy: Delete\\nFor example:\\nclasses:\\n  - name: fast-disks\\n    hostDir: /cosodir\\n    fsType: xfs\\n    storageClass:\\n      reclaimPolicy: Delete\\nOn the master (control plane) node, execute the following command to install the storage provisioner chart:\\nhelm install <helm_deployment_name> <chart_name> -n core [--set global.docker.registry=<Registry_given_during_OMT_installation>] [ --set glob\\nal.docker.orgName=<orgname_given_during_OMT_installation>] [-f <YAML file>]\\nwhere:\\n<helm_deployment_name>:\\n Any name you choose as the helm local storage provision chart deployment name. for example: \\nlpv\\n.\\n<chart_name>:\\n \\nitom-kubernetes-local-storage-provisioner-<chart version>.tgz\\n. The file is under \\n$CDF_HOME/charts\\n on your first master\\n(control plane) node. For example: \\n$CDF_HOME/charts/itom-kubernetes-local-storage-provisioner-<version>.tgz\\n.\\nglobal.docker.registry\\n: The default value is \\nlocalhost:5000\\n. If you specified a non-default value when you installed OMT, you must\\ngive the same value here.\\nglobal.docker.orgName:\\n The default value is \\nhpeswitom\\n. If you specified a non-default value when you installed OMT, you must\\ngive the same value here.\\n<YAML file>:\\n This is an optional parameter. The default path to mount new disks is \\n/mnt/disks.\\n If you have used a non-default\\nOPTIC Data Lake path during Create local persistent volumes on all worker nodes, then as mentioned earlier you should\\nspecify your newly created \\nlpv.yaml\\n file.\\nHere is an example of the command: \\nhelm install lpv $CDF_HOME/charts/itom-kubernetes-local-storage-provisioner-<version>.tgz -n core -f ./lpv.yaml\\n Example without using yaml:\\nhelm install lpv $CDF_HOME/charts/itom-kubernetes-local-storage-provisioner-<version>.tgz -n core  --set global.docker.orgName=hpeswitom\\nContainerized Operations Bridge 2022.11\\nPage \\n226\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bc530177bfb6a0e4f2237c252a3e0977'}>,\n",
              "  <Document: {'content': 'Network related application prerequisites\\nThis topic provides information on tasks that you must complete to configure the network required for deployment. This topic\\ncontains the following subtopics:\\nConfigure service to allow external access to the cluster\\nUpdate the load balancer configuration after OMT installation\\nContainerized Operations Bridge 2022.11\\nPage \\n228\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '757fe816182cb022995bdd282bc5bc85'}>,\n",
              "  <Document: {'content': \"Configure service to allow external access to the cluster\\nThis topic provides you information to configure the service to use external access according to the Kubernetes cluster. \\nOperations Bridge install uses an external access host for some services. You must complete the Network Load Balancer\\nconfiguration for OMT before the application deployment to enable the routing of the application's web services.\\nMake sure to perform the configuration task applicable to the Kubernetes provider as mentioned in the following table:\\nKubernetes\\nprovider\\nTask\\nAmazon\\nWeb\\nServices\\n(AWS)\\nMake sure to create an external network load balancer for port 5443 for OMT. You must perform this only\\nonce for a cluster. For more information on configuring the load balancer for OMT on AWS, see \\nOMT\\ndocumentation\\n.\\nMicrosoft\\nAzure\\nMake sure to add the DNS records to access the external services for OMT. Follow the steps in OMT\\ndocumentation to add a DNS record to the private DNS zone. For more information on configuring the load\\nbalancer for OMT on Azure, see \\nOMT documentation\\n.\\nRed Hat\\nOpenShift\\nYou must update the server entry for each worker node in the load balancer before you deploy NOM. For\\nmore information, see \\nUpdate the load balancer configuration after OMT installation\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n229\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5656bc97ee8b3d73ca9df58067eebf51'}>,\n",
              "  <Document: {'content': 'Other prerequisites\\nThis topic provides information on tasks that you must complete to configure additional prerequisites for deployment. This\\nsection contains the following topics:\\nCreate application namespace\\nUpload the application chart\\nObtain OBM CA certificate\\nCreate an Agent Metric Collector integration user\\nUpdate Security context constraints (SCCs) for application\\nContainerized Operations Bridge 2022.11\\nPage \\n231\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27939b15cb572fcf46c18e96dc4c7278'}>,\n",
              "  <Document: {'content': 'Update the load balancer configuration after OMT\\ninstallation\\nThis topic provides you steps to configure the load balancer on Red Hat OpenShift after you install OMT. The Load Balancer\\nneeds to be updated to direct traffic to each of the workers for web browsers and other external access.\\nYou must note down the \\nNodeport\\n of \\nportal-ingress-controller-svc\\n and update your load balancer to manage this port.\\n1\\n. \\nRun the following command to get the \\nNodeport\\n of a particular service:\\nkubectl get svc -n core\\nExample: \\n# kubectl get svc -n core\\nNAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                         AGE\\naa-svc                            ClusterIP   172.30.145.179   <none>        8443/TCP                        48m\\napphub-apiserver                  ClusterIP   172.30.40.173    <none>        8080/TCP                        42m\\napphub-ui                         ClusterIP   172.30.83.182    <none>        8443/TCP                        42m\\ncdf-svc                           ClusterIP   172.30.49.207    <none>        8443/TCP                        48m\\ncdfapiserver-postgresql           ClusterIP   172.30.5.147     <none>        5432/TCP                        48m\\nda-svc                            ClusterIP   172.30.77.129    <none>        8443/TCP                        48m\\nfrontend-ingress-controller-svc   NodePort    172.30.65.175    <none>        3000:31116/TCP                  48m\\nidm                               ClusterIP   172.30.154.170   <none>        443/TCP,444/TCP                 44m\\nidm-svc                           ClusterIP   172.30.228.217   <none>        443/TCP,444/TCP                 44m\\nitom-frontend-ui                  ClusterIP   172.30.251.118   <none>        8443/TCP                        48m\\nitom-idm                          ClusterIP   172.30.239.233   <none>        443/TCP,444/TCP                 42m\\nitom-idm-admin                    ClusterIP   172.30.141.241   <none>        18443/TCP                       42m\\nitom-idm-svc                      ClusterIP   172.30.141.62    <none>        18443/TCP,18444/TCP             42m\\nitom-pg-backup                    ClusterIP   172.30.243.166   <none>        8443/TCP,8080/TCP               42m\\nitom-postgresql                   ClusterIP   172.30.153.7     <none>        5432/TCP                        42m\\nitom-vault                        ClusterIP   172.30.172.5     <none>        8200/TCP,8201/TCP               48m\\nmng-portal                        ClusterIP   172.30.164.155   <none>        80/TCP                          42m\\nportal-ingress-controller-svc     NodePort    172.30.65.176    <none>        5443:30417/TCP,8444:31114/TCP   42m\\nsuite-installer-svc               ClusterIP   172.30.108.6     <none>        8443/TCP                        48m\\n2\\n. \\n Run the following command to get the \\nNodeport\\n of \\nportal-ingress-controller-svc\\n \\n5443:30417/TCP:\\n kubectl describe svc portal-ingress-controller-svc -n core\\n Example:\\n# kubectl describe svc portal-ingress-controller-svc -n core\\nName:                     portal-ingress-controller-svc\\nNamespace:                core\\nLabels:                   app.kubernetes.io/managed-by=Helm\\nAnnotations:              meta.helm.sh/release-name: apphub\\n                          meta.helm.sh/release-namespace: core\\nSelector:                 app.kubernetes.io/instance=apphub,app.kubernetes.io/name=portal-ingress-controller\\nType:                     NodePort\\nIP Family Policy:         SingleStack\\nIP Families:              IPv4\\nIP:                       172.30.164.246\\nIPs:                      172.30.164.246\\nPort:                     https  5443/TCP\\nTargetPort:               8443/TCP\\nNodePort:                 https  32369/TCP\\nEndpoints:                10.131.1.223:8443\\nPort:                     client-cert-auth  8444/TCP\\nTargetPort:               8444/TCP\\nNodePort:                 client-cert-auth  30417/TCP\\nEndpoints:                10.131.1.223:8444\\nSession Affinity:         None\\nExternal Traffic Policy:  Cluster\\nEvents:                   <none>\\nAfter you have made your edits to your load balancer configuration, restart the balancer services if required. After that, you\\nshould be able to use your web browser to access the ITOM management portal at: \\nhttps://<your external access node fqdn>:5443\\n.\\nTip: \\nYou will see two node port values, update with the \\nNodePort\\n \\nvalue starting with \\nhttps\\n.\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n230\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a10e0af911a33ba7a23c205aca9e3ab8'}>,\n",
              "  <Document: {'content': 'Create an Agent Metric Collector integration user\\nPerform these steps to create an Agent Metric Collector integration user only if you are using the classic OBM.\\nThe OPTIC Reporting capability enables you to send the system performance metrics collected by the Operations Agent to\\nOPTIC Data Lake. You can use either the Agent Metric Collector or Metric Streaming Policies to send the Agent metrics to OPTIC\\nData Lake.\\nIf you choose to use the Agent Metric Collector, you must create an integration user in OBM. This user would enable the Agent\\nMetric Collector to run a TQL to access the Operations Agent node CIs managed by the OBM, their related CIs like the IP\\naddress CI, and their associated attributes. These details are used by the Agent Metric Collector to:\\nConnect to Operations Agent nodes\\nCollect metrics\\nEnrich the collected metrics with attributes such as Agent core ID and Agent IP address before the metrics are sent to\\nOPTIC Data Lake   \\nFollow the steps to create an integration role and user in OBM:\\nCreate an integration role\\n1\\n. \\nOn OBM, go to \\nAdministration\\n > \\nUsers\\n > \\nUser, Groups, and Roles\\n.\\n2\\n. \\nUnder Roles, click \\nCreate new Role\\n.\\n3\\n. \\nOn the \\nProperties\\n tab, in the name box, enter a name for the user role and click \\nCreate Role\\n.\\n4\\n. \\nDo the following:\\nIf you are using a browser that supports Java applets: Under \\nAdvanced RTSM Permissions\\n, click \\nOpen RTSM\\nPermissions Editor\\n.\\nIf you are using a UCMDB local client: Go to \\nSecurity\\n > \\nRole Manage\\nr and select the role that you just created. \\n5\\n. \\nGo to the \\nResource Groups\\n tab:\\n1\\n. \\nSelect \\nAll Resources Group\\n2\\n. \\nUnder Available actions select \\nQueries >View \\nand move it to the \\nSelected Actions.\\n6\\n. \\nGo to\\n General Actions\\n tab:\\n1\\n. \\nUnder\\n Available Actions\\n, select\\n View CIs\\n, and move it to the \\nSelected Actions\\n.\\n2\\n. \\nUnder\\n Available Actions\\n, select\\n \\nRun Query by Definition\\n, and move it to the \\nSelected Actions\\n.\\nNote:\\n AMC integration username and integration role support only the lowercase characters.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n233\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a659e36200ba6b77f6dc3834bb8324d6'}>,\n",
              "  <Document: {'content': \"7\\n. \\nDo the following:\\nOn a browser: Click \\nApply\\n and then click\\n OK\\n.\\nOn a UCMDB local client: Click \\nSave\\n.\\n8\\n. \\nDo the following:\\nOn a browser: In the \\nAdvanced RTSM Permissions\\n tab, click \\nSave Role\\n to save the role. The new role is listed\\nunder \\nManage Roles\\n.\\nOn a UCMDB local client: No further action\\nCreate an integration user and assign the role\\n1\\n. \\nOn OBM, go to \\nAdministration\\n > \\nUsers\\n > \\nUser, Groups, and Roles\\n.\\n2\\n. \\nUnder \\nUsers\\n, click \\nCreate New User\\n.\\n3\\n. \\nOn the \\nProperties\\n tab, add all the mandatory details. \\nIn the\\n Name\\n and \\nLogin\\n box, add a display name and login id for the Agent Metric Collector user. For example,\\namcuser.\\n4\\n. \\nUnder \\nRole Assignment\\n, in the \\nAssigned roles \\nlist, select the role that you created.\\n5\\n. \\nClick \\nCreate User\\n. The new user is listed under \\nManage Users\\n.\\nYou will use this user to set the \\nglobal.amc.rtsmUsername\\n parameter, in the \\nvalues.yaml \\nfile. \\nYou will use this password while generating Kubernetes secrets to set the OBM_RTSM_PASSWORD. \\nNote: \\nIn case of edge chart installation don't execute generate secrets script, but add this password in\\nvalues.yaml.\\nContainerized Operations Bridge 2022.11\\nPage \\n234\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'aa34ef4370f43ed7c8792227262115cd'}>,\n",
              "  <Document: {'content': \"Obtain OBM CA certificate\\nThe \\nHyperscale Observability\\n capability requires Operations Bridge Manager (OBM). You can either use a containerized or\\na classic OBM. To use a classic OBM, you will need to provide the OBM server's CA certificate during deployment. Follow the\\nsteps to extract the OBM server's CA certificate:\\n1\\n. \\nEnter the following address on a browser to access the OBM server:\\nhttps://<hostname of classic OBM>:<port>\\nFor example: \\nhttps://myhost.mycomputer.net:8443\\n2\\n. \\nOpen the \\nCertificate\\n window and go to the \\nCertificate Path\\n tab. \\n3\\n. \\nExport the certificate of the \\nCertificate Authority\\n in \\nBase-6 encoded X.509 (.CER) \\nformat.\\n4\\n. \\nSave the certificate in a \\n.crt \\nformat. For example, \\nOBM-CA.crt\\n.\\n5\\n. \\nOpen the saved \\n.crt \\nfile (\\nOBM-CA.crt)\\n in an editor and copy the certificate.\\n6\\n. \\nSave the copied certificate \\n(OBM-CA.crt)\\n on the control plane node.\\nWhen deploying the Hyperscale Observability capability, you will trust this CA certificate.\\nContainerized Operations Bridge 2022.11\\nPage \\n232\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f57d3a2a552efd88837e03f9cbe53a17'}>,\n",
              "  <Document: {'content': 'Deployment checklist\\nThis topic provides you a checklist of configurations that you require to deploy Operations Bridge.\\nChecklist of configurations to deploy Operations Bridge on-premises on embedded Kubernetes\\nPrerequisite\\nChecklist of information required for deployment configuration\\nRelational database\\nRelational DB host name\\nDB certificate, \\ncapability specific DB user name and password\\nVertica\\nNodes, user, password, database names\\nStorage (not using NFS\\nprovisioner)\\nNFS server hostname\\nNFS server volume paths (Operations Bridge specific volumes)\\nStorage (using NFS provisioner)\\nStorage class name: \\ncdf-nfs\\nNetwork\\nLoad balancer is the external access host if using an external load balancer,\\notherwise, the Control Plane node can be set as external access host.\\nImage repository\\nImage repository hostname: \\nlocalhost:5000\\norgname\\nDocker user, password\\nKubernetes cluster\\nNone\\nAgent metric collector user\\n- Required if using external OBM\\nProtocol, \\nhostname, \\nport, \\nuser name and, passw\\nord. Ensure th\\ne integration user has\\nthe required permissions and roles\\nOBM certificate - required for\\nHyperscale Observability\\nMake sure the certificates required for communication with the OBM server is\\navailable for upload\\nChecklist of configurations to deploy Operations Bridge on-premises on external Kubernetes\\nPrerequisite\\nChecklist of information required for deployment configuration\\nRelational\\ndatabase\\nRelational DB service is configured\\nDB certificate, \\ncapability specific DB user name and password\\nVertica\\nNodes, user, password, database names\\nStorage (not\\nusing NFS\\nprovisioner)\\nStorage server hostname\\nStorage server volume paths (Operations Bridge specific volumes)\\nNetwork\\nLoad balancer is the external access host if using external load balancer, otherwise, the Bastion node\\ncan be set as external access host\\nImage\\nrepository\\nThe specific image registry for the chosen external Kubernetes, for example Azure Container Registry\\n(ACR) if deploying on Azure Kubernetes Services or Amazon Elastic Container Registry if deploying in\\nAmazon Elastic Kubernetes Service. Make sure you have set up the required repository and your\\ncredentials have the permissions to upload images.\\nYou also need the Docker user, password\\nKubernetes\\ncluster\\nAccess to bastion node and worker nodes within the cluster\\nAgent metric\\ncollector user\\n- Required if\\nusing external\\nOBM\\nProtocol, \\nhostname, \\nport, \\nuser name and, passwo\\nrd. Ensure \\nthe integration user has the required\\npermissions and roles\\nOBM certificate -\\nrequired for\\nHyperscale\\nObservability\\nMake sure the certificates required for communication with the OBM server is available for upload\\nContainerized Operations Bridge 2022.11\\nPage \\n236\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a59c2f2a458f30c63083837cf8101b30'}>,\n",
              "  <Document: {'content': 'Update Security context constraints (SCCs) for\\napplication\\nSecurity context constraints (\\nSCC\\ns) allow administrators to control permissions for pods. These permissions include actions\\nthat a \\npod\\n, a collection of containers, can perform and what resources it can access.\\nSCC\\ns allows an administrator to control:\\nWhether a pod can run privileged containers.\\nThe capabilities that a container can request.\\nThe use of host directories as volumes.\\nThe suite zip (\\nopsbridge-suite-chart-202x.xx.x.zip\\n)\\n contains the \\nitom-opsb-scc.yaml\\n file under the \\nsamples\\n directory (see \\nDownload\\nthe required installation packages for OpenShift\\n)\\n.\\n \\nEdit the \\nitom-opsb-scc.yaml \\nfile and replace \\nopsbnamespace\\n with your suite namespace.\\n1\\n. \\nFor example, when deploying \\nOpsB\\n suite in \\nopsb-helm\\n namespace, the \\nitom-opsb-scc.yaml \\nwill have these entries:\\ngroups:\\n  - system:serviceaccounts:opsb-helm\\n2\\n. \\nRun the command: \\nkubectl apply -f itom-opsb-scc.yaml\\nContainerized Operations Bridge 2022.11\\nPage \\n235\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '793ecb7276d1af45ea336d8f9262d862'}>,\n",
              "  <Document: {'content': 'Deploy Operations Bridge\\nYou must complete the following tasks to deploy an application.\\nCreate application namespace\\nCreate application namespace for Red Hat OpenShift\\nTo deploy the application using AppHub UI or CLI, you must create the application namespace manually before proceeding\\nwith deployment.\\nCreate application namespace for other Kubernetes providers\\nAppHub UI creates the application namespace during deployment. If you want to export the configured application and install\\nit manually at a later stage or if you want to install using CLI, you must perform this task.\\nTo create the namespace manually, see \\nCreate application namespace\\n.\\nUpload application chart\\nTo install an application, you need to upload a Helm chart file to the cluster. For more information, see \\nUpload application\\nchart\\n.\\nDeploy the application \\nPerform this step only for a shared OPTIC DL scenario\\n. Ensure to extract the Certificate from Providing deployment in a\\nshared OPTIC DL scenario before you proceed with the deployment. For shared OPTIC Data Lake, you must set up a two-way\\ntrust between providing and consuming applications. From the providing application, export RE and any custom certificate that\\nhas been used. Run the following command to extract the certificate: \\nkubectl get cm public-ca-certificates -n <namespace of the providing application> -o json | jq -r .data.\\\\\"RE_ca.crt\\\\\"> ProvidingAppCert.pem\\nExample:\\nkubectl get cm public-ca-certificates -n nom-helm -o json | jq -r .data.\\\\\"RE_ca.crt\\\\\" > ProvidingAppCert.pem\\nThere are two methods to deploy the application, either using AppHub UI or CLI. Before deploying the application you must\\nconfigure the application parameters in AppHub UI or \\nvalues.yaml\\n file. For more information, see the \\nDeploy application using\\nthe AppHub UI\\n or the \\nDeploy application using the CLI\\n.  \\nContainerized Operations Bridge 2022.11\\nPage \\n237\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a43bee2f1f87627216119a13144b28b1'}>,\n",
              "  <Document: {'content': 'Create application namespace\\nFollow these steps to install the application. Repeat the steps to install further applications.\\n1\\n. \\nLog on to the bastion or the control plane node.\\n2\\n. \\nRun the following commands to create the application namespace:\\ncd <OMT installation directory>/scripts\\n./cdfctl.sh deployment create -t helm -d <deployment name> -n <namespace>\\nIn these commands, \\n<OMT installation directory>\\n refers to the OMT installation directory of either the embedded Kubernetes\\nor BYOK, \\n<deployment name>\\n refers to the name that you wish to give the application deployment, and \\n<namespace>\\n refers\\nto the namespace which you are going to create and also in which you want to install the application.\\nFor example:\\ncd OMT_External_K8s_202x.xx.xxxx/scripts\\n./cdfctl.sh deployment create -t helm -d test -n demo\\nThe output resembles the following and indicates that you have created the deployment successfully:\\n[root@ExampleMachine]# cdfctl deployment create -t helm -d test -n demo\\n2021-04-13T10:00:29+08:00 INF Creating deployment ... name=test namespace=demo\\n2021-04-13T10:00:29+08:00 INF Created namespace \"demo\" ...\\n2021-04-13T10:00:29+08:00 INF Successfully created deployment \"test\" uuid=e8c02e36-008d-4f91-9777-d60ce7b70bd0\\n3\\n. \\nTo view the full list of options for the \\ncdfctl.sh\\n script, run the following command:\\n./cdfctl.sh deployment -h\\n4\\n. \\nTo check if the deployment got created, run the following command:\\n./cdfctl.sh deployment get\\nRed Hat OpenShift\\nUID and GID are unique for each namespace, hence you must get the \\nUID\\n and \\nfsGroup\\n ids of application namespace:\\n#kubectl get ns <application namespace>  -o yaml | grep groups \\n#You must copy the output: openshift.io/sa.scc.supplemental-groups: 1000870000/10000. \\n#Here 100064870000 will be used as ID and fsGroup id for opsb-helm namespace\\nYou must use these ids in the \\nvalues.yaml\\n for CLI deployment or in the security tab in AppHub deployment.\\nContainerized Operations Bridge 2022.11\\nPage \\n238\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eca8256e421e7e8eca4e5d027d0bf193'}>,\n",
              "  <Document: {'content': \"Upload the application chart\\nTo install an application, the application chart must be uploaded to the cluster. If you want to install the application through\\nAppHub UI, you must upload the chart.\\nBased on your choice of deployment, follow any one of the options to upload the application chart:\\nUpload the application chart using CLI\\n1\\n. \\nLog on to the bastion node or the control plane.\\n2\\n. \\nGo to the directory where you saved your Helm chart. For example, \\n/tmp\\n.\\n3\\n. \\nRun the following command to upload an application chart file or a chart tar file to the local repository:\\ncdfctl chart upload <absolute path to the Helm chart file>\\nFor example:\\ncdfctl chart upload /tmp/chart-202x.xx.x-xx.tgz\\n4\\n. \\nEnter \\nadmin\\n as the administrator username, and then enter the administrator password you set during OMT\\ninstallation. Your terminal resembles the following:\\n$CDF_HOME/bin/cdfctl chart upload /tmp/chart-202x.xx.x-xx.tgz -u admin -p Admin@111\\n2022-05-26T10:11:40+08:00 INF Start to upload the chart ...\\n2022-05-26T10:11:41+08:00 INF Upload chart successfully\\nUpload the application chart using AppHub\\nTo do this, follow these steps:\\nLog in to AppHub\\n1\\n. \\nCopy the installation portal URL to a supported browser. The URL uses the following format: \\nhttps://<external_access_host>:<a\\npphub-port>/apphub\\n.\\nFor example: \\nhttps://myhost.mycompany.com:5443/apphub\\n2\\n. \\nLog in using the cluster administrator credentials that you provided when you ran the \\n./install\\n command:\\nUser name\\n: \\nadmin\\nPassword\\n: Enter the password you provided during OMT installation.\\n3\\n. \\nClick \\nLOG IN\\n.\\nUpload a Helm chart\\n1\\n. \\nOn the \\nAPPLICATIONS\\n page, click the Upload an Application icon   \\n \\n  . An \\nUpload an Application\\n page appears. \\n2\\n. \\nFollow the screen prompt to upload an application's Helm chart (*.tgz) and an application's provenance file (*.tgz.prov)\\nand then click \\nUpload\\n.\\n3\\n. \\nWait a few seconds until you see \\nApplication Upload Complete\\n. Click \\nUpload Another\\n if you need to upload another\\nHelm chart or click \\nExit\\n. \\n4\\n. \\nRefresh on the \\nAPPLICATIONS\\n page and the uploaded application will appear on the page.\\nRelated topics:\\nSee \\nSignature validation\\nNote:\\nYou can only upload one application (two files) at a time. Otherwise, you'll get a warning and the upload will fail.\\nIf you attempt to upload a chart file without a provenance file, you will get a warning that this isn't recommended.\\nBecause AppHub won't be able to verify the integrity of the chart.  Micro Focus recommends always uploading the\\nchart with its provenance file.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n239\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '87b0c1198976cff556b887380d54751b'}>,\n",
              "  <Document: {'content': 'Deploy the application using AppHub\\nFollow the steps mentioned below to deploy Operations Bridge using AppHub: \\n1\\n. \\nDownload and upload the application images to the container registry - Before you deploy the application, you must\\ndownload the required images from Docker Hub or other registries and then upload the images to your registry.\\nFor AWS\\n, Ensure that the containers used by the application must be available in the Elastic Container Registry (ECR) for\\nthe desired region. For more information and steps, see \\nAWS ECR\\n topics.\\nFor Azure\\n, Ensure that the containers used by the application must be available in the Azure Container Registry (ACR)\\nfor the desired region. For more information and steps, see \\nAzure ACR\\n topics.\\nFor other K8s providers\\n, see \\nDownload and Upload installation images\\n.\\n2\\n. \\nLog into AppHub\\n \\n3\\n. \\nConfigure AppHub parameters\\n4\\n. \\nDeploy the application\\n5\\n. \\nVerify missing images\\n \\n6\\n. \\nFor AWS\\n, You must configure the listeners and target groups for the suite. Also, create or update the Route 53 records in\\nPrivate Hosted Zone. For more information, see \\nCreate listeners and target ports\\n.\\n7\\n. \\nIf you are using shared OPTIC Data Lake,\\n Upload OPTIC DL Client Authentication Certificates\\n and upgrade the\\nproviding deployment. For details, see \\nCertificate exchange for using Shared OPTIC Data Lake\\n.\\nNote\\n: If you\\'ve used the ITOM Cloud Deployment Toolkit and set up the AWS and Azure infrastructure, skip\\nstep 1 and step 6.\\n\\ue916\\n\\ue916\\nImportant:\\n \\n If you want to download only the required images for your specific deployment\\nconfiguration, you must configure the \\nvalues.YAML\\n and pass it with \\n-H\\n option while running the \\ngenerate_downlo\\nad_bundle.sh\\n script.  Otherwise, run the script without \\n-H\\n \\nvalues.YAML\\n to download all the images that are listed\\nin the specified chart. \\n\\ue91b\\n\\ue91b\\nTip\\n: If you see \\n\"No applications found.\"\\n message on this page, it means that you haven\\'t uploaded any\\nchart. Follow the steps in \\nUpload suite chart\\n to upload the application chart. \\n \\nNote: \\n For shared OPTIC Data Lake, upload the \\nProvidingAppCert.pem\\n under\\n \\nSecurity\\n > \\nUpload TLS\\nCertificates.\\n If you are using an existing \\nShared OPTIC DL from NOM\\n and you want to reconfigure the client\\nauthentication certificates, then you must upload them in the providing deployment\\'s \\nSecurity\\n > \\nUpload\\nOPTIC DL Client Authentication Certificates.\\nIf you are using existing\\n Shared OPTIC DL from DCA, \\nand you want to reconfigure the client authentication\\ncertificates, then you must upload them in the providing deployment using CLI only. DCA doesn\\'t\\noffer \\nSecurity > Upload OPTIC DL Client Authentication Certificates\\n in AppHub UI.\\nVertica Resource Pool \\nand \\nLocal Persistent Volumes PVC size for OPTIC Data Lake\\n won\\'t appear in\\nthe AppHub UI.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n240\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a5248dacd6f0dafdb5e56605dbbb720a'}>,\n",
              "  <Document: {'content': \"Configure AppHub Parameters\\nAfter you have uploaded one or more Helm charts to the repository, the associated application tiles appear on\\nthe \\nApplications \\npage.  To configure an application, perform the following steps:\\n1\\n. \\nDo one of the following:\\nClick on an application tile, select a release version, and then click \\nConfigure\\n.\\nClick the configure icon  \\n \\n or select a \\nRelease version\\n in the drop-down list, and then click the configure icon.\\nThe application details appear on the \\nDeployments\\n page, which displays the parameters defined in the Helm chart.\\n2\\n. \\nConfigure the application parameters for each category tab that appears in the side menu. Each tab contains a series of\\nfields and controls that enable you to configure the parameters that exist in the application’s Helm chart. When you are\\nconfiguring an application, note the following:\\nA tab with an  \\n \\n icon indicates that one or more parameters are incomplete or invalid.\\nA tab with a check mark  \\n \\n indicates that all parameters are complete and valid.\\nView help by hovering over parameter names.\\nA toggle \\n \\n represents a Boolean parameter that may show or hide additional parameters.\\nSome parameters are immutable (or read-only) after the successful deployment. These parameters are immutable\\nbecause editing them could cause configuration issues.\\n3\\n. \\nThe side menu contains the following category tabs.  Note that if there is nothing to configure for a category, the\\ncorresponding tab isn't displayed.\\nGeneral\\n–On the \\nGeneral \\ntab, configure external access parameters, including host names, ports, and deployment\\nsize settings.\\nServices\\n–On the \\nServices \\ntab, configure parameters related to services that you want to install with the\\ndeployment, including OPTIC Data Lake and more.\\nSecurity\\n–On the \\nSecurity \\ntab, configure the administrator password, override options for the user ID (UID) and\\ngroup ID (GID), and upload trusted CA certificates.  OPTIC AppHub provides a list of password complexity\\nrequirements that you must satisfy for the password.    \\nDatabases\\n–On the \\nDatabases \\ntab, configure database options and verify that the database connection\\ninformation is valid. OPTIC AppHub displays the validation response sent by the database.\\nAdvanced\\n–On the \\nAdvanced \\ntab, configure advanced parameters, including OPTIC Data Lake settings and multi\\ndeployment settings.\\n4\\n. \\nAfter you configure all the required parameters, click \\nDeploy \\nto deploy the application directly to Kubernetes. \\nThe following table provides the configuration parameters from AppHub UI, which you must update to deploy Operations\\nBridge. \\nConfigure General Settings\\nExternal Access Settings\\nConfiguration\\nparameter in UI\\nDescription\\nDefault\\nsetting\\nExternal\\nAccess Host\\n Enter the \\nFully Qualified Domain Name (FQDN) \\nof the external access host, which\\nyou provided during OMT install.\\nExternal\\nAccess Port\\nEnter the port available for Operations Bridge.\\n443\\nFor Red\\nHat\\nOpenShift -\\n30443\\nVERIFY\\nAVAILABLE\\nVerifies the availability of Kubernetes \\nNodePort\\n for \\nOpsBridge ingress controller\\nImportant:\\n \\nIf you are using an existing shared OPTIC Data Lake on \\nAWS\\n,\\nyou must have different external hostnames for providing and consuming\\napplications.\\n\\ue91b\\n\\ue91b\\nNote:\\n For OpenShift, the external access port range is 30000-\\n32767.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n241\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b2401b84e66a605775e84c238ecc109'}>,\n",
              "  <Document: {'content': \"Use\\nHardware\\nLoad Balancer\\nIf you have a load balancer, use the slider enable and enter the port of the Ingress load\\nbalancer under \\nCluster Ingress Port for Load Balancer\\n.\\nYou must enable this option and type the port number you have given for cloud. \\nIf you have used the toolkit to set up the AWS infrastructure, change the value to 30443.\\n443\\nConfiguration\\nparameter in UI\\nDescription\\nDefault\\nsetting\\nDocker Registry Settings\\nThe Docker Settings are already populated with the default Registry and Organization name. If you have configured the\\ndeployment with a different registry, enter the following details:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nDocker Registry Name\\nEnter the URL containing the repository hostname and the port number of your\\ndocker registry.\\nFor AWS\\n, enter the URL containing the ECR repository host name of your\\ndocker registry.\\nlocalhost:500\\n0\\nDocker Organization\\nName\\nEnter the organization name of the Docker repository.\\nhpeswitom\\nDeployment Size\\nExpected Deployment Size.\\nConfiguration\\nparameter\\nDescription\\nDefault\\nsetting\\nExpected\\nDeployment Size\\nSelect the deployment size. You can choose from \\nExtra Large, Large, Medium,\\n \\nLow\\nFootprint,\\n and \\nEvaluation \\ndeployment sizes.\\n \\nMedium\\nKubernetes Provider\\nSelect \\nKubernetes Provider\\n:\\nConfiguration\\nparameter\\nDescription\\nDefault setting\\nKubernetes\\nProvider\\nSelect one of the \\nKubernetes providers\\n from \\nMicro Focus OPTIC\\nManagement Platform embedded Kubernetes, Amazon EKS, Microsoft\\nAzure, Red Hat OpenShift, Generic\\n. \\nMicro Focus OPTIC Manage\\nment Platform embedded K\\nubernetes\\nConfigure Capabilities\\nStakeholder Dashboards\\nUse the slider to enable/disable \\nStakeholder Dashboard\\n. By default, it's enabled.\\nAutomatic Event Correlation\\nNote:\\n For embedded Kubernetes and Azure, the Ingress load\\nbalancer port number and the external access port number should\\nhave the same value.\\n\\ue916\\n\\ue916\\nImportant\\n: \\nEvaluation\\n deployment isn't supported on AWS and\\nAzure.\\n\\ue91b\\n\\ue91b\\nTip\\n: It's recommended to choose the \\nGeneric\\n option if you want to use K3s or Rancher as\\nKubernetes Provider.\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n242\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8ab77543c4152805df06e9dfb5b7d45a'}>,\n",
              "  <Document: {'content': \"Use the slider to enable/disable \\nAutomatic Event Correlation\\n. By default, it's enabled.\\nOperations Bridge Manager\\nUse the slider to enable \\nOperations Bridge Manager\\n, by default it's disabled.\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nEnable OBM HA\\nUse the slider to enable high availability for OBM\\nEnabled\\nAD Management Pack\\nUse the slider to enable deployment of \\nAD Management Pack\\nDisabled\\nApache Management Pack\\nUse the slider to enable deployment of \\nApache Management Pack\\nDisabled\\nExchange Management Pack\\nUse the slider to enable deployment of \\nExchange Management\\nPack\\nDisabled\\nHANA Management Pack\\nUse the slider to enable deployment of \\nHANA Management Pack\\nDisabled\\nInfra Management Pack\\nUse the slider to enable deployment of  \\nInfra Management Pack\\nEnabled\\nMSSQL Management Pack\\nUse the slider to enable deployment of \\nMSSQL Management Pack\\nDisabled\\nOra Management Pack\\nUse the slider to enable deployment of \\nOra Management Pack\\nDisabled\\nSAP Management Pack\\nUse the slider to enable deployment of \\nSAP Management Pack\\nDisabled\\nSAPSybaseASE Management\\nPack\\nUse the slider to enable deployment of  \\nSAPSybaseASE\\nManagement Pack\\nDisabled\\nWbs Management Pack\\nUse the slider to enable deployment of \\nWbs Management Pack\\nDisabled\\nWebLogic Management Pack\\nUse the slider to enable deployment of \\nWebLogic Management\\nPack\\nDisabled\\nHyperscale Observability\\nUse the slider to enable \\nHyperscale Observability\\n by default it's disabled.\\nConfiguration parameter\\nDescription\\nDefault setting\\nEnable Hyperscale\\nObservability\\nUse the slider to enable Enable Hyperscale\\nObservability\\nDisabled\\nUse External OBM \\nUse the slider to enable \\nexternal OBM.\\nDisabled\\nExternal OBM RTSM Protocol\\nEnter the protocol used by components to access\\nOBM and RTSM.\\nhttps\\nExternal UCMDB Server\\nHostName\\nEnter the UCMDB host name.\\nExternal UCMDB Server Port\\nEnter the UCMDB server port.\\n8443\\nExternal OBM RTSM\\nUsername\\nEnter the OBM RTSM host name.\\nExternal OBM RTSM\\nPassword\\nEnter the password for external UCMDB for cloud\\nmonitoring collections\\nUCMDB Probe certificate\\nvalidation level\\nSelect one of the following UCMDB Probe\\ncertificate validation levels:\\nFull validation\\nFull validation without revocation check\\n(default)\\nBasic validation\\nFull validation without revocation\\ncheck (default)\\nAgentless Monitoring\\nAgentless Monitoring is supported only on embedded Kubernetes. Use the slider to enable \\nEnable central management of\\nSiteScope,\\n by default it's disabled.\\nConfigure Email for Dashboard and Report Schedules (SMTP)\\nYou must give these parameters to schedule the reports to a specified email id. You can set the following if you have enabled\\nany or all these services - \\nOPTIC Data Lake\\n Reporting, Stakeholder Dashboards\\n. By default, it's disabled.\\nContainerized Operations Bridge 2022.11\\nPage \\n243\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '80dd8a5d5747cac2c0010b0281ae91aa'}>,\n",
              "  <Document: {'content': \"Configuration parameter\\nDescription\\nRequired\\nfield\\nDefault\\nsetting\\nEnable Email for Dashboard and\\nReport Schedules (SMTP)\\nUse the slider to enable the scheduling for reports to\\nthe specified email.\\nYes\\nDisabled\\nSMTP Server Host\\nEnter the SMTP server host name.\\nSMTP Server Port\\nEnter the SMTP server port. Click \\nVerify Available\\n to\\ncheck if the port is available.\\nSMTP Server Security Type\\nSelect one of the following security protocols for the\\nSMTP server:\\nTLS\\nSTARTTLS\\nYes\\nTLS\\nSMTP User\\nEnter the email id for the SMTP user.\\nYes\\nSMTP Password\\nEnter the password for the SMTP user.\\nYes\\nSMTP From\\nEnter the email address\\nYes\\nAnomaly Detection\\nUse the slider to enable/disable Anomaly Detection. By default, it's disabled.\\nOPTIC Reporting\\nUse the slider to enable/disable \\nOPTIC Data Lake\\n Reporting\\n. By default, it's enabled.\\nConfigure OPTIC Reporting Capability\\nUse the slider to enable/disable \\nEnable Agent Metric Collector \\nby default, it's disabled.  If you select\\n Enable Agent\\nMetric Collector\\n,\\n Use Classic Operations Bridge Manager (OBM) \\noption is available.\\nUse the slider to enable/disable \\nAuto start agent metric collector\\n by default, it's enabled.\\nConfiguration\\nparameter\\nDescription\\nDefault setting\\nUse Classic\\nOperations\\nBridge Manager\\n(OBM)\\nUse the slider to enable \\nUse Classic Operations Bridge Manager (OBM)\\n, If\\nEnabled, configure the required parameters for Classic OBM.\\nOBM Hostname\\nEnter the External OBM host name.\\nDisabled\\nOBM Port\\nEnter the OBM server port used by components to access OBM and RTSM.\\n443\\nEnable Agent\\nMetric Collector \\nUse the slider to enable Agent Metric Collector\\nAuto start\\nagent metric\\ncollector\\nUse the slider to enable Auto start agent metric collector\\nOBM RTSM\\nProtocol\\nProtocol used by components to access OBM and RTSM.\\nhttps\\nOBM RTSM User\\nName\\nEnter the username used by components to access OBM's RTSM. Use lowercase\\nto give the 'Agent Metric Collector integration user' that you had\\ncreated.  See \\nCreate an Agent Metric Collector integration user\\n. \\nOBM RTSM\\nPassword\\nEnter the password for External OBM's RTSM user. It's the password that you\\nset for the 'Agent Metric Collector integration user' (See \\nCreate an Agent Metric\\nCollector integration user\\n).\\nNote: \\n You need not set this user if you are using\\ncontainerized OBM (OBM capability).\\n\\ue916\\n\\ue916\\nNote:\\n  You need not set this password if you are using\\ncontainerized OBM (OBM capability).\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n244\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cf61692d9d20c307e542bc479e7b37f4'}>,\n",
              "  <Document: {'content': 'Deploy\\nOPTIC Data\\nLake\\nYou can choose to install OPTIC Data Lake as part of the deployment. \\nYou can select the following option if you plan to share the OPTIC DL configured in Operations\\nBridge with NOM or DCA for the Shared OPTIC DL deployment scenarios. \\nenabled\\nUse\\nExisting\\nOPTIC Data\\nLake\\nSelecting this option enables you to use an existing OPTIC Data Lake from another deployment.\\nYou need to select the method to connect to the existing OPTIC Data Lake.\\ndisabled\\nConfiguration\\nparameter\\nDescription\\nDefault\\nsetting\\nConnect using cross-namespace communication\\nShared OPTIC Data Lake can be connected using cross namespace communication, you must mention the providing\\ndeployment namespace.\\nConfiguration\\nparameter\\nDescription\\nExternal\\nAccess Host\\nof the\\nproviding\\ndeployment \\nExternal access host of the deployment where OPTIC Data Lake is deployed\\nIngress\\nController\\nPort\\nIngress Controller Port of the providing deployment\\nIDM\\nIntegration\\nUser of the\\nproviding\\ndeployment \\nThis is the IdM integration username that you created in the providing environment. (Example:\\nintegration_admin\\n)\\nIDM\\nIntegration\\nUser\\npassword of\\nthe\\nproviding\\ndeployment\\nThis is the IdM integration admin password from the providing environment. (Example: If NOM is your providing\\nenvironment enter the password for\\n idm_integration_admin_password\\n.)\\nTo retrieve the decoded password, run the command:\\nkubectl -n <namespace of the providing application> get secret <providing_suite_secret> --template={{.data.idm_integration_admin_password}} | base64 -d\\nExample:\\nkubectl -n <nom-helm> get secret <nom-secret> --template={{.data.idm_integration_admin_password}} | base64 -d\\n \\nNamespace\\nof the\\nproviding\\ndeployment\\nConnect using service specific FQDNs\\nConfiguration\\nparameter\\nDescription\\nExternal\\nAccess Host of\\nthe providing\\ndeployment \\nIngress\\nController Port\\nIngress Controller Port of the providing deployment\\nIDM\\nIntegration\\nUser of the\\nproviding\\ndeployment \\nThis is the IdM integration username that you have created in the providing environment. (Example:\\nintegration_admin\\nNote\\n: \\nIf you plan for Shared OPTIC Data Lake deployment where Operations\\nBridge is the provider application and the communication mechanism from the\\nconsumer application using FQDN, you must update the \\nsetFqdnInIngress\\n parameter in\\nthe YAML EDITOR. For more information on updating the parameter, see the YAML\\nEDITOR section at the end of this topic.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n246\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '96c7cef1b33ad62b1e45afa0049f111c'}>,\n",
              "  <Document: {'content': \"Data Broker\\nNodeport\\nThis is the external access port within the CDF cluster. The application uses this\\nport for OBM to agent metric collector communication.  Make sure the port is\\navailable.\\n1383\\n \\nFor Red Hat\\nOpenShift\\n -\\n31383\\nServer Port\\nThe \\nBBC \\nport used by the OBM server for incoming connections.\\n383\\nFor Red Hat\\nOpenShift \\n-port\\nrange of 30000-\\n32765\\nNumber of\\nParallel\\nConnections\\nThe maximum number of Operations Agent nodes that a single Agent Metric\\nCollector replica can connect to in parallel during regular metric collection.\\n25\\nNumber of\\nParallel History\\nConnections\\nThe maximum number of Operations Agent nodes that a single Agent Metric\\nCollector replica can connect to in parallel during historic metric collection.\\n10\\nCapacity Per\\nCollector\\nThe maximum number of Operations Agent nodes that a single Agent Metric\\nCollector replica can collect metrics from during regular metric collection. Use\\nthe sizing calculator to check the right value for your deployment.\\n750\\nCapacity Per\\nHistoric\\nCollector\\nThe maximum number of Operations Agent nodes that a single Agent Metric\\nCollector replica can collect metrics from during historic metric collection. Use\\nthe sizing calculator to check the right value for your deployment.\\n750\\nNode Refresh\\nInterval (in\\nminutes)\\nThe interval after which the Agent Metric Collector's node resolver component\\nconnects to OBM's RTSM to refresh the list of Operations Agent nodes.\\n360\\nCustom Tqls\\nUser-defined Tqls from OBM’s RTSM, used for getting the agent list for metric\\ncollection.\\n \\nConfiguration\\nparameter\\nDescription\\nDefault setting\\nConfigure OPTIC Data Lake\\nConfigure OPTIC Data Lake\\n will be enabled only for OPTIC Reporting, Automatic Event Correlation, and Hyperscale\\nObservability capabilities. \\nIf you want to deploy the OPTIC Data Lake capability as part of the application select \\nDeploy OPTIC Data Lake\\n.\\nIf you want to use an existing OPTIC Data Lake from another deployment, select \\nUse Existing OPTIC Data Lake. \\nExisting OPTIC Data lake can be connected through \\neither\\n of the 3 mechanisms: \\nConnect using cross-namespace communication\\nConnect using service specific FQDNs\\nConnect using External Access Host\\nConfiguration\\nparameter\\nDescription\\nDefault\\nsetting\\nTip:\\n For OpenShift deployment, the node port range should\\nbe between \\n30000-32767\\n.\\n\\ue917\\n\\ue917\\nNote:\\n Use the sizing calculator to check the right value for\\nyour deployment.\\n\\ue916\\n\\ue916\\nNote:\\n Use the sizing calculator to check the right value for\\nyour deployment.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n245\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5dba6233bc68fab65a2cf14cebb24789'}>,\n",
              "  <Document: {'content': 'IDM\\nIntegration\\nUser password\\nof the\\nproviding\\ndeployment\\nThis is the IdM integration admin password from the providing environment. (Example: If NOM is your providing\\nenvironment enter the password for\\n idm_integration_admin_password\\n). \\nTo retrieve the decoded password, run the command:\\nkubectl -n <namespace of the providing application> get secret <providing_suite_secret> --template={{.data.idm_integration_admin_password}} | base64 -d\\nExample:\\nkubectl -n <nom-helm> get secret <nom-secret> --template={{.data.idm_integration_admin_password}} | base64 -d\\nHostname of\\nOPTIC\\nReporting\\nIngestion\\nservice \\nPort of OPTIC\\nData Lake\\nIngestion\\nservice \\nPort of OPTIC\\nData Lake\\nAdministration\\nservice\\nPort of OPTIC\\nData Lake\\nData Access\\nservice\\nHostname of\\nOPTIC Data\\nLake Message\\nBus service \\nPort of OPTIC\\nData Lake\\nMessage Bus\\nSSL service\\nPort of OPTIC\\nData Lake\\nMessage Bus\\nWeb service\\nIngress\\nController port\\nof Providing\\nnamespace\\n Used to connect to the IDM deployed in the providing namespace\\nIngress\\nController\\nFQDN of\\nProviding\\nnamespace\\nUsed to connect to the IDM deployed in the providing namespace \\nIngress\\nController port\\nof Providing\\nnamespace\\nUsed to connect to the IDM deployed in the providing namespace *\\nConfiguration\\nparameter\\nDescription\\nConnect using External Access Host\\nConfiguration\\nparameter\\nDescription\\nExternal\\nAccess Host of\\nthe providing\\ndeployment \\nIngress\\nController Port\\nIngress Controller Port of the providing deployment\\nContainerized Operations Bridge 2022.11\\nPage \\n247\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f0c1654874bfb49235e3ab8348219ce7'}>,\n",
              "  <Document: {'content': \"IDM\\nIntegration\\nUser of the\\nproviding\\ndeployment \\nThis is the IdM integration username that you have created in the providing environment. (Example:\\nintegration_admin\\nIDM\\nIntegration\\nUser password\\nof the\\nproviding\\ndeployment\\nThis is the IdM integration admin password from the providing environment. (Example: If NOM is your providing\\nenvironment enter the password for\\n idm_integration_admin_password\\n). \\nTo retrieve the decoded password, run the command:\\nkubectl -n <namespace of the providing application> get secret <providing_suite_secret> --template={{.data.idm_integration_admin_password}} | base64 -d\\nExample:\\nkubectl -n <nom-helm> get secret <nom-secret> --template={{.data.idm_integration_admin_password}} | base64 -d\\nPort of OPTIC\\nData Lake\\nIngestion\\nservice \\nPort of OPTIC\\nData Lake\\nAdministration\\nservice\\nPort of OPTIC\\nData Lake\\nData Access\\nservice\\nPort of OPTIC\\nData Lake\\nMessage Bus\\nSSL service\\nPort of OPTIC\\nData Lake\\nMessage Bus\\nWeb service\\nConfiguration\\nparameter\\nDescription\\nConfigure settings for Cloud Deployment\\nYou must set the following if you have enabled services that include \\nOPTIC Data Lake.\\n By default, it's disabled.\\nConfiguration\\nparameter\\nDescription\\nRequired\\nfield\\nDefault\\nsetting\\nEnable\\nautomated\\nroute entry in\\nDNS\\nUse the slider to enable the automated route entry in DNS. You can enable the\\nslider to set the access to services outside the Kubernetes cluster. \\nThis section appears when you select \\nAmazon EKS \\nor \\nMicrosoft Azure\\n as the\\nKubernetes Provider\\n.\\nDisabled\\nString Value for\\npulsar proxy\\nDNS name\\n \\nThis section appears when you select \\nAmazon EKS \\nas the \\nKubernetes\\nProvider\\n.\\nFor optimized streaming, Vertica needs to connect to OPTIC DL Message Bus.\\nThe hostname is the hostname under which Vertica DB can reach OPTIC DL's\\npulsar proxy. The format is \\n<Prefix for Pulsar>.<Hosted zone>\\n.\\nFor example: \\nopbpulsar.itombyok.com\\nIf you have used ITOM Cloud Deployment Toolkit\\n, the value depends on toolkit's\\nconfiguration and must follow the format: \\n<PULSAR_PREFIX>.<TERRAFORM_ENVIRONMENT_PREFIX>.\\n<TERRAFORM_PRIVATE_DOMAIN>\\n, where:\\nPULSAR_PREFIX\\n can be any prefix you want as long as it's valid as part of a\\nURL.\\nTERRAFORM_ENVIRONMENT_PREFIX\\n corresponds to \\nenvironment-prefix\\n in\\nTerraforms' \\ntemplate.tfvars\\n.\\nTERRAFORM_PRIVATE_DOMAIN\\n corresponds to \\nprivate-domain\\n in Terraforms' \\nvari\\nables.tf\\n.\\nIf you deploy AWS infrastructure using the toolkit, the toolkit creates the pulsar\\nrecord automatically. However, if you manually deploy the AWS infrastructure,\\nyou must create the pulsar record manually after you deploy Operations Bridge.\\nyes\\npulsar.pri\\nvatehoste\\ndzone.co\\nm\\nContainerized Operations Bridge 2022.11\\nPage \\n248\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ffaeb647d5cc26c2f737b86ce9db6647'}>,\n",
              "  <Document: {'content': 'Private IP for\\nAzure Load\\nBalancer\\nconfiguration\\n \\nThis section appears when you select \\nMicrosoft Azure\\n as the \\nKubernetes\\nProvider\\n.\\nYou must give the Load Balancer Private IP as the value. To Get Private IP, check\\nthe \"\\nexternal IP\\n\" of \"\\nportal-ingress-controller-svc\\n\" in \\ncore\\n namespace, use the same\\nIP. Run the command: \\nkubectl get svc -n core\\nIf you are deploying OBM capability on Azure, \\nopen the YAML editor and\\nadd the following section and provide the load balancer IP value.\\n # Required only for Azure. Private IP for Azure Load Balancer configuration\\n global:\\n   loadBalancer:\\n     ip:\\nyes\\nString Value for\\nadministration\\nDNS name\\n \\nThis section appears only when you select \\nAmazon EKS \\nas the \\nKubernetes\\nProvider\\n.\\nFor optimized streaming, Vertica needs to connect to OPTIC DL Message Bus.\\nThe hostname is the hostname under which Vertica DB can reach OPTIC DL\\'s\\nadministration service. The format is \\n<Prefix for administration service>.<Hosted zone\\n>\\n.\\nyes\\nadministr\\nation.priv\\natehosted\\nzone.com\\nString Value for\\ndataAccess DNS\\nname\\n \\nThis section appears only when you select \\nAmazon EKS \\nas the \\nKubernetes\\nProvider\\n.\\nFor optimized streaming, Vertica needs to connect to OPTIC DL Message Bus.\\nThe hostname is the hostname under which Vertica DB can reach OPTIC DL\\'s\\ndata access service. The format is \\n<Prefix for data access service>.<Hosted zone>\\n.\\nyes\\ndataAcce\\nss.private\\nhostedzo\\nne.com\\nString Value for\\nreceiver DNS\\nname\\n \\nThis section appears only when you select \\nAmazon EKS \\nas the \\nKubernetes\\nProvider\\n.\\nFor optimized streaming, Vertica needs to connect to OPTIC DL Message Bus.\\nThe hostname is the hostname under which Vertica DB can reach OPTIC DL\\'s\\npulsar proxy. The format is \\n<Prefix for receiver service>.<Hosted zone>\\n.\\nyes\\nreceiver.\\nprivateho\\nstedzone.\\ncom\\nConfiguration\\nparameter\\nDescription\\nRequired\\nfield\\nDefault\\nsetting\\nConfigure Security\\nUsers and Passwords\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nCreate OpsBridge Administrator (user: admin) Password\\nOpsBridge Administrator\\n (user: admin)\\nPassword \\nConfirm Create OpsBridge Administrator Password\\n(user: admin)\\nConfirm the password for \\nAdministrator Us\\ner.\\nUpload TLS Certificates\\nEnsure that you have all the required certificates beforehand. For details.\\n1\\n. \\nUnder \\nTLS certificates\\n, click the box and then choose the TLS certificates (CA or self-signed certificates for\\nPostgreSQL\\n or \\nOracle\\n and \\nVertica\\n)  on your server. Instead, you can even drag the certificates into the box. Make\\nsure the certificates are the base64 encoded .crt files. \\n2\\n. \\nIf you are\\n deploying Hyperscale Observability with external OBM\\n. Upload the UCMDB certificate in \\n.crt\\n format.\\nSee \\nImport UCMDB server certificate\\n for details. \\n3\\n. \\nThis is applicable if you are \\ndeploying Agentless Monitoring\\n \\nwith SSL enabled SiteScope.\\n Upload the Sitescope\\ncertificate( CA or Self-signed certificate file). The certificate can be in\\n .crt\\n or \\n.pem\\n format. For steps to create SiteScope\\ncertificates, see \\nCreate SiteScope certificates\\n.\\n4\\n. \\nIn the case of shared OPTIC Data Lake, upload the exported \\nProvidingAppCert.pem\\n (RE, RID, and any custom certificate).\\nSee \\nDeploy\\n.\\n5\\n. \\n Click \\nVERIFY\\n. This will verify if the certificates are valid and in the correct format (\\n.crt \\nfor \\nPostgreSQL\\n or \\nOracle \\nand \\n.\\npem\\n or\\n .crt\\n for Vertica and UCMDB)\\nImportant:\\nMake sure that your Vertica certificate name is \"\\nvertica-ca.pem\\n or \\nvertica-ca.crt\\n\" before you upload the certificates.\\nPlease make sure the provided certificate information complies with the Certificate Path Validation algorithm as defined\\nin \\nRFC 5280\\n \\nwithout section 6.3\\n. This includes (but isn\\'t limited to) the following checks:\\nValidity Period:\\n The certification validity period has started already but didn\\'t end yet.\\nOnly X509 certificates with PEM \\n(Privacy Enhanced Mail) encoding (Base64 ASCII) are supported.\\nContainerized Operations Bridge 2022.11\\nPage \\n249\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ef71a148b5edccbc06888702390597d6'}>,\n",
              "  <Document: {'content': \"VERIFY CERTIFICATE\\nAfter uploading the (PKCS8 or PKCS12) certificates, click on VERIFY\\nCERTIFICATE for validation.\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nConfigure Database\\nOpsBridge Database Settings\\nSelect the \\nDatabase type\\n:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nExternal PostgreSQL\\nUse the toggle to select the  external\\nPostgreSQL database for deployment\\nSelected\\nExternal Oracle\\nUse the toggle to select the Oracle\\ndatabase for deployment.\\nEmbedded PostgreSQL (Evaluation and Low footprint\\nOpsBridge OPTIC Reporting deployment only)\\nUse the toggle to select the embedded\\nPostgreSQL database for deployment\\nIf you have selected an external PostgreSQL or Oracle database, enter the following details: \\nConfiguration\\nparameter\\nDescription\\nDefault\\nsetting\\nDatabase\\nHost Name\\nEnter the \\nFQDN\\n of the PostgreSQL database server.\\nDatabase\\nPort for\\nExternal\\nDatabase\\nEnter the port number of the PostgreSQL database server.\\nDatabase TLS\\nIf you have configured PostgreSQL database with TLS, enable this option using the slider.\\nNote:\\n By disabling \\nUse TLS\\n, you are disabling or bypassing security features, thereby\\nexposing the system to increased security risks. By using this option, you understand and\\nagree to assume all associated risks and hold Micro Focus harmless for the same. It's\\nrecommended to enable TLS.\\nEnabled\\nChoose\\nOracle\\nConnection\\nMethod \\nSelect \\nOracle Connection String\\n or \\nOracle Service Name \\nor \\nOracle SID.\\nOracle\\nConnection\\nString\\nEnter Oracle connection string.\\nOracle\\nService Name\\nEnter the Oracle service name, database hostname, and database port number for external\\ndatabase.\\nEnabled\\nOracle SID\\nEnter the Oracle\\n SID\\n name, database hostname, and database port number for external\\ndatabase.\\nDatabase\\nHost Name\\nEnter the \\nFQDN\\n of the Oracle database server.\\nNote: \\nOperations Bridge deployment on Azure with Oracle database isn't supported. \\nOperations Bridge deployment on AWS with Oracle database isn't supported if the ITOM Cloud Deployment\\ntoolkit is used to set up AWS infrastructure. However, for AWS infrastructure that's set up manually, you can\\ndeploy Operations Bridge with Oracle database without TLS. \\n\\ue916\\n\\ue916\\nImportant\\n: \\nIf you are using an existing Shared Optic Data Lake, and have changed\\nthe\\n idm\\n, \\nautopass\\n, \\nbvd\\n and \\nbtcd\\n database and user names based on the deployment scenario, ensure that you\\nprovide the correct user and database names in this section. For example, If you are using an existing Shared\\nOptic Data Lake from NOM, and your OpsB username and database names are \\nidm-opsb\\n, \\nautopass-opsb\\n, \\nbvd-\\nopsb\\n and \\nbtcd-opsb\\n provide the same in this section.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n251\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be11696b68e641fe719c71992fa44e93'}>,\n",
              "  <Document: {'content': 'The certificate chain received from Certificate Authority should be in a format that contains a chain of \\nX509\\n certificates\\nencoded in Base64 format. A good test is to see whether this command is able to decode the certificate chain: \\nopenssl \\nX5\\n09\\n -in CA-certificate-chain.crt\\n \\n-text -noout\\nUpload OPTIC Data Lake Client Authentication Certificates \\nThis is to set up the client certificate authentication, used for connecting the external OBM in the post install section. For\\ndetails see \\nConfigure Classic OBM\\n. \\n1\\n. \\nUnder \\nOPTIC Data Lake Client Authentication Certificates\\n, click the box and then choose the certificates on your\\nserver. Instead, you can even drag the certificates into the box. \\n2\\n. \\n In the case of shared OPTIC Data Lake,\\n \\nupload the\\n \\nConsumingAppCert.pem\\n in the providing deployment. See \\nCertificate\\nexchange for using Shared OPTIC Data Lake\\n.\\n3\\n. \\nClick \\nVERIFY\\n. This will verify if the certificates are valid and in the correct format.  You can upload multiple certificates if\\nneeded. \\nUser ID and Group ID \\nConfiguration parameter\\nDescription\\nDefault setting\\nUser ID (UID)\\n \\nAll containers in the deployment will run as this UID\\n1999\\nGroup ID (GID)\\n \\nAll containers in the deployment will run as this GID\\n1999\\nDeploy CA-signed Server Certificate for Application Ingress\\nMove the slider to enable Deploy CA-signed Server Certificate for Application Ingress.\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nPKCS8\\nYou can select PKCS8 Format for the Certificate Files.\\nUpload the File Containing the\\nIssuing Certificate Authority (CA)\\nCertificate\\nUpload the File Containing the Server\\nCertificate, Including Certificate\\nChain *\\nThe server certificate must be in base64-encoded PEM format, and\\nit must contain all the intermediate certificates in the certificate\\nchain\\nUpload the File Containing the\\nPrivate Key *\\nEnter Password for Private Key File, if\\nRequired\\nPKCS12\\nYou can select PKCS12 Format for the Certificate Files.\\nUpload the File Containing the\\nIssuing Certificate Authority (CA)\\nCertificate\\nUpload the PKCS12 File Containing\\nyour TLS Key and Certificate *\\nEnter Password for PKCS12 File, if\\nRequired\\nImportant: \\nOnly \\nX509\\n certificates with \\nPEM \\n(Privacy Enhanced Mail) encoding (Base64 \\nASCII\\n) are supported.\\n                          Ensure that the certificate name starts with \"client\" and ends with the \".crt\" extension. Example:\\nclient-production-obm.crt\\n      \\nImportant:\\n For OpenShift deployment use the \\nUID FsGID\\n values of application namespace. \\nContainerized Operations Bridge 2022.11\\nPage \\n250\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4399b7a171f5fdc0187b6c2b820f431'}>,\n",
              "  <Document: {'content': \"Database\\nPort for\\nExternal\\nDatabase\\nEnter the port number of the Oracle database server.\\nDatabase TLS\\nIf you have configured Oracle database with TLS, enable this option using the slider.\\nEnabled\\nAutomatically\\ncreate\\nrequired\\ndatabases\\nThis option creates the databases required for the application automatically using the\\ndatabase administrator credentials. You can enable this toggle if you haven't yet created the\\ndatabases. All the default databases and usernames appear in the respective database\\nsections. You must type the passwords for these databases. \\nDisabled\\nConfiguration\\nparameter\\nDescription\\nDefault\\nsetting\\nOpsBridge Database (Database Administrator)\\nThis section appears if you have enabled the \\nAutomatically create required databases \\ntoggle.\\nConfiguration parameter\\nDescription\\nDefault setting\\nDatabase Administrator User Name\\nEnter the database administrator user name.\\nDatabase Administrator Password\\nEnter the database administrator password.\\nAfter entering all the values, select \\nVerify \\nto validate the connection to the database server.\\nOpsBridge Database (IDM) \\nConfigures external database parameters:\\nConfiguration parameter\\nDescription\\nDefault setting\\nDatabase User Name (IDM)\\nEnter the IdM database username for external PostgreSQL/Oracle\\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostname>\\n \\nDatabase Password (IDM)\\nEnter the password for IdM database. \\nDatabase Name (IDM)\\nEnter the name of the IdM database. \\nVerify\\nVerify the connection\\nNote:\\n By disabling \\nUse TLS\\n, you are disabling or bypassing security\\nfeatures, thereby exposing the system to increased security risks. By\\nusing this option, you understand and agree to assume all associated\\nrisks and hold Micro Focus harmless for the same. It's recommended to\\nenable TLS.\\n\\ue916\\n\\ue916\\nImportant: \\nThis option is available only for external PostgreSQL database.\\n\\ue91b\\n\\ue91b\\nImportant:\\n For OpsBridge with shared OPTIC DL or multi deployments, if OpsBridge is deployed as the second\\ndeployment, make sure to change the \\nidm\\n, \\nautopass\\n, \\nbvd\\n, and \\nbtcd\\n database and user names. For example,\\ndatabase names can be \\nopsb_idm\\n, opsb_\\nautopass\\n, opsb_\\nbvd\\n, and opsb_\\nbtcd.\\nImportant:\\n \\nIf you are using an existing Shared Optic Data Lake, and have changed the \\nidm\\n database and\\nuser names based on the deployment scenario, ensure that you provide the correct user and database names in\\nthis section. For example, If you are using an existing Shared Optic Data Lake from NOM, and your OpsB\\nusername and database names are \\nidm-opsb,\\n provide the same in this section.\\n\\ue91b\\n\\ue91b\\nNote:\\n Not applicable when you select External Oracle.\\nContainerized Operations Bridge 2022.11\\nPage \\n252\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '362e603a5e92e88101b3dbc4ea971da4'}>,\n",
              "  <Document: {'content': \"OpsBridge Database (APLS) \\nConfigures external database parameters for the application:\\nEnsure that, you haven't customized autopass schema name to anything other than the default value '\\nautopassschema\\n'.\\nConfiguration parameter\\nDescription\\nDefault setting\\nDatabase User Name (APLS)\\nEnter the AutoPass database username for PostgreSQL/Oracle \\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostname>\\nDatabase Password (ALPS)\\nEnter the password for AutoPass database. \\nDatabase Name (ALPS)\\nEnter the name of the AutoPass database.\\nVerify\\nVerify the connection\\nOpsBridge Database (BVD) \\nConfigures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault setting\\nDatabase User Name (BVD)\\nEnter the BVD database username for PostgreSQL/Oracle \\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostname>\\nDatabase Password (BVD)\\nEnter the password for BVD database. \\nDatabase Name (BVD)\\nEnter the name of the BVD database.\\nVerify\\nVerify the connection\\nOracle Database Wallet Configuration\\nIf you have selected external Oracle update the wallet configuration. This section isn't applicable if you have selected AWS or\\nAzure as the Kubernetes provider.\\nConfiguration parameter\\nDescription\\nDefault setting\\nOracle wallet zip file\\nClick and select the wallet zip file \\nOpsBridge Database (OBM EVENT) \\nImportant\\n:\\n If you are using an existing Shared OPTIC Data Lake, and have changed the \\nautopass\\n database\\nand user names based on the deployment scenario, ensure that you provide the correct user and database names\\nin this section. For example, If you are using an existing Shared OPTIC Data Lake from NOM, and your OpsB\\nusername and database names are \\nautopass-opsb, \\nprovide the same in this section.\\n\\ue91b\\n\\ue91b\\nNote: \\nNot applicable when you select External Oracle.\\n\\ue916\\n\\ue916\\nImportant\\n:\\n If you are using an existing Shared OPTIC Data Lake, and have changed the \\nbvd\\n database and\\nuser names based on the deployment scenario, ensure that you provide the correct user and database names in\\nthis section. For example, If you are using an existing Shared Optic Data Lake from NOM, and your Operations\\nBridge username and database names are  \\nbvd-opsb,\\n provide the same in this section.\\n\\ue91b\\n\\ue91b\\nNote: \\nNot applicable when you select External Oracle.\\n\\ue916\\n\\ue916\\nNote: \\nNot applicable when you select External Oracle.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n253\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4000edceb623296ac459b9a3e433645'}>,\n",
              "  <Document: {'content': 'Database User Name for\\nMonitoringAdmin\\nEnter the \\nMonitoringAdmin\\n database username for\\nPostgreSQL/Oracle\\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostna\\nme>\\n \\nDatabase Password\\n(MonitoringAdmin)\\nEnter the password for \\nMonitoringAdmin\\n database. \\nDatabase Name (MonitoringAdmin)\\nEnter the name of the \\nMonitoringAdmin \\ndatabase.\\nVerify\\nVerify the connection\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nOpsBridge Database (CREDENTIALMANAGER)\\nConfigures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nDatabase User Name\\nfor CredentialManager\\nEnter the \\nCredentialManager\\n database username for\\nPostgreSQL/Oracle\\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostn\\name>\\n \\nDatabase Password\\n(CredentialManager)\\nEnter the password for \\nCredentialManager\\n database. \\nDatabase Name (CredentialManager)\\nEnter the name of the \\nCredentialManager \\ndatabase.\\nVerify\\nVerify the connection\\nOpsBridge Database (MONITORINGSNFDB)\\nConfigures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nDatabase User Name\\nfor MonitoringSNF\\nEnter the \\nMonitoringSNF\\n database username for\\nPostgreSQL/Oracle\\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostna\\nme>\\n \\nDatabase Password\\n(MonitoringSNF\\n)\\nEnter the password for \\nMonitoringSNF\\n database. \\nDatabase Name (MonitoringSNF\\n)\\nEnter the name of the \\nMonitoringSNF\\n \\ndatabase.\\nVerify\\nVerify the connection\\nOpsBridge Database (BTCD)\\nConfigures external \\nPostgreSQL\\n database parameters for the application:\\nNote:\\n Not applicable when you select External\\nOracle.\\n\\ue916\\n\\ue916\\nNote:\\n Not applicable when you select External\\nOracle.\\n\\ue916\\n\\ue916\\nNote:\\n Not applicable when you select External\\nOracle.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n255\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff3ab56eab6af06c5e170d3251b305af'}>,\n",
              "  <Document: {'content': 'Configures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nDatabase User Name for OBM\\nEvent\\nEnter the \\nOBM Event\\n database username for\\nPostgreSQL/Oracle \\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostna\\nme>\\nDatabase Password (OBM Event)\\nEnter the password for \\nOBM Event\\n database. \\nDatabase Name (OBM Event)\\nEnter the name of the \\nOBM Event\\n database.\\nVerify\\nVerify the connection\\nOpsBridge Database (OBM MGMT)  \\nConfigures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nDatabase User Name for OBM\\nMgmt\\nEnter the \\nOBM Mgmt\\n database username for\\nPostgreSQL/Oracle\\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostnam\\ne>\\n \\nDatabase Password (OBM Mgmt)\\nEnter the password for \\nOBM Mgmt\\n database. \\nDatabase Name (OBM Mgmt)\\nEnter the name of the \\nOBM Mgmt\\n database.\\nVerify\\nVerify the connection\\nOpsBridge Database (RTSM)  \\nConfigures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nDatabase User Name for OBM\\nRTSM\\nEnter the \\nOBM RTSM\\n database username for\\nPostgreSQL/Oracle \\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostnam\\ne>\\nDatabase Password (RTSM)\\nEnter the password for\\n RTSM\\n database. \\nDatabase Name (RTSM)\\nEnter the name of the \\nRTSM \\ndatabase.\\nVerify\\nVerify the connection\\nOpsBridge Database (MONITORINGADMINDB)\\nConfigures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nNote:\\n Not applicable when you select External Oracle.\\n\\ue916\\n\\ue916\\nNote: \\nNot applicable when you select External Oracle.\\n\\ue916\\n\\ue916\\nNote:\\n Not applicable when you select External Oracle.\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n254\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5015eb4d876c87a97fc481373f10ddc8'}>,\n",
              "  <Document: {'content': 'Default Storage\\nClass(RWX)\\nThe storage class name for \\no\\npsb pvc\\nFor OpenShift:\\n \\nocs-storagecluster-cephfs\\nFor AWS:\\n \\nopsb-default\\nFor Azure:\\n \\nopsb-default\\n \\nEmbedded Kubernetes: \\n <empty>\\ncdf-nfs\\n (Set this value to \\ncdf-nfs\\n if you use the NFS provisioner to\\ncreate volumes dynamically)\\nDefault Storage\\nClass(RWO)\\nThe Storage Class used by\\nomi-artemis\\nFor OpenShift: \\nocs-storagecluster-cephfs\\nFor AWS:\\n \\nopsb-default\\nFor Azure:\\n \\nopsb-default\\nEmbedded Kubernetes: \\n<empty>\\ncdf-nfs\\n (Only when OMT\\'s NFS provisioner is used\\n)\\nConfiguration\\nparameter\\nDescription\\nDefault setting\\nOBM Storage Class\\nConfiguration parameter\\nDescription\\nDefault setting\\nStorage class for the omi PVCs\\nThe storage class name for OBM PVC\\nFor OpenShift\\n: \\nocs-storagecluster-ceph-rbd\\nFor AWS\\n: \\nio2\\nFor Azure\\n: \\nmanaged-premium\\n \\nConfigure monitoring\\nThe Self Monitoring setting allows you to monitor the overall status of clusters, pods, and data flow in Operations Bridge. It\\'s\\nbased on the Prometheus and Grafana open source projects. To use this setting, you must have installed OMT with the value \\n\"\\nMonitoring=true\"\\n to the \\n--capabilities\\n option.\\nConfiguration\\nparameter\\nDescription\\nRequired\\nfield\\nDefault\\nsetting\\nDeploy\\nMonitoring\\nConfiguration\\nUse the slider to enable self monitoring for the application. You can use this\\nonly if you have deployed Prometheus monitoring in the cluster.\\nEnabled\\nDeploy Grafana\\nDashboard\\nUse the slider to enable or disable the Grafana dashboard.\\nEnabled\\nConfigure local persistent volumes\\nCustomize Local persistent volumes PVC size for \\nOPTIC Data Lake\\n. This section isn\\'t applicable if you have deployed only the\\nOBM and Stakeholder dashboard capabilities.\\nFor embedded Kubernetes\\n this section refers to the \\nCreate local persistent volumes on worker nodes\\n topic, where\\nyou configured the LPVs on each worker node from the Sizing Calculator for \\nledger, journal, \\nand \\nzookeeper\\n. You can edit these\\nLPV sizes to match the LPV sizes you had created on the worker nodes.\\nFor OpenShift\\n, this section refers to the step where you configured the additional disks on each worker nodes using the\\nOCS\\n Operator. Consider disk sizes for \\nledger,\\n \\njournal,\\n and \\nzookeeper\\n by referring to Sizing Calculator.\\nFor AWS and Azure\\n, this section refers to the storage created in the AWS and Azure section of \\nCreate PV manually\\n topic. \\nConfiguration parameter\\nDescription\\nDefault setting\\nImportant: \\nDon\\'t modify these values during reconfiguration post application deployment.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n257\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3ff60298b1f0dd92f965ac1196c2cc66'}>,\n",
              "  <Document: {'content': \"Configuration parameter\\nDescription\\nDefault setting\\nDatabase User Name for BTCD\\nEnter the \\nBTCD\\n database username for PostgreSQL/Oracle\\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostname>\\nDatabase Password (BTCD\\n)\\nEnter the password for \\nBTCD\\n database. \\nVerify\\nVerify the connection\\nVertica Database\\nSelect \\nExternal Vertica\\n or \\nEmbedded Vertica \\ndatabase. \\nConfiguration\\nparameter\\nDescription\\nDefault\\nsetting\\nVertica Database\\nSelect \\nExternal Vertica\\n or \\nEmbedded Vertica (Evaluation purpose only)\\n.\\nExternal\\nVertica\\nEmbedded Vertica\\nWhen you select Embedded Vertica no other parameters related to Vertica are\\nprompted.\\ndisabled\\nConfigure the external Vertica database for the application\\n.\\n This section isn't applicable if you have deployed OBM and\\nStakeholder dashboard capabilities.\\nConfiguration\\nparameter\\nDescription\\nDefault\\nsetting\\nVertica TLS\\nUse the slider to enable/disable TLS, if enabled, communication with Vertica services\\nwill be secure\\nenabled\\nVertica Host\\nName(s) or IP\\nAddress(es) \\nComma separated list of \\nFQDN\\n for Vertica database servers. In the case of embedded\\nVertica, you \\nmust set\\n this parameter to '\\nitom-di-vertica-svc\\n'.\\nPort for\\nExternal Vertica\\nPort number for external Vertica database server. In the case of embedded Vertica, you\\nmust\\n \\nset\\n this parameter to '5444'.\\n5433\\nVertica Read/Write\\nuser Name\\nA user name that has permissions to create tables, etc. In the case of embedded\\nVertica, you \\nmust set\\n this parameter to '\\ndbadmin\\n'.\\nvertica_r\\nwuser\\nVertica Read/Write\\nUser Password\\nEnter the password for the Vertica database user with read/write access\\nVertica\\ndatabase/schema\\nname \\n \\nName of schema within Vertica database \\nitomdb\\nVerify \\nVerify the connection to Vertica database for the \\nVertica Read/Write user\\nVertica Read-only\\nuser Name\\nA user name that has read-only permissions to access Vertica. In case of embedded\\nVertica, you \\nmust\\n \\nset\\n this parameter to \\ndbadmin\\n.\\nvertica_r\\nouser\\nVertica Read-only\\nUser Password\\nEnter the password for the Vertica database user with read-only access\\nVerify \\nVerify the connection to Vertica database for the \\nVertica Read-Only user\\nVertica Resource\\nPool Name\\nEnter the externally created and managed resource pool name, if the externally\\nmanaged resource pool name isn't the same as the default pool name otherwise it can\\nbe blank.\\nitom_di_\\nstream_r\\nes\\nConfigure Advanced Settings\\nDefault Storage Class\\nFor the default storage class, set the value \\ncdf-nfs\\n if you use the OMT's NFS provisioner, else do not change it.\\nConfiguration\\nparameter\\nDescription\\nDefault setting\\nImportant:\\n If you are using an existing Shared OPTIC Data Lake, and have changed the \\nbtcd\\n database and\\nuser names based on the deployment scenario, ensure that you provide the correct user and database names in\\nthis section. For example, If you are using an existing Shared OPTIC Data Lake from NOM, and your OpsB\\nusername and database names are \\nbtcd-opsb,\\n provide the same in this section.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n256\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '96a214a7cc5a33197f989f24d4ec1226'}>,\n",
              "  <Document: {'content': \"Ledgers Local Persistent Volumes Storage\\nClass Name\\n \\nThe storage class name for Ledgers\\nLPV\\nFor OpenShift:\\n \\nocs-storagecl\\nuster-ceph-rbd\\nFor AWS:\\n \\ngp3\\nFor Azure: \\nmanaged-premiu\\nm\\nLedgers Local Persistent Volumes Size\\nThe size for Ledgers LPV\\nFor AWS, Azure: \\n100 Gi\\nJournal Local Persistent Volumes Storage\\nClass Name\\n \\nThe storage class name for Journal\\nLPV\\nFor OpenShift:\\n \\nocs-storagecl\\nuster-ceph-rbd\\nFor AWS:\\n \\nio2\\nFor Azure:\\n \\nmanaged-premiu\\nm\\nJournal Local Persistent Volumes Size\\nThe size for Journal LPV\\nFor AWS, Azure: \\n50 Gi\\nZookeeper Data Local Persistent Volumes\\nStorage Class Name \\nThe storage class name for\\nZookeeper Data LPV\\nFor OpenShift:\\n \\nocs-storagecl\\nuster-ceph-rbd\\nFor AWS:\\n \\ngp3\\nFor Azure: \\nmanaged-premiu\\nm\\nZookeeper Data Local Persistent Volumes\\nSize\\nThe size for Zookeeper Data LPV\\nFor AWS, Azure: \\n50 Gi\\nZookeeper DataLog Local Persistent Volumes\\nStorage Class Name \\nThe storage class name for\\nZookeeper Data Log LPV\\nFor AWS:\\n \\ngp3\\nFor Azure:\\n \\nmanaged-premiu\\nm\\nZookeeper DataLog Local Persistent Volumes\\nSize\\nThe size for Zookeeper Data Log\\nLPV\\nFor AWS, Azure: \\n10 Gi\\nConfiguration parameter\\nDescription\\nDefault setting\\nYAML EDITOR\\nThe YAML EDITOR enables you to configure additional parameters that aren't exposed in AppHub UI but are available in the\\napplication Helm chart.\\nYou can click the \\nYAML EDITOR\\n, add the additional parameters that you want to configure, and click \\nSAVE\\n.\\nFor Shared OPTIC Data Lake deployment where Operations Bridge is the consumer application, enter the \\nbase64\\nencoded\\n secret of idm_integration_admin_password parameter used for NOM similar to the following YAML EDITOR example.\\nIf you have enabled Stakeholder Dashboards for NOM, and you want to schedule BVD reports using a specific email id, enter\\nthe \\nbase64 encoded\\n secret of \\nschedule_mail_password_key\\n parameter and \\nbvd\\n parameters similar to the following YAML\\nEDITOR example.\\nsecrets:\\n  schedule_mail_password_key: <base64 encoded password of SMTP user account>\\n  idm_integration_admin_password: <base64-encoded password from the same key given during OpsBridge installation>\\nbvd:\\n  smtpServer:\\n    host: <Give the smtpServer host name>\\n    port: <Give the smtpServer port>\\n    security: <STARTTLS or TLS depending on the SMTP server's configuration>\\n    user: <SMTP username>\\n    from: <SMTP user email id>\\nIf you plan for Shared OPTIC Data Lake deployment where Operations Bridge is the provider application and the\\ncommunication mechanism from the consumer application using FQDN, while you deploy Operations Bridge, enter the \\nsetFqdnI\\nnIngress\\n parameter similar to the following YAML EDITOR example.\\nglobal:\\n  setFqdnInIngress: false\\nIt's recommended to use the AppHub UI for all configurations unless otherwise absolutely necessary.\\nContainerized Operations Bridge 2022.11\\nPage \\n258\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2119e0bcc871aaf5725a09e73e6420f6'}>,\n",
              "  <Document: {'content': \"Deploy the application using CLI\\nFollow the steps mentioned below to deploy Operations Bridge using CLI:\\n1\\n. \\nUpdate the passwords for secrets in values.yaml\\n2\\n. \\nConfigure values.yaml file\\n \\n3\\n. \\nDownload and upload the application images to the container registry - Before you deploy the application, you must\\ndownload the required images from Docker Hub or other registries and then upload the images to your registry.\\nFor AWS\\n, ensure that the containers used by the application must be available in the Elastic Container Registry (ECR) for\\nthe desired region. For more information and steps, see \\nAWS ECR\\n topics.\\nFor Azure\\n, ensure that the containers used by the application must be available in the Azure Container Registry (ACR)\\nfor the desired region. For more information and steps, see \\nAzure ACR\\n topics.\\nFor other K8s providers\\n, see \\nDownload and Upload installation images\\n.\\n4\\n. \\nDeploy the application\\n \\n5\\n. \\nYou must configure the listeners and target groups for the application. Also, create or update the Route 53 records in\\nPrivate Hosted Zone. For more information, see \\nCreate listeners and target ports\\n.\\n6\\n. \\nIf you are using shared OPTIC Data Lake,\\n pass the OPTIC DL Client Authentication Certificates\\n and upgrade the\\nproviding deployment. For details, see \\nCertificate exchange for using Shared OPTIC Data Lake\\n.\\nNote\\n: If you've used the ITOM Cloud Deployment Toolkit and set up the AWS and Azure infrastructure, skip\\nstep 3 and step 5.\\n\\ue916\\n\\ue916\\nImportant:\\n \\n If you want to download only the required images for your specific deployment\\nconfiguration, you must configure the \\nvalues.YAML\\n and pass it with \\n-H\\n option while running the \\ngenerate_downlo\\nad_bundle.sh\\n script. Otherwise, run the script without \\n-H\\n \\nvalues.YAML\\n to download all the images that are listed\\nin the specified chart. \\n\\ue91b\\n\\ue91b\\nNote:\\n For shared OPTIC Data Lake, pass the generated certificate file \\nProvidingAppCert.pem\\n while installing\\nthe consuming application.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n259\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd8a47b5da966c98f49dc154850767ef'}>,\n",
              "  <Document: {'content': \"Update password for secrets\\n\\u200bA secret is an object that has sensitive data such as a password, a token, or a key.  Based on the capability that you have\\nplanned to deploy, you must update the password for all the applicable secrets. The secrets are already existing, but you must\\nupdate the password for all the required secrets in base64 format in the \\nsamples/values.yaml\\n or \\nsamples/aws/values.yaml\\n or \\nsample\\ns/azure/values.yaml\\n or \\nsamples/openshift/values.yaml\\n or \\nsamples/generic/values.yaml \\nfile depending on the Kubernetes\\ndistribution\\n \\nthat you have selected.\\nThe following table lists all the available secrets and the capability that uses them:\\nPassword \\nDescription\\nITOMDI_DB\\nA_PASSWO\\nRD_KEY\\nPassword of the Vertica database user with read and write permission. The application will give the\\nusername later in \\nvalues.yaml\\n file under \\nglobal.vertica.rwuser.\\nOPTIC\\nReporting,\\nAutomatic\\nEvent\\nCorrelation\\nITOMDI_RO\\n_USER_PAS\\nSWORD_KE\\nY\\nPassword of the Vertica database user with read-only permission. The application will give the username later in \\nv\\nalues.yaml\\n file under \\nglobal.vertica.rouser\\nOPTIC\\nReporting,\\nAutomatic\\nEvent\\nCorrelation\\nidm_opsbri\\ndge_admin\\n_password\\nPassword for the Operations Bridge admin user. This creates a user called admin with this password for access to\\nOperations Bridge IdM UI. The password must be 8 to 32 characters and must contain at least one lowercase, one\\nuppercase, one digit, and one special character.\\nThis is different from the admin credential entered during CDF installation. \\nAll\\nIDM_DB_U\\nSER_PASS\\nWORD_KEY\\nPassword for IdM database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under \\nd\\neployment.database.user\\n of IdM.\\nAll\\nAUTOPASS\\n_DB_USER_\\nPASSWORD\\n_KEY\\nPassword for \\nAUTOPASS\\n database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\nunder \\ndeployment.database.user\\n of \\nAUTOPASS\\n.\\nAll\\nBVD_DB_U\\nSER_PASS\\nWORD_KEY\\nPassword for BVD database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under \\nd\\neployment.database.user\\n of BVD.\\nStakeholder\\nDashboard\\nOBM_MGM\\nT_DB_USER\\n_PASSWOR\\nD_KEY\\nPassword for \\nMGMT\\n database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under\\ndeployment.mgmtDatabase.user\\n of OBM.\\nOBM\\nOBM_EVEN\\nT_DB_USER\\n_PASSWOR\\nD_KEY\\nPassword for \\nEVENT\\n database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under\\ndeployment.eventDatabase.user \\nof OBM.\\nOBM\\nRTSM_DB_\\nUSER_PASS\\nWORD_KEY\\nPassword for RTSM database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under \\ndeployment.database.user\\n of UCMDB.\\nOBM\\nMA_DB_US\\nER_PASSW\\nORD_KEY\\nPassword for monitoring admin database (PostgreSQL) user. The username is in \\nvalues.yaml\\n under \\ndeployment.datab\\nase.user\\n of monitoring admin.\\nHyperscale\\nObservability\\nCM_DB_PA\\nSSWD_KEY\\nPassword for PostgreSQL database for credentials manager.\\nHyperscale\\nObservability\\nOBM_RTSM\\n_PASSWOR\\nD\\nPassword for External OBM RTSM users. It's the password that you set for the 'Agent Metric Collector integration\\nuser' (See \\nCreate an Agent Metric Collector integration user\\n). If you didn't create this user, press enter. The\\nv generates a random password. The application will give the username later in \\nvalues.yaml \\nfile under \\nglobal.amc.rts\\nmUsername \\nNote:\\n You need not set this password if you are using containerized OBM (OBM capability).\\nOPTIC\\nReporting\\nsys_admin\\n_password\\nYou can use this Password in combination with the username sysadmin to log in to UCMDB’s JMX console.\\nOBM\\nucmdb_ma\\nster_key\\nSets the value for the master key of the UCMDB service that's used to encrypt all UCMDB keys.\\nOBM\\nUD_USER_\\nPASSWOR\\nD\\nPassword for External Universal Discovery user.The username will be provided in Helm values.yaml under\\nglobal.cms.udUsername\\nOBM_USER\\n_PASSWOR\\nD_KEY\\nPassword for External OBMs user with Content Packs upload permissions. The username will be provided in Helm\\nvalues.yaml under global.monitoringService.obmUsername\\nContainerized Operations Bridge 2022.11\\nPage \\n260\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f4275c38ae9db9ddc491143b4e3f2730'}>,\n",
              "  <Document: {'content': \"SNF_DB_U\\nSER_PASS\\nWORD_KEY\\nPassword for temporary storage PostgreSQL database for Hyperscale Observability.\\nHyperscale\\nObservability\\nschedule_\\nmail_passw\\nord_key\\nPassword for scheduling the reports to an email for BVD.\\nOPTIC\\nReporting\\nBTCD_DB_\\nPASSWD_K\\nEY\\nPassword for NOM metric transformation and Hyperscale Observability.\\nHyperscale\\nObservability\\nOPTIC_DAT\\nALAKE_INT\\nEGRATION_\\nPASSWOR\\nD\\nIDM Integration user password of the providing deployment when using Shared OPTIC DL. \\nIf you plan to use Shared OPTIC DL, retrieve the password as mentioned below and pass the same in\\nthe secrets section of values.yaml.\\nTo retrieve the decoded password, execute the command:\\nkubectl -n <namespace of the providing application> get secret <providing_suite_secret> --template={{.data.idm_integration_admin_password}}\\nExample:\\nkubectl -n <nom-helm> get secret <nom-secret> --template={{.data.idm_integration_admin_password}}\\nShared\\nOPTIC\\nReporting\\nIf you want to install Stakeholder Dashboard only, then the Vertica, OBM RTSM, and OPTIC Data Lake Health Insights secrets\\naren't applicable. \\nIf you want to install Automatic Event Correlation without OPTIC Reporting, the OBM RTSM secret isn't applicable. For\\nAutomatic Event Correlation, the application configures the OBM credentials later.\\nUpdate the passwords in Base64 format\\nThe samples/values.yaml\\n or \\nsamples/<kubernetes distribution>/values.yaml\\n file will have all the required secret keys. You must update\\nrespective passwords in the base64 format.\\nExample:  \\n   \\nIf the password for \\nidm_opsbridge_admin_password\\n is Testing@123, follow the below example to update the password in base64\\nformat:          \\necho –n Testing@123 | base64 \\nCopy the output value which needs to be updated in the \\nvalues.yaml\\n  \\nfor the same key\\n idm_opsbridge_admin_password.\\nExample:\\nidm_opsbridge_admin_password: VGVzdGluZ0AxMjM=\\nNote:\\n If you change the password for\\n \\nidm_opsbridge_admin_password\\n, then you need to update the same password\\nfor \\nbvd_admin_password\\n \\nand \\nidm_admin_admin_password\\n keys also.\\nContainerized Operations Bridge 2022.11\\nPage \\n261\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3be6f8ba967900daa0e25a181b6f4251'}>,\n",
              "  <Document: {'content': 'global.expose.internalLoadBalancer.ip\\nType the Load balancer IP address for the\\ncommunication.\\nglobal.expose.internalLoadBalancer.sourceRanges\\nType the range of Load balancer IPs allowed\\nfor the communication.\\nParameter group\\nValues\\nDescription\\nServices\\nAll the Capabilities deployment is disabled by default, you must delete the \\'\\'\\n#\\n\" character for the specific line to deploy. \\nParameter group\\nValues\\nDescription\\nglobal.services.automaticEventCorrelation.deploy\\nPossible\\nvalues: \\ntrue\\nDefault\\nvalue: \\ntrue\\nEnable/disable deployment of the Automatic\\nEvent Correlation capability. \\nInstalls the Automatic Event Correlation\\ncapability with these components: Automatic\\nEvent Correlation (AEC), OPTIC DL.\\nT\\no deploy, you must delete the \\'\\'\\n#\\n\" character\\nfor this parameter.\\nglobal.services.stakeholderDashboard.deploy\\nPossible\\nvalues: \\ntrue\\nDefault\\nvalue: \\ntrue\\nEnable/disable deployment of the Stakeholder\\nDashboard capability. \\nInstalls the Stakeholder Dashboard capability\\nwith the BVD component.\\nT\\no deploy, you must delete the \\'\\'\\n#\\n\" character\\nfor this parameter.\\nglobal.services.obm.deploy\\nPossible\\nvalues: \\ntrue\\nDefault\\nvalue: \\ntrue\\nEnable/disable deployment of the OBM\\ncapability. \\nInstalls the OBM capability.\\nT\\no deploy, you must delete the \\'\\'\\n#\\n\" character\\nfor this parameter.\\nglobal.services.hyperscaleObservability\\n.deploy\\nPossible\\nvalues: \\ntrue\\nDefault\\nvalue: \\ntrue\\nEnable/disable deployment of Hyperscale\\nObservability capability. \\nT\\no deploy, you must delete the \\'\\'\\n#\\n\" character\\nfor this parameter.\\nglobal.services.agentlessMonitoring\\n.deploy\\nPossible\\nvalues: \\ntrue\\nDefault\\nvalue: \\ntrue\\nEnable/disable deployment of Agentless\\nMonitoring capability. \\nInstalls Agentless Monitoring capability.\\nT\\no deploy, you must delete the \\'\\'\\n#\\n\" character\\nfor this parameter.\\nglobal.services.anomalyDetection.deploy\\nPossible\\nvalues: \\ntrue\\nDefault\\nvalue: \\ntrue\\nEnable/disable deployment of Anomaly\\nDetection capability. \\nInstalls Anomaly Detection capability.\\nT\\no deploy, you must delete the \\'\\'\\n#\\n\" character\\nfor this parameter.\\nglobal.services.opticReporting.deploy\\nPossible\\nvalues: \\ntrue\\nDefault\\nvalue: \\ntrue\\nEnable/disable deployment of the OPTIC\\nReporting capability. \\nT\\no deploy, you must delete the \\'\\'\\n#\\n\" character\\nfor this parameter.\\nNote:\\n This parameter is applicable only\\nfor Generic Kubernetes providers.\\nNote:\\n This parameter is applicable only\\nfor Generic Kubernetes providers.\\nContainerized Operations Bridge 2022.11\\nPage \\n263\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e6c691c347e6398fa99d6de304b1bc0'}>,\n",
              "  <Document: {'content': \"Configure values.yaml\\nThe suite zip has the \\nvalues.yaml\\n file under \\nsamples\\n directory. You can edit the required file according to your Kubernetes\\nplatform. Following are the available sample \\nvalues.yaml\\n files:\\nAWS - \\n<installer chart unzip location>/samples/aws\\nAzure - \\n<installer chart unzip location>/samples/azure\\nGeneric - \\n<installer chart unzip location>/samples/generic\\nEmbedded Kubernetes - \\n<installer chart unzip location>/samples\\nOpenShift - \\n<installer chart unzip location>/samples/openshift\\nDon't change any indentation in the YAML file. Update the required values and keep the YAML syntax. Don't change the\\nparameters which have explicit comment \\n[DON'T CHANGE]\\n in the \\nvalues.yaml\\n file.\\nAfter you deploy the suite using \\nvalues.yaml\\n, you can either save the same \\nvalues.yaml\\n or even retrieve it from the system later.\\nThe automatically retrieved \\nvalues.yaml\\n may not have ordered parameters compared to user created or the saved \\nvalues.yaml\\n. \\nEnd user license agreement (EULA) \\nParameter group\\nValues\\nDescription\\nacceptEula\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault\\nvalue:\\n false\\nAccept Micro Focus \\nEULA\\n to proceed\\nfurther\\nYou can find the End User License\\nAgreement (\\nEULA\\n) at \\nSoftware License\\n.\\nExternal access host\\nParameter group\\nValues\\nDescription\\nglobal.externalAccessHost\\nThe hostname/FQDN (Load balancer or Master\\nNode) that you can access externally. \\nglobal.externalAccessPort\\nDefault\\nvalue:\\n443\\nThe port you can access externally (Load\\nbalancer OR Master Node). Operations Bridge\\nuses External Access Port along with an\\nExternal Access Host to access Operations\\nBridge. Make sure that any other program isn't\\nusing this port.\\nglobal.expose.type\\nGive the service type used for the\\ncommunication. This can be either a node port,\\nload balancer, or cluster IP.\\nglobal.expose.internalLoadBalancer.annotations\\nGive the annotations required for the Load\\nbalancer type.\\nImportant:\\n If you are using an existing\\nshared OPTIC Data Lake on \\nAWS\\n, you\\nmust have different external host names\\nfor providing and consuming applications.\\nNote:\\n For OpenShift, the external access\\nport range is 30000-32767.\\nNote:\\n This parameter is applicable only\\nfor Generic Kubernetes providers.\\nNote:\\n This parameter is applicable only\\nfor Generic Kubernetes providers.\\nContainerized Operations Bridge 2022.11\\nPage \\n262\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2fd3ed612946673233c21625c888381e'}>,\n",
              "  <Document: {'content': 'global.services.opticDataLake\\n.deploy\\nPossible\\nvalues: \\nfalse\\nDefault\\nvalue: \\nfalse\\nEnable/disable deployment of the OPTIC Data\\nLake. \\nThis parameter decides whether \\nOPTIC \\nData\\nLake will be deployed or not. If this parameter\\nis commented then \\nOPTIC \\nData Lake will be\\ndeployed, else provide parameters required to\\naccess Shared OPTIC DL.\\nYou must delete the \\'\\'\\n#\\n\" character for this\\nparameter only when you \\nDO NOT\\n want to\\ndeploy OPTIC Data Lake pods in your\\nnamespace and use OPTIC Data Lake\\ndeployed in a different namespace. To Deploy\\nOPTIC Data Lake, keep this parameter\\ncommented.\\n \\nParameter group\\nValues\\nDescription\\nConnection methods for shared OPTIC Data Lake\\nParameter group\\nValues\\nDescription\\nglobal.services.opticDataLake.externalOpticDataLake\\nUpdate values for all the parameters required\\nto access \\nOPTIC \\nDL deployed in a different\\nnamespace.\\n \\nglobal.services.opticDataLake.externalOpticDataLake.externalAccess\\nHost\\nDefault\\nvalue: \\nnull\\nExternal Access Host of the providing\\ndeployment, this is a compulsory parameter\\nfor using shared OPTIC Data Lake.\\nglobal.services.opticDataLake.externalOpticDataLake.externalAccess\\nPort\\nDefault\\nvalue: \\nnull\\nIngress Controller Port of the providing\\ndeployment, this is a compulsory parameter\\nfor using shared OPTIC Data Lake.\\nglobal.services.opticDataLake.externalOpticDataLake.integrationUser\\nDefault\\nvalue: \\nnull\\nIdM Integration User of the providing\\ndeployment, this is a compulsory parameter\\nfor using shared OPTIC Data Lake.\\nThis is the IdM integration username which you\\nhave created in the providing environment. ( \\nE\\nxample:integration_admin)\\nConnect to shared OPTIC Data Lake using namespace\\nTo connect to shared OPTIC Data Lake \\nusing namespace\\n (\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingNamespac\\ne)\\n, you need to provide only the namespace in which OPTIC Data Lake has been deployed.\\nParameter group\\nValues\\nDescription\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingN\\namespace\\nConnect using cross-namespace\\ncommunication within the same cluster.\\nYou can only use one of the four connection\\nmechanisms to connect to shared OPTIC Data\\nLake.\\nIf you choose this method, then you need not\\nupdate the parameters for \\nconnectUsingExternalA\\nccessHost.\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingN\\namespace.namespace\\nShared \\nOPTIC\\n Data Lake can be connected\\nusing cross-namespace communication, you\\nmust mention the providing deployment\\nnamespace.\\nConnect to shared OPTIC Data Lake using external access host\\nTo connect to shared OPTIC Data Lake \\nusing external access host\\n(\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingExternalAccessHost), you\\n need to provide the node ports of the OPTIC\\nData Lake services accessible over an external access host.\\nParameter group\\nValues\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n264\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '814202618bbb148138622d559ef78c0'}>,\n",
              "  <Document: {'content': 'global.services.opticDataLake.externalOpticDataLake.connectUsingE\\nxternalAccessHost\\nShared OPTIC Data Lake can be connected\\nusing External Access Host.\\nYou can only use one of the four connection\\nmechanisms to connect to shared OPTIC DL\\nIf you choose this method, then you need not\\nupdate the parameters for \\nconnectUsingNamespa\\nce\\n.\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingE\\nxternalAccessHost.diReceiverPort\\n30001 \\n(External Host) Port of OPTIC Reporting\\nIngestion Service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingE\\nxternalAccessHost.diAdminPort\\n30004\\n(External Host) Port of OPTIC Reporting\\nAdministration Service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingE\\nxternalAccessHost.diDataAccessPort\\n30003\\n(External Host) Port of OPTIC Reporting Data\\nAccess Service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingE\\nxternalAccessHost.diPulsarSslPort\\n31051\\n(External Host) Port of OPTIC Reporting\\nMessage Bus SSL service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingE\\nxternalAccessHost.diPulsarWebPort\\n31001\\n(External Host) Port of OPTIC Reporting\\nMessage Bus Web service\\nParameter group\\nValues\\nDescription\\nConnect to shared OPTIC Data Lake using service-specific FQDN (Not Applicable for Azure)\\nTo connect to shared OPTIC Data Lake \\nusing service-specific FQDN\\n(\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingServiceFQDN)\\n, you need to provide only the namespace in which\\nOPTIC Data Lake has been deployed.\\nParameter group\\nValues\\nDescription\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN\\nShared OPTIC Data Lake can be connected\\nusing service specific FQDNs\\nYou can only use one of the four connection\\nmechanisms to connect to shared OPTIC DL\\nIf you choose this method, then you need not\\nupdate the parameters for \\nconnectUsingNamespa\\nce\\n.\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.diReceiverHost\\nThe hostname of OPTIC Reporting Ingestion\\nService\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.diReceiverPort\\n30001 \\n(Service FQDN) Port of OPTIC Reporting\\nIngestion Service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.diAdminHost\\nThe hostname of OPTIC Reporting\\nAdministration Service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.diAdminPort\\n30004\\n(Service FQDN) Port of OPTIC Reporting\\nAdministration Service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.diDataAccessHost\\nThe hostname of OPTIC Reporting Data Access\\nService\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.diDataAccessPort\\n30003\\n(Service FQDN) Port of OPTIC Reporting Data\\nAccess Service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.diPulsarHost\\nThe hostname of OPTIC Reporting Pulsar\\nservice\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.diPulsarSslPort\\n31051\\n(Service FQDN) Port of OPTIC Reporting Pulsar\\nSSL service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.diPulsarWebPort\\n31001\\n(Service FQDN) Port of OPTIC Reporting Pulsar\\nWeb service\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.ingressControllerHost\\nIngress Controller FQDN of Providing\\nnamespace: Used to connect to the IdM\\ndeployed in the providing namespace\\nglobal.services.opticDataLake.externalOpticDataLake.connectUsingS\\nerviceFQDN.ingressControllerPort\\n18443\\nIngress Controller port of Providing\\nnamespace: Used to connect to the IdM\\ndeployed in the providing namespace\\nPersistent volume claim\\nParameter group\\nValues\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n265\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2425c0598511b24a1a884fcf5eb4f36'}>,\n",
              "  <Document: {'content': 'global.persistence.enabled\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value: \\ntrue\\nIf set to \\ntrue\\n then the PVCs will be\\nautomatically created.\\nIf set to\\n \\nfalse\\n then you must create the\\nPVCs.\\nAll PVCs are automatically created.  You\\ndon\\'t need to fill in or change anything in\\nthis section.\\nglobal.persistence.storageClasses.default-rwx\\nDefault value: \\nFor AWS/Azure -\\n\"\\nopsb-default\"\\nFor RedHat OpenShift \\n-\\n\"ocs-storagecluster-\\ncephfs\"\\nPossible values:\\nFor Embedded\\nkubernetes -\\n \"cdf-nfs\"\\nThe four PVCs are created with\\nthis storage class name.\\nIn case of Embedded Kubernetes and\\nAWS, if you have used OMT\\'s\\nNfsProvisioner to create Pvs, then you\\nneed to change the default values of the \\ns\\ntorageClass\\n parameters to \\ncdf-nfs\\nglobal.persistence.storageClasses.default-rwo\\nDefault value: For\\nAWS/Azure - \"\\nopsb-\\ndefault\", \\nFor RedHat OpenShift \\n-\\n\"ocs-storagecluster-\\ncephfs\"\\nPossible\\nvalues: \\n F\\nor Embedded\\nkubernetes -\\n \\n\"cdf-\\nnfs\"\\nomi-artemis\\n PVC is created using\\n\"\\ndefault-rwo\\n\"\\nIn case of Embedded Kubernetes and\\nAWS, if you have used OMT\\'s\\nNfsProvisioner to create Pvs, then you\\nneed to change the default values of the \\ns\\ntorageClass\\n parameters to \\ncdf-nfs\\nParameter group\\nValues\\nDescription\\nPrometheus\\nParameter group\\nValues\\nDescription\\nglobal.prometheus.deployPrometheusConfig\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value: true\\nEnable Prometheus monitoring\\nconfigurations to deploy scraping rules\\n(\\nServiceMonitor\\n, \\nPodMonitor\\n) and\\nalerts (\\nPrometheusRule\\n)\\nglobal.prometheus.deployGrafanaConfig\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault\\nvalue: \\ntrue\\nEnable Grafana monitoring dashboard\\nconfigurations to deploy config maps\\ndefining the Grafana dashboards\\nglobal.prometheus.prometheusSelector.prometheus_config\\nDefault value: \\n1\\nDefine the label(s) that Prometheus looks\\nfor when discovering rules\\nglobal.prometheus.grafanaSelector.grafana_dashboard\\nDefault value: \\n1\\nDefine the label(s) that Grafana looks for\\nwhen discovering dashboards\\nDocker\\nParameter group\\nValues\\nDescription\\nglobal.docker.registry\\nDefault value: For AWS\\n-  \\nECR_URL.amazonaws.com\\nFor Azure\\n-\\n ACR_URL.azurecr.io\\nThe Docker registry URL\\nglobal.docker.orgName\\nDefault value: \\nhpeswitom\\nDocker registry org name\\nglobal.docker.imagePullSecret\\nName of the secret used to log on to the\\nDocker registry. \\nImagepullsecret\\n is a secret that holds the\\nusername/password of a docker registry\\n(internal or external). For the local cluster\\nregistry, you can access images without\\na username/password. Hence you can\\nleave this blank. If you have configured\\nan external registry and want to use it\\ndirectly (without doing a\\ndownload/upload of images), you can\\nspecify the registry and give the\\nusername/password in this secret.\\nFor the local OMT registry, you don’t\\nneed to use a username/password or \\nima\\ngepullsecret\\n. The suite uses \\nregistry-\\nadmin\\n for modifying images in the local\\nregistry.\\nContainerized Operations Bridge 2022.11\\nPage \\n266\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f755201b613358fa18174e2e3e6a5603'}>,\n",
              "  <Document: {'content': 'global.vertica.resourcepoolname\\nDefault value:\\nitom_di_stream_respool_provider_default\\nVertica resource pools are a pre-\\nallocated subset of system\\nresources for running a queue of\\nqueries. You must give the\\nResource pool name for External\\nVertica. If not, the suite uses the\\ndefault name \\nitom_di_stream_respool_\\nprovider_default\\n.\\nParameter group\\nValues\\nDescription\\nNginx\\nThis parameter is applicable for cloud deployment.\\nParameter group\\nValues\\nDescription\\nglobal.nginx.httpsPort\\nDefault\\nvalue: For\\nAWS\\n- \\n30443\\nFor Azure -\\n443\\nThe port number for the Nginx load balancer\\nport.\\nKubernetes provider\\nThis parameter is applicable for cloud deployment.\\nParameter group\\nValues\\nDescription\\nglobal.cluster.k8sProvider\\nDefault\\nvalue:\\nFor AWS -\\n\"aws\"\\nFor Azure -\\n\"azure\"\\nFor\\nRancher\\n -\\n\"generic\"\\nThe Kubernetes provider for cloud\\ndeployment. \\nRelational database\\nParameter group\\nValues\\nDescription\\nglobal.database.internal\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\nfalse\\nEdit this section only for external relational\\ndatabases. The default for \\nglobal.database.inte\\nrnal\\n is \"\\nfalse\\n\". If you want to use embedded\\nPostgreSQL you would need to set \\nglobal.data\\nbase.internal \\nto \"\\ntrue\\n\", you don\\'t need to\\nchange any other parameters in this section.\\nSet \\ndatabase.internal\\n to \\nfalse\\n to use an\\nexternal PostgreSQL.\\nSet \\ndatabase.internal\\n to \\ntrue\\n to use the\\nembedded PostgreSQL database. The suite\\nsupports embedded PostgreSQL for non-\\nproduction use only. \\nglobal.database.type\\nDefault value:\\npostgresql\\nThe default is \\npostgresql\\n to use external\\nPostgreSQL.\\nglobal.database.host\\nFQDN of DB Server. Required for\\nPostgreSQL.\\nglobal.database.port\\nDefault value:\\n5432\\nDB Port. Required for PostgreSQL.\\nNote: \\nOperations Bridge deployment on Azure with Oracle database isn\\'t supported. \\nOperations Bridge deployment on AWS with Oracle database isn\\'t supported if you\\'ve used the ITOM Cloud\\nDeployment toolkit to set up AWS infrastructure. However, for AWS infrastructure that\\'s set up manually, you\\ncan deploy Operations Bridge with Oracle database without TLS. \\nContainerized Operations Bridge 2022.11\\nPage \\n268\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '72d86d12f15c0513eabce3475d632b7e'}>,\n",
              "  <Document: {'content': \"global.docker.imagePullPolicy\\nDefault value: \\nIfNotPresent\\nDocker image pull policy\\nParameter group\\nValues\\nDescription\\nSecurity\\nParameter group\\nValues\\nDescription\\nglobal.securityContext.user\\nDefault\\nvalue:\\n1999\\nUser ID which has the ownership of persistent\\nstorage and runtime deployment.\\nglobal.securityContext.fsGroup\\nDefault\\nvalue:\\n1999\\nGroup ID which has the ownership of persistent\\nstorage and runtime deployment.\\nVertica\\nParameter group\\nValues\\nDescription\\nglobal.vertica.embedded\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nIf you plan to use the suite with\\nany or all of these capabilities:\\nOPTIC Reporting, Automatic Event\\nCorrelation, Hyperscale\\nObservability, you must enable\\nVertica database parameters.\\nSet to \\nfalse\\n to use external\\nVertica. If you set 'embedded' to\\ntrue, update the parameters\\n‘\\nhost\\n’, ’\\nrwuser\\n’, ‘\\nport\\n’, and\\n‘\\nrouser\\n’. \\nglobal.vertica.host\\nThe FQDN of the Vertica server. If\\nusing a Vertica cluster with 3\\nnodes, enter a comma separated\\nlist of FQDN of all the 3 nodes.\\nglobal.vertica.rwuser\\nDefault value: \\nitom_db_rw\\nDB User with \\nREAD\\n and \\nWRITE\\nPermissions\\nglobal.vertica.rwuserkey\\nDefault value:\\nITOMDI_DBA_PASSWORD_KEY\\nPassword key for '\\nrwuser\\n' \\n[DO\\nNOT CHANGE]\\n.\\nglobal.vertica.rouser\\nDefault value: \\nitom_db_ro\\nDB User with \\nREAD ONLY\\npermission\\nglobal.vertica.rouserkey\\nDefault value:\\nITOMDI_RO_USER_PASSWORD_KEY\\nPassword key for '\\nrouser\\n' \\n[DO\\nNOT CHANGE]\\n.\\nglobal.vertica.db\\nDefault value: \\nitomdb\\nVertica database name\\nglobal.vertica.port\\nDefault value: \\n5433\\nExternal Vertica port for both TLS\\nenabled and non-TLS.\\nglobal.vertica.tlsEnabled\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\ntrue\\nIf you enable TLS, you must give\\nthe Vertica Server Certificate while\\nrunning the suite install.\\nImportant\\n: Embedded Vertica\\nisn't supported in the\\nOperations Bridge suite\\ndeployment on AWS.\\nImportant\\n: There shouldn't\\nbe any space between a\\ncomma separated list of FQDN\\nof the Vertica cluster nodes.\\nContainerized Operations Bridge 2022.11\\nPage \\n267\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '310f8f6228cd8d28fd725b0a4e154fef'}>,\n",
              "  <Document: {'content': \"global.amc.\\nobmHostna\\nme\\nFQDN of the OBM gateway or load balancer.\\nglobal.amc.\\nport\\nDefault value: \\n443\\nThe OBM server port used by components to access OBM and RTSM.\\nglobal.amc.\\nrtsmProtoco\\nl\\nDefault value:\\nhttps\\nThe protocol used by components to access OBM and RTSM.\\nglobal.amc.\\nrtsmUserna\\nme\\nThe username used by components to access OBM's RTSM. Use lowercase to give the 'Agent\\nMetric Collector integration user' that you had created. See \\nCreate an Agent Metric Collector\\nintegration user\\n. \\nglobal.amc.\\ndataBroker\\nNodePort\\nDefault value:\\n1383\\nThe data broker component of the agent metric collector uses the externally accessible port\\nwithin the CDF cluster. The suite uses this port for OBM to agent metric collector communication.\\nIf there is a need to change this port, note that:\\na. You can't use Port 383 as it's reserved within the cluster for different usage.\\nb. You must do a corresponding change on OBM. For more information, see the topic \\nConfigure a\\nsecure connection between DBC and OBM\\n.\\nglobal.amc.\\nserverPort\\nDefault value: \\n383\\nThe BBC port used by the OBM server for incoming connections.\\nThe Agent Metric Collector uses this port to communicate with OBM. The default port used by\\nOBM is 383, therefore this setting should only be changed if the default BBC port has been\\nchanged on the OBM server.\\nglobal.amc.\\nnumOfParall\\nelCollection\\ns\\nDefault value: \\n25\\nThe Agent Metric Collector can connect to this many Operations Agent nodes in parallel during\\nmetric collection.\\nUse one of 5, 10, 20, 25.\\nNote that higher parallel connections would consume more CPU and Memory resources than\\nlower parallel connections.  \\n   \\nglobal.amc.\\nnumOfParall\\nelHistoryCol\\nlections\\nDefault value: \\n10\\nThe Agent Metric Collector can connect to this many Operations Agent nodes in parallel history\\ncollection.\\nUse one of 5, 10, 20, 25.\\nNote that higher parallel connections would consume more CPU and Memory resources than\\nlower parallel connections.\\nglobal.amc.\\nCustomTqls\\nDefault value: null\\nUser-defined Tqls from OBM’s RTSM, used for getting the agent list for metric collection.\\n \\nParameter\\ngroup\\nValues\\nDescription\\nHyperscale Observability\\nNote\\n: Ignore this parameter if the \\nexternalOBM\\n is '\\nfalse\\n'.\\nNote\\n:\\nIf OBM is configured to be accessed as http, set this parameter to 80.\\nIgnore this parameter if the \\nexternalOBM\\n is '\\nfalse\\n'.\\nNote\\n:\\nIf OBM is configured to be accessed http, set this parameter to http.\\nIgnore this parameter if the \\nexternalOBM\\n is '\\nfalse\\n'.\\nNote:\\n Ignore this parameter if the \\nexternalOBM\\n is '\\nfalse\\n'.\\nNote:\\n Ignore this parameter if the \\nexternalOBM\\n is '\\nfalse\\n'.\\nNote:\\n Ignore this parameter if the \\nexternalOBM\\n is '\\nfalse\\n'.\\nContainerized Operations Bridge 2022.11\\nPage \\n270\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c79c169b306c40581fdb886c350bfa7'}>,\n",
              "  <Document: {'content': \"global.database.tlsEnabled\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\ntrue\\nIf TLS is enabled, the \\nPostgreSQLServer\\nCertificate\\n must be provided later while\\nrunning the suite install.\\nglobal.database.oracleConnectionString\\nIf Connection String is provided then the\\nother parameter ('\\noracleSid\\n') isn't used. \\nExample:\\n \\n(DESCRIPTION=(ADDRESS=(PROTOCOL\\n=TCP)(HOST=ora-01.net)(PORT=1521))(CONNECT_D\\nATA=(SID=ORACLEDB)))\\nIn an Oracle RAC environment pass\\nthe Single Client Access Name (\\nSCAN\\n) for\\nHOST parameter as mentioned in\\nthese examples:\\noracleConnectionString\\n:\\n (DESCRIPTION=(ADDRESS\\n=(PROTOCOL=TCP)(HOST=oraclerac-scan.example.\\nnet)(PORT=1521))(CONNECT_DATA=(SID=orcl1))\\noracleConnectionString: (DESCRIPTION=(ADDRESS\\n=(PROTOCOL=TCP)(HOST=oraclerac-scan.example.\\nnet)(PORT=1521))(CONNECT_DATA=(SERVICE_NAM\\nE =orcl.example.net)))\\nglobal.database.oracleSid\\nGive  \\noracleSid\\n.\\nParameter group\\nValues\\nDescription\\nAgent metric collection\\nParameter group\\nValues\\nDescription\\nglobal.isAgentMetricCollectorEnabled\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault\\nvalue: \\ntrue\\nThe OPTIC Reporting capability uses the\\nAgent Metric Collection settings. The Agent\\nMetric Collector can pull metrics from\\nOperations Agents to store in OPTIC Data\\nLake. It queries RTSM for the list of nodes\\nand agent CIs from which it collects data.\\nSet this flag to 'true' to enable Agent Metric\\nCollector.\\nYou can set this flag to 'true' even after\\ninstallation.\\nglobal.autoStartAgentMetricCollector\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\ntrue\\nSet this flag to 'true' to start metric\\ncollection using Agent Metric Collector\\nimmediately after deployment.\\nSet this flag to 'true' if:\\nYou have up to 750 agent nodes in\\nyour environment\\nAll the agents are trusted by your OBM\\nserver and using default\\ncommunications (For example: default\\nBBC port 383, no proxies)\\nYou want to start with the default\\nsettings for metric collections.\\nSet this flag to 'false' if:\\nYou have more than 750 agent nodes\\nin your environment. \\nAgent nodes are using non-default\\nports and proxies\\nYou want to change the default settings\\nfor metric collections.\\nYou can make the changes and then start\\nthe collection manually.\\nAgent metric collection settings\\nParameter\\ngroup\\nValues\\nDescription\\nglobal.amc.\\nexternalOB\\nM\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\nfalse\\nThe location of the OBM server to which the Agent Metric Collector registers itself and from which\\nthe Operations Agent nodes list is retrieved.\\nSet this flag to 'false' if you are using the containerized OBM (OBM capability).\\nContainerized Operations Bridge 2022.11\\nPage \\n269\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6a47e2d41055ea0893f8986335ece581'}>,\n",
              "  <Document: {'content': \"secrets.ucmdb\\n_master_key\\nSets the value for the master key of the UCMDB service that's used to encrypt all UCMDB keys.\\nsecrets.sys_ad\\nmin_password\\nYou can use this Password in combination with the username sysadmin to log in to UCMDB’s JMX\\nconsole.\\nsecrets.UD_US\\nER_PASSWOR\\nD\\nPassword for External Universal Discovery user.The username will be provided in Helm\\nvalues.yaml under global.cms.udUsername\\nsecrets.sched\\nule_mail_pass\\nword_key\\nPassword for scheduling the reports to an email for BVD.\\nsecrets.OPTIC_\\nDATALAKE_INT\\nEGRATION_PAS\\nSWORD\\nIDM Integration User password of the providing deployment when shared optic DL is used.\\nParameter\\ngroup\\nValues\\nDescription\\nIdM settings\\nParameter group\\nValues\\nDescription\\nidm.deployment.database.dbName\\nDefault value: \\nidm\\nGive database name for \\nidm\\n.\\nidm.deployment.database.user\\nDefault value: \\nidm\\nGive the user name for \\nidm \\ndatabase.\\nidm.deployment.database.userPasswordKey\\nDefault value:\\nIDM_DB_USER_PASSWORD_KEY\\nThis is set while running\\ngen_secrets.sh script and refers to the\\npassword for \\nidm \\ndatabase\\n(PostgreSQL/Oracle) user. Oracle\\nuses this as \\nidm\\n schema password.\\nDon't change the \\nuserPasswordKey\\n. \\nAutopass settings\\nParameter group\\nValues\\nDescription\\nautopass.deployment.database.dbName\\nDefault value: \\nautopass\\nGive database name for \\nautopass\\n.\\nautopass.deployment.database.user\\nDefault value: \\nautopass\\nGive the user name for\\nautopass \\ndatabase.\\nautopass.deployment.database.schema\\nDefault value: \\nautopassschema\\nSchema name should be what has\\nbeen set in during the\\nPostgreSQL prepare step for \\nautopas\\nsdb\\n.\\nIf Databases are created using the\\nscript, then don't change the value.\\nautopass.deployment.database.userPasswor\\ndKey\\nDefault value:\\nAUTOPASS_DB_USER_PASSWORD_KEY\\nThis is set while running\\ngen_secrets.sh script and refers to\\nthe password for \\nautopass\\n database\\n(PostgreSQL/Oracle) user. Oracle\\nuses this as \\nautopass \\nschema\\npassword. Don't change\\n userPasswor\\ndKey\\n. \\nBVD settings\\nImportant:\\n If you are using an existing Shared OPTIC Data Lake, and have changed the \\nidm\\n database and\\nuser names based on the deployment scenario, ensure that you provide the correct user and database names in\\nthis section. For example, If you are using an existing Shared OPTIC Data Lake from NOM, and your Operations\\nBridge username and database names are \\nidm-opsb\\n, provide the same in this section.\\n\\ue91b\\n\\ue91b\\nImportant\\n:\\n If you are using an existing Shared OPTIC Data Lake, and have changed the \\nautopass\\n database\\nand user names based on the deployment scenario, ensure that you provide the correct user and database names\\nin this section. For example, If you are using an existing Shared OPTIC Data Lake from NOM, and your OpsB\\nusername and database names are \\nautopass-opsb\\n, provide the same in this section.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n273\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bafa882041fa2b7ed7edb00fc5e6135d'}>,\n",
              "  <Document: {'content': \"Parameter group\\nValues\\nDescription\\nglobal.cms.externalOBM\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nYou must give the values for all\\nthe \\ncms\\n parameters if you have set \\nglob\\nal.amc.externalOBM\\n to \\ntrue\\n to deploy the\\nOperations Bridge suite with Hyperscale\\nObservability to use external OBM.\\nIf you plan to deploy the Operations\\nBridge suite with Hyperscale\\nObservability to use external OBM, set\\nthis parameter to \\ntrue\\n. Make sure that\\nyou set \\nglobal.amc.externalOBM\\nparameter to \\ntrue\\n.\\nglobal.cms.udProtocol\\nDefault value: \\nhttps\\nThe protocol used by components to\\naccess OBM and UCMDB.\\nglobal.cms.udHostname\\nGive the external UCMDB server host\\nname. Ignore this flag if the value for \\nex\\nternalOBM\\n is 'false'.\\nglobal.cms.port\\nDefault value: \\n8443\\nGive the external UCMDB server port.\\nIgnore this flag if the value for \\nexternalO\\nBM\\n is 'false'.\\nglobal.cms.udUsername\\nGive the external UCMDB server details.\\nIgnore this flag if the value for \\nexternalO\\nBM\\n is 'false'.\\ncms.externalOBM.secrets.UISysadmin\\nDefault value:\\nucmdb_uisysadmin_password\\nIncase \\nglobal.cms.externalOBM\\n is set to\\ntrue, set \\nglobal.cms.secrets.UISysadmin\\n to\\nUD_USER_PASSWORD\\nExternal DNS\\nThis parameter is applicable for deployment on AWS.\\nParameter group\\nValues\\nDescription\\nglobal.di.cloud.externalDNS.enabled\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\ntrue\\nIf you plan to deploy the OpsBridge\\nReporting, Hyperscale Observability and/or\\nAEC capabilities, they use OPTIC DL\\ninternally. You must set this parameter if\\nyou want to access OPTIC DL services\\noutside EKS cluster.\\nglobal.di.cloud.externalAccessHost.pulsar\\nDefault value:\\npulsar.opsbridge.example.com\\nIf you plan to deploy the OpsBridge\\nReporting,Hyperscale Observability and/or\\nAEC capabilities, they use OPTIC DL\\ninternally. If you have deployed the cloud\\ninfrastructure manually, you must give the\\nsame Record name while creating the DNS\\nrecord. \\nFor example: If the \\nHosted zone\\n is \\nopsb.ito\\nmbyok.com\\n and \\nEXTERNAL_ACCESS_HOST\\n is \\ndemo.opsb.itombyok.com\\n, then the \\nRecord\\nname\\n while creating DNS record will be \\npul\\nsar.opsb.itombyok.com\\n.\\nAzure load balancer\\nThis parameter is applicable for deployment on Azure.\\nParameter group\\nValues\\nDescription\\nglobal.expose.internalLoadBalancer.ip\\nType the Private IP for Azure Load Balancer\\nconfiguration.\\nNote\\n:\\nIf OBM is configured to be\\naccessed as HTTP, set this\\nparameter to \\nhttp\\n.\\nIgnore this flag if the value for \\nexternalOBM\\n is 'false'.\\nContainerized Operations Bridge 2022.11\\nPage \\n271\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e533353d3d49f6389f808de28e991896'}>,\n",
              "  <Document: {'content': 'global.loadBalancer.ip\\n-\\nType the Private IP for Azure Load Balancer\\nconfiguration.\\nIf you are deploying OBM capability on\\nAzure, you must add this parameter also.\\nParameter group\\nValues\\nDescription\\nSecrets\\nYou must provide all the required secrets password in \\nBase64 encoded \\nformat.\\nParameter\\ngroup\\nValues\\nDescription\\nsecrets.idm_o\\npsbridge_admi\\nn_password\\nAdmin Password for IDM admin user. This password will be used to log into IDM UI.\\nThe password must meet the following requirements:\\nLength must be at least 8 characters\\nLength cannot exceed 64 characters\\nMust contain 1 or more upper case characters\\nMust contain 1 or more lower case characters\\nMust contain 1 or more digit (0-9) characters\\nMust contain 1 or more special characters in: -+\"?/.,<>:;[]{}`~!@#%^&*()_=|$\\nsecrets.ITOMD\\nI_DBA_PASSW\\nORD_KEY\\nPassword of the Vertica database user with read and write permission.\\nsecrets.ITOMD\\nI_RO_USER_PA\\nSSWORD_KEY\\nPassword of the Vertica database user with read-only permission.\\nsecrets.AUTOP\\nASS_DB_USER_\\nPASSWORD_KE\\nY\\nPassword for \\nAUTOPASS\\n database (PostgreSQL) user. The application will give the username in\\nHelm \\nvalues.yaml\\n under \\ndeployment.database.user\\n of \\nAUTOPASS\\n.\\nsecrets.BVD_D\\nB_USER_PASS\\nWORD_KEY\\nPassword for BVD database (PostgreSQL) user. The application will give the username in Helm \\nval\\nues.yaml\\n under \\ndeployment.database.user\\n of BVD.\\nsecrets.CM_DB\\n_PASSWD_KEY\\nPassword for PostgreSQL database for credentials manager.\\nsecrets.IDM_D\\nB_USER_PASS\\nWORD_KEY\\nPassword for IdM database (PostgreSQL) user. The application will give the username in Helm \\nvalu\\nes.yaml\\n under \\ndeployment.database.user\\n of IdM.\\nsecrets.MA_DB\\n_USER_PASSW\\nORD_KEY\\nPassword for monitoring admin database (PostgreSQL) user. The username is in \\nvalues.yaml\\n under\\ndeployment.database.user\\n of monitoring admin.\\nsecrets.SNF_D\\nB_USER_PASS\\nWORD_KEY\\nPassword for temporary storage PostgreSQL database for Hyperscale Observability.\\nsecrets.OBM_\\nMGMT_DB_USE\\nR_PASSWORD_\\nKEY\\nPassword for \\nMGMT\\n database (PostgreSQL) user. The application will give the username in Helm \\nv\\nalues.yaml\\n under \\ndeployment.mgmtDatabase.user\\n of OBM.\\nsecrets.OBM_E\\nVENT_DB_USE\\nR_PASSWORD_\\nKEY\\nPassword for \\nEVENT\\n database (PostgreSQL) user. The application will give the username in Helm \\nv\\nalues.yaml\\n under \\ndeployment.eventDatabase.user \\nof OBM.\\nsecrets.RTSM_\\nDB_USER_PASS\\nWORD_KEY\\nPassword for RTSM database (PostgreSQL) user. The application will give the username in Helm \\nv\\nalues.yaml\\n under \\ndeployment.database.user\\n of UCMDB.\\nsecrets.BTCD_\\nDB_PASSWD_K\\nEY\\nPassword for NOM metric transformation and Hyperscale Observability.\\nsecrets.OBM_R\\nTSM_PASSWOR\\nD\\nPassword for External OBM RTSM users. It\\'s the password that you set for the \\'Agent Metric\\nCollector integration user\\' (See \\nCreate an Agent Metric Collector integration user\\n). If you didn\\'t\\ncreate this user, press enter. The v generates a random password. The application will give the\\nusername later in \\nvalues.yaml \\nfile under \\nglobal.amc.rtsmUsername \\nNote:\\n You need not set this password if you are using containerized OBM (OBM capability).\\nsecrets.OBM_\\nUSER_PASSWO\\nRD_KEY\\nPassword for External OBMs user with Content Packs upload permissions. The username will be\\nprovided in Helm values.yaml under global.monitoringService.obmUsername\\nContainerized Operations Bridge 2022.11\\nPage \\n272\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f5e817ceff735c7ac436605f28471abe'}>,\n",
              "  <Document: {'content': \"Parameter group\\nValues\\nDescription\\nbvd.deployment.database.dbName\\nDefault value: \\nbvd\\nGive \\nbvd \\ndatabase name.\\nbvd.deployment.database.user\\nDefault value: \\nbvd\\nGive the user name for \\nbvd\\n database.\\nbvd.deployment.database.userPasswordKey\\nDefault value:\\nBVD_DB_USER_PASSWORD_KEY\\nThis is set while running \\ngen_secrets.sh\\nscript and refers to the password for \\nb\\nvd\\n database (PostgreSQL/Oracle) user.\\nOracle uses this as \\nbvd\\n schema\\npassword. Don't change\\n userPasswordKe\\ny\\n. \\nbvd.smtpServer.host\\nYou must give these parameters\\nto schedule the reports to a specified\\nemail id. Give the \\nsmtpServer\\n host\\nname.\\nbvd.smtpServer.port\\nGive the \\nsmtpServer\\n port.\\nbvd.smtpServer.security\\nGive the value as \\nTLS\\n or \\nSTARTTLS\\n.\\nbvd.smtpServer.user\\nGive the user id for the \\nsmtpServer\\n.\\nbvd.smtpServer.from\\nGive the email id for the \\nsmtpServer use\\nd \\nto send the emails\\nDepending upon the SMTP server\\n'from' and 'user' fields can be the\\nsame or different. \\nbvd.smtpServer.passwordKey\\nDefault value:\\nschedule_mail_password_key\\nThis is set while running \\ngen_secrets.sh\\nscript and refers to the password for \\ns\\nmtpServer\\n (PostgreSQL/Oracle) user.\\nDon't change \\npasswordKey\\n. This is the\\npassword key for the mail Proxy user\\noracleWallet\\nPass the oracle wallet zip's base64\\nfile \\nOBM settings\\nParameter group\\nValues\\nDescription\\nobm.deployment.eventDatabase.dbNa\\nme\\nDefault value: \\nobm_event\\nGive PostgreSQL database name\\nfor the event database.\\nobm.deployment.mgmtDatabase.dbNa\\nme\\nDefault value: \\nobm_mgmt\\nGive PostgreSQL database name\\nfor the management (\\nmgmt\\n)\\ndatabase.\\nobm.deployment.eventDatabase.user\\nDefault value: \\nobm_event\\nGive Oracle user name for the\\nevent database.\\nobm.deployment.mgmtDatabase.user\\nDefault value: \\nobm_mgmt\\nGive Oracle user name for the\\nmanagement database.\\nobm.deployment.mgmtDatabase.userP\\nasswordKey\\nDefault value:\\nOBM_MGMT_DB_USER_PASSWORD_KEY\\nThis is set while running\\ngen_secrets.sh script and refers\\nto the password for \\nmgmt\\ndatabase (PostgreSQL/Oracle)\\nuser. Oracle uses this as  \\nmgmt\\nschema password. Don't change\\nuserPasswordKey\\n. \\nobm.deployment.eventDatabase.userP\\nasswordKey\\nDefault\\nvalue: \\nOBM_EVENT_DB_USER_PASSWORD_KEY\\nThis is set while running\\ngen_secrets.sh script and refers\\nto the password for \\nevent\\ndatabase (PostgreSQL/Oracle)\\nuser. Oracle uses this as\\nevent\\n schema password. Don't\\nchange\\n userPasswordKey\\n. \\nobm.bbc.port\\nDefault value: \\n383\\nThe BBC port used by the OBM\\nserver.\\nImportant\\n:\\n If you are using an existing Shared OPTIC Data Lake, and have changed the \\nbvd\\n database and\\nuser names based on the deployment scenario, ensure that you provide the correct user and database names in\\nthis section. For example, If you are using an existing Shared OPTIC Data Lake from NOM, and your Operations\\nBridge username and database names are  \\nbvd-opsb\\n, provide the same in this section.\\nIf you are planning to Configure Email for Dashboard and Report Schedules using SMTP, \\nbvd.smtpServer.*\\nparameters mentioned in the below table are \\nmandatory\\n.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n274\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fb142c529a18280c9849e337d0e7fb3f'}>,\n",
              "  <Document: {'content': 'Parameter group\\nValues\\nDescription\\nitomdipulsar.bookkeeper.volumes.ledgers.local_storage\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault\\nvalue: \\nfalse\\nThis applies if you have selected the OPTIC\\nReporting or Automatic Event Correlation\\ncapabilities. The OPTIC DL Message Bus\\nuses Local Persistent Volumes on the\\nworker nodes. The OPTIC DL Message Bus\\ncomponents \\n(ledger, journal, \\nand\\n zookeeper\\n)\\nas calculated based on your expected\\nOPTIC DL ingestion load in the sizing\\ncalculator. \\nDuring suite deployment, Persistent Volume\\nClaims are created automatically from\\nthese Persistent Volumes based on the\\nminimum disk space requirements defined\\nin your \\nvalues.yaml\\n. \\nYou must edit the \\nvalues.yaml \\nfile\\nbefore the helm install to be equal to\\nor less than the capacity of your LPVs.\\nitomdipulsar.bookkeeper.volumes.ledgers.storageClassName\\nDefault value: For\\nAWS - \\ngp3\\nFor Azure\\n- \\n\"managed-\\npremium\"\\nLocal volume storage class name\\nitomdipulsar.bookkeeper.volumes.ledgers.size\\nDefault value:\\n100Gi\\nLocal volume size\\nitomdipulsar.bookkeeper.volumes.journal.local_storage\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault\\nvalue: \\nfalse\\nitomdipulsar.bookkeeper.volumes.journal.storageClassName\\nDefault value: For\\nAWS - \\nio2\\nFor Azure\\n- \\n\"managed-\\npremium\"\\nitomdipulsar.bookkeeper.volumes.volumes:journal.size\\nDefault value:\\n50Gi\\nitomdipulsar.bookkeeper.volumes.configData.journalSyncData\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value: \\ntrue\\nzookeeper.volumes.data.local_storage\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault\\nvalue: \\nfalse\\nzookeeper.volumes.data.storageClassName\\nDefault value: For\\nAWS - \\ngp3\\nFor Azure\\n- \\n\"managed-\\npremium\"\\nzookeeper.volumes.data.size\\nDefault value:\\n50Gi\\nzookeeper.volumes.dataLog.local_storage\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault\\nvalue: \\nfalse\\nzookeeper.volumes.dataLog.size\\nDefault value:\\n10Gi\\nzookeeper.volumes.dataLog.storageClassName\\nDefault value: For\\nAWS - \\ngp3\\nFor Azure\\n- \\n\"managed-\\npremium\"\\nCredential manager settings\\nParameter group\\nValues\\nDescription\\ncredentialmanager.deployment.database.dbName\\nDefault value:\\ncredentialmanager\\nGive the \\ncredentialmanager \\ndatabase\\nname.\\ncredentialmanager.deployment.database.user\\nDefault value:\\ncredentialmanageruser\\nGive the \\ncredentialmanager \\ndatabase user\\nname.\\nContainerized Operations Bridge 2022.11\\nPage \\n276\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '60206b8859e2e8c8f5a912c8dd9025e5'}>,\n",
              "  <Document: {'content': 'obm.omi.storageClassName\\nDefault value: For AWS - \"io2\", for Azure -\\n\"managed-premium\" , For RedHat OpenShift -\\n\"ocs-storagecluster-ceph-rbd\"\\n \\nParameter to configure the\\nstorage class for the PVCs in the\\nomi-0 or omi-1 pods template.  If\\nyou do not give any storage\\nclass name, the PVCs in the omi-\\n0/1 template will use the default\\nstorage class.\\nobm.params.haEnabled\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\ntrue\\nEnable \\nomi-1\\n as a HA failover\\npod for \\nomi-0\\n.\\nOBM comes with HA enabled for\\nmedium and large deployments.\\nTo Disable HA, set the \\nobm.param\\ns.haEnabled\\n to \\nfalse\\n and \\nucmdbser\\nver.deployment.replicaCount\\n to 1.\\nobm.params.managementPacks.ADMP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy AD Management Pack\\nobm.params.managementPacks.Apach\\neMP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy Apache Management\\nPack\\nobm.params.managementPacks.Excha\\nngeMP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy Exchange Management\\nPack\\nobm.params.managementPacks.HANA\\nMP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy HANA Management Pack\\nobm.params.managementPacks.InfraM\\nP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\ntrue\\nDeploy Infra Management Pack\\nobm.params.managementPacks.MSSQ\\nLMP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy MSSQL Management\\nPack\\nobm.params.managementPacks.OraM\\nP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy Ora Management Pack\\nobm.params.managementPacks.SAPM\\nP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy SAP Management Pack\\nobm.params.managementPacks.SAPSy\\nbaseASEMP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy SAP Sybase ASE\\nManagement Pack\\nobm.params.managementPacks.WbsM\\nP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy Wbs Management Pack\\nobm.params.managementPacks.WebL\\nogicMP\\nPossible values: \\ntrue\\n/\\nfalse\\nDefault value: \\nfalse\\nDeploy WebLogic Management\\nPack\\nParameter group\\nValues\\nDescription\\nRTSM settings \\nParameter group\\nValues\\nDescription\\nucmdbserver.deployment.replicaCount\\nDefault value: \\n2\\nGive the number of replicas of \\nitom-uc\\nmdb\\n pods\\nucmdbserver.deployment.database.dbName\\nDefault value: \\nrtsm\\nGive Postgres database name for the \\nrtsm\\n database.\\nucmdbserver.deployment.database.user\\nDefault value: \\nrtsm\\nGive database user for the \\nrtsm\\ndatabase\\nucmdbserver.deployment.database.userPassword\\nKey\\nDefault value:\\nRTSM_DB_USER_PASSWORD_KEY\\nThis is set while running\\ngen_secrets.sh script and refers to\\nthe password for \\nrtsm \\ndatabase\\n(PostgreSQL/Oracle) user. Oracle uses\\nthis as \\nrtsm \\nschema password. Don\\'t\\nchange\\n userPasswordKey\\n. \\nOPTIC Data Lake Message Bus \\nNote\\n: The enabled\\nmanagement packs will be\\ndeployed after the next \\nomi\\npod restart. If you disable\\nthem later, they won\\'t be\\nuninstalled.\\nContainerized Operations Bridge 2022.11\\nPage \\n275\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '837ff70e00852a2227c834dff958debb'}>,\n",
              "  <Document: {'content': 'cmsgateway.deployment.ucmdb.protocol\\nDefault\\nvalue:\\n\"https\"\\nYou must give the values for all the \\ncmsgatewa\\ny\\n parameters if you have set \\nglobal.cms.externa\\nlOBM\\n to \\ntrue\\n to deploy the Operations Bridge\\nsuite with Hyperscale Observability to use\\nexternal OBM.\\nConfiguration parameters required by\\nCMS gateway for external OBM (UCMDB\\nserver) integration\\ncmsgateway.deployment.ucmdb.host\\nDefault\\nvalue: \\nitom-\\nucmdb-svc\\nGive the external UCMDB server host name.\\ncmsgateway.deployment.ucmdb.port\\nDefault\\nvalue: \\n8443\\nGive the external UCMDB Server port.\\ncmsgateway.deployment.ucmdb.username\\nDefault\\nvalue:\\nUISysadmin\\nGive the external OBM RTSM user name.\\nParameter group\\nValues\\nDescription\\nMonitoring service data broker\\nParameter group\\nValues\\nDescription\\nitomopsbridgedatabroker.deployment.config.obmDataCollectorProtoc\\nol\\nDefault\\nvalue:\\nhttps\\nYou must give the values for all the \\nitomopsbrid\\ngedatabroker\\n parameters if you have set \\nglobal.c\\nms.externalOBM\\n to \\ntrue\\n to deploy the Operations\\nBridge suite with Hyperscale Observability to\\nuse external OBM. Give the values to these\\nparameters if the Data Collector URL is\\ndifferent from the OBM user URL.\\nGive the protocol used by the Data broker to\\nconnect to the external OBM server.\\nitomopsbridgedatabroker.deployment.config.obmDataCollectorHostn\\name\\nGive the host name used by the Data broker to\\nconnect to the external OBM server.\\nitomopsbridgedatabroker.deployment.config.obmDataCollectorPort\\nDefault\\nvalue:\\n383\\nThe port used by the Data broker to connect to\\nthe external OBM server.\\nNOM metric transformation\\nApplicable only if you want to create database for NOM integration.\\nParameter group\\nValues\\nDescription\\nnommetricstransform.deployment.database.dbName\\nDefault value: \\nbtcd\\nGive the PostgreSQL database name for\\nNOM metric transform.\\nnommetricstransform.deployment.database.user\\nDefault value: \\nbtcd\\nGive the PostgreSQL database user\\nname for NOM metric transform.\\nnommetricstransform.deployment.database.userPassword\\nKey\\nDefault value:\\nBTCD_DB_PASSWD_KEY\\nGive the PostgreSQL database user\\npassword for NOM metric transform.\\nAnamoly detection\\nParameter group\\nValues\\nDescription\\nitom-oba-config.deployment.oba.protocol\\nDefault\\nvalue:\\nhttps\\nitom-oba-config.deployment.oba.host\\nGive the host name of the Operations Bridge\\nAnalytics application server.\\nImportant:\\n If you are using an existing Shared OPTIC Data Lake, and have changed the \\nbtcd\\n database and\\nuser names based on the deployment scenario, ensure that you provide the correct user and database names in\\nthis section. For example, If you are using an existing Shared OPTIC Data Lake from NOM, and your Operations\\nBridge username and database names are \\nbtcd-opsb,\\n provide the same in this section.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n278\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3eb6726da1d18bef360e1ec1ad1e718c'}>,\n",
              "  <Document: {'content': \"credentialmanager.deployment.database.userPasswordKe\\ny\\nDefault value:\\nCM_DB_PASSWD_KEY\\nThis is set while running \\ngen_secrets.sh\\nscript and refers to the password for \\ncre\\ndentialmanager\\n database (PostgreSQL)\\nuser. Don't change\\n userPasswordKey\\n.\\nParameter group\\nValues\\nDescription\\nMonitoring administrator settings\\nParameter group\\nValues\\nDescription\\nitomopsbridgemonitoringadmin.deployment.databas\\ne.dbName\\nDefault value:\\nmonitoringadmindb\\nGive the \\nmonitoringadmindb\\n database\\nname.\\nitomopsbridgemonitoringadmin.deployment.databas\\ne.user\\nDefault value:\\nmonitoringadminuser\\nGive the \\nmonitoringadmindb\\n database\\nuser name.\\nitomopsbridgemonitoringadmin.deployment.databas\\ne.userPasswordKey\\nDefault value:\\nMA_DB_USER_PASSWORD_KEY\\nThis is set while running \\ngen_secrets.s\\nh\\n script and refers to the password\\nfor \\nmonitoringadmin\\n database\\n(PostgreSQL) user. Don't change\\n user\\nPasswordKey\\n. \\nStore and forward database settings\\nParameter group\\nValues\\nDescription\\nitommonitoringsnf.deployment.database.dbName\\nDefault value: \\nmonitoringsnfdb\\nGive the \\nmonitoringsnfdb\\n database\\nname.\\nitommonitoringsnf.deployment.database.user\\nDefault value:\\nmonitoringsnfdbuser\\nGive the \\nmonitoringsnfdb\\n database\\nuser name.\\nitommonitoringsnf.deployment.database.userPassw\\nordKey\\nDefault value:\\nSNF_DB_USER_PASSWORD_KEY\\nThis is set while running \\ngen_secrets.s\\nh\\n script and refers to the password\\nfor \\nmonitoringsnfdb \\ndatabase\\n(PostgreSQL) user. Don't change\\n user\\nPasswordKey\\n. \\nUCMDB probe settings\\nParameter group\\nValues\\nDescription\\nucmdbprobe.deployment.type\\nDefault\\nvalue:\\nembedded\\nYou must give the values for all the \\nucmdbprob\\ne\\n parameters if you have set \\nglobal.cms.external\\nOBM\\n to \\ntrue\\n to deploy the Operations Bridge\\nsuite with Hyperscale Observability to use\\nexternal OBM.\\nConfiguration parameter for UCMDB probe\\nintegration with external OBM Server (UCMDB\\nServer)\\nucmdbprobe.deployment.ucmdbServer.hostName\\nDefault\\nvalue:\\nitom-\\nucmdb-\\nwritersvc\\nGive the external UCMDB server host name.\\nucmdbprobe.deployment.ucmdbServer.port\\nDefault\\nvalue:\\n8443\\nGive the external UCMDB Server port.\\nucmdbprobe.deployment.ucmdbServer.probeSSLFullValidation\\nDefault\\nvalue: \\n1\\nGive any of the following validation levels for\\nthe UCMDB Probe certificate:\\n  \\n0\\n: Full validation\\n  \\n1\\n: Full validation without revocation check\\n  \\n2\\n: Basic validation\\nCMS gateway for external OBM\\nParameter group\\nValues\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n277\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6aac5df4a94b120d312e6f688b227cc7'}>,\n",
              "  <Document: {'content': 'kubectl get secret opsbridge-suite-secret -n opsb-helm -o yaml | grep ucmdb_master_key\\nucmdb_master_key: US9rNV8xYXI1RGh5Qm84Tl0uay5dTXhlZndhLkdlXWQ=\\n        f:ucmdb_master_key: {}\\nContainerized Operations Bridge 2022.11\\nPage \\n281\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8a850750c495b98df39ebe7db128ef3e'}>,\n",
              "  <Document: {'content': 'itom-oba-config.deployment.oba.configParameterServicePort\\nDefault\\nvalue:\\n9090\\nGive the port.\\nParameter group\\nValues\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n279\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e667723c9ca5244bba42b0f95579a6c1'}>,\n",
              "  <Document: {'content': 'Deploy using CLI\\nYou can deploy the Operations Bridge in the following different deployment sizes:\\nMedium\\nLarge \\nExtra Large\\nLow Footprint \\nTo deploy the Operations Bridge, run the following command on the control plane or installer, or bastion node:\\nhelm install <helm deployment name> -n <application namespace> -f <values.yaml> <chart> [--set-file \"caCertificates.vertica-ca\\\\.crt\"=<vertica ce\\nrtificate file> ] [--set-file \"caCertificates.oracle\\\\.crt\"=<relational database certificate file>  [--set-file oracleWallet=<base64\\n \\nencoded wallet text file\\n> ] [--set-file \"caCertificates.postgres\\\\.crt\"=<relational database certificate file>]\\n \\n] \\n \\n[\\n \\n--set-file \"caCertificates.obm-ca\\\\.crt\"=<obm crt file> ]\\n \\n[--set\\n-file \"caCertificates.sitescope-ca\\\\.crt\"= <sitescope crt file> ] [--set-file \"caCertificates.sharedoptic-ca\\\\.crt\"=ProvidingAppCert.pem] [--set global.setFq\\ndnInIngress=false] [-f <deployment.yaml>\\n ]\\n -f <secrets.yaml>\\nWhere:\\n<helm deployment name>\\n: Deployment name you want to create. \\n<application namespace>\\n: The namespace where the application will get deployed. \\n<values.yaml>\\n: Give the full path to the \\nvalues.yaml\\n file. \\n<deployment.yaml>\\n: Give the full path to the \\ndeployment.yaml\\n files. Depending on the size of deployment that you choose\\naccording to the sizing calculator, each deployment type has a corresponding deployment file (\\nMedium-\\nDeployment.yaml\\n, \\nLarge-Deployment.yaml, \\nExtra-Large-Deployment.yaml\\n) in the path where you have\\nunzipped \\nopsbridge-suite-chart-2022.05.zip\\n, under \\ndeployment\\n directory. If you don\\'t specify a deployment size YAML,\\nby default medium size will get deployed. \\n<vertica certificate file>\\n: Give the full path to the \\nvertica-ca.pem\\n file. If an intermediate CA has issued the Vertica server\\ncertificate, make sure that \\nvertica-ca.pem\\n includes the intermediate CA certificate chain, starting with the certificate of the\\nCA that issued the Vertica server certificate, followed by other intermediate certificates (if any) and with the root CA\\ncertificate at the end. You must follow this sequence. TLS setup will fail if you use the opposite sequence. \\n<chart>:\\n The absolute path to the chart package. Example: \\nopsbridge-suite-2022.05.0.tgz\\n. Chart file is available\\nunder \\ncharts\\n directory.\\n<relational database certificate file>\\n: Give the full path to the Oracle or PostgreSQL database TLS certificate file. \\n<base64 encoded wallet text file>\\n: Give the full path to the \\nbase64 \\nencoded wallet text file. This file has the base64 value of\\nthe \\nwallet.zip\\n file. Give this when the application uses External Oracle and you are using OPTIC Reporting or Stakeholder\\nDashboard.\\n<obm crt file>\\n: This is applicable only if you are\\n deploying Hyperscale Observability capability with external\\nOBM\\n. Give the full path of the OBM CA certificate. See \\nObtain OBM CA certificate\\n for details. \\n<sitescope crt file>\\n: This is applicable if you are deploying Agentless Monitoring capability with SSL enabled SiteScope. Full\\npath to the CA or Self signed certificate file. The certificate can be in \\n.crt\\n or \\n.pem\\n format. For steps to create SiteScope\\ncertificates, see \\nCreate SiteScope certificates\\n.\\nProvidingAppCert.pem\\n: This is applicable only if you are using Shared OPTIC Data Lake from another deployment. This is the\\ncertificate file extracted from the providing deployment. See \\nDeploy\\n.\\nsecrets.yaml\\n: Use this option for secrets created using the \\ngen_secrets\\n script. This file contains all the secrets.\\nExample:\\nhelm install deployment01 -n opsb-helm -f values.yaml opsbridge-suite-2022.05.x.tgz --set-file \"caCertificates.vertica-ca\\\\.crt\"=/dbStuffs/ca.pem  --set-file oracleWallet=/root/helm/oracle_wallet_base64.txt --set-file \"caCertificates.oracle\\\\.crt\"=/dbStuffs/simple-certificate.crt -f mysecretsfile.yaml\\nhelm install deployment01 -n opsb-helm -f values.yaml opsbridge-suite-2022.05.x.tgz --set-file \"caCertificates.vertica-ca\\\\.crt\"=/dbStuffs/ca.pem --set-file \"caCertificates.postgres\\\\.crt\"=/dbStuffs/server.crt -f /pub/opsbridge-suite-chart/deployment/Large-Deployment.yaml \"caCertificates.extucmdb\\\\.crt\"=./ext_obm.crt  --set-file \"caCertificates.sitescope-ca\\\\.crt\"=MyCA.crt   -f mysecretsfile.yaml\\nImportant:\\n If you plan for Shared OPTIC Data Lake deployment where Operations Bridge is the provider\\napplication and use the FQDN communication mechanism from the consumer application, while you deploy\\nOperations Bridge as a provider, you must use the  \\n--set global.setFqdnInIngress=false\\n parameter in the install\\ncommand.\\n\\ue91b\\n\\ue91b\\nNote: \\nOperations Bridge deployment on Azure with Oracle database isn\\'t supported. \\nOperations Bridge deployment on AWS with Oracle database isn\\'t supported if you\\'ve used the ITOM\\nCloud Deployment toolkit to set up AWS infrastructure. However, for AWS infrastructure that\\'s set up\\nmanually, you can deploy Operations Bridge with Oracle database without TLS. \\nTip\\n: When you uninstall the application using \\nhelm uninstall\\n command, it removes the passwords along with the\\nsecret and you won\\'t be able to use the database again. If you plan to use the same database, you must take a\\nbackup of the \\nucmdb_master_key\\n. Run the following command after the application install completes:\\nkubectl get secret opsbridge-suite-secret -n <suite namespace> -o yaml | grep <database key>\\nExample:\\nContainerized Operations Bridge 2022.11\\nPage \\n280\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ee7be183edc2eb84c9db86fcb1c5d026'}>,\n",
              "  <Document: {'content': 'Download and upload application images to ECR\\ncontainer registry\\nPrepare the container image lists\\nBased on the downloaded installation packages, run the following commands to extract the JSON lists:\\nunzip -j <PATH_TO_SUITE_ZIP> \\'opsbridge-suite-chart/charts/*.tgz\\' -d /tmp\\ntar -xf /tmp/opsbridge-suite-*.tgz -C /tmp/ --strip-components=1 \\'opsbridge-suite/_image-set.json\\'\\nCreate ECR repositories\\n1\\n. \\nTo create the ECR repository using the given script you require Python 3. Run the following command to install Python 3\\non the system with AWS CLI.\\nsudo yum install -y python3\\n2\\n. \\nThe script to create the repository is in \\n<unzipped opsbridge-suite-chart>/scripts/byok\\n folder. Run the following commands to\\ncreate the ECR repository:\\npython3 create_aws_repositories.py -r <aws_region> -o <orgname> -i /tmp/_image-set.json \\n<ecr_region>\\n is the region where you will put your ECR repositories.\\nDownload and upload images\\nRun the following commands on the system with AWS CLI:\\nregion=<ecr_region>\\necrURL=`aws ecr get-authorization-token --region $region --query=\"authorizationData[0].proxyEndpoint\"| grep -oE \"[0-9]+[^\\\\\"]*\"`\\necrUserName=AWS\\necrUserPassword=`aws --region $region ecr get-login-password`\\npython3 image-transfer.py -su <source-username> -sp <source-password> -so <source-orgname> -sr <source-registry> -p /tmp/_image-set.json -tu $ecrUserName -tp $ecrUserPassword -to <target-orgname> -tr $ecrURL\\nThe \\nimage-transfer.py\\n is in \\n<unzipped opsbridge-suite-chart>/scripts/byok\\n folder.\\nIn these commands:\\n<ecr_region>\\n is the region where you will put your ECR repositories.\\n<source-username>\\n is the user name of the source docker registry. Get it from Micro Focus if you use Docker Hub, or\\ncontact the admin of the registry.\\n<source-password>\\n is the password of the source docker registry. Get it from Micro Focus if you use Docker Hub, or contact\\nthe admin of the registry.\\n<source-registry>\\n is the URL of the source registry. For example, \\nregistry.hub.docker.com\\n.\\n<source-orgname>\\n is the organization name in the source docker registry. It\\'s \\nhpeswitom\\n if you use Docker Hub, or contact\\nthe admin of the registry.\\n<target-orgname>\\n is the organization name in the target docker registry. You can get it from the admin of the registry.\\nTip\\n: If you get an upload error while running the \\nimage-transfer.py\\n script, see the \\nImage upload error while image\\ndownload and upload\\n topic to resolve the issue.\\nContainerized Operations Bridge 2022.11\\nPage \\n282\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '722f27d521ae56df334dfb378e5386f4'}>,\n",
              "  <Document: {'content': 'Create listeners and target ports for application\\nListener\\nPort\\nTarget\\nPort\\nProtocol\\nComponent\\nService name\\n443\\n30443\\nTLS\\nTLS Port- Operations Bridge Ingress Controller - Target port is\\nconfigurable in \\nglobal.nginx.httpsPort\\nitom-ingress-controller-\\nsvc-internal\\n383\\n383\\nTCP\\nOMI-BBC\\n - Target Port configurable using \\nomi.bbc.ports\\nomi-bbc\\n30001\\n5050\\nTCP\\nOPTIC DL Receiver\\nitom-di-receiver-svc\\n30003\\n28443\\nTCP\\nOPTIC DL Data Access\\nitom-di-data-access-\\nsvc\\n30004\\n18443\\nTCP\\nOPTIC DL Administration\\nitom-di-administration-s\\nvc\\n31051\\n6651\\nTCP\\nOPTIC DL Message Bus\\nitomdipulsar-proxy\\nPerform the following steps to configure the target groups and listeners for the application namespace. You can list all the load\\nbalancers in the namespace using the command: \\nkubectl get svc -n <namespace> | grep -i loadbalancer\\nYou must perform these steps for all the load balancers:\\n1\\n. \\nRun the following command to create a Target group for Port:\\nTG_ARN_443=$(aws elbv2 create-target-group --name <Target_Group_Name> --protocol TLS --port <Target_Port_Number> --target-type ip --vpc-id <VPC_ID> --query \"TargetGroups[].TargetGroupArn\" --output text)\\n where, the \\nTarget_Port_Number\\n for which you will create the Target group is available in the table. The \\nVPC_ID\\n is the ID you\\nnoted down from the AWS console.\\n2\\n. \\nRun the following command to get the load balancer with the service for which the target group is created: \\nloadBalancer_443=$(kubectl get svc <SERVICE_NAME> -n <NAMESPACE> | grep -v EXTERNAL-IP | awk \\'{print $4}\\')\\nYou can refer to the table in this topic for port information. \\n3\\n. \\n Run the following command to associate target with the load balancer:\\nNetInt_443=$(aws elbv2 describe-load-balancers --query \"LoadBalancers[?DNSName==\\\\`$loadBalancer_443\\\\`].LoadBalancerName\" --output text)\\nPPIPs_443=$(aws ec2 describe-network-interfaces --filters \"Name=vpc-id,Values=<VPC_ID>\" \"Name=description,Values=*${NetInt_443}*\" --query \"NetworkInterfaces[].PrivateIpAddresses[?Primary==\\\\`true\\\\`].PrivateIpAddress\" --output text)\\n4\\n. \\nRun the following command to format the target:\\n  Targets_443=\"\"\\n      for ip_443 in $PPIPs_443; do\\n      Targets_443=$Targets_443\" Id=$ip_443\"\\n  done\\n5\\n. \\nRun the following command to register Target to the Target group created in step 1:\\naws elbv2 register-targets --target-group-arn $TG_ARN_443 --targets $Targets_443 \\n6\\n. \\nFrom the \\nvalues.yaml\\n, get the Certificate Domain Name for the FQDN given as external Access host name.\\n7\\n. \\nRun the following command to get the \\nCertificate ARN\\n:\\nCert_ARN=$(aws acm list-certificates --query \"CertificateSummaryList[?DomainName==\\\\`<CERTIFICATE_DOMAIN_NAME>\\\\`].CertificateArn\" --output text)\\n You may also get the \\nCertificate ARN\\n from the \\nAWS Console\\n > \\nACM\\n.\\n8\\n. \\nRun the following command to create a listener for the network load balancer: \\naws elbv2 create-listener --load-balancer-arn $NLB_ARN --protocol TLS --port <Listener_port_number> --ssl-policy ELBSecurityPolicy-2016-08 --certificates CertificateArn=${Cert_ARN} --default-actions Type=forward,TargetGroupArn=${TG_ARN_443}\\nYou created the \\nNLB_ARN\\n \\nhere\\n.\\nUpdate record\\nIf you plan to use the OPTIC Reporting or AEC capabilities, you must create and update the DNS record for the OPTIC DL\\nMessage Bus service:\\n1\\n. \\nTo create a DNS record, go to \\nAWS Console\\n > \\nRoute 53\\n > \\nHosted Zone\\n and click on your hosted zone. For\\ninformation to create a private hosted zone, see \\nAmazon documentation\\n.\\n2\\n. \\nClick \\nCreate Record\\n. The DNS record name must match with the \\nexternalAccessHost.pulsar\\n value given in \\nvalues.yaml\\n.\\nTip\\n: While creating a listener with TCP, you need not give the \\n--ssl-policy\\n and \\n--certificates\\n parameters.\\nContainerized Operations Bridge 2022.11\\nPage \\n284\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a27c580ad670688091b655cbbd1f4eb1'}>,\n",
              "  <Document: {'content': \"Download and upload suite images to ACR container\\nregistry\\nDownload and upload the images\\nCheck the path of the following before uploading images to ACR:\\nimage-transfer.py\\n: The \\nbyok\\n folder of the application installer package contains this script. For example:  \\n<extracted installer p\\nackage>/scripts/byok/image-transfer.py.\\n_image-set.json\\n: The application installer package contains this file. For example: \\n<extracted\\ninstaller package>/charts/<extracted package.tgz>/_image-set.json.\\nTo download and upload the generated images, follow the steps below:\\n1\\n. \\nRun the following commands to transfer the application images from the source registry to your registry:\\ncd path of opsb-suite-metadata-202x.xx-xxx.tgz\\npython3 image-transfer.py -sr <source-registry> -su <source-username> -sp <source-password> -so <source-orgname> -tr <target-registry> -tu <target-username> -tp <target-password> -to <target-orgname> -p _image-set.json\\n2\\n. \\nIn these commands,\\n<source-registry>\\n is the URL of the source registry. For example, '\\nregistry.hub.docker.com'\\n.\\n<source-username>\\n is the username of the source docker registry. \\n<source-password>\\n is the password of the source docker registry. \\n<source-orgname>\\n is the organization name in the source docker registry. Use \\n'hpeswitom'\\n if you use Docker Hub;\\notherwise, contact the admin of the source registry.\\n<target-registry>\\n is the URL of the target registry. If you don't have an Azure Container Registry, go\\nto \\nhttps://docs.microsoft.com/en-gb/azure/container-registry/\\n to create one in the same region as the AKS cluster.\\n<target-username>\\n is the username of the target docker registry. Obtain it from the admin of the target registry.\\n<target-password>\\n is the password of the target docker registry. Obtain it from the admin of the target registry.\\n<target-orgname>\\n is the organization name in the target docker registry. Obtain it from the admin of the target\\nregistry.\\nNote  \\nPython 3 is required to run the Python script below. Ensure Python 3 is installed on the bastion. \\nContainerized Operations Bridge 2022.11\\nPage \\n283\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e6f272bd8f7e984a359004a01085ff6f'}>,\n",
              "  <Document: {'content': 'For example: If the \\nHosted zone\\n is \\nopsb.itombyok.com\\n and \\nEXTERNAL_ACCESS_HOST\\n is \\ndemo.opsb.itombyok.com\\n, then\\nthe \\nRecord name\\n while creating DNS record will be \\npulsar.opsb.itombyok.com\\n.\\n3\\n. \\nSelect the \\nRecord type\\n as \\nA\\n and in \\nValue\\n enable the toggle to \\nAlias\\n. In \\nChoose Endpoint\\n drop-down, select \\nAlias\\nto Network Load Balancer\\n. In \\nChoose Region\\n drop-down, select your deployment region. In \\nChoose network load\\nbalancer\\n, search or type the load balancer configured for the \\nPulsar proxy service\\n. To get the load balancer configured\\nfor the \\nPulsar proxy service\\n, run the command on the bastion node: \\nkubectl get svc -n <suite namespace> | grep pulsar-proxy\\nNote down the \\npulsar-proxy\\n name for the service type \\nLoadBalancer\\n and run the following command:\\nkubectl -n <suite namespace> get svc itomdipulsar-proxy\\nNote down the \\nExternal-IP\\n value. You must type the record name as mentioned in step 2 and the \\nExternal-IP\\n value for\\nthe value of the \\nPulsar proxy service.\\n4\\n. \\nClick \\nCreate records\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n285\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd0498c86c837db94031d0fb6d2dd344'}>,\n",
              "  <Document: {'content': 'Certificate exchange for using Shared OPTIC Data Lake\\nFor shared OPTIC Data Lake, you must install the providing application first, the one which provides OPTIC Data Lake, for\\nexample, NOM. Deploy the consuming application, for example, Opsbridge in the second step. You must decide on the\\nmechanism through which you want to connect and update the values.YAML or AppHub UI  parameters accordingly. You must\\nset up a two-way trust between providing and consuming applications.\\nUpgrade the providing deployment\\nYou must upgrade the providing deployment and add the certificate extracted from the consuming application through \\nhelm up\\ngrade\\n.\\nRun this command to extract the RE, \\nRID\\n certificates from the consuming application\\'s \\npublic-ca-certificate:\\nkubectl get cm public-ca-certificates -n <namespace of consuming application> -o json | jq -r .data.\\\\\"RE_ca.crt\\\\\" > ConsumingAppRECert.pem\\nkubectl get cm public-ca-certificates -n <namespace of consuming application> -o json | jq -r .data.\\\\\"RID_ca.crt\\\\\" > ConsumingAppRIDCert.pem\\nExample:\\nkubectl get cm public-ca-certificates -n opsb-helm -o json | jq -r .data.\\\\\"RE_ca.crt\\\\\"> ConsumingAppRECert.pem\\nkubectl get cm public-ca-certificates -n opsb-helm -o json | jq -r .data.\\\\\"RID_ca.crt\\\\\" > ConsumingAppRIDCert.pem\\nRun the following command to upgrade the providing application:\\nCLI\\nhelm upgrade <providing deployment name  > -n <namespace or providing application > -f <providing application values.yaml > <providing application chart> [--set-file \"authorizedClientCAs.opsbridge-ca_RE\\\\.crt\"=ConsumingAppRECert.pem --set-file \"authorizedClientCAs.opsbridge-ca_RID\\\\.crt\"=ConsumingAppRIDCert.pem] -f <providing application secrets.yaml> \\nExample:\\nhelm upgrade deployment-nom -n nom-helm -f nom-values.yaml  $HOME/nom/charts/nom-1.6.0+202xxxxx.xxx.tgz [--set-file \"authorizedClientCAs.opsbridge-ca_RE\\\\.crt\"=ConsumingAppRECert.pem --set-file \"authorizedClientCAs.opsbridge-ca_RID\\\\.crt\"=ConsumingAppRIDCert.pem] -f nom-secrets.yaml \\nAppHub\\n1\\n. \\nLogin to AppHub UI of the Providing deployment, go to \\nDEPLOYMENTS\\n.\\n2\\n. \\nFrom the actions menu select\\n Edit \\nto edit the deployment.\\n3\\n. \\nGo to \\nSecurity\\n > \\nUpload OPTIC DL Client Authentication Certificates\\n.\\n4\\n. \\nClick or drag and drop to upload the\\n ConsumingAppRECert.pem\\n and \\nConsumingAppRIDCert.pem\\n.\\n5\\n. \\nClick on \\nREDEPLOY\\n.\\nImportant:\\n \\nIf you are using existing\\n Shared OPTIC DL from DCA, \\nand you want to reconfigure the\\nclient authentication certificates, then you must upload them in the providing deployment using CLI and\\nrestart OPTIC DL pods. DCA doesn\\'t provide \\nSecurity > Upload OPTIC DL Client Authentication\\nCertificates\\n in AppHub UI.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n286\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e34c4f5b49b1d9fcd8cec9b235fc0744'}>,\n",
              "  <Document: {'content': 'Verify the install\\n1\\n. \\nVerify that each of the PVCs are bound to a NFS volume.\\nExample: \\n# kubectl get pvc --all-namespaces\\nNAMESPACE        NAME                                                                               STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE\\ncore             db-backup-vol                                                                      Bound    db-backup           5Gi        RWX            cdf-default    29h\\ncore             db-single-vol                                                                      Bound    db-single           5Gi        RWX            cdf-default    29h\\ncore             itom-logging-vol                                                                   Bound    itom-logging        5Gi        RWX            cdf-default    29h\\ncore             itom-vol-claim                                                                     Bound    itom-vol            5Gi        RWX            cdf-default    29h\\ncore             prometheus-itom-prometheus-prometheus-db-prometheus-itom-prometheus-prometheus-0   Bound    itom-monitor        5Gi        RWX            cdf-default    28h\\nopsb-helm   deployment-opsb-helm-configvolumeclaim                                                  Bound    opsbvol3            10Gi       RWX                           27h\\nopsb-helm   deployment-opsb-helm-datavolumeclaim                                                    Bound    opsbvol1            10Gi       RWX                           27h\\nopsb-helm   deployment-opsb-helm-dbvolumeclaim                                                      Bound    opsbvol4            10Gi       RWX                           27h\\nopsb-helm   deployment-opsb-helm-logvolumeclaim                                                     Bound    opsbvol8            10Gi       RWX                           27h\\nopsb-helm   deployment-opsb-helm-omi-artemis-pvc                                                    Bound    opsbvol5            10Gi       RWO                           27h\\nopsb-helm   deployment-opsb-helm-pvc-omi-0                                                          Bound    opsbvol6            10Gi       RWO                           27h\\nopsb-helm   itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-0                               Bound    local-pv-7034901f   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-1                               Bound    local-pv-a248a6b9   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-2                               Bound    local-pv-6c0e5e9f   98Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-0                               Bound    local-pv-f1720991   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-1                               Bound    local-pv-8057665    98Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-2                               Bound    local-pv-bcfc6ca0   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-0                          Bound    local-pv-2b68689f   98Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-1                          Bound    local-pv-16db1ccb   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-2                          Bound    local-pv-1ca96a82   29Gi       RWO            fast-disks     27h\\n                                      \\n2\\n. \\nRun the following command to see the pod status in a namespace:\\nkubectl get pods -n <application namespace> \\nExample:\\n#  kubectl get pods -n opsb-helm\\nNAME                                                            READY   STATUS      RESTARTS      AGE\\nbvd-controller-deployment-6788dbc75c-7rpjd                      2/2     Running     0             27h\\nbvd-explore-deployment-7b66fb7d6c-2ngm9                         2/2     Running     0             24h\\nbvd-quexserv-5b8894749f-zv6l2                                   2/2     Running     0             24h\\nbvd-receiver-deployment-849c7fddb7-2r4fh                        2/2     Running     0             24h\\nbvd-redis-c6465485c-8mxqh                                       2/2     Running     0             27h\\nbvd-www-deployment-648d9b8b75-qzf4p                             2/2     Running     0             24h\\ncredential-manager-7f47fd64b7-tx6qz                             2/2     Running     0             27h\\nitom-analytics-aec-explained-56cd8d484-grvch                    2/2     Running     0             27h\\nitom-analytics-aec-pipeline-jm-564dc96877-j4nx5                 2/2     Running     0             27h\\nitom-analytics-aec-pipeline-tm-7d7668668b-wjv7p                 1/1     Running     0             27h\\nitom-analytics-auto-event-correlation-job-27554680-z44gd        0/1     Completed   0             26m\\nitom-analytics-auto-event-correlation-job-27554690-9svww        0/1     Completed   0             16m\\nitom-analytics-auto-event-correlation-job-27554700-mk4l4        0/1     Completed   0             6m51s\\nitom-analytics-data-retention-job-27553680-vgll7                0/1     Completed   0             17h\\nitom-analytics-data-retention-job-27554400-xbfcn                0/1     Completed   0             5h6m\\nitom-analytics-datasource-registry-796c948d6c-46r6d             2/2     Running     0             27h\\nitom-analytics-ea-config-b9c4556f6-jhw7l                        2/2     Running     0             27h\\nitom-analytics-event-attribute-reader-5948b55f84-q87q4          2/2     Running     0             27h\\nitom-analytics-flink-controller-7bd6585759-2snnk                2/2     Running     0             27h\\nitom-analytics-opsbridge-notification-8488597bdb-rwhq9          2/2     Running     0             27h\\nitom-analytics-text-clustering-server-58bf4f968d-sr7sn          2/2     Running     0             27h\\nitom-autopass-lms-54bf656cfc-cdkfx                              2/2     Running     0             27h\\nitom-cms-gateway-74fd99fb74-4gvlb                               2/2     Running     0             27h\\nitom-di-administration-77c95bb44b-s7zbw                         2/2     Running     0             27h\\nitom-di-data-access-dpl-559f56c6-mn7tj                          2/2     Running     0             27h\\nitom-di-metadata-server-778fcc756c-6d8tx                        2/2     Running     0             27h\\nitom-di-postload-taskcontroller-758798784c-dnzqw                2/2     Running     0             27h\\nitom-di-postload-taskexecutor-65b96b8b6-64twv                   2/2     Running     0             27h\\nitom-di-receiver-dpl-6f7cbc464d-97q4p                           2/2     Running     1 (27h ago)   27h\\nitom-di-receiver-dpl-6f7cbc464d-9fd9h                           2/2     Running     1 (27h ago)   27h\\nitom-di-scheduler-udx-f9cc5974f-nj9nh                           2/2     Running     0             27h\\nitom-idm-646446589c-bvs5b                                       2/2     Running     0             27h\\nitom-idm-646446589c-nwmf7                                       2/2     Running     0             27h\\nitom-ingress-controller-84f5494f66-49f9q                        2/2     Running     0             27h\\nitom-ingress-controller-84f5494f66-5v7z2                        2/2     Running     0             27h\\nitom-monitoring-admin-69bbb99b8b-bvbcf                          2/2     Running     0             27h\\nitom-monitoring-aws-discovery-collector-845559fdb-8nfl4         4/4     Running     0             27h\\nitom-monitoring-aws-metric-collector-67597f46b6-bzpmd           4/4     Running     0             27h\\nitom-monitoring-azure-discovery-collector-8469b597bc-lq25d      4/4     Running     0             27h\\nitom-monitoring-azure-metric-collector-6c7856f876-c9b5l         4/4     Running     0             27h\\nitom-monitoring-baseline-cfg-preload-job-zmyir-qszjf            0/1     Completed   0             27h\\nitom-monitoring-collection-autoconfigure-job-ucqmt-k85wb        0/1     Completed   0             27h\\nitom-monitoring-collection-manager-7cd8dc96c6-fwv5j             2/2     Running     0             27h\\nitom-monitoring-job-scheduler-5c85486b96-99rzc                  2/2     Running     0             27h\\nitom-monitoring-kubernetes-discovery-collector-c54b5799-qgshw   4/4     Running     0             27h\\nitom-monitoring-kubernetes-metric-collector-744c79949-gw2gw     4/4     Running     0             27h\\nitom-monitoring-oa-discovery-collector-7d8f7cd6c8-4265k         4/4     Running     0             27h\\nitom-monitoring-oa-metric-collector-65df67487f-wzrnz            4/4     Running     0             27h\\nContainerized Operations Bridge 2022.11\\nPage \\n287\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '18981246894f40a29877989997bb9aa4'}>,\n",
              "  <Document: {'content': 'itom-monitoring-oa-metric-collector-bg-864cb956f9-f7ctd         4/4     Running     0             27h\\nitom-monitoring-pt-api-server-7bfd8fb5-4lnhb                    2/2     Running     0             27h\\nitom-monitoring-pt-api-server-7bfd8fb5-6wswd                    2/2     Running     0             27h\\nitom-monitoring-pt-api-server-7bfd8fb5-7bb6m                    2/2     Running     0             27h\\nitom-monitoring-pt-coso-dl-data-access-0                        2/2     Running     0             27h\\nitom-monitoring-pt-ui-555d96c7b4-jcrg8                          2/2     Running     0             27h\\nitom-monitoring-pt-ui-555d96c7b4-jr9f8                          2/2     Running     0             27h\\nitom-monitoring-pt-ui-555d96c7b4-m7nkr                          2/2     Running     0             27h\\nitom-monitoring-pt-ui-config-job-dsowmut-h5dff                  0/1     Completed   0             27h\\nitom-monitoring-pt-zookeeper-0                                  1/1     Running     0             27h\\nitom-monitoring-service-data-broker-c7ddb44bc-pw5zg             2/2     Running     0             27h\\nitom-monitoring-sis-adapter-app-5fc9bd7f48-g7rv4                2/2     Running     0             27h\\nitom-monitoring-snf-6756557869-4kkzg                            2/2     Running     0             27h\\nitom-monitoring-threshold-processor-6787649978-cslqv            2/2     Running     0             27h\\nitom-nom-metric-transformation-0                                3/3     Running     8 (27h ago)   27h\\nitom-nom-metric-transformation-1                                3/3     Running     0             27h\\nitom-nom-metric-transformation-2                                3/3     Running     0             27h\\nitom-nom-zookeeper-0                                            1/1     Running     0             27h\\nitom-oba-config-74b7447c4c-8w97h                                2/2     Running     0             27h\\nitom-omi-aec-integration-164ay-d9k7v                            0/1     Completed   0             27h\\nitom-omi-aec-integration-watcher-27554680-v24k5                 0/1     Completed   0             26m\\nitom-omi-aec-integration-watcher-27554690-blxzd                 0/1     Completed   0             16m\\nitom-omi-aec-integration-watcher-27554700-n7hxz                 0/1     Completed   0             6m51s\\nitom-omi-csr-granter-laega-xzpwm                                0/1     Completed   0             27h\\nitom-omi-di-integration-wvsam-bg4gr                             0/1     Completed   0             27h\\nitom-opsb-content-management-job-9ulzt-rx4jm                    0/1     Completed   0             27h\\nitom-opsb-content-manager-58d8c76575-jbrmt                      2/2     Running     0             27h\\nitom-opsb-db-connection-validator-job-9kw9b                     0/1     Completed   0             27h\\nitom-opsb-resource-bundle-55596dfc87-nlcvz                      1/1     Running     0             27h\\nitom-opsbridge-cs-redis-6bd5f6b554-hh5x2                        2/2     Running     0             27h\\nitom-opsbridge-data-enrichment-service-b6487778f-zdmzx          2/2     Running     5 (27h ago)   27h\\nitom-opsbridge-des-cicache-builder-64dfb69d76-4pr84             2/2     Running     0             27h\\nitom-opsbridge-monitoring-gateway-f775df645-5hzc9               2/2     Running     0             27h\\nitom-reloader-575b4b9c9f-qcc9s                                  1/1     Running     0             27h\\nitom-ucmdb-0                                                    2/2     Running     0             27h\\nitom-ucmdb-probe-ff5b7f695-xc86l                                2/2     Running     0             27h\\nitom-vault-766b46f66d-gzzks                                     1/1     Running     0             27h\\nitomdimonitoring-verticapromexporter-6b9c9f6578-r9q2w           2/2     Running     0             27h\\nitomdipulsar-autorecovery-0                                     2/2     Running     0             27h\\nitomdipulsar-bastion-0                                          2/2     Running     0             27h\\nitomdipulsar-bookkeeper-0                                       2/2     Running     0             27h\\nitomdipulsar-bookkeeper-1                                       2/2     Running     0             27h\\nitomdipulsar-bookkeeper-2                                       2/2     Running     0             27h\\nitomdipulsar-bookkeeper-init-bf7x70z-fvlth                      0/1     Completed   0             27h\\nitomdipulsar-broker-78f77d94fd-l5bgx                            2/2     Running     0             27h\\nitomdipulsar-broker-78f77d94fd-n2f9w                            2/2     Running     0             27h\\nitomdipulsar-broker-78f77d94fd-pkz7k                            2/2     Running     0             27h\\nitomdipulsar-broker-apply-admin-settings-j1kr2lm-mmgzv          0/1     Completed   0             27h\\nitomdipulsar-minio-connector-post-upgrade-job-99x0c7x-m65k6     0/1     Completed   0             27h\\nitomdipulsar-proxy-5d486c7974-d6nqz                             2/2     Running     0             27h\\nitomdipulsar-proxy-5d486c7974-fr7dh                             2/2     Running     0             27h\\nitomdipulsar-zookeeper-0                                        2/2     Running     0             27h\\nitomdipulsar-zookeeper-1                                        2/2     Running     0             27h\\nitomdipulsar-zookeeper-2                                        2/2     Running     0             27h\\nitomdipulsar-zookeeper-metadata-nl85xha-7kn8t                   0/1     Completed   0             27h\\nomi-0                                                           2/2     Running     0             27h\\nomi-artemis-74c967d97d-gdf2v                                    2/2     Running     0             27h\\nuif-upload-job-b8q7nqn-lghc2                                    0/1     Completed   0             27h\\nwebtopdf-deployment-7cf4c58f79-n4f9c                            2/2     Running     0             24h\\n\\u200b\\nThe STATUS column indicates the current lifecycle state of the pod.  For example,\\n \\nPending, Running, Completed, Init,\\nCrashLoopBackoff\\n. You will see many pods in \\nInit\\n state, which eventually becomes \\nPodInitializing\\n state, and then Running\\nstate. \\nIn addition, if the pod status is Running, check the \\nREADY\\n column to see if the pod is fully started up.  The \\nREADY\\n column\\ncontains two numbers in the form X/Y.  \\nX indicates the number of containers running in the pod\\nY indicates the number of containers that should be running\\nFor example, 1/2 means that one out of two containers is running, so the pod isn\\'t fully started yet.\\nThe lifecycle state may take a long time to change, for example, 45 minutes. \\n3\\n. \\nExecute this command to output only those pods that aren\\'t running correctly:\\nkubectl get pods --all-namespaces -o wide | awk -F \" *|/\" \\'($3!=$4 || $5!=\"Running\") && $5!=\"Completed\" {print $0}\\'\\n  \\nIf after ample time has passed (~45 minutes) and the readiness status of the pod hasn\\'t changed, the installation didn\\'t\\ncomplete successfully and will require troubleshooting.\\nYou can further verify the status of each pod. Run the following commands:\\na\\n. \\nIf the pod is in  \\nPending\\n, \\nImagePullErorr\\n, or not yet Running state with all containers (for example, 2/2), then you can\\nget more information about that pod by running the following command:  \\nkubectl describe pod <pod name> -n <\\napplication\\n namespace>\\nb\\n. \\nYou can view logs for a container by running the following command: \\nkubectl logs <pod name> -n <\\napplication\\n namespace> -c <container name> [-f]\\nContainerized Operations Bridge 2022.11\\nPage \\n288\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a92a5e629161300def48642573f91a23'}>,\n",
              "  <Document: {'content': 'You can get \\n<container name>\\n from the output of  the command: \\nkubectl describe pod <pod name> -n <\\napplication\\n namespace>\\nIf you omit \\n-c <container name>\\n, the output displays the list of container names for that pod. \\n-f\\n is optional.  It behaves like \\ntail -f\\n by streaming the output and not exiting until you enter \\nctrl-c\\n.\\n Example: \\n# kubectl describe pod bvd-www-deployment-55c6f745d-8zf45 -n opsb-helm \\n# kubectl logs bvd-www-deployment-74b895c686-x7bps -n opsb-helm -c bvd-www\\nVerify the URL access\\nBased on the capabilities that you have selected for the application deployment, you will be able to access the following URLs:\\nCapabilities\\nApplications\\nURL\\nAll\\nUser Management\\nUI\\nhttps://<\\nFQDN_of_the_external_access_host\\n>:\\n<port_number_of_the_exter\\nnal_access_host>\\n/idm-admin\\nAll\\nLicense\\nManagement UI\\nhttps://<\\nFQDN_of_the_external_access_host\\n>:\\n<port_number_of_the_exter\\nnal_access_host>/\\nautopass\\nOPTIC Reporting, Stakeholder\\ndashboards, AEC\\nReporting UI\\nhttps://<\\nFQDN_of_the_external_access_host\\n>:\\n<port_number_of_the_exter\\nnal_access_host>/\\nbvd\\nOPTIC Reporting, Hyperscale\\nObservability\\nPerformance\\nTroubleshooter\\nhttps://<\\nFQDN_of_the_external_access_host\\n>:\\n<port_number_of_the_exter\\nnal_access_host>/\\ndashboard\\nFor the URL you will have to use one of these based on your installation type:\\nCLI install:\\nValues \\nglobal.externalAccessHost\\n and \\nglobal.externalAccessPort \\ncan be obtained from your OpsBridge helm values, for\\nexample:\\nhelm get values <helm deployment name> -n <application namespace> -o json | jq \\'.global.externalAccessHost\\'\\nhelm get values <helm deployment name> -n <application namespace> -o json | jq \\'.global.externalAccessPort\\'\\nAppHUb install:\\nValues from\\n \\nthe\\n General \\ntab >\\n External Access Host\\n and \\nExternal Access Port.\\nLog in using your OpsBridge administrator user for example, \\n\"admin\"\\n and your OpsBridge administrator password.\\nVerify Agent Metric Collector\\nTo verify the creation and deployment of the AMC configurations, see \\nSysInfraAgentAMCClassicOBMReport\\n.\\nTip:\\n If you see \"\\nNot Secure\\n\" message on the browsers when you access the service URLs, follow the steps\\nmentioned in \\nRequest certificates\\n from OMT doc to validate the certificates and establish a secure connection.\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n289\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e57f2460b71f55e12bcc922121eb306'}>,\n",
              "  <Document: {'content': 'Required for capabilities - AEC\\nVerify AEC\\nVerify AEC on a containerized OBM. For more information, see \\nVerify AEC\\n.\\nValidate the connection between BVD and OPTIC Data Lake\\nDuring the Operations Bridge suite deployment, Business Value Dashboard (BVD) is automatically configured with a connection\\nto the OPTIC Data Lake Vertica database.\\nIn this task, you validate the connection between BVD and OPTIC Data Lake. For more information, see \\nValidate the connection\\nbetween BVD and OPTIC Data Lake\\n.\\nConfigure the performance troubleshooting\\nRequired for capabilities - OPTIC Reporting, Hyperscale Observability\\nIn OPTIC Reporting, the Performance Troubleshooting enables you to plot and compare time series graphs of system\\ninfrastructure metrics. In Hyperscale Observability, it enables you to graph performance metrics and compare them. For more\\ninformation, see \\nPerformance Troubleshooting for OPTIC Reporting\\n, \\nPerformance Troubleshooting for Azure\\nmetrics\\n, \\nPerformance Troubleshooting for Kubernetes metrics\\n, and \\nPerformance Troubleshooting for AWS metrics\\n.\\nImportant\\n: After you complete the suite deployment and configuration on AWS, you must observe the\\nthroughput usage and update the mode according to the throughput utilization of the data volume. For steps, see\\nthe \\nMonitor and update the AWS EFS throughput\\n. \\nContainerized Operations Bridge 2022.11\\nPage \\n292\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9812e629ef63c60e3dc03371818e285f'}>,\n",
              "  <Document: {'content': \"Post install configurations\\nApply license\\nYou can either use a perpetual suite license or use the built-in 60 day trial license (\\nInstantOn\\n) after installation. If you use the\\ntrial license, you will need to apply for the perpetual license after the trial license expires. For more information, see \\nApply\\nlicense\\n.\\nUpdate the load balancer configuration after suite install\\nFor Red Hat OpenShift\\n  \\nAfter suite installation, you must create a listener for the external access port that's passed during suite install, for the service\\n \\nitom-ingress-controller-svc\\n. Also, configure \\npulsar proxy, des node port, data broker svc, di receiver, di admin \\nand \\ndata access\\n ports. For\\nmore information, see \\nUpdate the load balancer configuration after suite install\\n.\\nUse custom certificates\\nOperations Bridge is installed with a self-signed certificate for external access to the browser URLs. This includes BVD, IDM,\\nAutoPass, OPTIC Data Lake Health Insights, and OBM. Follow the steps on this page if you want to use your own CA issued\\ncertificate, see \\nUse custom certificates\\n.\\nConfigure the suite capabilities\\nPerform these configurations according to the capabilities that you selected during the suite install.\\nRequired for capabilities - Containerized OBM \\nGet integration tools\\nThe OBM integration tools are required to:\\n1\\n. \\nConfigure a secure connection between OBM and OPTIC Data Lake.\\n2\\n. \\nConfigure event forwarding from OBM to OPTIC Data Lake \\n3\\n. \\nConfigure Automatic Event Correlation\\nFor more information, see \\nGet integration tools\\n.\\nConfigure external OBM\\nIf you plan to use the external OBM, you must configure OBM for correlating events and forwarding them to OPTIC Data\\nLake. For more information, see \\nConfigure Classic OBM\\n.\\nConfigure a secure connection between containerized OBM and OPTIC Data Lake\\nUse the OBM Integration tools to configure a secure connection between containerized OBM and OPTIC Data Lake. For more\\ninformation, see \\nConfigure a secure connection between containerized OBM and OPTIC Data Lake\\n.\\nRequired for capabilities - Stakeholder Dashboard\\nConfigure stakeholder dashboard\\nThe stakeholder dashboard gives you visual information of live data using tables, charts, and widgets. Real time data can be\\nstreamed from any data source in JSON format via HTTP post. For more information, see \\nCreate Stakeholder Dashboard\\n.\\nValidate the connection between BVD and OPTIC Data Lake\\nDuring the Operations Bridge suite deployment, Business Value Dashboard (BVD) is automatically configured with a connection\\nto the OPTIC Data Lake Vertica database.\\nIn this task, you validate the connection between BVD and OPTIC Data Lake. For more information, see \\nValidate the connection\\nbetween BVD and OPTIC Data Lake\\n.\\nRequired for capabilities - Agentless Monitoring\\nAgentless Monitoring offers a centralized UI for managing SiteScope. Ensure that you \\ninstall SiteScope 2022.05\\n or later before\\nconfiguring Agentless Monitoring. To configure Agentless Monitoring, see \\nConfigure Agentless Monitoring\\n.\\nRequired for capabilities - Hyperscale Observability\\nGrant certificate request on OBM\\n \\nNote: \\nTo use Agentless Monitoring, you don't need to apply for an additional license. The Sitescope license\\nenables you to access Agentless Monitoring.\\nContainerized Operations Bridge 2022.11\\nPage \\n290\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2dd94f007a54e3f6f0beedd2f0ed5002'}>,\n",
              "  <Document: {'content': 'You can configure thresholds for the AWS metrics collected by Hyperscale Observability. If a metric breaches a threshold, an\\nevent is generated and forwarded to OBM using the Data Broker Container (DBC). DBC is registered as an Agent in OBM during\\nOperations Bridge startup. Hence, you must grant its certificate request on OBM. For more information, see \\nGrant certificate\\nrequest on OBM\\n.\\nDeploy the event mapping policy\\nAfter importing Hyperscale Observability content pack, you must deploy the Operations Bridge Manager (OBM) event mapping\\npolicy to the Operations Agent running on the Data Broker Container ((\\nitom-monitoring-service-data-broker-svc\\n). This will\\nforward the events generated by Hyperscale Observability to OBM. For more information, see \\nDeploy the event mapping\\npolicy\\n.\\nConfigure IDM URL for \\nops-monitoring-ctl\\nRun the following command:\\nops-monitoring-ctl config set cs.server <IDM_URL>\\nRequired for capabilities - OPTIC Reporting\\nConfigure System Infrastructure reports\\nSystem Infrastructure reports give you information about the availability and performance of the servers in your environment.  These\\nreports are available for metrics collected by Operations Agent and SiteScope. \\nYou can use one of the following mechanisms to send Operations Agent data to OPTIC Data Lake:\\nUsing Agent Metric Collector\\nUsing metric streaming policies\\nTo send SiteScope data to OPTIC Data Lake, see Configure System Infrastructure Reports using SiteScope\\nFor more information, see \\nConfigure System Infrastructure Reports using Agent Metric Collector\\n, \\nConfigure System\\nInfrastructure Reports using metric streaming policies\\n, \\nConfigure System Infrastructure Reports using SiteScope\\n.\\nConfigure synthetic transaction reports using BPM\\nSynthetic Transaction Reports give you information about end user experience, availability, and performance of applications.\\nBusiness Process Monitor (BPM) enables you to run synthetic transactions and collect metrics. Perform this task to send the\\nmetrics collected by BPM to OPTIC Data Lake and generate synthetic transaction reports on Business Value Dashboard (BVD).\\nFor more information, see \\nConfigure synthetic transaction reports using BPM\\n.\\nConfigure real user monitoring reports using RUM\\nReal User Monitor (RUM) reports give you information about end user experience, availability, and performance of applications\\nby monitoring real user traffic.  Perform this task to enable RUM to send data to OPTIC Data Lake. RUM forwards these data to\\nOPTIC Data Lake: Pages, Actions, Session, Transactions, Crashes, Events, TCP, and Requests. For more information, see\\nConfigure real user monitoring reports using RUM\\n.\\nConfigure IDM URL for \\nops-content-ctl\\nRun the following command:\\nops-content-ctl config set cas.server <IDM_URL>\\nConfigure IDM URL for \\nops-monitoring-ctl\\nRun the following command:\\nops-monitoring-ctl config set cs.server <IDM_URL>\\nRequired for capabilities - OPTIC Reporting, AEC\\nConfigure AEC and event reports\\nTo populate the event reports in BVD, you must configure OBM to forward events and topology to OPTIC Data Lake. Perform\\nthis task to forward events and topology from an OBM server to OPTIC Data Lake. For more information, see \\nConfigure event\\nreports with classic OBM\\n.\\nValidate the connection between BVD and OPTIC Data Lake\\nDuring the Operations Bridge suite deployment, Business Value Dashboard (BVD) is automatically configured with a connection\\nto the OPTIC Data Lake Vertica database.\\nIn this task, you validate the connection between BVD and OPTIC Data Lake. For more information, see \\nValidate the connection\\nbetween BVD and OPTIC Data Lake\\n.\\nNote:\\n You must complete the configuration of \\nSystem Infrastructure Reports using Agent Metric Collector\\nwithin two hours of Operations bridge installation for the configuration to be successful.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n291\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '70abd78a992fcb78c9cf7f6f6758c2fa'}>,\n",
              "  <Document: {'content': \"Get Integration Tools\\nThe OBM integration tools are required to:\\n1\\n. \\nConfigure a secure connection between OBM and OPTIC Data Lake.\\n2\\n. \\nConfigure event forwarding from OBM to OPTIC Data Lake (Both Reporting and Automatic Event Correlation capabilities\\nuse event forwarding)\\n3\\n. \\nConfigure Automatic Event Correlation\\nFollow the steps on the suite master node to get the integration tools:\\n1\\n. \\nThe \\nopsb-obm-integration-tools.zip\\n file is present in the \\nstatic-files-provider\\n container of the \\nopsb-resource-bundle\\n pod.\\nTo get the zip file, run the following command on the master (control plane) node:\\nwget https://<externalAccessHost>:<externalAccessPort>/staticfiles/opsb-obm-integration-tools.zip --no-check-certificate\\nFor example: \\nwget https://myhost.mycompany.net:443/staticfiles/opsb-obm-integration-tools.zip --no-check-certificate\\n2\\n. \\nRun the following commands to extract the contents of \\nopsb-obm-integration-tools.zip\\n to the \\nintegration-tools\\n directory: \\nunzip opsb-obm-integration-tools.zip -d integration-tools\\ncd integration-tools\\nYou will use the tools in the \\nintegration-tools\\n directory to perform the post-installation tasks.\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nTip: \\nRun the following command to get the \\n<externalAccessPort>\\n:\\nhelm get values -n $(helm list -A | grep opsbridge | awk '{print $2,$1}') | grep -i externalAccess\\nCommand output:\\nexternalAccessHost: mycomp.mysystem.net\\nexternalAccessPort: 443\\nContainerized Operations Bridge 2022.11\\nPage \\n294\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a601686daf6759a3e9b6aaad4b23a8e'}>,\n",
              "  <Document: {'content': 'Apply license\\nYou can either use a perpetual suite license or use the built-in 60-day trial license (InstantOn) after installation. If you use the\\ntrial license, you will need to apply for the perpetual license after the trial license expires. \\nActivate perpetual suite license\\n1\\n. \\nGo to the \\nSoftware Entitlement Portal\\n.\\n2\\n. \\nObtain an Operations Bridge suite perpetual license.\\n3\\n. \\nEnter any valid IP address in the \\nLocking Information\\n field - this must not be the IP address of your master or worker\\nnodes. It can be IP address of the external access host. In a multi master environment this can correspond to the IP of the\\nLoad Balancer if it exists.\\n4\\n. \\nDownload the perpetual license file to your local drive.  \\n5\\n. \\nLaunch the License Administration UI from a supported web browser:\\nhttps://<externalAccessHost>:<externalAccessPort>/autopass\\n<externalAccessHost>:\\n Give \\nexternalAccessHost\\n (FQDN of the external access host) here.\\n<externalAccessPort>\\n: Give \\nexternalAccessPort\\n (port number on which containerized Operations Bridge is accessible).\\n6\\n. \\nLog on as the administrator. The AutoPass License Server page opens.\\n7\\n. \\nClick \\nLICENSE > INSTALL\\n.\\n8\\n. \\nClick \\nAdd File(s)\\n to browse to the license file on your local drive, and then click \\nNext\\n. The license details appear.\\n9\\n. \\nSelect the listed license, and then click \\nInstall Licenses\\n.\\n10\\n. \\nWhen the installation is complete, click \\nView \\nto view the installed licenses.\\nContainerized Operations Bridge 2022.11\\nPage \\n293\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c8fb0d8e128126a60c63840ccd4d5e52'}>,\n",
              "  <Document: {'content': 'Configure Agentless Monitoring\\nAgentless Monitoring enables you to understand the health, availability, and performance of a hybrid set of systems and\\napplications deployed on-premise and cloud in your infrastructure.   \\nFollow the instructions on the pages associated with the tasks to configure Agentless Monitoring:\\n1\\n. \\nAdd users, roles, and groups in IDM. See, \\nAdd users, roles, and groups\\n.\\n2\\n. \\nAdd SiteScope certificates. See, \\nAdd SiteScope certificates\\n.\\n3\\n. \\nSet up monitoring CLI to control the collection service configuration. See, \\nSet up monitoring CLI\\n.\\n4\\n. \\nOnboard SiteScope systems to the new centralized UI, Agentless Monitoring using CLI. See, \\nOnboard SiteScope systems to\\nnew centralized UI (Agentless Monitoring)\\n.\\n5\\n. \\nLaunch Agentless Monitoring.\\nLaunch Agentless Monitoring\\ni\\n. \\nEnter the following address in your browser: \\nhttps://<FQDN of control plane>/ui/\\nii\\n. \\nClick Administration  \\n \\n.\\niii\\n. \\nClick on the \\nMonitoring\\n folder.\\niv\\n. \\nClick on Agentless Monitoring.\\nContainerized Operations Bridge 2022.11\\nPage \\n295\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '76aa424ee05e3a4b8fa54fea84243f28'}>,\n",
              "  <Document: {'content': \"Configure Classic OBM\\nThe topic provides the steps to configure a classic OBM for correlating events and forwarding them to OPTIC Data Lake. \\nTasks for configuring classic OBM\\nCreate the \\nobm-configurator.jar\\n tool and install the OBM CA certificate on the suite by using the \\nIntegration Tools\\n. \\nConfigure OBM and create \\nconfigureSuite.zip\\n by executing \\nobm-configurator.jar. \\nThe \\nconfigureSuite.zip \\ncontains\\nthe\\n setup-obm.sh\\n. \\nConfigure the suite by extracting \\nconfigureSuite.zip \\non the control plane (master) node and executing \\nsetup-obm.sh\\n.\\nCreate the obm-configurator.jar tool\\n On the control plane (master) node, in the \\nintegration-tools \\ndirectory, execute the following command:\\n./get-obm-setup-tool.sh\\nThe \\nobm-configuration.jar\\n tool is created in the same directory where \\nget-obm-setup-tool.sh\\n resides.\\nHere's a sample output after running the command: \\n./get-obm-setup-tool.sh\\ninfo: Logfile: /tmp/get-obm-setup-tool-2021-10-11-16-56-35.log\\ninfo: Working dir: /root/tools2\\ninfo: AEC Namespace:  opsb-helm\\ninfo: COSO Namespace: opsb-helm\\ninfo: Getting files from the suite ...\\ninfo: Fetching suite certificates ...\\ninfo: Copying file issue_ca.crt from container idm to /root/tools2/obm-configurator-interim/issue_ca.crt ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied suite certificates\\ninfo: Fetching IDL configuration script ...\\ninfo: Name of the file: /artifacts/idl-config-1.5.0.zip\\ninfo: Copying file idl-config-1.5.0.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/idl-config-1.5.0.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully fetched IDL configuration script\\ninfo: Fetching OBM IDL configuration script ...\\ninfo: Name of the file: /artifacts/opr-config-idl-11.20.004.010.zip\\ninfo: Copying file opr-config-idl-11.20.004.010.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/opr-config-idl-11.20.004.010.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully fetched OBM configuration script\\ninfo: Fetching 'COSO_Data_Lake_Event_Integration' ...\\ninfo: Name of the file: /artifacts/COSO_Data_Lake_Event_Integration_CP-3.01.zip\\ninfo: Copying file COSO_Data_Lake_Event_Integration_CP-3.01.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/COSO_Data_Lake_Event_Integration_CP-3.01.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied 'COSO_Data_Lake_Event_Integration'\\ninfo: Fetching 'COSO_Data_Lake_AEC_Integration_CP' ...\\ninfo: Name of the file: /artifacts/COSO_Data_Lake_AEC_Integration_CP-3.04.zip\\ninfo: Copying file COSO_Data_Lake_AEC_Integration_CP-3.04.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/COSO_Data_Lake_AEC_Integration_CP-3.04.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied 'COSO_Data_Lake_AEC_Integration_CP'\\ninfo: Fetching OBM Setup Tool ...\\ninfo: Name of the file: /artifacts/obm-configurator.jar\\ninfo: Copying file obm-configurator.jar from container itom-static-files-provider to /root/tools2/obm-configurator.jar ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied OBM setup tool\\ninfo: Getting deployment information ...\\ninfo: Creating files package ...\\ninfo: Creating package of files ...\\n/root/tools2\\ninfo: Creating tool jar ...\\ninfo: Unpacking tool package ...\\ninfo: Updating tool files ...\\ninfo: Creating the updated tool package ...\\n/root/tools2\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\nIf an existing OBM is connected to OPTIC Data Lake and BVD, refer \\nConnected Servers\\n.\\nNote\\n: The\\n obm-configurator.jar\\n tool configures a classic OBM for correlating events and forwarding the events\\nto OPTIC Data Lake. This tool can't change the configuration of a configured classic OBM or Operations Bridge\\nsuite.\\nContainerized Operations Bridge 2022.11\\nPage \\n296\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7687e268c75edbe66acd5ff8ce468f9e'}>,\n",
              "  <Document: {'content': 'info: Cleanup ...\\nSuccessfully created OBM setup tool in file /root/tools2/obm-configurator.jar\\nRemove old certificates from the OBM trust store\\n(Optional) If the classic OBM is connected to an earlier instance of OPTIC Data Lake, remove the old suite certificates from OBM\\ntrust store before executing the \\nobm-configurator.jar \\nfile. Refer to the Remove the OBM Configuration page in related\\ntopics and perform the steps, as required.\\nInstall an OBM CA certificate\\nPerform the following steps to install an OBM CA certificate on the Operations Bridge suite:\\n1\\n. \\nTo get the list of trusted certificates, run the following command on the classic OBM gateway server:\\nOn Linux\\n/opt/OV/bin/ovcert -list\\nOn Windows\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcert\" -list\\n2\\n. \\nFrom the list of certificates, locate the OBM\\n Trusted Certificate\\n.\\nFor example, \\nCA_297819c4-f266-75c1-0518-a723aacc1fde_2048\\n3\\n. \\nTo export the trusted certificate to a file, run the following command:\\nOn Linux\\n/opt/OV/bin/ovcert -exporttrusted -file obm_ca.crt -alias \"CA_297819c4-f266-75c1-0518-a723aacc1fde_2048\" -ovrg server\\nOn Windows\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcert\" -exporttrusted -file obm_ca.crt  -alias \"CA_297819c4-f266-75c1-0518-a723aacc1fde_2048\" -ovrg server\\n4\\n. \\nTo install an OBM CA certificate, do one of the following options:\\nInstall OBM CA certificate in Operations Bridge using the CLI\\na\\n. \\nCopy the \\nobm_ca.crt\\n file to the control plane (master) node of the Operations Bridge suite.\\nb\\n. \\nOn the control plane (master) node, install the OBM CA certificate using the\\n idl_config.sh\\n tool:\\nidl_config.sh -cacert <cert_file> -chart <chart> -namespace <namespace> [-release <release>]\\nFor example, run the following command after changing to the \\nintegration-tools\\n directory:\\ncd integration-tools/obm-configurator-interim\\n./idl_config.sh -cacert /tmp/obm_ca.crt -chart path/to/charts/\\nopsbridge-suite-2021.05.tgz -namespace opsb-suite\\nInstall OBM CA certificate in Operations Bridge using the AppHub\\nTip: \\nThe OBM Trusted Certificate is usually with a * in the Trusted Certificates section.\\n\\ue917\\n\\ue917\\nNote: \\nYou can find the\\n idl_config.sh\\n tool in the \\nobm-configurator-interim \\ndirecto\\nry, which is in\\nthe \\nintegration-tools\\n direc\\ntory.\\n\\ue916\\n\\ue916\\nImportant:\\n \\nIf you have used an existing Shared OPTIC Data Lake, you must enter the Providing\\ndeployment\\'s chart name and Providing deployment application namespace.\\n  \\nFor example, if you have used\\nNOM\\'s Shared OPTIC DL, then you must provide NOM chart name and NOM application namespace.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n297\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd57c468697cd422c9bd87703e867316f'}>,\n",
              "  <Document: {'content': \"Client Certificates\\nTool Parameter\\nFormat\\nDescription\\n--admin-client-cert\\nPKCS12\\nOBM admin user client certificate and key chain\\n--client-cert\\nPEM\\nIntegration user client certificate\\n--client-key\\nPEM\\nIntegration user client key\\nConfigure the Operations Bridge suite\\nThe obm-configurator.jar tool creates the \\nconfigureSuite.zip\\n file. Perform the following steps to use the\\ncreated \\nconfigureSuite.zip\\n file and configure the suite:\\n1\\n. \\nTo copy the \\nconfigureSuite.zip\\n file to the \\n/var/tmp \\ndirectory of the control plane (master) node, run the following\\ncommand:\\nscp configureSuite.zip root@<suite_master_hostname>:/var/tmp\\nFor example:\\nscp configureSuite.zip root@suitemaster.example.com:/var/tmp\\nIf you have installed the OBM system on Windows, manually copy the package to the control plane (master) node.\\n2\\n. \\nOn the control plane (master) node, extract the package:\\ncd /var/tmp\\nunzip configureSuite.zip -d configureSuite\\n3\\n. \\nTo run the \\nsetup-obm.sh\\n tool in the \\n/var/tmp/configureSuite\\n directory, run the following command.\\ncd configureSuite\\nbash setup-obm.sh -chart <path>\\nwhere \\nchart\\n is the path to either a directory containing the chart or a path to a \\ngzipped TAR\\n file.\\nFor example:\\ncd configureSuite\\nbash setup-obm.sh -chart /path/to/opsbridge-suite-2020.10.0-149.tgz\\nThe \\nsetup-obm.sh\\n tool does the following in the Operations Bridge suite:\\nInstalls the OBM CA certificate in the suite.\\nConfigures the OBM system as a data source and receiver for AEC events (if you set the \\n--configuration-type\\n to \\nAEC\\n).\\nNote:  \\nIt's recommended that the certificates should have a valid Subject Alternative Name (SAN). You\\ncan run the following command to verify if a certificate has a SAN: \\nopenssl x509 -noout -ext subjectAltName -in <certificate file>\\nFor example:\\nopenssl x509 -noout -ext subjectAltName -in server.crt\\nThe sample output after running the command is as follows:\\nX509v3 Subject Alternative Name:\\n    DNS:omi, DNS:omi-0, DNS:omi-0.omisvc, DNS:omi-0.omisvc.opsb-helm, DNS:omi-0.omisvc.opsb-helm.svc, DNS:omi-0.omi\\nsvc.opsb-helm.svc.cluster.local, DNS\\n:omi-0.omisvc.opsb-helm.svc.cluster.local.\\nImportant:\\n \\nIf you have used an existing Shared OPTIC Data Lake, you must enter the Providing\\ndeployment's chart name with the absolute path.\\n  \\nFor example, if you have used NOM's Shared OPTIC DL,\\nthen you must provide NOM chart name.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n301\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '490e02e91e65d0d19867901f5f9339d5'}>,\n",
              "  <Document: {'content': 'a\\n. \\nChange the name of the \\nobm_ca.crt\\n file to a unique name that qualifies your classic OBM system. The name must start\\nwith \"client\" and end with the \".crt\" extension. Ensure that the filename must not be more than 20 characters.\\nFor example, \\nclient-abc-obm.crt\\nb\\n. \\nOn the AppHub UI, choose \\nDeployments\\n > \\nEdit \\nand click the \\nSecurity \\ntab. Refer to \\nReconfigure a deployment\\n topic.\\nc\\n. \\nUpload the new \\nUpload OPTIC Data Lake Client Authentication Certificates\\n certificate by using the option \\nClick\\nhere \\nor drag and add files for \\nUpload OPTIC Data Lake Client Authentication Certificates\\n.\\nd\\n. \\nClick on \\nVERIFY CERTIFICATE\\n. Make sure that the validation is successful.\\ne\\n. \\nClick on the \\nDatabases\\n tab and click on \\nVERIFY\\n for each of the databases. Then click \\nREDEPLOY\\n.\\nf\\n. \\nAfter some time, run the command to verify the pods which aren\\'t running:\\nkubectl get pods --all-namespaces -o wide | awk -F \" *|/\" \\'($3!=$4 || $5!=\"Running\") && $5!=\"Completed\" {print $0}\\'\\nConfigure the OBM system\\nThe \\nobm-configurator.jar\\n file is the setup tool for a classic OBM system. You must execute the tool on a classic OBM\\nGateway system. This tool configures OBM with the Operations Bridge suite and creates the \\nconfigureSuite.zip\\n file in the\\nsame directory.\\nYou can use the tool to:\\nEstablish trust between OBM and OPTIC Data Lake \\nConfigure event forwarding\\nConfigure Automatic Event Correlation (AEC)\\nPerform the following steps:\\n1\\n. \\nTo copy the \\nobm-configurator.jar\\n file to the OBM Gateway system, run the following command :\\nscp integration-tools/obm-configurator.jar root@<obm_system>:/var/tmp\\nIf you have installed the OBM Gateway system on Windows, manually copy the tool to the system.\\n2\\n. \\nExecute the \\nobm-configurator.jar\\n tool on the OBM Gateway system. Enter passwords for \\nadmin\\n and \\nZIP file\\nencryption\\n when prompted.\\nUse the following syntax when executing the \\nobm-configurator.jar\\n with only the required parameters:\\n/opt/HP/BSM/JRE/bin/java -jar/var/tmp/obm-configurator.jar --endpoint-id <id> --suite-service-hostname <host> --obm-ca-cert-alias <cert-alias>\\nExecute the command as a \\nsudo\\n \\nuser\\n if you aren\\'t using the \\nroot user\\n.\\nThe required \\nparameters are as follows:\\n--endpoint-id\\n: Defines the identifier used to register the OBM system in the suite. This parameter should be a readable\\nstring and is used to identify the OBM system when checking the registered OBM systems in the suite. \\nImportant:\\n \\nYou must upload the obm_ca.crt in AppHub UI and reconfigure the deployment as mentioned in\\nthis section. If you skip this step and later try to upgrade using AppHub, the certificates will not be present in\\nAppHub and the integration will not work.\\n\\ue91b\\n\\ue91b\\nNote: \\nYou can\\'t use the \\nobm-configuration.jar \\ntool to configure a containerized OBM system.\\n\\ue916\\n\\ue916\\nNotes: \\nThe event forwarding from the OBM to OPTIC Data Lake is enabled immediately after you configure OBM. It\\'s\\npossible that OBM immediately tries to forward events to the configured OPTIC Data Lake, while OPTIC Data\\nLake and the suite are still not configured. This might result in some warning events that mention that the\\nevent forwarding to OPTIC Data Lake has failed.\\nWhen you rerun the tool, it might abort because the suite certificates are already installed when the tool was\\nexecuted previously. In such a situation, add the \\n--force\\n parameter. The inclusion of the --force parameter in\\nthe command ensures that the tool execution proceeds even when the suite certificates are installed. Ensure\\nthat you rerun the tool with this parameter only when the installed certificates are from the current suite,\\nwhich was installed when the tool was run previously.\\nContainerized Operations Bridge 2022.11\\nPage \\n298\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a813a82ec7607a08e5a1a6358d4f0004'}>,\n",
              "  <Document: {'content': 'Configure event forwarding from OBM to OPTIC Data Lake using basic authentication for the OBM administration\\nuser and the event integration user:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm\\\\\\n--configuration-type FORWARDING \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--admin-user obmadmin \\nConfigure Automatic Event Correlation where OBM uses client authentication for the OBM administration user\\nand the event integration user (default configuration type is AEC):\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--integration-user obm_integration_user \\\\\\n--admin-client-cert admin.user_cert.p12 \\\\\\n--client-cert integration.user_cert.pem \\\\\\n--client-key integration.user.key.pem \\nConfigure Automatic Event Correlation where you have set OBM without TLS. In this case, you must specify the\\nOBM URL:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--configuration-type AEC \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--admin-user obmadmin --integration-user obm_integration_user \\\\\\n--obm-url \"http://obm.company.com:8081\"\\nFor Windows, replace \\n/opt/HP/BSM/JRE/bin/java\\n with \\n%TOPAZ_HOME%\\\\JRE\\\\bin\\\\java.exe\\nFor more examples and a detailed description of all possible tool parameters, see the\\n OBM Configurator Tool\\ntopic.\\nSpecial certificate handling\\nDepending on the certificates that OBM uses, it might be necessary to specify the alias of the certificates, which are as follows:\\nOBM Web Certificate\\nWhen you use a CA-signed certificate to access the web (for example, accessing the OBM web services), you must specify\\nthe alias of the installed web certificate using the \\n--web-cert-alias\\n parameter. Run the following command to find the alias:\\nOn Linux\\n/opt/HP/BSM/bin/opr-cert-mgmt.sh -list\\nOn Windows\\n%TOPAZ_HOME%\\\\bin\\\\opr-cert-mgmt.bat -list\\nOBM CA Certificate\\nWhen you install additional trusted certificates on OBM, it\\'s recommended that you use only the OBM CA certificate\\nto configure the suite. To prevent an import of all trusted certificates from OBM into the Operations Bridge suite, you must\\nspecify the CA certificate alias using the \\n--obm-ca-cert-alias\\n parameter. Run the following command to find the alias of the\\nOBM CA certificate: \\nOn Linux\\n/opt/OV/bin/ovcert -list -ovrg server\\nOn Windows\\n%OvInstallDir%\\\\bin\\\\win64\\\\ovcert.exe -list -ovrg server\\nOBM Client Certificates\\nIf you use client certificates to access OBM, you must specify the certificate files for the setup. When using client\\ncertificates, make sure that:\\nthe certificate for the OBM administration user is in \\nPKCS12\\n format\\nthe certificate and key for the integration user is in \\nPEM \\nformat\\nContainerized Operations Bridge 2022.11\\nPage \\n300\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '52cf62549e28defa3d2951c91b752b49'}>,\n",
              "  <Document: {'content': \"--suite-service-hostname\\n: Defines the FQDN of the system on which OBM can reach the \\nitom-di-receiver-svc\\n service. This\\nis typically the FQDN of the control plane (master) node.\\n--obm-ca-cert-alias\\n: After you install additional trusted certificates on OBM, it's recommended to use only the OBM CA\\ncertificate to configure the suite. To prevent an import of all trusted certificates from OBM into the suite, ensure that\\nyou specify the CA certificate alias by using the parameter, \\n--obm-ca-cert-alias\\n.\\nPassword parameters: You're prompted for passwords if you haven't specified them on the command line.\\nThe following optional parameters are updated with the default settings or operations if they're not specified:\\n--configuration-type\\n: Defines the type of configuration especially if event correlation isn't desired. By default, the\\nvalue is AEC. You can set it to \\nFORWARDING\\n (to configure only event forwarding) or \\nTRUST_ONLY\\n (to\\nexchange certificates only). The valid options are as follows:\\nTRUST_ONLY: Exchanges certificates to establish trust between OBM and OPTIC Data Lake. Choose this\\noption if you want to configure OPTIC Reporting.\\nFORWARDING: Configures the classic OBM and the suite for event forwarding. Choose this option if you\\nwant to configure OPTIC Reporting, specifically the Event reports.\\nAEC: Configures OBM and the Operations Bridge suite for event forwarding and Automatic Event\\nCorrelation. By default, the option is set to AEC. Choose this option if you want to configure OPTIC Reporting\\nand AEC.\\n--integration-user\\n: Gets set to \\nOBM_event_submit_user\\n--obm-url\\n: Gets set to https://localhost:443, if not specified. If you haven't used TLS, you can specify the OBM\\nHTTP URL.\\n--itom-di-receiver-port\\n: You can use this parameter to overwrite the port, especially when you install the Operations\\nBridge suite on a cloud platform. This isn't a required option. The port gets automatically detected during tool\\ncreation.\\n--itom-di-administration-port\\n: You can use this parameter to overwrite the port, especially when you install the\\nOperations Bridge suite on a cloud platform. This isn't a required option. The port gets automatically detected\\nduring tool creation.\\n--itom-di-data-access-port\\n: You can use this parameter to overwrite the port, especially when you install the\\nOperations Bridge suite on a cloud platform. This isn't a required option. The port gets automatically detected\\nduring tool creation.\\n--force\\n: You can use this parameter to allow the tool execution to proceed when the suite certificates are already\\ninstalled after the tool was run previously. Although this isn't a required option, the tool execution might abort\\nbecause the suite certificates are already installed when the tool was executed previously.\\nExamples\\nEstablish trust between OBM and OPTIC Data Lake using basic authentication for OBM:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--configuration-type TRUST_ONLY \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--admin-user obmadmin\\nNote:\\nIf you add BVD after integrating through \\nobm-configurator.jar\\n tool fails due to duplicate servers then\\nupdate the DNS entry for the server. Run the following command:\\nopr-connected-server.bat -username -password -update -dns itom-di-receiver-svc\\nTo find the ID of the connected server, run the following command:\\nopr-connected-server.bat -list -username -password\\n\\ue916\\n\\ue916\\nImportant\\n: In cloud deployments, the default ports for DI Receiver, DI Data Access, and DI\\nAdministration are 30001, 30003, and 30004 respectively. If you didn't use the\\n Cloud Automation\\nToolkit\\n provided by Micro Focus and instead provisioned your cloud infrastructure manually using\\ndifferent ports, then specify these ports explicitly with the corresponding parameters that are mentioned.\\nNote\\n: The \\nTRUST_ONLY\\n is a subset of \\nFORWARDING\\n, which in turn is a subset of AEC.\\nThis means that if you specify AEC, \\nobm-configurator.jar\\n establishes trust between OBM and\\nOPTIC Data Lake, and then configures event forwarding and also configures Automatic Event\\nCorrelation.\\nContainerized Operations Bridge 2022.11\\nPage \\n299\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8c97b4bea27c5d6a399c499738579dc5'}>,\n",
              "  <Document: {'content': 'On Linux:\\nGo to \\n/opt/HP/BSM/opr/support\\n and run the command,\\n ./sendEvent.sh -j -t TestEvent -s normal\\nHere\\'s a sample output after running the command:\\n2\\n. \\nFrom the OBM menu, choose \\nWorkspaces \\n> \\nOperations Console\\n and click \\nEvent Perspective\\n.\\n \\nCheck if the test\\nevent is listed and verify if the \\nState\\n of the event shows as, \\nForwarded\\n.\\n3\\n. \\nYou can also check the \\nopr_event\\n table in the \\nmf_shared_provider_default \\nschema to verify if the event has reached\\nOPTIC Data Lake.\\nOn a system that has the vsql command, such as a Vertica node, run the command:\\n/opt/vertica/bin/vsql -U dbadmin -c \"select node_hint,title,timestamp from mf_shared_provider_default.opr_event where title ilike \\'testEvent\\' limit 10;\"\\nYou are prompted \\nto enter the password for the \\ndbadmin\\n user. You can specify a different user such as the user, \\n<vertica_r\\nouser> \\nthat you created in the \\nPrepare Vertica database\\n section.\\nVerify AEC\\nWait for five minutes before verifying the AEC configuration because it can take up to five minutes for the configuration from\\nthe previous steps to be applied.\\n1\\n. \\nTo send a test event to OPTIC Data Lake, run the following command on the OBM Gateway server:\\nOn Windows\\n\"%TOPAZ_HOME%\\\\opr\\\\support\\\\sendEvent.bat\" -j -t \"Test Start\" -eh AutoCorrelationTest:Start -nx second -t \"Test End\" -eh AutoCorrelationTest:End\\nHere\\'s the sample output after running the command:\\nOn Linux\\n/opt/HP/BSM/opr/support/sendEvent.sh -j -t \"Test Start\" -eh AutoCorrelationTest:Start -nx second -t \"Test End\" -eh AutoCorrelationTest:End \\nHere\\'s the sample output after running the command on the Linux platform:\\n2\\n. \\nFrom the OBM menu, choose \\nWorkspaces \\n>\\n Operations Console \\nand click \\nEvent Perspective. \\nCheck the OBM Event\\nContainerized Operations Bridge 2022.11\\nPage \\n303\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6219b09448a1d3d3a55361474137d7a6'}>,\n",
              "  <Document: {'content': 'Browser after some time for a new event with the \\nAutomatically Correlated Event: … \\ntitle.\\nIf the event is visible in the browser, it indicates that you have configured Automatic Event Correlation correctly. \\nThe following image shows that AEC is configured.\\nRelated topics\\nIntegration Tools\\nOBM Configurator Tool\\nReconfigure a deployment\\nRemove the OBM Configuration\\nContainerized Operations Bridge 2022.11\\nPage \\n304\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b18bb889608e93d3129fe48390ee12fe'}>,\n",
              "  <Document: {'content': '1\\n. \\nOn OBM, go to \\nAdministration\\n > \\nRTSM Administration\\n and click \\nLocal Client\\n.\\n2\\n. \\nIn the \\nLocal Client \\nwindow, based on your operating system, click a link to download the UCMDB Local Client.\\n3\\n. \\nExtract the contents of the UCMDB Local Client zip file (For example, \\nUCMDB_Local_Client_Win\\n) and execute the script.\\n4\\n. \\nIn the UCMDB Local Client window, click \\nAdd\\n. In the \\nAdd/Edit Configuration\\n window, add all the required details: \\n \\n \\nThe required credentials are the same that you use for the Web UI.\\n5\\n. \\nInstall the UD content pack:\\n1\\n. \\nLog in to the UCMDB UI using the UCMDB Local Client. Give the same admin user credentials that you used to log in\\nto OBM.\\n2\\n. \\nGo to \\nAdministration > Package Manager.\\n3\\n. \\nOn the toolbar, click the \\nInstall Content Pack\\n \\n \\nbutton.\\n4\\n. \\nIn the Install Content Pack dialog box that opens, select the version of the content pack, and click \\nInstall\\n. It takes\\nabout 15 minutes to install the content pack\\n6\\n. \\nVerify the installation: Verify that there are no errors in the \\nmam.packaging.log\\n file located at:\\nOn Windows: \\n%TOPAZ_HOME%\\\\ucmdb\\\\\\nruntime\\\\log\\nOn Linux: \\n/opt/HP/BSM/ucmdb\\n/runtime/log\\nVerify that there are no errors in the \\nmam.packaging.log\\n file.\\nOn the UCMDB Local Client, go to \\nData Flow Management > Adapter Management\\n \\nCheck if \\nPulsarPushAdapter\\n is present.\\nyour browser does not support Java Applets.\\nSkip this step if you already have the UCMDB Local Client installed.\\nContainerized Operations Bridge 2022.11\\nPage \\n306\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '127458ca394bb8e465700477ed5aa56e'}>,\n",
              "  <Document: {'content': 'Verify the OBM configuration\\nTo verify the OBM configuration:\\nVerify import of all OPTIC Data Lake certificates\\nEnsure that all OPTIC Data Lake certificates are imported into OBM.\\n1\\n. \\nRun the following command on OBM:\\nOn Linux\\n/opt/OV/bin/ovcert -list\\nOn Windows\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcert\" -list\\nThe command lists all trusted certificates. Make sure that the certificate starting with \\nMF CDF\\n exists in the trusted list.\\nHere\\'s a sample output after running the command:\\nVerify event forwarding\\n1\\n. \\nTo verify the configuration, run the following command on the OBM Gateway server and send a test event to OPTIC Data\\nLake:\\nOn Windows:\\nGo to\\n \\n%TOPAZ_HOME%\\\\opr\\\\support\\n and run the command, \\nsendEvent.bat -j -t TestEvent -s normal\\nHere\\'s a sample output after running the command:\\nNote\\n: The following certificate verification step is valid for on-premises installations only. The certificate names\\nvary in AWS and Azure environments.\\nNote\\n: The certificate name starting with \\nMF CDF\\n must exist for on-premises installations, and this name might\\nvary in cloud deployments such as in AWS, Azure, or OpenShift environments.  In cloud deployments, the\\ncertificate names are dependent on the cloud environments that you\\'re using.\\nNote\\n: The events that occur after you activate the Event Forwarding Rule, flow into OPTIC Data Lake.\\nContainerized Operations Bridge 2022.11\\nPage \\n302\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ace693a6dbc0162ad2f224698d503bc'}>,\n",
              "  <Document: {'content': 'Forward topology from classic OBM to OPTIC Data Lake\\nTopology gets used to populate the Events by CI report and the \"Top event flow by CI\" section of the Event Executive\\nSummary report. It\\'s also required to enable Automatic Event Correlation (AEC) to correlate events across multiple CIs. The\\nsection provides you with the steps to forward topology from classic OBM to OPTIC Data Lake.\\nPrerequisites\\nA secure connection between OBM and OPTIC Data Lake. See \\nConfigure Classic OBM\\n.\\nTask 1: Install the UD content pack\\nThe Topology based correlation requires UD content pack.\\nFollow the steps to install the UD content pack:\\n1\\n. \\nDownload the installation file \\nUD Content Pack 202x.xx 202x.xx.zip\\n from the \\nMarketplace\\n. \\nAlthough there are newer versions available make sure to download\\n UD Content Pack \\n202x.xx 202x.xx.zip\\n by clicking\\non \"See previous releases\" as seen in the pictures below\\n \\n2\\n. \\nExtract the contents of the \\nCP202x_xx_installation.zip\\n. You will find \\nCP202x.xx.xxx.zip\\n.\\n3\\n. \\nCopy the \\nCP202x.xx.xxx.zip \\nfile on to the OBM DPS:\\nOn Windows: \\n%TOPAZ_HOME%\\\\ucmdb\\\\content\\\\content_packs \\nOn Linux: \\n/opt/HP/BSM/ucmdb/content/content_packs \\n4\\n. \\n(Optional) Download and execute the UCMDB Local Client:\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nNote:\\n Skip this page if you have already configured Event Reports as mentioned in the \\nConfigure event reports\\nwith classic OBM\\n section of this document.\\nImportant:\\nSkip this task if you are using classic OBM 2021.05.\\nPerform this task only if you are using the classic OBM versions earlier than 2021.05.\\n\\ue91b\\n\\ue91b\\nNote\\n: \\nThe UCMDB Local Client allows you to launch the RTSM application from your local system. Use this option if\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n305\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cfdcc2593abce8824b0e6fe999caab31'}>,\n",
              "  <Document: {'content': \"Probe Domain Config\\n (shown only if \\nUse Default UCMDB Domain\\n isn't selected in the previous step):\\nData Flow Probe domain type\\n: You can leave this on the default option \\nCustomer.\\n \\nData Flow Probe domain\\n: 'DefaultDomain' is the default value. You can keep the default value or change\\nit.\\nProbe Working Mode\\n (Windows only): You may leave this to the default option \\nNo\\n.\\nProbe Memory Size\\n: You may leave this to the default option.\\nAccount Configuration\\n: Create the following passwords:\\nConfigure PostgreSQL Data Flow Probe Account\\n: The Data Flow Probe uses this password to connect to\\nthe embedded PostgreSQL database. The password must contain 8 to 16 characters and must include at least\\none of each of the following four types of characters:\\nUppercase alphabetical characters\\nLowercase alphabetical characters\\nNumeric characters\\nSpecial characters: : / . _ + - [ ]\\nConfigure PostgreSQL Root Account\\n:  You require this password to perform administrative tasks on the\\nembedded PostgreSQL database. \\nSystem Administration (sysadmin)\\n:  This password is for the system administrator (sysadmin) account. \\nSet Up Keystore Password\\n: This password is for the keystore.\\nSet Up Truststore Password\\n: This password is for the truststore.\\nContainerized Operations Bridge 2022.11\\nPage \\n309\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4201821a33793441a95ca67633c414f6'}>,\n",
              "  <Document: {'content': 'Task 2: Download and install the Data flow probe\\nFollow the steps:\\nTask A: Verify the version of RTSM\\nOBM on Linux:\\n1\\n. \\nGo to \\n/opt/HP/BSM/ucmdb\\n2\\n. \\nRun the command: \\ncat version.dat\\n. The version gets displayed.\\nOBM on Windows:\\n1\\n. \\nGo to \\n\\\\HPBSM\\\\ucmdb\\n2\\n. \\nOpen the \\nversion.dat \\nto check the version.\\nTask B: Download the Data Flow Probe installation file\\nThe Data Flow Probe (DFP) installation\\n .zip\\n is a part of the OBM package and it contains installation files for Linux and\\nWindows.\\nDownload the Data Flow Probe installation file from \\nMicro Focus Software Licenses and Downloads\\n. The filename would be\\nOBM_202x.xx_DataFlowProbe.zip\\n. You may get the DFP from \\nhere\\n.\\nExtract the contents and go to:\\nOn Linux:\\n \\nSoftware\\\\Linux\\\\UCMDB_DataFlowProbe-<version>.bin\\nOn Windows: \\nSoftware/Windows/UCMDB_DataFlowProbe-<version>.exe\\nTask C: Install the Data Flow Probe\\nYou can install the Data Flow Probe anywhere but you should establish a connection between the Data Flow Probe and\\nthe OBM gateway server. You can install multiple Data Flow Probes, one on each server. Each Data Flow Probe can connect\\nonly to one OBM gateway server.\\nFollow the steps:\\n1\\n. \\nExecute the installation file:\\nOn Linux:\\n  \\nSoftware/Linux/UCMDB_DataFlowProbe_<version>.bin\\nOn Windows\\n: \\nSoftware\\\\Windows\\\\UCMDB_DataFlowProbe_<version>.exe\\nA splash screen appears.\\n2\\n. \\nChoose a language from the drop-down menu and click \\nOK\\n to continue.\\n3\\n. \\nThe installation wizard appears. For every page, complete the necessary information and click \\nNext\\n to go to the next\\npage. On the last page click \\nInstall\\n.\\nNote\\n:\\n If you are installing the Content Pack on a High Availability system, copy all files on the active UCMDB\\nserver to the same directory on the passive UCMDB server:\\nOn Windows: \\n%TOPAZ_HOME%\\\\runtime\\\\fcmdb\\\\CodeBase\\nOn Linux: \\n/opt/HP/BSM/ucmdb/runtime/fcmdb/CodeBase\\n\\ue916\\n\\ue916\\nImportant: \\nThe version of the Data Flow Probe (DFP) must be the same as the RTSM version\\n.\\nNote\\n:  To install the Data Flow Probe on Red Hat Linux 7, you must first create a symbolic link from \\nlibsasl2.so.3.0.0 \\nto libsasl2.so.2\\n before installing the Data Flow Probe.\\nFor example:\\ncd /usr/lib64\\nln -s ./libsasl2.so.3.0.0 ./libsasl2.so.2\\nNote\\n: To execute the installer, you require a system with GUI. On Linux, enable the \\nX11\\n server display\\nmode.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n307\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '766f578e7f49593fb8ebd0664492f42f'}>,\n",
              "  <Document: {'content': \" \\n       \\nIntroduction\\n: This shows the welcome screen.\\nLicense Agreement\\n: This shows the License Agreement for using the Data Flow Probe. Select \\nI accept the terms\\nof the License Agreement \\nand click \\nNext.\\nSetup Type \\n(For Windows only): Leave the selection on the default option \\nFull Data Flow Probe Installation\\n.\\nInstallation type\\n: Choose \\nNew Installation \\nand click\\n Next.\\nSelect Installation Folder\\n: To choose a different installation path, you can either enter the path manually in the\\ntext field or click \\nChoose…\\n (to open the file explorer). If you want to restore the default installation path,\\nclick \\nRestore Default Folder\\n.\\nProbe Configuration\\n: To connect the Data Flow Probe to OBM:\\n   \\nUnder \\nApplication to report to\\n, select \\nBSM.\\nIn the \\nApplication Server address\\n box, enter the FQDN of the OBM gateway server. If you use a load\\nbalancer, this should be the FQDN of the load balancer.\\nIn the \\nData Flow Probe address box\\n, enter the IP address or FQDN of the machine on which the Data Flow\\nProbe is currently installed, or accept the default.\\nClick \\nNext.\\nIn the \\nCustomer ID \\nbox, leave the value to default (The default value is 1.) and click \\nNext.\\nIs Basic Authentication enabled for the UCMDB server? - \\nSelect \\nNo \\nand click\\n Next.\\nData Flow Probe identifier - \\n This is the display name of the Data Flow Probe. This name is visible in\\nthe OBM GUI and helped to identify it. If you select \\nUse Default UCMDB Domain\\n, the Data Flow Probe gets\\ncreated under the 'DefaultDomain' domain. If you don't select \\nUse Default UCMDB Domain\\n, you must create\\na custom domain name in the next step under \\nProbe Domain Config\\n.\\nNote\\n: If you install the Data Flow Probe on an OBM gateway server, make sure to choose an\\ninstallation path different from the OBM installation path.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n308\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4516d1c712b0cd67feffde7af781af46'}>,\n",
              "  <Document: {'content': 'Scan File Upload Config\\n: Create the username (You may leave this to the default option) and password for\\nuploading scan files.\\nPre-Installation Summary\\n: Displays the product name (always: UCMDB Data Flow Probe), the installation folder,\\nand disk space information. Click \\nInstall\\n to start the installation.\\nSelect one of the following options:\\nHigh. Enable CA certificate validation with CRL check.\\nMedium. Enable CA certificate validation without a CRL check.\\nLow. Check for the existence of a certificate.\\nIf you select the \\nHigh\\n or \\nMedium\\n option, you will get the following error: \\'Invalid UCMDB server certificate\\ndetected!\\' Click \\nIgnore\\n.\\nInstalling…\\n: Shows the progress of the installation.\\nYou may choose to remove or keep the Users group access privilege from the Probe installation folder and click\\nNext\\n.\\nYou will get a confirmation message. Click \\nDone\\n to exit the installer wizard.\\nYou can find the install logs at: \\n<DFP install folder>/UninstallerData/Logs/UCMDB_Data_Flow_Probe_Install_<date and time>.lo\\ng\\n4\\n. \\n For Linux only: After the installation, open the \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n file. Verify that the \\nappilog.agent.probe.integrationsOnlyProbe\\n parameter gets set to \\nfalse\\n.\\n5\\n. \\nIf OBM isn\\'t using TLS, edit the \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n file and change the \\nappilog.agent.pro\\nbe.protocol\\n to HTTP\\n6\\n. \\nIf you enable TLS in OBM, enable TLS in the Data Flow Probe:\\n1\\n. \\nOpen the file \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n.\\n2\\n. \\nChange the property \\nserverPortHttps\\n from 8443 to 443.\\n7\\n. \\nIf you enable TLS in OBM, import the corresponding certificates to the \\nJava Keystore\\n of the Data Flow Probe: \\n1\\n. \\nTo get the certificate, run the following command on the OBM system:\\nOBM on Windows: \\n%TOPAZ_HOME%\\\\bin\\\\opr-cert-mgmt.bat -export \"OBM Webserver CA Certificate\" PEM \"C:\\\\certificate.pem\"\\nOBM on Linux:\\necho | openssl s_client -showcerts -servername ${HOSTNAME} -connect ${HOSTNAME}:443 2>/dev/null | sed -n \\'/-----BEGIN CERTIFICATE\\n-----/,/-----END CERTIFICATE-----/p\\' > /root/certificate.pem\\nImportant\\n: \\n{HOSTNAME}\\n would be the FQDN of the OBM server. \\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n310\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd4ccd7e1bca054616febad4a53721415'}>,\n",
              "  <Document: {'content': \"2\\n. \\nIf OBM and DFP are on separate systems, copy \\ncertificate.pem \\nto the Data Flow Probe system.\\nFor example: \\nOBM on Linux:\\n \\nscp /root/certificate.pem <DFP Node>:/root\\n \\n3\\n. \\nRun the following command to import the OBM certificate to the Data Flow Probe (DFP) system:\\nOn Linux:\\n/opt/UCMDB/DataFlowProbe/bin/jre/bin/keytool -import -trustcacerts -file /root/certificate.pem -alias obmcert -keystore  /opt/UCMDB/DataF\\nlowProbe/bin/jre/lib/security/cacerts  \\n  \\nOn Windows:\\nC:\\\\UCMDB\\\\DataFlowProbe\\\\bin\\\\jre\\\\bin\\\\keytool.exe -import -trustcacerts -file C:\\\\certificate.pem -alias obm_smperfqa02 -keystore  C:\\\\UCMD\\nB\\\\DataFlowProbe\\\\bin\\\\jre\\\\lib\\\\security\\\\cacerts \\n \\nTask D: Restart the Data Flow Probe\\nFollow the steps:\\nOn Windows:\\n1\\n. \\nGo to \\n<Data Flow Probe install folder>/bin\\n2\\n. \\nRun \\ngateway.bat.\\nFor example: \\n<Data Flow Probe install folder>/bin/gateway.bat restart\\nStarts the Data Flow Probe on a Windows server.\\nOr\\nUse the stop/start menu entries.\\nOn  Linux:\\n1\\n. \\nGo to\\n /opt/UCMDB/DataFlowProbe/bin\\n2\\n. \\nRun \\nProbeGateway.sh\\n with the argument restart.\\nFor example: \\n/opt/UCMDB/DataFlowProbe/bin/ProbeGateway.sh\\n \\nrestart\\nStarts the Data Flow Probe on a Linux server.\\nTask E: Verify the Data Flow Probe Connection\\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Data Flow Probe Setup\\nIf the Data Flow Probe is correctly connected, the domain for which it's created is visible under the \\nDomains and Probes\\n root\\nnode. For example, \\nDefaultDomain (Default)\\n. Under this domain, you will find two nodes: \\nCredentials\\n and \\nData Flow\\nProbes\\n.\\nUnder \\nData Flow Probe\\n, the newly created Data Flow Probe with the given Data Flow Probe name gets displayed.\\nVerify that the status of the probe is 'Connected'.\\nTask 3: Configure topology streaming from RTSM to OPTIC Data Lake \\nFollow the steps to forward topology from OBM's RTSM to OPTIC Data Lake:\\nNote\\n: Set the password to \\nchangeit\\n and when prompted type \\nyes\\n to trust the certificate.\\n\\ue916\\n\\ue916\\nNote\\n:\\n Make sure you use the Internet Explorer browser or the UCMDB Local Client.\\n\\ue916\\n\\ue916\\nImportant:\\nBy default, DFP tries to start on port 80. If DFP and OBM (OBM isn't using TLS) are on the same machine, OBM\\nmay occupy port 80 and you may not see the newly created Data Flow Probe. In such a case, go to \\n<Data Flow Prob\\ne install folder>/conf/DataFlowProbe.properties\\n  and set the value of  \\nappilog.agent.callhome.port\\n to an available port (For\\nexample: \\nappilog.agent.callhome.port = 8081\\n) and restart Data Flow Probe.\\n\\ue91b\\n\\ue91b\\nImportant:\\nUse the Internet Explorer browser or the UCMDB Local Client.\\nTopology integration from an external UCMDB instance into OPTIC DL is currently not supported, but only\\nthrough (classic) OBM/RTSM.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n311\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4352b240f4d2dcfd10b2c3f8fe35d22f'}>,\n",
              "  <Document: {'content': '1\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Adapter Management >\\nPulsar Push Adapter > Configuration Files > PulsarPushAdapter/settings.json \\nIf you are using the UCMDB Local Client go to \\nData Flow Management > Adapter Management > Pulsar Push\\nAdapter > Configuration Files > PulsarPushAdapter/settings.json.\\n2\\n. \\nTo check if you have configured the Pulsar Push Adapter, change the value of \"\\ntest.connection.topic\\n\" to\\n\"persistent://public/default/mf_shared_cmdb_entity_configuration_item_raw\\n\":\\n3\\n. \\nOn OBM Data Processing Server (DPS), run the following to create a client certificate signed by OBM CA. This certificate\\nenables secure communication between Data Flow Probe and OPTIC Data Lake.\\nOn Linux:\\nrm -f /tmp/dfp_client*\\ncd /opt/OV/bin/\\n./ovcm -issue -file /tmp/dfp_client.p12 -name \"UCMDB Data Flow Probe\" -coreid \"itom-cms-dataflowprobe\" -pass \"dataflowprobe\"\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.crt -nokeys -passin pass:dataflowprobe\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.key -nocerts -passin pass:dataflowprobe -nodes\\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in /tmp/dfp_client.key -out /tmp/dfp_client-key-pk8.pem -nocrypt\\nOn Windows:\\n1\\n. \\nGo to \\nC:\\\\Program Files\\\\HP\\\\HP BTO Software\\\\bin\\\\win64\\n2\\n. \\nRun the following command: \\novcm -issue -file C:\\\\dfp_client.p12 -name \"UCMDB Data Flow Probe\" -coreid \"itom-cms-dataflowprobe\" -pass \"dataflowprobe\"\\n3\\n. \\nManually copy \\ndfp_client.p12\\n certificate from OBM to the \\n/tmp\\n directory on the master (control plane) node.\\n4\\n. \\nLogin into the master (control plane) node and go to the \\n/tmp\\n directory \\n5\\n. \\nRun the following commands: \\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.crt -nokeys -passin pass:dataflowprobe\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.key -nocerts -passin pass:dataflowprobe -nodes\\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in /tmp/dfp_client.key -out /tmp/dfp_client-key-pk8.pem -nocrypt\\n6\\n. \\nManually copy \\ndfp_client.crt\\n, \\ndfp_client-key-pk8.pem\\n certificates from the master (control plane) node to the OBM.\\n4\\n. \\nIf OBM and DFP are on different systems, copy the generated files \\ndfp_client.crt\\n and \\ndfp_client-key-pk8.pem\\n to the DFP\\nmachine and use them to configure the probe. \\n5\\n. \\nCopy the \\nissue_ca.crt\\n to the DFP machine and use it to configure the probe (\\nissue_ca.crt\\n is a part of the OBM integration\\ntools that you procured earlier. See \\nConfigure Classic OBM\\n).\\n6\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Integration Studio \\nto create\\nan integration point.\\nIf you are using the UCMDB Local client, go to \\nData Flow Management > Integration Studio\\n to create an integration\\npoint.\\n1\\n. \\nClick  \\n \\n. The \\nNew Integration Point\\n window appears.\\nThis Pulsar topic exists in OPTIC Data Lake. You will use this later to test the communication from RTSM to\\nPulsar.\\nContainerized Operations Bridge 2022.11\\nPage \\n312\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '95732c78818f15217074941818af9d48'}>,\n",
              "  <Document: {'content': \"Related topics\\nFor more information about the UD content pack installation, see \\nContent Pack Installation\\n.\\nTo customize the data retention, see \\nCustomize data retention\\n.\\nNote:\\nFor every full or delta synchronization, the CI details including attribute changes are sent from RTSM to OPTIC\\nData Lake.\\nIf a CI gets deleted from RTSM, the status of the CI gets updated as 'deleted' but the CI isn't deleted from\\nOPTIC Data Lake. \\nContainerized Operations Bridge 2022.11\\nPage \\n315\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '89cc5437de8858046d0e558a3e27f3e0'}>,\n",
              "  <Document: {'content': 'Add the following:       \\nIntegration Name\\n: Enter a name. For example: RTSM_COSO_Topology_Streaming\\nAdapter\\n: Click \\n \\nSelect Adapter\\n, you will see a list of all adapters. Select \\nPulsar Push Adapter\\n.       \\nIs Integration Activated\\n: Select the check box      \\nHostname/IP\\n: Add hostname. Note that hostname must be in the following format: \\npulsar+ssl://<externalAccessHo\\nst>:31051\\nContainerized Operations Bridge 2022.11\\nPage \\n313\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd1e96b7bada11a530be76b992f0f549d'}>,\n",
              "  <Document: {'content': \"Credentials ID\\n: Click  \\n \\n \\nCreate New Credentials\\n. Choose Credentials window appears. Click \\n \\n and add\\nthe following:    \\nProtocol\\n: Choose \\nTWO-WAY-SSL\\n   \\nTrust Cert File Path\\n: Enter the path of the \\nissue_ca.crt \\n(\\nissue_ca.crt\\n is a part of the OBM integration tools\\nthat you procured earlier. See \\nConfigure Classic OBM\\n.)\\nTLS Cert File Path\\n: Enter the path of the \\ndfp_client.crt \\n(You generated the \\ndfp_client.crt\\n in step 3.)\\nTLS Key File Path\\n: Enter the path of the \\ndfp_client-key-pk8.pem \\n(You generated the \\ndfp_client-key-pk8.pem\\n in\\nstep 3.)\\nClick \\nOK.\\n  \\nClick \\nOK\\nData Source Id\\n: Enter an Id. Default Id is UCMDB.           \\nData Flow Probe\\n: Select the name of the Data Flow Probe.       \\n2\\n. \\nClick \\nTest Connection\\n. Here you are testing access to the\\n mf_shared_cmdb_entity_configuration_item_raw\\n  that you\\nspecified earlier. If the connection is successful, click \\nOk\\n. If the connection isn't successful, verify the aforementioned\\nsteps, and test again.\\n7\\n. \\nIn the Integration studio, select the \\nIntegration Point \\n(Pulsar Push Job) created in OBM and click \\n \\nFull\\nSynchronization\\n.\\n8\\n. \\nClick \\nRefresh\\n to see the status of the topology synchronization.\\n9\\n. \\nFollow the steps to schedule delta synchronization:\\n1\\n. \\nIn the Integration studio, select the Integration Point (Pulsar Push Job) and click \\nEdit\\n.\\n2\\n. \\nIn the \\nEdit Integration Job \\nwindow, go to the \\nDelta Synchronization \\ntab, and select the \\nScheduler enabled\\ncheck box.\\n3\\n. \\nSelect a specific interval of your choice and click \\nOK\\n.\\n4\\n. \\nClick \\nRefresh.\\nImportant:\\n In cloud deployments that use private domain names, set the port to 6651.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n314\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '908551683115f72f7c3e98c0c0c345d7'}>,\n",
              "  <Document: {'content': 'Verify event forwarding and AEC on a classic OBM\\n \\nVerify event forwarding\\n1\\n. \\nThe events that occur after you activate the Event Forwarding Rule flow in to OPTIC Data Lake. To verify the\\nconfiguration, run the command on the OBM gateway server to send a test event to OPTIC Data Lake:\\nOn Windows\\n:\\nGo to\\n \\n%TOPAZ_HOME%\\\\opr\\\\support\\nRun the command: \\nsendEvent.bat -j -t TestEvent -s normal\\nSample output:\\n \\nOn Linux\\n:\\nGo to \\n/opt/HP/BSM/opr/support\\nRun the command:\\n ./sendEvent.sh -j -t TestEvent -s normal\\n2\\n. \\nGo to \\nWorkspaces > Operations Console > Event Perspective. \\nCheck if the test event gets listed and verify if the\\nState\\n of the event is \\nForwarded.\\n3\\n. \\nYou can also check the \\nopr_event\\n table in the \\nmf_shared_provider_default \\nschema to verify if the event has reached\\nOPTIC Data Lake.\\nOn a system that has the vsql command, such as a Vertica node, run the command:\\n/opt/vertica/bin/vsql -U dbadmin -c \"select node_hint,title,timestamp from mf_shared_provider_default.opr_event where title ilike \\'testEvent\\' lim\\nit 10;\"\\nYou are prompted to enter the password for the \\ndbadmin\\n user. You can specify a different user such as the \\n<vertica_rouser>\\n \\nyou created in the \\nPrepare Vertica database\\n section.\\nVerify AEC\\nIt can take up to 5 minutes for the configuration to be applied. Wait for 5 minutes before verifying the AEC configuration.\\n1\\n. \\nRun the command on the OBM gateway server to send a test event to OPTIC Data Lake:\\nOn Windows:\\n\"%TOPAZ_HOME%\\\\opr\\\\support\\\\sendEvent.bat\" -j -t \"Test Start\" -eh AutoCorrelationTest:Start -nx second -t \"Test End\" -eh AutoCorrelationTest:\\nEnd\\nSample output:\\nOn Linux:\\n /opt/HP/BSM/opr/support/sendEvent.sh -j -t \"Test Start\" -eh AutoCorrelationTest:Start -nx second -t \"Test End\" -eh AutoCorrelationTest:End\\n2\\n. \\nOn OBM, go to \\nWorkspaces > Operations Console > Event Perspective. \\nCheck the OBM event browser after some\\ntime for a new event with the title: “Automatically Correlated Event:: …”. If the event is visible in the browser, it indicates\\nthat you have configured Automatic Event Correlation correctly. \\nSample:\\nContainerized Operations Bridge 2022.11\\nPage \\n316\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e680e4b44f815bdf74116c55c099c183'}>,\n",
              "  <Document: {'content': 'Configure containerized OBM\\nFollow the steps to configure containerized OBM to forward events and topology:\\n1\\n. \\nConfigure a secure connection between containerized OBM and OPTIC Data Lake\\n2\\n. \\nConfigure event forwarding from containerized OBM\\n3\\n. \\nForward topology from containerized OBM to OPTIC Data Lake\\n4\\n. \\nVerify AEC on a containerized OBM\\nContainerized Operations Bridge 2022.11\\nPage \\n317\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '904790c88450f7b1fd633a0f1efcf9df'}>,\n",
              "  <Document: {'content': \"Configure a secure connection between containerized\\nOBM and OPTIC Data Lake\\n\\u200bThis section provides you with the steps to use the OBM Integration tools to configure a secure connection between\\ncontainerized OBM and OPTIC Data Lake.\\nTask 1: Get suite files\\nThe OBM Integration tools simplify the process of establishing a secure connection between OBM and OPTIC Data Lake. These\\ntools are also required to configure the \\nOPTIC Reporting\\n and the \\nAutomatic Event Correlation\\n capabilities. Make sure\\nyou save these tools.\\n \\n1\\n. \\nOn the suite master system, in the \\nintegration-tools\\n directory, execute the following command:\\n./get-suite-files.sh\\nThe \\nget-suite-files.sh\\n command creates a zip file called \\nobm-integration-files.zip \\nand extracts the following files from the \\nobm-i\\nntegration-files.zip\\n: \\nClose_Cause_if_Symptoms_Closed-<version>.zip - \\nAn OBM content pack used to automatically close cause events when all\\nsymptoms get closed.\\nCOSO_Data_Lake_AEC_Integration_CP-<version>.zip\\n - An OBM content pack used to configure event correlation on OPTIC\\nData Lake.\\nCOSO_Data_Lake_Event_Integration_CP<version>.zip\\n - An OBM content pack used to configure event forwarding to OPTIC\\nData Lake.\\nidl_config.sh\\n - A helper script to import certificates into OPTIC Data Lake.\\nissue_ca.crt\\n - Certificate bundle to establish trust between OPTIC Data Lake and other systems.\\nopr-config-idl.bat\\n - A helper script to import certificates into OBM on Windows.\\nopr-config-idl.sh\\n - A helper script to import certificates into OBM on Linux.\\nSample output \\n[root@ObmDoc integration-tools]# ./get-suite-files.sh\\ninfo: Logfile: /tmp/get-suite-files-2021-10-11-16-27-37.log\\ninfo: Namespace: opsb-helm\\ninfo: Getting files from the suite ...\\ninfo: Fetching suite certificates ...\\ninfo: Copying file issue_ca.crt from container idm to /root/tools2/obm-integration-files/issue_ca.crt ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied suite certificates\\ninfo: Fetching IDL configuration script ...\\ninfo: Name of the file: /artifacts/idl-config-1.5.0.zip\\ninfo: Copying file idl-config-1.5.0.zip from container itom-static-files-provider to /root/tools2/obm-integration-files/idl-config-1.5.0.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully fetched IDL configuration script\\ninfo: Fetching OBM IDL configuration script ...\\ninfo: Name of the file: /artifacts/opr-config-idl-11.20.004.010.zip\\ninfo: Copying file opr-config-idl-11.20.004.010.zip from container itom-static-files-provider to /root/tools2/obm-integration-files/opr-config-idl-11.20.004.010.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully fetched OBM configuration script\\ninfo: Fetching 'COSO_Data_Lake_Event_Integration' ...\\ninfo: Name of the file: /artifacts/COSO_Data_Lake_Event_Integration_CP-3.01.zip\\ninfo: Copying file COSO_Data_Lake_Event_Integration_CP-3.01.zip from container itom-static-files-provider to /root/tools2/obm-integration-files/COSO_Data_Lake_Event_Integration_CP-3.01.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied 'COSO_Data_Lake_Event_Integration'\\ninfo: Fetching 'COSO_Data_Lake_AEC_Integration_CP' ...\\ninfo: Name of the file: /artifacts/COSO_Data_Lake_AEC_Integration_CP-3.04.zip\\ninfo: Copying file COSO_Data_Lake_AEC_Integration_CP-3.04.zip from container itom-static-files-provider to /root/tools2/obm-integration-files/COSO_Data_Lake_AEC_Integration_CP-3.04.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied 'COSO_Data_Lake_AEC_Integration_CP'\\ninfo: Fetching 'Close_Cause_if_Symptoms_Closed' ...\\ninfo: Name of the file: /artifacts/Close_Cause_if_Symptoms_Closed-01.20.zip\\ninfo: Copying file Close_Cause_if_Symptoms_Closed-01.20.zip from container itom-static-files-provider to /root/tools2/obm-integration-files/Close_Cause_if_Symptoms_Closed-01.20.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied 'Close_Cause_if_Symptoms_Closed'\\ninfo: Creating files package ...\\ninfo: Creating package of files ...\\n/root/tools2\\ninfo: Successfully packaged files into ./obm-integration-files.zip\\nList of fetched files:\\nClose_Cause_if_Symptoms_Closed-01.20.zip\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nNote:\\n Containerized OBM is the OBM capability that's enabled in your containerized deployment.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n318\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c7e4655429d894a6e9fc849eabe49344'}>,\n",
              "  <Document: {'content': 'customAttributeFromCustomAttribute(\"CA Name 1\", \"CA Name 01\", \"Default value\")\\nIf the value of the custom attribute is obtained through a groovy closure applied on an OBM event and the closure\\nreturns a string, use the following syntax:\\ncustomAttributeFromClosure(\"<CA name that will appear in opr_event>\", <groovy closure which will be applied to the OBM event and must return a string>)\\nExample\\n:\\ncustomAttributeFromClosure(\"time of event creation\", { event -> event.timeCreated.toString() })\\n \\nThe following example shows how CMAs can be added to the \\nCOSO Data Lake Event Forwarding\\n script:\\n \\n\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\nIn the above example, the CMAs get logged in the following order: \\n \\nca01_name = \"Business unit value\", ca01_value = <value of the CMA Business unit value. By default, the value if Null>\\nca02_name = \"Location_opticDL\", ca02_value = <value of the location. By default, the value is set to None in the script>\\nca03_name = \"Severity of the event\", ca03_value = <value of the CMA Severity Level. For example, Normal/Minor/Major/Critical>\\nca04_name = \"Time of event creation\", ca04_value = <value of the time of event creation. For example, Fri Feb 04 03:11:18 PST 2022>\\nNote:\\n In the OPR event table the CMAs gets logged in the same order as given in the \\nCOSO Data Lake\\nEvent Forwarding\\n Script.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n321\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e2ef31fc4ff2f772b028ee492bd2eb5a'}>,\n",
              "  <Document: {'content': 'Installed Component:\\n    Reporting\\n    Event Analytics\\n    Stakeholder Dashboard\\nDatabase Used:\\n    External Vertica, Host:  vdb.mambo.net\\n    Internal Postgres for Suite (BVD/IDM/Autopass) (EVALUATION PURPOSE ONLY)\\nConfiguration:\\n    Agent Metric Collection is disabled.\\nShown below are important URLs for you:\\nReporting UI (Business Value Dashboard):\\n    https://obmdoc.mambo.net:443/bvd\\nLicense Management UI:\\n    https://obmdoc.mambo.net:443/autopass\\nUser Management UI:\\n    https://obmdoc.mambo.net:443/idm-admin\\nIMPORTANT: If you perform a Helm upgrade in the future using *the same* chart, you MUST use the --reuse-values flag, otherwise the configured trust will be lost. A backup YAML has been created in /var/tmp/idl_config_log/external_certs_backup.yaml\\n(Optional)\\n Enable forwarding of custom message attributes\\nYou can add the Custom Message Attributes (CMAs) to the forwarding script to ensure that all events that arrive to OPTIC DL\\nhave these CMAs defined. They appear in opr_event as key-value pairs under columns (caXX_name,caXX_value). You can add\\n50 CMAs to the forwarding script and any additional CMAs are ignored.\\nGuidelines for CMA names and maintaining order of CMAs\\nCMA names are case sensitive, can contain alphanumeric characters, and the following characters: _-+=[]{};:\\'/?\\n<>,.äÄéöÖüÜß()`~!@#%^&* \\nIf you want to use $ in the CMA name, you must escape it with a backslash. For example, CA Name \\\\$1.\\nOnce you specify the CMAs, their order in the opr_event table is fixed.\\nIf you want to add a new CMA, always append it to the end of the list.\\nIf you remove a CMA or add it in between the other CMAs it will disturb the initial order. Therefore, it\\'s highly\\nrecommended to add new attributes at the end of the list and to avoid deleting attributes.\\nIf the forwarding script has any syntax errors, the events won\\'t be forwarded.\\nFollow these steps to add CMAs to the forwarding script:\\n1\\n. \\nGo to \\nOBM > SETUP AND MAINTENANCE > Connected Servers\\n.\\n2\\n. \\nSelect \\nExternal Event Processing\\n.\\n3\\n. \\nClick \\nManage Scripts\\n. The \\nScript \\npage opens in a new tab.\\n4\\n. \\nSelect \\nCOSO Data Lake Event Forwarding Script\\n and click \\nEdit\\n.\\n5\\n. \\nLocate \\nstatic final List<CustomAttributeProvider> customAttributes = []\\n in the script.\\n6\\n. \\nAdd the names of CMAs as comma separated values enclosed in double quotes, in \\n[]\\n. You have the following options to\\nadd the names:\\nSpecify the names as strings.\\nExample\\n:\\n\"Custom_CA1\"\\nWhen the custom attribute derives its value from an event property like Severity or State, use the following syntax.\\ncustomAttributeFromEventProperty(\"<CA name that will appear in opr_event>\", \"<Event property from which the CA must derive the value>\",\"<Value if the property doesn\\'t exist or isn\\'t defined (optional)>\")\\nExample\\n:\\n\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\u200bcustomAttributeFromEventProperty(\"severity CA\", \"severity\")\\nIf you want the custom attribute name to be different in opr_event table and OBM, use the following syntax:\\n \\ncustomAttributeFromCustomAttribute(\"<CA name that will appear in opr_event>\", \"<CA name that will be used on OBM>\", \"<Default CA value if no value is assigned (optional)>\") \\nExample\\n:\\nContainerized Operations Bridge 2022.11\\nPage \\n320\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'df5a430020a7e7bc85f3cc52d2160cec'}>,\n",
              "  <Document: {'content': 'COSO_Data_Lake_AEC_Integration_CP-3.04.zip\\nCOSO_Data_Lake_Event_Integration_CP-3.01.zip\\nidl_config.sh\\nissue_ca.crt\\nopr-config-idl.bat\\nopr-config-idl.sh\\ninfo: Done\\nPlease find the suite files in: ./obm-integration-files.zip\\nThe files are also available uncompressed in: /root/tools2/obm-integration-files\\nTask 2: Enable OPTIC Data Lake to accept secure connections from OBM\\nFollow the steps:\\n1\\n. \\nOpen a shell in the \\nomi\\n container. Run the following command on the master node: \\nkubectl exec -ti -n $(kubectl get pods -A | awk \\'/omi-0/ {print $1,$2}\\') -c omi -- bash\\n2\\n. \\nRun the command in the \\nomi \\ncontainer of the \\nomi-0\\n pod to export a list of trusted CA certificates (You may choose any\\nfilename for the certificate file):\\n \\n /opt/OV/bin/ovcert -exporttrusted -ovrg server -file /var/tmp/<certificate filename>\\n For example: \\n/opt/OV/bin/ovcert -exporttrusted -ovrg server -file /var/tmp/ca.crt\\n3\\n. \\nExit from the \\nomi-0\\n pod. \\n4\\n. \\nCopy certificate (\\nca.crt)\\n from the \\nomi \\ncontainer to the master (control plane) node \\n(For example: \\n/var/tmp/ca.crt)\\n. On the\\nmaster (control panel) node: \\nkubectl cp $(kubectl get pods -A | awk \\'/omi-0/ {print $1}\\')/omi-0:/var/tmp/ca.crt /var/tmp/ca.crt -c omi\\n5\\n. \\nRun the following commands on the master (control plane) node:\\ncd obm-integration-files\\nchmod +x ./idl_config.sh\\n./idl_config.sh -namespace <provider namespace> -chart <path of provider suite> [-release <release name>] -cacert /var/tmp/<certificate filename>\\nFor example: \\n./idl_config.sh -namespace $(kubectl get pods -A | awk \\'/omi-0/ {print $1}\\') -chart /var/tmp/opsbridge-suite-2020.10.0-128.tgz -cacert /var/tmp/ca.crt\\nThis configures OPTIC Data Lake to trust the OBM CA certificate and restarts the OPTIC Data Lake pods. It also ensures\\nthat OPTIC Data Lake trusts Operations Agents with the OBM CA certificate.\\nNote:\\n If you install more than one release in the environment, you have to specify the Operations Bridge release using\\nthe \\n\\'-release\\'\\n parameter when executing the \\n\\'idl_config.sh\\'\\n tool. Run this command to get a list of all the releases in the\\nenvironment: \\nhelm list --all-namespaces\\nSample output: \\n[root@ObmDoc obm-integration-files]# ./idl_config.sh -namespace opsbridge-helm -chart /var/tmp/opsbridge-suite-2020.08.0-261.tgz -cacert /var/tmp/ca.crt\\n2020-09-24 13:54:47,674 INFO idl_config:main: Logging to /var/tmp/idl_config_log/idl_config.log\\n2020-09-24 13:54:48,041 INFO idl_config:main: Performing Helm upgrade.\\nRelease \"opsb-suite\" has been upgraded. Happy Helming!\\nNAME: opsb-suite\\nLAST DEPLOYED: Thu Sep 24 13:54:59 2020\\nNAMESPACE: opsbridge-helm\\nSTATUS: deployed\\nREVISION: 2\\nTEST SUITE: None\\nNOTES:\\nThank you for deploying Opsbridge Suite 2020.08.0-261\\nBelow are the Installation Summary:\\nNote:\\n For OBM containers hosted on the Operations bridge, you must give the namespace and chart for\\nthe Operations bridge, otherwise give the namespace and chart for Network Operations Management (NOM).\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n319\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '541af99739f138e9678d830d00f37826'}>,\n",
              "  <Document: {'content': 'Verify event forwarding from containerized OBM\\nAutomatic Event Correlation (AEC) analyses events that are forwarded from OBM to OPTIC Data Lake.  It can detect patterns\\nbased on the Node CI ID in the events or based on the topology that\\'s forwarded from OBM to OPTIC Data Lake.\\nFollow the steps to verify the event forwarding from a containerized OBM:\\n1\\n. \\nThe events that occur after you activate the Event Forwarding Rule are sent to OPTIC Data Lake. To verify the\\nconfiguration, run the command on the master (control plane) node to send a test event to OPTIC Data Lake:\\nkubectl exec -it omi-0 -n OpsB-helm -c omi -- bash\\ncd /opt/HP/BSM/opr/support \\n./sendEvent.sh -j -t TestEvent -s normal\\nSample output:\\n2\\n. \\nOn OBM, go to \\nWorkspaces > Operations Console > Event Perspective. \\nCheck that the event is visible in the \\nEvent\\nPerspective\\n and its\\n \\nState\\n is \\nForwarded. \\n3\\n. \\nYou can also check the \\nopr_event\\n table in the \\nmf_shared_provider_default\\n schema to verify if the event has reached\\nOPTIC Data Lake.\\nOn a system that has the vsql command, such as a Vertica node, run the command:\\n/opt/vertica/bin/vsql -U dbadmin -c \"select node_hint,title,timestamp from mf_shared_provider_default.opr_event where title ilike \\'testEvent\\' lim\\nit 10;\"\\nYou are prompted to enter the password for the dbadmin user. You can specify a different user such as the \\n<vertica_rouser\\n> \\nyou created in the \\nPrepare Vertica database\\n.\\nRelated topics\\nTo stream topology, see \\nForward topology from containerized OBM to OPTIC Data Lake\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n322\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ddb12f25e4e95176cc622c963777b2d'}>,\n",
              "  <Document: {'content': '3\\n. \\nThe installation wizard appears. For every page, complete the necessary information and click \\nNext\\n to go to the next\\npage. On the last page click \\nInstall\\n.\\n       \\nIntroduction\\n: Shows the welcome screen.\\nLicense Agreement\\n: Shows the License Agreement for using the Data Flow Probe. Select \\nI accept the terms of\\nthe License Agreement\\n and click \\nNext.\\nSetup Type \\n(For Windows only): Leave the selection on the default option \\nFull Data Flow Probe Installation\\n.\\nInstallation type\\n: Choose \\nNew Installation \\nand click\\n Next.\\nSelect Installation Folder\\n: To choose a different installation path, you can either enter the path manually in the\\ntext field or click \\nChoose…\\n (to open the file explorer). If you want to restore the default installation path,\\nclick \\nRestore Default Folder\\n.\\nProbe Configuration\\n: To connect the Data Flow Probe to OBM:\\n   \\nUnder \\nApplication to report to\\n, select \\nBSM.\\nIn the \\nApplication Server address\\n box, enter the FQDN of the external access host of the suite. If you use a\\nload balancer, this should be the FQDN of the load balancer.\\nContainerized Operations Bridge 2022.11\\nPage \\n324\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ab20fed3a14a3dfbec424bb9e7413cda'}>,\n",
              "  <Document: {'content': 'Forward topology from containerized OBM to OPTIC Data\\nLake\\nTopology gets used to populate the Events by CI report and the \"Top event flow by CI\" section of the Event Executive\\nSummary report. It\\'s also required to enable Automatic Event Correlation (AEC) to correlate events across multiple CIs. The\\nsection provides you with the steps to forward topology from containerized OBM to OPTIC Data Lake: \\nTask 1: Download and install the Data Flow Probe\\nFollow the steps: \\nTask A: Verify the version of RTSM\\nFollow the steps:\\n1\\n. \\nLog in to the UCMDB container:\\nkubectl -n <suite namespace> exec -it <ucmdb pod name> -c itom-ucmdb -- bash\\nFor example: \\nkubectl -n $(kubectl get pods -A | awk \\'/itom-ucmdb-0/ {print $1}\\') exec -it itom-ucmdb-0 -c itom-ucmdb -- bash\\n2\\n. \\nRun the command to view the version of RTSM:\\ncat version.dat\\nThe version gets displayed.\\nTask B: Download the Data Flow Probe installation file\\nThe Data Flow Probe installation .zip is a part of the OBM package and it contains installation files for Linux and Windows.\\nDownload the Data Flow Probe installation file from \\nMicro Focus Software Licenses and Downloads\\n. The filename would be \\nOps\\nB_202x.xx_DataFlowProbe.zip\\n. You may get the DFP from \\nhere\\n.\\nExtract the contents and go to:\\nOn Linux: \\nSoftware/Linux/UCMDB_DataFlowProbe_<version>.bin\\nOn Windows: \\nSoftware\\\\Windows\\\\UCMDB_DataFlowProbe_<version>.exe\\nTask C: Install the Data Flow Probe\\nYou can install the Data Flow Probe anywhere but you should establish a connection between the Data Flow Probe and\\nthe OBM. You can install multiple Data Flow Probes, one on each server. Each Data Flow Probe can connect only to one OBM\\nserver.\\nFollow the steps:\\n1\\n. \\nExecute the installation file:\\nOn Linux:\\n  \\nSoftware/Linux/UCMDB_DataFlowProbe_<version>.bin\\nOn Windows\\n: \\nSoftware\\\\Windows\\\\UCMDB_DataFlowProbe_<version>.exe\\nA splash screen appears.\\n2\\n. \\nChoose a language from the drop-down menu and click \\nOK\\n to continue.\\nNote:\\n Skip this page if you have already configured Event Reports as mentioned in the \\nConfigure event reports\\nwith containerized OBM \\n section of this document.\\nImportant: \\nThe version of the Data Flow Probe (DFP) must be the same as the RTSM version\\n.\\nNote\\n:  To install the Data Flow Probe on Red Hat Linux 7, you must first create a symbolic link from \\nlibsasl2.so.3.0.0 \\nto libsasl2.so.2\\n before installing the Data Flow Probe.\\nFor example:\\ncd /usr/lib64\\nln -s ./libsasl2.so.3.0.0 ./libsasl2.so.2\\nNote\\n: To execute the installer, you require a system with GUI. On Linux, enable the\\n X11\\n server display\\nmode.\\nContainerized Operations Bridge 2022.11\\nPage \\n323\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ee7e982ca0bb9250a46f8c208052f477'}>,\n",
              "  <Document: {'content': \"In the \\nData Flow Probe\\n address box, enter the IP address or FQDN of the machine on which the Data Flow\\nProbe is currently installed.\\nClick \\nNext.\\nIn the \\nCustomer ID \\nbox, leave the value to default (The default value is 1.) and click \\nNext.\\nIs Basic Authentication enables for the UCMDB server? - \\nSelect \\nYes \\nand click\\n Next.\\nIn the \\nBA Username \\nbox, enter\\n \\nUCMDB_BA_1\\n. \\nIn the \\nBA Password\\n box, enter the password.\\nTo obtain the password, execute the following command on the master node:\\nkubectl -n <suite namespace> get secret opsbridge-suite-secret -o jsonpath='{.data.UCMDB_BA_1_PASSWORD}' | base64 --\\ndecode\\nFor example: \\nkubectl -n $(kubectl get pods -A | awk '/itom-ucmdb-0/ {print $1}') get secret opsbridge-suite-secret -o jsonpath='{.data.UCMDB_BA_1_PASSWORD}' | base64 --decode\\nData Flow Probe identifier - \\n This is the display name of the Data Flow Probe. This name is visible in\\nthe OBM GUI and helped to identify it. If you select \\nUse Default UCMDB Domain\\n, the Data Flow Probe gets\\ncreated under the 'DefaultDomain' domain. If you don't select \\nUse Default UCMDB Domain\\n, you must create\\na custom domain name in the next step under \\nProbe Domain Config\\n.\\nProbe Domain Config\\n (shown only if \\nUse Default UCMDB Domain\\n isn't selected in the previous step):\\nData Flow Probe domain type\\n: You can leave this on the default option \\nCustomer.\\n \\nData Flow Probe domain\\n: 'DefaultDomain' is the default value. You can keep the default value or change\\nit.\\nProbe Working Mode (Windows only):  You may leave this to the default option No.\\nProbe Memory Size\\n: You may leave this to the default option.\\nAccount Configuration\\n: Create the following passwords:\\nConfigure PostgreSQL Data Flow Probe Account: The Data Flow Probe uses this password to connect to the\\nembedded PostgreSQL database.  The password must contain 8 to 16 characters and must include at least one\\nof each of the following four types of characters:\\nUppercase alphabetical characters\\nLowercase alphabetical characters\\nNumeric characters\\nSpecial characters: : / . _ + - [ ]\\nConfigure PostgreSQL Root Account:   You require this password to perform administrative tasks on the\\nembedded PostgreSQL database. \\nSystem Administration (sysadmin):  This password is for the system administrator (sysadmin) account. \\nScan File Upload Config: Create the username (You may leave this to the default option.) and password for uploading\\nscan files.\\nPre-Installation Summary\\n: Displays the product name (always: UCMDB Data Flow Probe), the installation folder,\\nand disk space information. Click \\nInstall\\n to start the installation.\\nSelect one of the following options:\\nHigh. Enable CA certificate validation with CRL check.\\nMedium. Enable CA certificate validation without CRL check.\\nLow. Check for existence of a certificate.\\nIf you select the \\nHigh\\n or \\nMedium\\n option, you will get the following error: 'Invalid UCMDB server certificate\\ndetected!' Click \\nIgnore\\n.\\nInstalling…\\n: Shows the progress of the installation.\\nYou may choose to remove or keep the Users group access privilege from the Probe installation folder and click\\nNext\\n.\\nYou will get a confirmation message. Click \\nDone\\n to exit the installer wizard.\\nYou can find the install logs at: \\n<DFP install folder>/UninstallerData/Logs/UCMDB_Data_Flow_Probe_Install_<date and time>.lo\\ng\\nTask D: Configure the Data Flow Probe \\n1\\n. \\nFor Linux only: After the installation, open the \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n file. Verify if the \\napp\\nilog.agent.probe.integrationsOnlyProbe\\n parameter gets set to \\nfalse\\n.\\n2\\n. \\nIf you enable TLS in OBM, enable TLS in the Data Flow Probe:\\nContainerized Operations Bridge 2022.11\\nPage \\n325\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fff0b2b5c3b85393ad1fbc55d8687fcb'}>,\n",
              "  <Document: {'content': \"1\\n. \\nOpen the file \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n.\\n2\\n. \\nChange the property \\nserverPortHttps\\n from 8443 to 443.\\n3\\n. \\nIf you installed the Data Flow Probe on a host running the containerized operations bridge single server evaluation\\ndeployment:\\n1\\n. \\nOpen the file \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n.\\n2\\n. \\nSet \\nappilog.agent.callhome.enabled\\n to false.\\n3\\n. \\nSet \\nappilog.agent.callhome.port\\n to 8081.\\n4\\n. \\nImport the corresponding certificates to the\\n Java Keystore\\n of the Data Flow Probe: \\n1\\n. \\nTo get the certificate and integrate with DFP, run the following command on the master node of the OBM system:\\necho | openssl s_client -showcerts -servername ${HOSTNAME} -connect ${HOSTNAME}:443 2>/dev/null | sed -n '/-----BEGIN CERTIFICATE\\n-----/,/-----END CERTIFICATE-----/p' > /root/certificate.pem\\n2\\n. \\nCopy  \\ncertificate.pem \\nto the Data Flow Probe system.\\nFor example\\n: scp /root/certificate.pem <DFP Node>:/root \\n3\\n. \\nRun the following command to import the OBM certificate to the Data Flow Probe (DFP) system:\\nOn Linux:\\n/opt/UCMDB/DataFlowProbe/bin/jre/bin/keytool -import -trustcacerts -file /root/certificate.pem -alias obmcert -keystore  /opt/UCMDB/DataF\\nlowProbe/bin/jre/lib/security/cacerts  \\n  \\nOn Windows:\\nC:\\\\UCMDB\\\\DataFlowProbe\\\\bin\\\\jre\\\\bin\\\\keytool.exe -import -trustcacerts -file C:\\\\certificate.pem -alias obm_smperfqa02 -keystore  C:\\\\UCMD\\nB\\\\DataFlowProbe\\\\bin\\\\jre\\\\lib\\\\security\\\\cacerts \\n \\nTask E: Restart the Data Flow Probe\\nFollow the steps:\\nOn Windows:\\n1\\n. \\nGo to \\n<Data Flow Probe install folder>/bin\\n2\\n. \\nRun \\ngateway.bat.\\nFor example: \\n<Data Flow Probe install folder>/bin/gateway.bat restart\\nStarts the Data Flow Probe on a Windows server.\\nOr\\nUse the stop/start menu entries.\\nOn  Linux:\\n1\\n. \\nGo to \\n/opt/UCMDB/DataFlowProbe/bin\\n2\\n. \\nRun \\nProbeGateway.sh\\n with the argument restart.\\nFor example: \\n/opt/UCMDB/DataFlowProbe/bin/ProbeGateway.sh restart\\nStarts the Data Flow Probe on a Linux server.\\nTask F: Verify the Data Flow Probe Connection\\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Data Flow Probe Setup\\nIf the Data Flow Probe is correctly connected, the domain for which it's created is visible under the \\nDomains and Probes\\n root\\nnode. For example, \\nDefaultDomain (Default)\\n. Under this domain, you will find two nodes: \\nCredentials\\n and \\nData Flow\\nProbes\\n.\\nUnder \\nData Flow Probe\\n, the newly created Data Flow Probe with the given Data Flow Probe name gets displayed.\\nVerify that the status of the probe is 'Connected'.\\nImportant:\\n{HOSTNAME}\\n would be the FQDN of the external access host.    \\nNote\\n: Set the password to \\nchangeit\\n and when prompted type \\nyes\\n to trust the certificate.\\nNote\\n:\\n Make sure you use the Internet Explorer browser or the UCMDB local client.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n326\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7748408f0b7831f0328d88c2992cd7f4'}>,\n",
              "  <Document: {'content': 'Add the following:       \\nIntegration Name: Enter a name. For example: RTSM_COSO_Topology_Streaming\\nAdapter: Click \\n \\nSelect Adapter\\n, a list of all adapters are displayed. Select \\nPulsar Push Adapter\\n.       \\nIs Integration Activated: Select the check box      \\nHostname/IP: Add hostname. Note that hostname must be in the following format: \\npulsar+ssl://<externalAccessHost\\n>:31051\\nContainerized Operations Bridge 2022.11\\nPage \\n328\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7329120d03cafcd3872446c53c9a3216'}>,\n",
              "  <Document: {'content': \"Credentials ID: Click  \\n \\n \\nCreate New Credentials\\n. Choose Credentials window appears. Click \\n \\n and add the\\nfollowing:    \\nProtocol: Choose \\nTWO-WAY-SSL\\n   \\nTrust Cert File Path: Enter the path of the \\nissue_ca.crt \\n(issue_ca.crt is a part of the OBM integration tools that\\nyou procured earlier. See \\nConfigure a secure connection between containerized OBM and OPTIC Data Lake\\n.)\\nTLS Cert File Path: Enter the path of the \\ndfp_client.crt \\n(You generated the \\ndfp_client.crt\\n in step 3.)\\nTLS Key File Path: Enter the path of the \\ndfp_client-key-pk8.pem \\n(You generated the \\ndfp_client-key-pk8.pem \\nin\\nstep 3.)\\nClick \\nOK.\\n  \\nClick \\nOK\\nData Source Id: Enter an Id. Default Id is UCMDB.         \\nData Flow Probe: Select the name of the Data Flow Probe.       \\n2\\n. \\nClick \\nTest Connection\\n. Here you are testing access to the \\nmf_shared_cmdb_entity_configuration_item_raw\\n that you\\nspecified earlier. If the connection is successful, click \\nOk\\n. If the connection isn't successful, verify the aforementioned\\nsteps, and test again.\\n5\\n. \\nIn the Integration studio, select the Integration Point (Pulsar Push Job) created in OBM and click \\n \\nFull\\nSynchronization\\n.\\n6\\n. \\nClick \\nRefresh\\n to see the status of the topology synchronization. \\n7\\n. \\nFollow the steps to schedule delta synchronization:\\nImportant:\\n In cloud deployments that use private domain names, set the port to 6651.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n329\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '454433f2ba192eb80174ab7484f5c686'}>,\n",
              "  <Document: {'content': \"1\\n. \\nIn the Integration studio, select the Integration Point (Pulsar Push Job) and click \\nEdit\\n.\\n2\\n. \\nIn the \\nEdit Integration Job \\nwindow, in the \\nDelta Synchronization \\ntab, and select the \\nScheduler enabled\\ncheck box.\\n3\\n. \\nSelect a specific interval of your choice and click \\nOK\\n.\\n4\\n. \\nClick \\nRefresh.\\nRelated topics\\nTo enable automatic event correlation, see \\nConfigure Automatic Event Correlation\\n.\\nFor more information about the UD content pack installation, see \\nContent Pack Installation\\n.\\nTo customize the data retention, see \\nCustomize data retention\\n.\\nNote:\\nFor every full or delta synchronization, the CI details including attribute changes are sent from RTSM to OPTIC\\nData Lake.\\nIf a CI gets deleted from RTSM, the status of the CI gets updated as 'deleted' but the CI isn't deleted from\\nOPTIC Data Lake. \\nContainerized Operations Bridge 2022.11\\nPage \\n330\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1556a69b7573f135b0bb62d45e468ea5'}>,\n",
              "  <Document: {'content': 'Task 2: Configure topology streaming from RTSM to OPTIC Data Lake\\nFollow the steps to forward topology from OBM\\'s RTSM to OPTIC Data Lake:\\n1\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Adapter Management >\\nPulsar Push Adapter > Configuration Files > PulsarPushAdapter/settings.json  \\nIf you are using the UCMDB Local Client go to \\nData Flow Management > Adapter Management > Pulsar Push\\nAdapter > Configuration Files > PulsarPushAdapter/settings.json.\\n2\\n. \\nTo check if you have configured the Pulsar Push Adapter, change the value of \"\\ntest.connection.topic\\n\"\\nto \\n\"persistent://public/default/mf_shared_cmdb_entity_configuration_item_raw\":\\n3\\n. \\nOn OBM, run the following to create a client certificate signed by OBM CA. This certificate enables secure communication\\nbetween Data Flow Probe and OPTIC Data Lake.\\nTo access the OBM pod, e.g: \\nkubectl exec -it -n $(kubectl get pods -A | awk \\'/ omi-0 / { print $1}\\') omi-0 -c omi -- bash\\nrm -f /tmp/dfp_client*\\ncd /opt/OV/bin/\\n./ovcm -issue -file /tmp/dfp_client.p12 -name \"UCMDB Data Flow Probe\" -coreid \"itom-cms-dataflowprobe\" -pass \"dataflowprobe\"\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.crt -nokeys -passin pass:dataflowprobe\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.key -nocerts -passin pass:dataflowprobe -nodes\\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in /tmp/dfp_client.key -out /tmp/dfp_client-key-pk8.pem -nocrypt\\n1\\n. \\nOn the master node,  run the following commands to copy the certificate (\\ndfp_client.crt\\n and \\ndfp_client-key-pk8.pem\\n) from\\nOBM to the \\ntmp\\n directory on the master node:\\n \\nkubectl cp omi-0:/tmp/dfp_client.crt /tmp/dfp_client.crt -c omi -n $(kubectl get pods -A | awk \\'/omi-0/ {print $1}\\') \\nkubectl cp omi-0:/tmp/dfp_client-key-pk8.pem /tmp/dfp_client-key-pk8.pem -c omi -n $(kubectl get pods -A | awk \\'/omi-0/ {print $1}\\')\\n2\\n. \\nCopy the generated files \\n/tmp/dfp_client.crt\\n and \\n/tmp/dfp_client-key-pk8.pem\\n to the DFP machine and use them to configure\\nthe probe. \\n3\\n. \\nCopy the \\nissue_ca.crt\\n to the DFP machine and use it to configure the probe (issue_ca.crt is a part of the OBM integration\\ntools that you procured earlier. See \\nConfigure a secure connection between containerized OBM and OPTIC Data Lake\\n). \\n4\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Integration Studio \\nto create\\nan integration point.\\nIf you are using the UCMDB Local client, go to \\nData Flow Management > Integration Studio \\nto create an integration\\npoint.\\n1\\n. \\nClick  \\n \\n. The \\nNew Integration Point\\n window appears.\\nImportant:\\nBy default, DFP tries to start on port 80. If port 80 isn\\'t available, you may not see the newly created Data Flow\\nProbe. In such a case, go to \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n  and set the value of  \\nappilog.a\\ngent.callhome.port\\n to an available port (For example: \\nappilog.agent.callhome.port = 8081\\n) and restart Data Flow Probe.\\nUse the Internet Explorer browser or the UCMDB Local Client.\\nThis Pulsar topic exists in OPTIC Data Lake. You will use this later to test the communication from RTSM to\\nPulsar.\\nContainerized Operations Bridge 2022.11\\nPage \\n327\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2d83866f0f8bd29d8ec486227ef1309e'}>,\n",
              "  <Document: {'content': 'Verify AEC on a containerized OBM\\nIt can take up to 5 minutes for the configuration to take effect. Wait for about 5 minutes before verifying the configuration.\\nFollow the steps:\\n1\\n. \\nRun the command on OBM to send a test event to OPTIC Data Lake:\\nkubectl -n opsbridge-coqre exec -it omi-0 -c omi -- bash\\ncd /opt/HP/BSM/opr/support/\\n./sendEvent.sh -j -t \"Test Start\" -eh AutoCorrelationTest:Start -nx second -t \"Test End\" -eh AutoCorrelationTest:End\\nSample output:\\n2\\n. \\nOn OBM, go to \\nWorkspaces > Operations Console > Event Perspective. \\nCheck the OBM event browser after some\\ntime for a new event with the title: “CORRELATED [2 Evts]: …”. If the event is visible in the browser, the Automatic Event\\nCorrelation configuration is correct. \\nSample:\\nContainerized Operations Bridge 2022.11\\nPage \\n331\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '23878da8258ca0cfff0830dd5a74f651'}>,\n",
              "  <Document: {'content': \"Configure reporting\\nReports consolidate performance metrics, event metrics, and response time data into tables, graphs, and dashboards and\\nenable you to view data from several valuable and unique perspectives. It helps you to predict infrastructure resource\\nutilization, detect problems, and take corrective actions before critical business availability is impacted.\\nReports are a graphical representation of the data stored in OPTIC Data Lake. You can configure Operations Agent, Operations\\nBridge Manager (OBM), SiteScope, Business Process Monitor (BPM), or Real User Monitor (RUM) to send data to OPTIC Data\\nLake.  If you choose Operations Agent, you can use either the Agent Metric Collector or Metric Streaming Policies to send\\nmetrics to OPTIC Data Lake. You can use the data in OPTIC Data Lake to populate reports using the Business Value Dashboard\\n(BVD), or any other Business Intelligence (BI) tool. \\nOperations Bridge offers several reports that are categorized as follows: \\nSystem Infrastructure reports:\\n These reports provide you with information about the availability and performance of\\nphysical systems in your environment. They also provide you with information about the average utilization of resources\\nfor a chosen period of time. \\nSystem Infrastructure\\n \\nreports are available for metrics collected by Operations Agent and SiteScope. \\nYou can use one of the following mechanisms to send Operations Agent data to OPTIC Data Lake:\\nConfigure System Infrastructure Reports using Agent Metric Collector\\nConfigure System Infrastructure Reports using metric streaming policies\\nTo send SiteScope data to OPTIC Data Lake, see \\nConfigure System Infrastructure Reports using SiteScope\\nEvent reports:\\n These reports display the statistics about the events that are sent to OBM from various data sources. You\\ncan view the information about event trends, the severity of events, and the number of events fixed by users and user\\ngroups. \\nTo populate the event reports in BVD, you must configure OBM to forward events and topology to OPTIC Data Lake.\\nPerform this task to forward the events and topology from an OBM server to OPTIC Data Lake:\\nIf you have chosen classic OBM: \\nConfigure event reports with classic OBM\\nIf you have chosen containerized OBM: \\nConfigure event reports with containerized OBM\\nSynthetic Transaction reports:\\n These reports provide you with information about end-user experience, availability,\\nand performance of applications by running synthetic transactions. Currently, there are no out of the box reports. You can\\nuse the data in OPTIC Data Lake to generate reports using either BVD or any other Business Intelligence tools of your\\nchoice. \\nFor information about configuring and viewing the reports, see \\nConfigure synthetic transaction reports using BPM\\n. \\nReal User Monitor reports:\\n \\nThese reports provide you with information about end-user experience, availability, and\\nperformance of applications by monitoring real user traffic. \\nFor information about configuring and viewing the reports, see \\nConfigure real user monitoring reports using RUM\\n.\\nNote: \\nThe BVD reports use the following Google fonts:\\nRoboto Light\\nRoboto\\nRoboto Black\\nIf your client browser does not have Internet access, then it substitutes with some other locally available font,\\nwhich may cause issues like text overlapping. \\nYou can install the Google fonts on the client's desktop or you can put the fonts on a central web server that is\\naccessible to the client, such as the \\nOpsBridge suite's nginx web server\\n.\\nTo configure BVD to use a local web server, see 'Use Custom Fonts in Your Dashboards' on the \\n'Modify the\\nsettings'\\n page. \\n \\nContainerized Operations Bridge 2022.11\\nPage \\n332\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '50893feec39c961aecc45f010b88aa91'}>,\n",
              "  <Document: {'content': \"Validate the connection between BVD and OPTIC Data\\nLake\\nDuring the Operations Bridge suite deployment, Business Value Dashboard (BVD) is automatically configured with a connection\\nto the OPTIC Data Lake Vertica database.\\nFollow these steps to validate the connection between BVD and OPTIC Data Lake:\\n1\\n. \\nLog on to BVD:\\nEnter the following address on a browser to access BVD:\\nhttps://<externalAccessHost>:<port>/bvd\\nThe user name is \\nadmin\\nThe password is what you entered as the admin password when installing CDF.\\nBy default \\n<port>\\n is 443\\n2\\n. \\nOn the title bar, click \\n \\nAdministration\\n and then click \\nData Collectors\\n. The Data Collectors page appears.\\n3\\n. \\nClick  \\n \\n and then click \\nDB connection settings\\n. \\n4\\n. \\nValidate if the following are aligned to your Vertica database settings:\\nHost name: You can connect BVD to either a single Vertica host or a Vertica cluster. If you want to connect BVD to a\\nVertica cluster, enter the host names as a comma separated list. This ensures that BVD picks a live node from the\\ncluster. \\nIf you are using an embedded Vertica, use \\nitom-di-vertica-svc\\n.\\nPort: You set the Vertica port in the \\nvertica.port\\n parameter in the \\nvalues.yaml\\n file during suite installation.  The default\\nport is 5433.\\nSecurity: The \\nEnable TLS for secure communication\\n check box gets cleared if you set the \\nvertica.tlsEnabled\\nparameter to \\nfalse\\n in the \\nvalues.yaml\\n file during the suite installation.  The default is that TLS is enabled. See the\\nsection 'Vertica' in the \\nConfigure Values.yaml\\n page.\\nDatabase name: You set the Vertica database name in the\\n vertica.db\\n parameter in the \\nvalues.yaml\\n file during the suite\\ninstallation.  The default is \\nitomdb\\n.\\nLogin: You may set the Vertica read-only user login name in the \\nvertica.rouser\\n parameter in the \\nvalues.yaml\\n file during\\nthe suite installation.  The default is \\nvertica_rouser\\n.\\nPassword: Set to the password of the Vertica read-only user. \\n5\\n. \\nClick \\nTEST CONNECTION\\n to test the connection. \\nTEST CONNECTION \\nmust be successful.\\n6\\n. \\nClick \\nSAVE SETTINGS\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n333\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '404cdd9b51ab43c24846d95903cb0bdd'}>,\n",
              "  <Document: {'content': \"System Infrastructure Reports\\nSystem Infrastructure reports give you information about the availability and performance of the servers in your environment. \\nThese reports are available for metrics collected by Operations Agent and SiteScope. This drives the \\nSystem Performance data\\nmodel\\n for purposes of reporting in BVD, performance dashboards (using only raw tables), and custom reporting use cases. \\nIt's not recommended to have the same metrics collected from the same managed node ingested through multiple channels.\\nOperations Agent\\nThe Operations Agent collects an extensive set of metrics that indicate the health, performance, resource utilization, and\\navailability of essential elements of the system.\\nYou can send the system performance metrics collected by the Operations Agent into OPTIC Data Lake. This drives the \\nSystem\\nPerformance data model\\n for purposes of reporting in BVD, PD, and custom reporting use cases. \\nYou can use one of the following mechanisms to send Operations Agent data to OPTIC Data Lake:\\nAgent Metric Collector\\nMetric Streaming Policies\\nThe Agent Metric Collector queries an OBM RTSM to get the list\\nof nodes from which to collect metrics.\\nMetric Streaming Policies configure different collection\\nfrequencies at a granular level, by tuning the\\nparameters in the System Metrics Metric Store aspect\\nin OBM.\\nYou can use this channel for system performance metric\\ncollection and data ingestion if your Operations Agent version\\nis 12.00 or later.\\nYou can use this channel for system performance\\nmetric collection and data ingestion if your Operations\\nAgent version is 12.14 or later.\\nBy default, system performance metrics summarized at five\\nminute granularity are ingested by the Agent Metric Collector\\ninto OPTIC Data Lake at five minute intervals.\\nBy default, system performance metrics summarized at\\nfive minute granularity are pushed from the Operations\\nAgents into OPTIC Data Lake at five minute intervals.\\nYou can collect metrics for Agent node CIs that exist in one\\nOBM RTSM, per collection configuration. You can collect\\nperformance metrics from Agent nodes and send them to\\nOPTIC Data Lake using the Agent Metric Collector.\\nAgent nodes that belong to different OBM servers can\\nall stream metrics to the same OPTIC Data Lake\\ninstance by configuring the same OPTIC Data Lake\\nendpoint in the metric streaming policy.\\nThe collector connects to the Operations Agents at port 383\\nthrough HTTPS.  You can configure the collector to connect to\\na non-default port and also to connect using an HTTP Proxy or\\nReverse Channel Proxy (RCP). To configure system\\ninfrastructure reports using Agent Metric Collector, see\\nConfigure System Infrastructure Reports using Agent Metric\\nCollector\\n.\\nThe Operations Agents send the metric data to OPTIC\\nData Lake using HTTPS POST and uses the proxy\\nsetting in the Operations agent to connect to OPTIC\\nData Lake. To configure system infrastructure reports\\nusing the metric streaming policies, see \\nConfigure\\nSystem Infrastructure Reports using metric streaming\\npolicies\\n.\\nSiteScope\\nSiteScope provides simple and powerful agentless infrastructure and application monitoring, through a remote access\\narchitecture that's lightweight and you can customize.\\nYou can ingest the system performance metrics collected by SiteScope into OPTIC Data Lake.\\nYou can also send other SiteScope monitor data into the \\nopsb_agentless_generic\\n table in OPTIC Data Lake. You can use this\\ndata to generate custom reports and performance dashboards.\\nTo configure system infrastructure reports using SiteScope, see \\nConfigure System Infrastructure Reports using SiteScope\\n.\\nRelated topics\\nFor more information about the \\nopsb_agentless_generic\\n table, see the  \\nSystem Performance data model\\n.\\nFor information about specific monitors that populate the SiteScope raw tables, see \\nSource of SiteScope raw tables\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n334\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '258b7e0c088c16eb449628c8ccf702e2'}>,\n",
              "  <Document: {'content': \"Configure System Infrastructure Reports using Agent\\nMetric Collector\\nTo view system infrastructure reports, you must send the performance metrics collected by the Operations Agent to OPTIC\\nData Lake. You can use either the Agent Metric Collector (AMC) or Metric Streaming Policies to send Agent metrics to OPTIC\\nData Lake. Agent Metric Collector can pull metrics from all supported Operations Agent versions. You can use the Agent Metric\\nCollector to collect metrics for Agent node CIs that exist in one OBM RTSM, per collection configuration. This section provides\\nyou with the steps to collect performance metrics from Agent nodes and send them to OPTIC Data Lake using the Agent Metric\\nCollector.\\nPrerequisites\\nOPTIC Reporting\\n capability\\nRun the command on the master node (control plane) to check if you have installed the \\nOPTIC Reporting \\ncapability: \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting -A 1\\nFor example:\\nhelm get values deployment01 -n opsb-helm | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true\\nTo add the \\nOPTIC Reporting\\n capability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page. \\nOnly if you are using a classic OBM:\\nMake sure that you have installed the Operations Bridge Manager (OBM). \\nMake sure that you have created the Agent Metric Collector integration user. See \\nCreate an Agent Metric Collector\\nintegration user\\n.\\nMake sure that you have set all the Agent Metric Collector parameters. Specifically, check for the OBM integration\\ndetails (\\nglobal.amc.obmHostname\\n), and the Agent Metric Collector integration user (\\nglobal.amc.rtsmUsername\\n) parameters\\nas they don't have default values:\\nIf you have installed using Apphub UI, use the \\nReconfigure the deployment\\n option on the Apphub\\nDeployments\\n page to set the following Agent Metric Collector parameters:\\nobmHostname\\nrtsmUsername\\nisAgentMetricCollectorEnabled\\n  (Enable this parameter)\\nautoStartAgentMetricCollector\\n (Disable this parameter)\\nFor more information, see \\nReconfigure a deployment\\n. \\nIf you have installed using CLI:\\na\\n. \\nRun the following command to check if the \\nglobal.amc.obmHostname \\nand\\n global.amc.rtsmUsername\\n parameters\\nare set in the \\nvalues.yaml \\nfile (for more information, see Agent Metric Collector settings on the \\nConfigure\\nvalues.yaml\\n page):\\nhelm get values <helm_deployment_name> -n <suite namespace> | grep -E 'Hostname|rtsmUsername'\\nIf these parameters aren't set, run the following command to set the \\nglobal.amc.obmHostname \\nand\\n global.amc.r\\ntsmUsername\\n parameters and also to set the \\nisAgentMetricCollectorEnabled\\n parameter to \\ntrue\\n and the \\nautoStartA\\ngentMetricCollector\\n parameter to \\nfalse\\n:\\nhelm upgrade <helm deployment name> <chart> -n <suite namespace> --set global.amc.obmHostname=<FQDN of OBM gateway or load balancer> --set global.amc.rtsmUsername=<Agent Metric Collector integration user> --set global.isAgentMetricCollectorEnabled=true --set global.autoStartAgentMetricCollector=false --reuse-values\\n<chart>\\n is the absolute path to the suite chart package. For example: \\nopsbridge-suite-<version>.tgz\\n. The chart\\nfile is available under\\nthe \\ncharts\\n directory mentioned in the topic \\nDownload the required Installation Packages\\n. \\nIf you are installing containerized Operations Bridge suite or upgrading to the current version of the containerized\\nOperations Bridge suite:\\nRun the following command to check if the \\nautoStartAgentMetricCollector\\n parameter is set to \\ntrue \\nor\\n false: \\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nImportant: \\nMake sure that you use the \\n--reuse-values\\n parameter in the command, else the\\ndeployment values provided during installation will be overwritten with the default values.\\n \\nContainerized Operations Bridge 2022.11\\nPage \\n335\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6333dbeaaeaa1c739448e4a58c466f4e'}>,\n",
              "  <Document: {'content': 'helm get values <helm_deployment_name> -n <suite namespace> | grep autoStartAgentMetricCollector\\nRun the command to check if the \\nisAgentMetricCollectorEnabled\\n parameter is set to \\ntrue:  \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep isAgentMetricCollectorEnabled\\nRun the following command to set \\nautoStartAgentMetricCollector\\n to \\nfalse \\nand set\\n  \\nisAgentMetricCollectorEnabled\\n to \\ntrue \\n :\\nhelm upgrade <helm deployment name> <chart> -n <suite namespace> --set global.autoStartAgentMetricCollector=false --set global.isAgentMetricCollectorEnabled=true --reuse-values \\n<chart>\\n is the absolute path to the suite chart package. For example: \\nopsbridge-suite-<version>.tgz\\n. The chart file is\\navailable under the \\ncharts\\n directory mentioned in the topic \\nDownload the required Installation Packages\\n.\\n \\nComponents and their supported versions\\nComponent\\nSupported version\\nClassic OBM \\n2020.05 and higher\\nContainerized OBM\\n2021.08 and higher\\nOperations Agent \\n12.xx and higher\\nTask 1: Configure a secure connection between DBC and OBM\\nThe Data Broker Container (DBC) contains Operations Agent managed by OBM. DBC communicates with OBM and receives\\ncertificate updates which are used by the Agent Metric Collector.\\nAfter you install the reporting capability, the DBC gets deployed and the \\nitom-monitoring-service-data-broker \\npod is in the 2/2\\nrunning state.\\nPerform the steps on the OBM data processing and gateway servers to configure a secure connection between DBC and OBM:\\n1\\n. \\nGo to\\nOn Linux: \\n/opt/OV/bin/ovconfchg -edit\\nOn Windows:\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovconfchg.exe\" -edit\\n2\\n. \\nConfigure OBM to communicate with DBC: If \\nPORTS\\n is already defined in the [\\nbbc.cb.ports\\n] namespace, append \\n<externalAc\\ncessHost>:<NODEPORT>\\n to the \\nPORTS\\n setting, otherwise, add the following lines:\\n[bbc.cb.ports]\\nPORTS=\\n<externalAccessHost>:<NODEPORT>\\n<NODEPORT>\\n must be the same port number that you configured as \\ndataBrokerNodePort\\n in the \\nvalues.yaml\\n file. The default\\nvalue of the\\n dataBrokerNodePort\\n is 1383.\\n3\\n. \\nGo to\\nOn Linux: \\n/opt/OV/bin/ovconfchg -ovrg server -edit\\nOn Windows:\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovconfchg.exe\" -ovrg server -edit\\n4\\n. \\nConfigure OBM to communicate with DBC: If \\nPORTS\\n is already defined in the [\\nbbc.cb.ports\\n] namespace, append \\n<externalAc\\ncessHost>:<NODEPORT>\\n to the \\nPORTS \\nsetting, otherwise, add the following lines:\\n[bbc.cb.ports]\\nPORTS=<externalAccessHost>:<NODEPORT>\\nImportant\\n:\\nYou must complete the task within two hours of installation for the autoconfigure job to succeed.\\nPerform this task only if you are using a classic OBM.\\n\\ue91b\\n\\ue91b\\nNote:\\n If you want to check the port number, run the command on the master (control plane) node:\\nhelm get values <helm_deployment_name> -n <suite namespace> | grep dataBrokerNodePort\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n336\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2126ac09dd0357d6994bce353d431446'}>,\n",
              "  <Document: {'content': '<NODEPORT>\\n must be the same port number that you configured as \\ndataBrokerNodePort\\n in the \\nvalues.yaml\\n file. The default\\nvalue of the\\n dataBrokerNodePort\\n is 1383.\\nTask 2: Grant the certificate request on OBM\\nFollow the steps:\\n1\\n. \\nGo to \\nADMINISTRATION \\n>\\n SETUP AND MAINTENANCE \\n>\\n Certificate Requests\\n.\\n2\\n. \\nSelect the certificate request from the \\n<externalAccessHost>\\n.\\n3\\n. \\nRight click and select \\nGrant Item\\n.\\nTask 3: Enable Agent Metric Collector\\nImport custom configurations\\nIf you have a custom collection configuration, make sure that you have backed it up. You must have backed up the collection\\nconfiguration before uninstalling suite.  See \\nBackup custom collection configuration\\n. Use this command to import the custom\\nconfiguration:  \\n./ops-monitoring-ctl create -f <filename>\\n \\nRun the following commands to import the custom configurations if any:\\n1\\n. \\nDownload and configure the CLI\\n. \\n2\\n. \\nTo import custom credentials run the following command \\n ./ops-monitoring-ctl create -f custom_amc_obm_basic_auth.yaml\\n3\\n. \\nTo import custom targets run the following command\\n ./ops-monitoring-ctl create -f custom_amc_obm_rtsm.yaml\\n4\\n. \\nTo import other custom configurations for nodefilter/proxy/ports/hosts\\n./ops-monitoring-ctl create -f <file name>\\n5\\n. \\nTo import custom collectors run the following command.\\n ./ops-monitoring-ctl create -f custom-agent-collector-sysinfra.yaml\\n6\\n. \\nTo check the collection status run the following command:\\n ./ops-monitoring-ctl get collector-status -o yaml\\nValidate autoconfigurejob\\nDuring the suite installation, \\nautoConfigure\\n job creates and deploys the AMC credential, target, and system infrastructure\\ncollection configuration files. You may run the following command to verify the creation and deployment of the AMC\\nconfigurations: \\nNote:\\n If you want to check the port number, run the command on the master (control plane) node:  \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep dataBrokerNodePort\\n\\ue916\\n\\ue916\\nTip:\\nBy default, you see the certificate requests received in the last 24 hours.  If you installed the \\nOPTIC\\nReporting\\n capability before that, select a longer date range in the time filter list.\\nIf the certificate request doesn\\'t appear, run the command on OBM data processing server to grant the\\ncertificate request:\\nOn Linux:\\n/opt/OV/bin/ovcm -listpending\\n/opt/OV/bin/ovcm -grant <Certificate-ID-from-above-command>\\nOn Windows:\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcm\" -listpending \\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcm\" -grant <Certificate-ID-from-above-command>\\nNote\\n: This section is applicable only if you\\'ve uninstalled and installing the application again. If you\\'re performing\\nfresh installation and configuring reports, then ignore this import custom configuration section.\\nContainerized Operations Bridge 2022.11\\nPage \\n337\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1c900d929bfa81432070bef65a7ac34f'}>,\n",
              "  <Document: {'content': \"kubectl logs  -n $(kubectl get pod -A | awk '/autoconfigure/ {print $1, $2}')\\nIf the configuration files are created, the output of the above command displays the \\nStatus \\nof \\nTarget\\n, \\nCredential\\n,\\nand \\nCollector \\nas \\nCreated\\n. If the default configuration files aren't created, follow the steps described in \\nAMC configuration files\\naren't deployed after installation\\n.\\nIf you want to customize the default collection configuration, see \\nAMC collector configuration\\n.\\nPrerequisites to enable AMC:\\n \\nFor classic OBM: Make sure that you have assigned the correct roles to the Agent Metric Collector integration user. The\\nAgent Metric Collector won't list any nodes or start metrics collection if you assign the wrong roles. To add and assign\\nroles, see \\nCreate an Agent Metric Collector integration user\\n.\\nFor containerized OBM: Make sure that the\\n omi-0\\n (and \\nomi-1\\n in HA) pod is running. Run the following command to see the\\npod status in a namespace:\\nkubectl get pods --namespace <suite namespace>\\nFor example: \\nkubectl get pods -n opsbridge1\\nFollow these steps to enable the Agent Metric Collector: \\n1\\n. \\nRun the following command in the folder containing the \\nops-monitoring-ctl\\n tool to check the status of the collection\\nconfigurations: \\n \\n./ops-monitoring-ctl get collector-status\\n2\\n. \\nRun the following command in the folder containing the \\nops-monitoring-ctl\\n to start the collection:\\n./ops-monitoring-ctl enable collector -n <name of the configuration>\\nWhere \\n<name of the configuration>\\n is the metadata name in the collector yaml file.\\nFor example:\\n./ops-monitoring-ctl enable collector -n agent-collector-sysinfra\\nAfter you start the performance metrics collection, metrics from the Agent nodes flow into OPTIC Data Lake. BVD uses these\\nmetrics to generate the system infrastructure reports. After completing the aforementioned configurations, it would take about\\n30 minutes for you to see the last hour data in the System Resource Top 3 report on BVD. To view the system infrastructure\\nreports, see \\nSystem Infrastructure\\n. \\nYou can also use these metrics to generate dashboards using Performance Dashboards. For configuration steps, see \\nConfigure\\nPerformance Dashboards\\n. \\nIf Agent Metric Collector is unable to collect metrics from the Operations Agents on the worker nodes, see the troubleshooting\\ntopic, \\nAgent Metric Collector is unable to collect metrics from the Operations Agents on the worker nodes.\\nAgent Metric Collector supports the ingestion of only those metrics that are summarized.  Here is a list of supported metric\\nclasses:\\nMetric class\\nSummarization interval\\nglobal\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\napplication\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\ndisk device\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\nlvolume\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\ntransaction\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\nconfiguration\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\nnetif\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\ncpu\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\nfilesystem\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\nhost bus adapter\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\ncore\\n5, 15, 30, 60, 180, 360, 720, 1440, 10080 minutes.\\nRelated topics\\nFor details about the metrics collected by Operations Agent, see \\nSystem Infrastructure schema tables.\\nContainerized Operations Bridge 2022.11\\nPage \\n338\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bbac55c6ef3e0c24574109d170eb8bb5'}>,\n",
              "  <Document: {'content': 'Follow the steps to create a new policy and add a \\ntenant_id\\n:\\n1\\n. \\nOn OBM, go to \\nAdministration\\n > \\nMonitoring\\n > \\nPolicy Template\\n2\\n. \\nClick \\nTemplate by Type\\n, expand \\nConfiguration\\n.\\n3\\n. \\nClick the \\nNode Info\\n folder.\\n4\\n. \\nClick \\nNew\\n and then click the \\nNew Policy Template\\n.\\n5\\n. \\nCreate Raw Editor\\n page appears. In the \\nProperties\\n tab, enter a \\nDisplay\\n name and click \\nNext\\n.\\n6\\n. \\nIn the \\nPolicy Data\\n tab, enter the following:\\n;XPL config\\n[eaagt]\\nTENANT_ID=<value>\\nHere, replace <\\nvalue\\n> with a name of your choice.\\n7\\n. \\nClick \\nSave\\n. A new policy with the tenant id gets created.\\n8\\n. \\nSelect the new policy and click   \\n \\n \\nAssign and Deploy Items\\n.\\n9\\n. \\nIn the \\nAssign And Deploy Aspect\\n window, select the Operations Agent node to which you want to deploy the policy.\\n10\\n. \\nClick \\nAssign\\n. The updated policy gets deployed and the \\ntenant_id \\ninformation flows to OPTIC Data Lake.\\nAfter you deploy the \\nSystem Metrics Metric Store Streaming \\naspect, metrics from the nodes flow into OPTIC Data Lake\\nand used to generate the system infrastructure reports on BVD. After completing the aforementioned configurations, it would\\ntake about 30 minutes for you to see the last hour data in the System Resource Top 3 report on BVD. To view the system\\ninfrastructure reports, see \\nSystem Infrastructure\\n.\\nYou can also use these metrics to generate dashboards using Performance Dashboards. For configuration steps, see \\nConfigure\\nPerformance Dashboards\\n.   \\nRelated topics\\nFor details about the metrics collected by Operations Agent, see \\nSystem Infrastructure schema tables.\\nTo troubleshoot issues, see \\nTroubleshoot System Infrastructure Reports - Metric Streaming Policy\\nFor information about enabling tenant specific OpsBridge OPTIC Data Lake Reports, see \\nEnable tenant specific access to\\ndata in OpsBridge OPTIC Data Lake Reports\\n on the Marketplace.\\nImportant:\\n \\ntenant_id \\nis supported only from the Operations Agent version 12.20. \\n\\ue91b\\n\\ue91b\\nNote: \\nThe \\ntenant_id\\n can have a maximum of 80 characters. The \\ntenant_id\\n can have alpha-numeric\\ncharacters. If a \\ntenant_id\\n isn\\'t configured or if the \\ntenant_id\\n  has more than 80 characters, the following\\nwarning message gets logged in \\nhpcs.log\\n file:\\n\"\\ntenant_id\\n isn\\'t configured. Configure the \\ntenant_id\\n and make sure that you don\\'t exceed 80 characters.\\nPlease restart the Operations Agent after updating the \\ntenant_id\\n.\"\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n342\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '97b608337702b1e9d482f541b565af6a'}>,\n",
              "  <Document: {'content': 'To customize collection frequency or filter nodes for collection using Agent Metric Collector, see \\nAMC collector\\nconfiguration\\n.\\nTo manage the reporting content and Agent metric Collection (create, update, or delete Agent Metric Collector),\\nsee \\nManage monitor configurations using Parameters\\n. \\nTo modify the default data retention period, see \\nConfigure Data Retention\\n.\\nTo scale out the collection, see \\nScale out the collection\\n.\\nConfigure System Infrastructure Reports using metric streaming policies\\n.\\nConfigure metrics collections from Operations Agent nodes in secure zones.\\nIf the communication between the Agent Metric Collector and the Operations Agent fails with an SSL error, see \\nAn SSL\\nconnection IO error has occurred\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n339\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f4071ef65431f39248e786d5576011e5'}>,\n",
              "  <Document: {'content': \"Lake.\\nThe System Metrics Metric store Streaming (version 2.0) aspect includes the following policies:\\nSys_SystemMetrics_MetricStoreStreaming (version 2.0)\\n - The \\nSys_SystemMetrics_MetricStoreStreaming\\nmetric streaming policy is responsible for forwarding the metrics defined in the policy to OPTIC Data Lake.  In Task A, you\\nedit this policy and select the URL of your OPTIC Data Lake Data Receiver Endpoint.\\nSys_SystemMetrics_MetricStoreInterval (version 1.0)\\n - In the \\nSys_SystemMetrics_MetricStoreInterval\\n node\\ninfo policy, you may define how often you want to send metrics to OPTIC Data Lake.  The default is every 300 seconds.\\n You can change this from 10 seconds to 3600 seconds (1 hour).  Since this setting is a parameter, when you assign the\\naspect you can specify different intervals for different sets of nodes.\\nSys_SystemMetrics_MetricStoreMapping (version 1.0)\\n - The \\nSys_SystemMetrics_MetricStoreMapping\\nConfigFile\\n policy translates the Operations Agent classes and metric names to normalized OPTIC DL Message Bus topics,\\ntables, and column names.\\nAfter you deploy the \\nSystem Metrics Metric Store\\n aspect in Task B, the Operations Agent's \\nhpsensor\\n sends the metric data\\nas an \\nHTTPS POST\\n to OPTIC Data Lake.  If the Operations Agent has to communicate to the external access host through a\\nproxy, configure the proxy setting.  See \\nConfiguring the Operations Agent in a Secure Environment\\n.\\nFollow the steps:\\nTask A: Update the policy with the target URL\\nFollow the steps:\\n1\\n. \\nOn OBM, go to \\nAdministration > Monitoring > Management Templates & Aspects\\n.\\n2\\n. \\nExpand \\nConfiguration Folders, \\nexpand \\nInfrastructure Management, \\nclick\\n Systems Infrastructure Aspects\\n.\\n3\\n. \\nExpand \\nSystem Metrics Metric Store Streaming\\n aspect, select a version of the aspect and then click \\n \\n \\nEdit\\n.\\n4\\n. \\nOn the \\nEdit Aspect: System Metrics Metric Store Streaming \\npage, click \\nPolicy Templates\\n.\\n5\\n. \\nSelect \\nSys_SystemMetrics_MetricStoreStreaming\\n policy and click \\n \\n \\nEdit\\n.\\n6\\n. \\nOn the \\nEdit Metric Streaming Configuration Policy \\npage, click \\nTarget Endpoint\\n.\\n7\\n. \\nSelect \\nOPTIC Data Lake, \\n1\\n. \\nIn the drop-down list select the URL to stream the metric data. The list displays the URL configured in Infrastructure\\nSettings.\\nFor example:\\nhttps://<MetricStore_Server>:<Port>/itomdi/receiver\\nFor example: \\nhttps://myexternalaccesshost:30001/itomdi/receiver\\n2\\n. \\nSpecify the metric mapping file name. You would have defined the metric mapping file name in the\\n Mapping\\nConfig file\\n policy.\\n8\\n. \\nClick \\nSAVE AND CLOSE\\n.  The policy version gets incremented by 0.1.\\n9\\n. \\nClick \\nRefresh\\n  \\n \\n to see the version increment. \\n10\\n. \\nClick \\nOK\\n to save the aspect.  The aspect version gets incremented by 0.1.\\nTask B: Deploy the System Metrics Metric Store  Streaming aspect\\nFollow the steps: \\n1\\n. \\nGo to \\nAdministration > Monitoring > Management Templates & Aspects\\n.\\n2\\n. \\nExpand \\nConfiguration Folders, \\nexpand \\nInfrastructure Management, \\nclick\\n Systems Infrastructure Aspects\\n.\\n3\\n. \\nSelect the updated \\nSystem Metrics Metric Store Streaming\\n aspect, and click  \\n \\n \\nAssign and Deploy Items\\n. The\\nAssign and Deploy wizard appears.\\n4\\n. \\nIn the \\nConfiguration Item\\n tab, select the managed node (that you want to monitor) to deploy the \\nSystem Metrics\\nMetric Store\\n aspect.\\n5\\n. \\nUnder \\nParameters\\n, you may specify how often you want to summarize and send metrics to OPTIC Data Lake.  The\\ndefault collection schedule is 300 seconds (5 minutes). To change the default value, click  \\n \\n.\\n6\\n. \\nThe \\nEnable Assignment(s)\\n check box gets selected by default. If you don't want to enable the assignment immediately,\\nclear the \\nEnable Assignment(s)\\n check box. You can then enable the assignment later using the \\nAssignments &\\nTuning pane\\n.\\n7\\n. \\nClick \\nASSIGN\\n.\\nA confirmation message appears after you deploy the \\nSystem Metrics Metric Store Streaming\\n aspect.\\n(Optional) \\nAdd the tenant_id\\nA tenant id enables you to send tenant information to OPTIC Data Lake. \\nNote\\n:\\n The metric list includes all the metrics required to populate the System Infrastructure reports on\\nBVD.  You can specify if you want to collect data for all or specific instances of multiple instance classes. By\\ndefault, data for all instances get collected. The \\nOPTIC Reporting\\n capability supports the streaming of the\\nfollowing metric classes to OPTIC Data Lake: \\nGLOBAL, CPU, DISK, NETIF, FILESYSTEM\\n.  If you want to integrate your\\nown metrics to OPTIC Data Lake, see \\nCustom Metric Ingestion\\n. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n341\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6dc711049a1db04b45d516a5c1f154a6'}>,\n",
              "  <Document: {'content': 'Configure System Infrastructure Reports using metric\\nstreaming policies\\nThis section provides you steps to collect metrics from Agent nodes and send them to OPTIC Data Lake using the Metric\\nStreaming policies. Agent nodes that belong to different OBM servers can all stream metrics to the same OPTIC Data Lake\\ninstance by configuring the same OPTIC Data Lake endpoint in the metric streaming policy.\\nTo view system infrastructure reports, you must send the performance metrics collected by the Operations Agent to OPTIC\\nData Lake. You can use either the Agent Metric Collector or Metric Streaming policies to send Agent metrics to OPTIC Data\\nLake.\\nPrerequisites\\nOPTIC Reporting\\n capability\\nRun the command on the master (control plane) node to check if you have installed the \\nOPTIC Reporting\\n capability: \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting:\\nFor example:\\nhelm get values opsb -n opsbs | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true \\nTo add the \\nOPTIC Reporting\\n capability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page. \\nOperations Bridge Manager (OBM). For installation steps, see \\nOBM 2020.05 Install\\n,  \\nOBM 2020.10 Install\\n, \\nOBM 2021.05\\nInstall\\n, \\nInstall OBM 2021.11\\n, or \\nInstall OBM 2022.05\\n.\\nOBM Management Pack for Infrastructure (OBM MP for Infrastructure) version 2020.08. If you choose to use OBM 2020.10,\\ninstall the OBM MP for Infrastructure that comes out of the box with OBM 2020.10. If you decide to install OBM 2020.05,\\nthen download the OBM MP for Infrastructure version 2020.08 from \\nITOM Marketplace\\n and install it.\\nConfigure a secure connection between OBM and OPTIC Data Lake:\\nTo configure classic OBM, see \\nConfigure classic OBM\\nTo configure containerized OBM, see \\nConfigure a secure connection between containerized OBM and OPTIC Data\\nLake\\nCheck if you have configured the BVD database connection. See \\nValidate the connection between BVD and OPTIC Data\\nLake\\n.\\nOperations Agent version 12.14 or higher with real-time license enabled. \\nTo install Operations Agent, see \\nInstall\\n.\\nRun the command to check if you have enabled the real-time license:\\noalicense -get -all\\nSample output:\\nLICENSE NAME                                                    TYPE        ACTIVATION    EXPIRY        EXTN\\n--------------------------------------------------------------------------------------------------------------------------------------------\\nHP Operations OS Inst Adv SW LTU            PERMANENT   18/Jun/2020   N/A           N/A\\nHP Ops OS Inst to Realtime Inst LTU           PERMANENT   20/Jun/2020   N/A           N/A\\nIf \"\\nHP Ops OS Inst to Realtime Inst LTU\"\\n isn\\'t shown as active,  run the commands to activate the license:\\noalicense -set -type PERMANENT \"HP Ops OS Inst to Realtime Inst LTU\"\\novc -restart hpsensor\\nComponents and their supported versions\\nComponent\\nSupported version\\nClassic OBM \\n2020.05 and higher\\nContainerized OBM\\n2021.08\\nOperations Agent on the target nodes\\n12.14 and higher\\nOBM MP for Infrastructure\\n2020.08\\nDeploy the aspect\\nThe System Metrics Metric Store aspect is available with the OBM MP for Infrastructure. The OBM MP for Infrastructure is\\navailable on the \\nMarketplace\\n. \\nYou must deploy the System Metrics Metric Store aspect to enable the streaming of metrics from the nodes to OPTIC Data\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n340\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f4b511d4654c864b2d9de39349908fd3'}>,\n",
              "  <Document: {'content': 'Component\\nSupported version\\nClassic OBM \\n2020.05 and higher\\nContainerized OBM\\n2021.05 and higher\\nOperations Agent \\n12.14 and higher\\nSiteScope\\n2020.10 and higher\\nSiteScope Metric Streaming OPTIC\\nData Lake content\\n2020.05\\nTask 1: Deploy the metrics streaming aspect\\nThe SiteScope Metrics Steaming policy is available with the SiteScope Metric Streaming OPTIC Data Lake content. \\nInstall the SiteScope Metric Streaming OPTIC Data Lake content\\nFollow the steps:\\n1\\n. \\nDownload the SiteScope Metric Streaming OPTIC Data Lake content from the \\nMarket Place\\n.\\n2\\n. \\nOn OBM, go to \\nAdministration\\n >\\nSETUP AND MAINTENANCE\\n > \\nContent Packs\\n.\\n3\\n. \\nClick \\nImport\\n. Import Content Pack window appears.\\n4\\n. \\nBrowse to the location where you have saved the SiteScope Metric Streaming OPTIC Data Lake content and then click\\nImport\\n.\\n5\\n. \\nThe required aspect gets imported. Click \\nClose\\n.\\nDeploy the SiteScope Metrics Streaming Aspect\\nFollow the steps:\\n1\\n. \\nOn OBM, go to \\nAdministration > Monitoring > Management Templates & Aspects\\n.\\n2\\n. \\nIn the Configurations Folder pane, click \\nSiteScope Metric Streaming\\n.\\n3\\n. \\nIn Management Templates & Aspects pane, right-click the \\nSiteScope Metric Streaming\\n aspect and then click \\nAssign and Deploy\\n item.\\n4\\n. \\nIn the Configuration Item tab, click the CI of the SiteScope server (Operations Agent node) on which you want to deploy\\nthe Aspect, and then click \\nNext\\n.\\n5\\n. \\nIn the Required Parameters tab, enter the OPTIC Data Lake receiver URL in the following format: \\nhttps://<externalAccessHost\\n>:30001/itomdi/receiver\\n6\\n. \\nClick \\nNext \\nand then click \\nFinish\\n.\\nValidate the installation of the SiteScope policies\\nRun the command on the SiteScope server:\\nOn Linux:\\n \\n/opt/OV/bin/ovpolicy -list\\nOn Windows:\\n \\novpolicy -list\\nThe SiteScope policies are listed as follows:\\nTask 2: (Optional) Add the tenant_id\\nA tenant id enables you to configure multiple tenants.\\nFollow the steps to update the tenant id:\\n1\\n. \\nOn SiteScope server, go to:\\nOn Windows:\\n \\n\"<SITESCOPE_HOME>\\\\templates.applications\\\\COSO_tenant.properties\"\\nOn Linux:\\n \\n/opt/HP/SiteScope/templates.applications/COSO_tenant.properties\\n2\\n. \\nIn the \\nCOSO_tenant.properties\\n file, add a tenant id to the \\n_tenantIdForCOSO\\n parameter.\\nNote: \\nSiteScope supports ingestion of tenant id into OPTIC Data Lake\\nonly from version 2020.10 and higher.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n344\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '615dc4f6e7b78e9c23c74ac423bb6785'}>,\n",
              "  <Document: {'content': 'Follow the steps:\\n1\\n. \\nOn \\nMonitors\\n context, right-click \\nSiteScope\\n root (or the group or monitor in the monitor tree).\\n2\\n. \\nSelect \\nGlobal Search and Replace\\n from the context menu. The Global Search and Replace window opens.\\n3\\n. \\nSelect the \\nMonitor\\n option and click \\nNext\\n.\\n4\\n. \\nIn the \\nSelect Subtype\\n tab, select the monitors for which you want to assign the OPTIC Data Lake tag.\\nFor Agentless system infrastructure reporting, select the monitors shown in the following image:\\n \\n \\n5\\n. \\nClick \\nNext\\n.\\n6\\n. \\nIn the \\nReplace Mode\\n tab. Select the \\nReplace\\n option.\\n7\\n. \\nClick \\nNext\\n.\\n8\\n. \\nIn the \\nChoose Changes\\n tab, select the OPTIC Data Lake tag as shown in the image:\\nContainerized Operations Bridge 2022.11\\nPage \\n346\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bccf0cf1ca97bdc7893483e2be161f74'}>,\n",
              "  <Document: {'content': 'Task 3: Restart SiteScope\\nUse one of the following options to restart SiteScope:\\nOn Windows:\\n1\\n. \\nOpen the Services Window.\\n2\\n. \\nSelect the SiteScope service and click \\nStop\\n.\\n3\\n. \\nSelect the SiteScope service and click \\nStart\\n.\\nIt takes a few minutes for the SiteScope server to start. \\nOn Linux:\\n1\\n. \\nOpen a terminal window on the server where you have installed SiteScope.\\n2\\n. \\nRun the stop command:\\n /opt/HP/SiteScope/stop\\n3\\n. \\nRun the start command: \\n/opt/HP/SiteScope/start\\nIt takes a few minutes for the SiteScope server to start. \\nTask 4: Create monitors\\nCPU Monitor\\n. See \\nCPU Monitor\\n.\\nMemory Monitor\\n. See \\nMemory Monitor\\n.\\nNetwork Bandwidth Monitor\\n. See \\nNetwork Bandwidth Monitor\\n.\\nDynamic Disk Space Monitor\\n. See \\nDynamic Disk Space Monitor\\n.\\nMicrosoft Windows Resources Monitor\\n. See \\nMicrosoft Windows Resources Monitor\\n.\\nUNIX Resources Monitors\\n. See \\nUNIX\\n Resources Monitor\\n.\\nTask 5: Enable monitors\\nYou must add the OPTIC Data Lake tag to monitors to enable them to stream data into OPTIC Data Lake.\\nFollow the steps:\\nI. Add the OPTIC Data Lake tag\\n1\\n. \\nIn the SiteScope UI, select the \\nPreferences\\n context.\\n2\\n. \\nSelect \\nSearch/Filter Tags.\\n Follow the steps to create the \\nCOSO tag\\n if it doesn\\'t exist:\\n1\\n. \\nClick \\n \\nNew tag\\n. \\n2\\n. \\nIn the \\nTag\\n name box enter \\'COSO\\' (It must be in upper case).\\n3\\n. \\nTo add a Value, click \\n \\n. A new row gets created.\\n4\\n. \\nIn the \\nValue Name\\n column, double-click and enter \\'COSO\\' (It must be in upper case).\\n5\\n. \\nClick \\nOk\\n. The tag gets created and available for assignments.\\nII. Assign the OPTIC Data Lake tag to monitors \\nAssign the OPTIC Data Lake tag to each monitor individually or a group of monitors.\\nOption 1: Assign the OPTIC Data Lake tag to a single monitor\\nFollow the steps:\\n1\\n. \\nSelect the \\nMonitors\\n context. In the monitor tree, expand the group directory that contains the monitor, and select the\\nmonitor. For the complete list of monitors that are required to populate BVD Reports, see \\'\\nList of Monitors\\n\\' section on\\nthis page.\\n2\\n. \\nIn the right pane, click the \\nProperties\\n tab, and select \\nSearch/Filter Tags\\n.\\n3\\n. \\nSelect the OPTIC Data Lake tag with the value as OPTIC Data Lake and then click \\nSave\\n.\\nOption 2: Assign the OPTIC Data Lake tag to a group of monitors\\nNote:\\nThe tenant_id can have a maximum of 80 characters.\\nIf a \\ntenant_id\\n isn\\'t configured or if the \\ntenant_id\\n  has more than 80 characters, the following warning message\\ngets logged in \\nerror.log\\n file:\\n\"tenant_id isn\\'t configured. Configure the tenant_id and make sure that you don\\'t exceed 80 characters.\\nPlease restart the SiteScope after updating the tenant_id.\"\\nTip: \\nThe section \\'\\nList of Monitors\\n\\' on this page, provides a complete list of Agentless Infrastructure Monitors.\\nUse only the Internet Explorer browser or the SiteScope local client to view the UI.\\nContainerized Operations Bridge 2022.11\\nPage \\n345\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '22b7a3564e4562c406b82c49bdb6178'}>,\n",
              "  <Document: {'content': \"Configure System Infrastructure Reports using SiteScope\\nTo view system infrastructure reports, you must send the performance metrics collected by the SiteScope to OPTIC Data\\nLake. You can send metrics for any SiteScope monitor type to OPTIC Data Lake for custom reporting and Performance\\nDashboards.  Business Value Dashboard (BVD) uses the data from a specific set of monitors to populate the System\\nInfrastructure Reports. The section '\\nList of Monitors\\n' on this page, provides a complete list of monitors that are used to\\npopulate the System Infrastructure Reports on BVD.\\nPrerequisites\\nOPTIC Reporting \\ncapability\\nRun the command on the master (control plane) node to check if you have installed the \\nOPTIC Reporting\\n capability: \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting:\\nFor example:\\nhelm get values opsb -n opsbs | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true\\nTo add the \\nOPTIC Reporting\\n capability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page. \\nOperations Bridge Manager (OBM). For installation steps, see \\nInstall\\n.\\nConfigure a secure connection between OBM and OPTIC Data Lake:\\nTo configure classic OBM, see \\nConfigure classic OBM\\nTo configure containerized OBM, see \\nConfigure a secure connection between containerized OBM and OPTIC Data\\nLake\\nValidate the connection between BVD and OPTIC Data Lake. See \\nValidate the connection between BVD and OPTIC Data\\nLake\\n.\\nSiteScope. For installation steps, see \\nInstall\\n.\\nInstall and integrate Operations Agent on the SiteScope server with OBM.\\nTo stream SiteScope data into the OPTIC Data Lake, you must integrate the Operations Agent which is on the SiteScope\\nserver with OBM.\\nPerform the following steps to check if Operations Agent is installed:\\n1\\n. \\nLog on to the SiteScope server:\\nOn Linux as root\\nOn Windows as Administrator\\n2\\n. \\nRun the following commands:\\nOn Linux:\\ncd /opt/OV/bin\\n./opcagt -version\\nOn Windows:\\n%ovinstalldir%\\\\bin\\nopcagt -version\\nThe version of the Operations Agent is displayed. Make sure that the version is 12.14.\\nInstall and integrate Operations Agent with OBM\\nRun the following commands if you want to install and integrate Operations Agent (on a SiteScope server) with OBM:\\nOn Linux:\\n \\n./oainstall.sh -i -a -s <OBM load balancer or gateway server> \\nOn Windows:\\n \\ncscript oainstall.vbs -i -a -s <OBM load balancer or gateway server> \\nGrant the SiteScope server certificate\\nFollow the steps:\\n1\\n. \\nOn OBM, go to \\nAdministration > SETUP AND MAINTENANCE > Certificate Request\\n2\\n. \\nClick \\n \\nto grant the certificate.\\nComponents and their supported versions\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nNote:\\nFor containerized OBM 2019.11, \\n<OBM load balancer or gateway server> \\nis the  FQDN of the external access host. \\nFor more information, see \\nOperations Agent Install\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n343\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '32aab532a78d840bf6f87969f62f1c61'}>,\n",
              "  <Document: {'content': '9\\n. \\nClick \\nNext.\\n10\\n. \\nIn the Affected Objects tab, you can see the list of monitors that are tagged with the OPTIC Data Lake tag.\\n11\\n. \\nClick \\nNext\\n.\\n12\\n. \\nIn the \\nReview summary\\n tab, you can review the changes.\\n13\\n. \\nClear the \\nVerify monitor properties with remote server\\n check box.\\n14\\n. \\nClick \\nApply\\n.\\n15\\n. \\nThe Summary page displays the result. Click \\nFinish\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n347\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ec47932c9c663ecd0787907c97579a69'}>,\n",
              "  <Document: {'content': ' \\n \\nList of monitors\\nFor SiteScope reports, enable the following Agentless Infrastructure Monitors:\\nCPU Monitor\\nMemory Monitor\\nNetwork Bandwidth Monitor\\nDynamic Disk Space Monitor\\nMicrosoft Windows Resources Monitor\\nUNIX Resources Monitors\\nTable name\\nMonitor type\\nopsb_agentless_node\\nCPU, Memory, Windows Resources, UNIX Resources\\nopsb_agentless_cpu\\nCPU\\nopsb_agentless_disk\\nWindows Resources\\nopsb_agentless_filesys\\nDynamic Disk Space, UNIX Resources \\nopsb_agentless_netif\\nNetwork Bandwidth, Windows Resources, UNIX Resources \\nopsb_agentless_generic\\nAll SiteScope monitors\\nFor information about specific monitors that populate the SiteScope raw tables, see \\nSource of SiteScope raw tables\\n.\\nAfter you enable the monitors, metrics from the nodes are sent to OPTIC Data Lake. After completing the aforementioned\\nconfigurations, it would take about 30 minutes for you to see the last hour data in the System Resource Top 3 report on BVD. \\nYou can also use these metrics to generate dashboards using Performance Dashboards. For configuration steps, see \\nConfigure\\nPerformance Dashboards\\n.\\nTask 6: \\n(Optional) \\nEnable downtime\\nSiteScope sends the downtime flag to the OPTIC Data Lake. For more information see, \\nConfigure downtime\\n.\\nRelated topics\\nFor details about the metrics collected by SiteScope, see \\nSystem Infrastructure schema tables\\n.\\nFor information about specific monitors that populate the SiteScope raw tables, see \\nSource of SiteScope raw tables\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n348\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b91ab088cab0c25fbdde17babaa49ca4'}>,\n",
              "  <Document: {'content': 'Configure event reports with classic OBM\\nThis section provides you with the steps to forward events and topology from an OBM server to OPTIC Data Lake. To populate\\nthe event reports in BVD, you must configure OBM to forward events and topology to OPTIC Data Lake.\\nIf you want to forward only events from OBM to OPTIC Data Lake, perform the following tasks:\\n1\\n. \\nCreate the \\nobm-configurator.jar\\n tool.\\n2\\n. \\nConfigure the OBM system.\\n3\\n. \\nConfigure the suite.\\nIf you also want to forward topology from OBM to OPTIC Data Lake, perform the following tasks:\\n1\\n. \\nInstall the UD content pack.\\n2\\n. \\nDownload and install the Data Flow Probe.\\n3\\n. \\nConfigure topology data streaming from OBM/RTSM to OPTIC Data Lake.\\nTo forward topology, each OBM server requires a Data Flow Probe. The Events by CI report and the \"Top event flow by CI\"\\nsection of the Event Executive Summary report show information about topology.\\nPrerequisites\\nOPTIC Reporting\\n capability\\nRun the command on the master (control plane) node to check if you have installed the \\nOPTIC Reporting\\n capability: \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting:\\nFor example:\\nhelm get values opsb -n opsbs | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true\\nTo add the \\nOPTIC Reporting\\n capability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page. \\nValidate the connection between BVD and OPTIC Data Lake. See \\nValidate the connection between BVD and OPTIC Data\\nLake\\n.\\nComponents and their supported versions\\nThe components and their supported versions that are required for reporting are listed:\\nComponent\\nSupported version\\nClassic OBM \\n2020.05 and higher\\nThe topic provides the steps to configure a classic OBM for correlating events and forwarding them to OPTIC Data Lake. \\nTasks for configuring classic OBM\\nCreate the \\nobm-configurator.jar\\n tool and install the OBM CA certificate on the suite by using the \\nIntegration Tools\\n. \\nConfigure OBM and create \\nconfigureSuite.zip\\n by executing \\nobm-configurator.jar. \\nThe \\nconfigureSuite.zip \\ncontains\\nthe\\n setup-obm.sh\\n. \\nConfigure the suite by extracting \\nconfigureSuite.zip \\non the control plane (master) node and executing \\nsetup-obm.sh\\n.\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nNote\\n:\\n Skip tasks 1-3 below if you have already configured Event Forwarding as mentioned in the \\nConfigure\\nClassic OBM\\n section of this document.\\n\\ue916\\n\\ue916\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\nIf an existing OBM is connected to OPTIC Data Lake and BVD, refer \\nConnected Servers\\n.\\nNote\\n: The\\n obm-configurator.jar\\n tool configures a classic OBM for correlating events and forwarding the events\\nContainerized Operations Bridge 2022.11\\nPage \\n349\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '26d8afd04d6725b949ca8a5ddf497fe4'}>,\n",
              "  <Document: {'content': 'Create the obm-configurator.jar tool\\n On the control plane (master) node, in the \\nintegration-tools \\ndirectory, execute the following command:\\n./get-obm-setup-tool.sh\\nThe \\nobm-configuration.jar\\n tool is created in the same directory where \\nget-obm-setup-tool.sh\\n resides.\\nHere\\'s a sample output after running the command: \\n./get-obm-setup-tool.sh\\ninfo: Logfile: /tmp/get-obm-setup-tool-2021-10-11-16-56-35.log\\ninfo: Working dir: /root/tools2\\ninfo: AEC Namespace:  opsb-helm\\ninfo: COSO Namespace: opsb-helm\\ninfo: Getting files from the suite ...\\ninfo: Fetching suite certificates ...\\ninfo: Copying file issue_ca.crt from container idm to /root/tools2/obm-configurator-interim/issue_ca.crt ...\\ntar: Removing leading `/\\' from member names\\ninfo: Successfully copied suite certificates\\ninfo: Fetching IDL configuration script ...\\ninfo: Name of the file: /artifacts/idl-config-1.5.0.zip\\ninfo: Copying file idl-config-1.5.0.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/idl-config-1.5.0.zip ...\\ntar: Removing leading `/\\' from member names\\ninfo: Successfully fetched IDL configuration script\\ninfo: Fetching OBM IDL configuration script ...\\ninfo: Name of the file: /artifacts/opr-config-idl-11.20.004.010.zip\\ninfo: Copying file opr-config-idl-11.20.004.010.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/opr-config-idl-11.20.004.010.zip ...\\ntar: Removing leading `/\\' from member names\\ninfo: Successfully fetched OBM configuration script\\ninfo: Fetching \\'COSO_Data_Lake_Event_Integration\\' ...\\ninfo: Name of the file: /artifacts/COSO_Data_Lake_Event_Integration_CP-3.01.zip\\ninfo: Copying file COSO_Data_Lake_Event_Integration_CP-3.01.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/COSO_Data_Lake_Event_Integration_CP-3.01.zip ...\\ntar: Removing leading `/\\' from member names\\ninfo: Successfully copied \\'COSO_Data_Lake_Event_Integration\\'\\ninfo: Fetching \\'COSO_Data_Lake_AEC_Integration_CP\\' ...\\ninfo: Name of the file: /artifacts/COSO_Data_Lake_AEC_Integration_CP-3.04.zip\\ninfo: Copying file COSO_Data_Lake_AEC_Integration_CP-3.04.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/COSO_Data_Lake_AEC_Integration_CP-3.04.zip ...\\ntar: Removing leading `/\\' from member names\\ninfo: Successfully copied \\'COSO_Data_Lake_AEC_Integration_CP\\'\\ninfo: Fetching OBM Setup Tool ...\\ninfo: Name of the file: /artifacts/obm-configurator.jar\\ninfo: Copying file obm-configurator.jar from container itom-static-files-provider to /root/tools2/obm-configurator.jar ...\\ntar: Removing leading `/\\' from member names\\ninfo: Successfully copied OBM setup tool\\ninfo: Getting deployment information ...\\ninfo: Creating files package ...\\ninfo: Creating package of files ...\\n/root/tools2\\ninfo: Creating tool jar ...\\ninfo: Unpacking tool package ...\\ninfo: Updating tool files ...\\ninfo: Creating the updated tool package ...\\n/root/tools2\\ninfo: Cleanup ...\\nSuccessfully created OBM setup tool in file /root/tools2/obm-configurator.jar\\nRemove old certificates from the OBM trust store\\n(Optional) If the classic OBM is connected to an earlier instance of OPTIC Data Lake, remove the old suite certificates from OBM\\ntrust store before executing the \\nobm-configurator.jar \\nfile. Refer to the Remove the OBM Configuration page in related\\ntopics and perform the steps, as required.\\nInstall an OBM CA certificate\\nPerform the following steps to install an OBM CA certificate on the Operations Bridge suite:\\n1\\n. \\nTo get the list of trusted certificates, run the following command on the classic OBM gateway server:\\nOn Linux\\n/opt/OV/bin/ovcert -list\\nOn Windows\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcert\" -list\\nto OPTIC Data Lake. This tool can\\'t change the configuration of a configured classic OBM or Operations Bridge\\nsuite.\\nContainerized Operations Bridge 2022.11\\nPage \\n350\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ebcef7b818867adb6752d284a4e5b5b9'}>,\n",
              "  <Document: {'content': '2\\n. \\nFrom the list of certificates, locate the OBM\\n Trusted Certificate\\n.\\nFor example, \\nCA_297819c4-f266-75c1-0518-a723aacc1fde_2048\\n3\\n. \\nTo export the trusted certificate to a file, run the following command:\\nOn Linux\\n/opt/OV/bin/ovcert -exporttrusted -file obm_ca.crt -alias \"CA_297819c4-f266-75c1-0518-a723aacc1fde_2048\" -ovrg server\\nOn Windows\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcert\" -exporttrusted -file obm_ca.crt  -alias \"CA_297819c4-f266-75c1-0518-a723aacc1fde_2048\" -ovrg server\\n4\\n. \\nTo install an OBM CA certificate, do one of the following options:\\nInstall OBM CA certificate in Operations Bridge using the CLI\\na\\n. \\nCopy the \\nobm_ca.crt\\n file to the control plane (master) node of the Operations Bridge suite.\\nb\\n. \\nOn the control plane (master) node, install the OBM CA certificate using the\\n idl_config.sh\\n tool:\\nidl_config.sh -cacert <cert_file> -chart <chart> -namespace <namespace> [-release <release>]\\nFor example, run the following command after changing to the \\nintegration-tools\\n directory:\\ncd integration-tools/obm-configurator-interim\\n./idl_config.sh -cacert /tmp/obm_ca.crt -chart path/to/charts/\\nopsbridge-suite-2021.05.tgz -namespace opsb-suite\\nInstall OBM CA certificate in Operations Bridge using the AppHub\\na\\n. \\nChange the name of the \\nobm_ca.crt\\n file to a unique name that qualifies your classic OBM system. The name must start\\nwith \"client\" and end with the \".crt\" extension. Ensure that the filename must not be more than 20 characters.\\nFor example, \\nclient-abc-obm.crt\\nb\\n. \\nOn the AppHub UI, choose \\nDeployments\\n > \\nEdit \\nand click the \\nSecurity \\ntab. Refer to \\nReconfigure a deployment\\n topic.\\nc\\n. \\nUpload the new \\nUpload OPTIC Data Lake Client Authentication Certificates\\n certificate by using the option \\nClick\\nhere \\nor drag and add files for \\nUpload OPTIC Data Lake Client Authentication Certificates\\n.\\nd\\n. \\nClick on \\nVERIFY CERTIFICATE\\n. Make sure that the validation is successful.\\ne\\n. \\nClick on the \\nDatabases\\n tab and click on \\nVERIFY\\n for each of the databases. Then click \\nREDEPLOY\\n.\\nf\\n. \\nAfter some time, run the command to verify the pods which aren\\'t running:\\nkubectl get pods --all-namespaces -o wide | awk -F \" *|/\" \\'($3!=$4 || $5!=\"Running\") && $5!=\"Completed\" {print $0}\\'\\nConfigure the OBM system\\nThe \\nobm-configurator.jar\\n file is the setup tool for a classic OBM system. You must execute the tool on a classic OBM\\nTip: \\nThe OBM Trusted Certificate is usually with a * in the Trusted Certificates section.\\n\\ue917\\n\\ue917\\nNote: \\nYou can find the\\n idl_config.sh\\n tool in the \\nobm-configurator-interim \\ndirecto\\nry, which is in\\nthe \\nintegration-tools\\n direc\\ntory.\\n\\ue916\\n\\ue916\\nImportant:\\n \\nIf you have used an existing Shared OPTIC Data Lake, you must enter the Providing\\ndeployment\\'s chart name and Providing deployment application namespace.\\n  \\nFor example, if you have used\\nNOM\\'s Shared OPTIC DL, then you must provide NOM chart name and NOM application namespace.\\n\\ue91b\\n\\ue91b\\nImportant:\\n \\nYou must upload the obm_ca.crt in AppHub UI and reconfigure the deployment as mentioned in\\nthis section. If you skip this step and later try to upgrade using AppHub, the certificates will not be present in\\nAppHub and the integration will not work.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n351\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5c1731577ce9a02e5ac9f73ea997056d'}>,\n",
              "  <Document: {'content': \"Gateway system. This tool configures OBM with the Operations Bridge suite and creates the \\nconfigureSuite.zip\\n file in the\\nsame directory.\\nYou can use the tool to:\\nEstablish trust between OBM and OPTIC Data Lake \\nConfigure event forwarding\\nConfigure Automatic Event Correlation (AEC)\\nPerform the following steps:\\n1\\n. \\nTo copy the \\nobm-configurator.jar\\n file to the OBM Gateway system, run the following command :\\nscp integration-tools/obm-configurator.jar root@<obm_system>:/var/tmp\\nIf you have installed the OBM Gateway system on Windows, manually copy the tool to the system.\\n2\\n. \\nExecute the \\nobm-configurator.jar\\n tool on the OBM Gateway system. Enter passwords for \\nadmin\\n and \\nZIP file\\nencryption\\n when prompted.\\nUse the following syntax when executing the \\nobm-configurator.jar\\n with only the required parameters:\\n/opt/HP/BSM/JRE/bin/java -jar/var/tmp/obm-configurator.jar --endpoint-id <id> --suite-service-hostname <host> --obm-ca-cert-alias <cert-alias>\\nExecute the command as a \\nsudo\\n \\nuser\\n if you aren't using the \\nroot user\\n.\\nThe required \\nparameters are as follows:\\n--endpoint-id\\n: Defines the identifier used to register the OBM system in the suite. This parameter should be a readable\\nstring and is used to identify the OBM system when checking the registered OBM systems in the suite. \\n--suite-service-hostname\\n: Defines the FQDN of the system on which OBM can reach the \\nitom-di-receiver-svc\\n service. This\\nis typically the FQDN of the control plane (master) node.\\n--obm-ca-cert-alias\\n: After you install additional trusted certificates on OBM, it's recommended to use only the OBM CA\\ncertificate to configure the suite. To prevent an import of all trusted certificates from OBM into the suite, ensure that\\nyou specify the CA certificate alias by using the parameter, \\n--obm-ca-cert-alias\\n.\\nPassword parameters: You're prompted for passwords if you haven't specified them on the command line.\\nThe following optional parameters are updated with the default settings or operations if they're not specified:\\nNote: \\nYou can't use the \\nobm-configuration.jar \\ntool to configure a containerized OBM system.\\n\\ue916\\n\\ue916\\nNotes: \\nThe event forwarding from the OBM to OPTIC Data Lake is enabled immediately after you configure OBM. It's\\npossible that OBM immediately tries to forward events to the configured OPTIC Data Lake, while OPTIC Data\\nLake and the suite are still not configured. This might result in some warning events that mention that the\\nevent forwarding to OPTIC Data Lake has failed.\\nWhen you rerun the tool, it might abort because the suite certificates are already installed when the tool was\\nexecuted previously. In such a situation, add the \\n--force\\n parameter. The inclusion of the --force parameter in\\nthe command ensures that the tool execution proceeds even when the suite certificates are installed. Ensure\\nthat you rerun the tool with this parameter only when the installed certificates are from the current suite,\\nwhich was installed when the tool was run previously.\\nNote:\\nIf you add BVD after integrating through \\nobm-configurator.jar\\n tool fails due to duplicate servers then\\nupdate the DNS entry for the server. Run the following command:\\nopr-connected-server.bat -username -password -update -dns itom-di-receiver-svc\\nTo find the ID of the connected server, run the following command:\\nopr-connected-server.bat -list -username -password\\n\\ue916\\n\\ue916\\nImportant\\n: In cloud deployments, the default ports for DI Receiver, DI Data Access, and DI\\nAdministration are 30001, 30003, and 30004 respectively. If you didn't use the\\n Cloud Automation\\nToolkit\\n provided by Micro Focus and instead provisioned your cloud infrastructure manually using\\ndifferent ports, then specify these ports explicitly with the corresponding parameters that are mentioned.\\nContainerized Operations Bridge 2022.11\\nPage \\n352\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b57e71130542d9567ba426aadccd43ec'}>,\n",
              "  <Document: {'content': \"--configuration-type\\n: Defines the type of configuration especially if event correlation isn't desired. By default, the\\nvalue is AEC. You can set it to \\nFORWARDING\\n (to configure only event forwarding) or \\nTRUST_ONLY\\n (to\\nexchange certificates only). The valid options are as follows:\\nTRUST_ONLY: Exchanges certificates to establish trust between OBM and OPTIC Data Lake. Choose this\\noption if you want to configure OPTIC Reporting.\\nFORWARDING: Configures the classic OBM and the suite for event forwarding. Choose this option if you\\nwant to configure OPTIC Reporting, specifically the Event reports.\\nAEC: Configures OBM and the Operations Bridge suite for event forwarding and Automatic Event\\nCorrelation. By default, the option is set to AEC. Choose this option if you want to configure OPTIC Reporting\\nand AEC.\\n--integration-user\\n: Gets set to \\nOBM_event_submit_user\\n--obm-url\\n: Gets set to https://localhost:443, if not specified. If you haven't used TLS, you can specify the OBM\\nHTTP URL.\\n--itom-di-receiver-port\\n: You can use this parameter to overwrite the port, especially when you install the Operations\\nBridge suite on a cloud platform. This isn't a required option. The port gets automatically detected during tool\\ncreation.\\n--itom-di-administration-port\\n: You can use this parameter to overwrite the port, especially when you install the\\nOperations Bridge suite on a cloud platform. This isn't a required option. The port gets automatically detected\\nduring tool creation.\\n--itom-di-data-access-port\\n: You can use this parameter to overwrite the port, especially when you install the\\nOperations Bridge suite on a cloud platform. This isn't a required option. The port gets automatically detected\\nduring tool creation.\\n--force\\n: You can use this parameter to allow the tool execution to proceed when the suite certificates are already\\ninstalled after the tool was run previously. Although this isn't a required option, the tool execution might abort\\nbecause the suite certificates are already installed when the tool was executed previously.\\nExamples\\nEstablish trust between OBM and OPTIC Data Lake using basic authentication for OBM:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--configuration-type TRUST_ONLY \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--admin-user obmadmin\\nConfigure event forwarding from OBM to OPTIC Data Lake using basic authentication for the OBM administration\\nuser and the event integration user:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm\\\\\\n--configuration-type FORWARDING \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--admin-user obmadmin \\nConfigure Automatic Event Correlation where OBM uses client authentication for the OBM administration user\\nand the event integration user (default configuration type is AEC):\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--integration-user obm_integration_user \\\\\\n--admin-client-cert admin.user_cert.p12 \\\\\\n--client-cert integration.user_cert.pem \\\\\\n--client-key integration.user.key.pem \\nConfigure Automatic Event Correlation where you have set OBM without TLS. In this case, you must specify the\\nOBM URL:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--configuration-type AEC \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\nNote\\n: The \\nTRUST_ONLY\\n is a subset of \\nFORWARDING\\n, which in turn is a subset of AEC.\\nThis means that if you specify AEC, \\nobm-configurator.jar\\n establishes trust between OBM and\\nOPTIC Data Lake, and then configures event forwarding and also configures Automatic Event\\nCorrelation.\\nContainerized Operations Bridge 2022.11\\nPage \\n353\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f84e4f746e39e79bb04655e1bb3c3c41'}>,\n",
              "  <Document: {'content': '--admin-user obmadmin --integration-user obm_integration_user \\\\\\n--obm-url \"http://obm.company.com:8081\"\\nFor Windows, replace \\n/opt/HP/BSM/JRE/bin/java\\n with \\n%TOPAZ_HOME%\\\\JRE\\\\bin\\\\java.exe\\nFor more examples and a detailed description of all possible tool parameters, see the\\n OBM Configurator Tool\\ntopic.\\nSpecial certificate handling\\nDepending on the certificates that OBM uses, it might be necessary to specify the alias of the certificates, which are as follows:\\nOBM Web Certificate\\nWhen you use a CA-signed certificate to access the web (for example, accessing the OBM web services), you must specify\\nthe alias of the installed web certificate using the \\n--web-cert-alias\\n parameter. Run the following command to find the alias:\\nOn Linux\\n/opt/HP/BSM/bin/opr-cert-mgmt.sh -list\\nOn Windows\\n%TOPAZ_HOME%\\\\bin\\\\opr-cert-mgmt.bat -list\\nOBM CA Certificate\\nWhen you install additional trusted certificates on OBM, it\\'s recommended that you use only the OBM CA certificate\\nto configure the suite. To prevent an import of all trusted certificates from OBM into the Operations Bridge suite, you must\\nspecify the CA certificate alias using the \\n--obm-ca-cert-alias\\n parameter. Run the following command to find the alias of the\\nOBM CA certificate: \\nOn Linux\\n/opt/OV/bin/ovcert -list -ovrg server\\nOn Windows\\n%OvInstallDir%\\\\bin\\\\win64\\\\ovcert.exe -list -ovrg server\\nOBM Client Certificates\\nIf you use client certificates to access OBM, you must specify the certificate files for the setup. When using client\\ncertificates, make sure that:\\nthe certificate for the OBM administration user is in \\nPKCS12\\n format\\nthe certificate and key for the integration user is in \\nPEM \\nformat\\nClient Certificates\\nTool Parameter\\nFormat\\nDescription\\n--admin-client-cert\\nPKCS12\\nOBM admin user client certificate and key chain\\n--client-cert\\nPEM\\nIntegration user client certificate\\n--client-key\\nPEM\\nIntegration user client key\\nNote:  \\nIt\\'s recommended that the certificates should have a valid Subject Alternative Name (SAN). You\\ncan run the following command to verify if a certificate has a SAN: \\nopenssl x509 -noout -ext subjectAltName -in <certificate file>\\nFor example:\\nopenssl x509 -noout -ext subjectAltName -in server.crt\\nThe sample output after running the command is as follows:\\nX509v3 Subject Alternative Name:\\n    DNS:omi, DNS:omi-0, DNS:omi-0.omisvc, DNS:omi-0.omisvc.opsb-helm, DNS:omi-0.omisvc.opsb-helm.svc, DNS:omi-0.omi\\nsvc.opsb-helm.svc.cluster.local, DNS\\n:omi-0.omisvc.opsb-helm.svc.cluster.local.\\nContainerized Operations Bridge 2022.11\\nPage \\n354\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a68dfc02fa39c0abd04cad91bade890d'}>,\n",
              "  <Document: {'content': 'Configure the Operations Bridge suite\\nThe obm-configurator.jar tool creates the \\nconfigureSuite.zip\\n file. Perform the following steps to use the\\ncreated \\nconfigureSuite.zip\\n file and configure the suite:\\n1\\n. \\nTo copy the \\nconfigureSuite.zip\\n file to the \\n/var/tmp \\ndirectory of the control plane (master) node, run the following\\ncommand:\\nscp configureSuite.zip root@<suite_master_hostname>:/var/tmp\\nFor example:\\nscp configureSuite.zip root@suitemaster.example.com:/var/tmp\\nIf you have installed the OBM system on Windows, manually copy the package to the control plane (master) node.\\n2\\n. \\nOn the control plane (master) node, extract the package:\\ncd /var/tmp\\nunzip configureSuite.zip -d configureSuite\\n3\\n. \\nTo run the \\nsetup-obm.sh\\n tool in the \\n/var/tmp/configureSuite\\n directory, run the following command.\\ncd configureSuite\\nbash setup-obm.sh -chart <path>\\nwhere \\nchart\\n is the path to either a directory containing the chart or a path to a \\ngzipped TAR\\n file.\\nFor example:\\ncd configureSuite\\nbash setup-obm.sh -chart /path/to/opsbridge-suite-2020.10.0-149.tgz\\nThe \\nsetup-obm.sh\\n tool does the following in the Operations Bridge suite:\\nInstalls the OBM CA certificate in the suite.\\nConfigures the OBM system as a data source and receiver for AEC events (if you set the \\n--configuration-type\\n to \\nAEC\\n).\\nTo verify the OBM configuration:\\nVerify import of all OPTIC Data Lake certificates\\nEnsure that all OPTIC Data Lake certificates are imported into OBM.\\n1\\n. \\nRun the following command on OBM:\\nOn Linux\\n/opt/OV/bin/ovcert -list\\nOn Windows\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcert\" -list\\nThe command lists all trusted certificates. Make sure that the certificate starting with \\nMF CDF\\n exists in the trusted list.\\nImportant:\\n \\nIf you have used an existing Shared OPTIC Data Lake, you must enter the Providing\\ndeployment\\'s chart name with the absolute path.\\n  \\nFor example, if you have used NOM\\'s Shared OPTIC DL,\\nthen you must provide NOM chart name.\\n\\ue91b\\n\\ue91b\\nNote\\n: The following certificate verification step is valid for on-premises installations only. The certificate names\\nvary in AWS and Azure environments.\\nNote\\n: The certificate name starting with \\nMF CDF\\n must exist for on-premises installations, and this name might\\nvary in cloud deployments such as in AWS, Azure, or OpenShift environments.  In cloud deployments, the\\nContainerized Operations Bridge 2022.11\\nPage \\n355\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c915d1ff8de3f333bf9e5b15fed54d10'}>,\n",
              "  <Document: {'content': \"Here's a sample output after running the command:\\nVerify event forwarding\\n1\\n. \\nTo verify the configuration, run the following command on the OBM Gateway server and send a test event to OPTIC Data\\nLake:\\nOn Windows:\\nGo to\\n \\n%TOPAZ_HOME%\\\\opr\\\\support\\n and run the command, \\nsendEvent.bat -j -t TestEvent -s normal\\nHere's a sample output after running the command:\\nOn Linux:\\nGo to \\n/opt/HP/BSM/opr/support\\n and run the command,\\n ./sendEvent.sh -j -t TestEvent -s normal\\nHere's a sample output after running the command:\\n2\\n. \\nFrom the OBM menu, choose \\nWorkspaces \\n> \\nOperations Console\\n and click \\nEvent Perspective\\n.\\n \\nCheck if the test\\nevent is listed and verify if the \\nState\\n of the event shows as, \\nForwarded\\n.\\n3\\n. \\nYou can also check the \\nopr_event\\n table in the \\nmf_shared_provider_default \\nschema to verify if the event has reached\\ncertificate names are dependent on the cloud environments that you're using.\\nNote\\n: The events that occur after you activate the Event Forwarding Rule, flow into OPTIC Data Lake.\\nContainerized Operations Bridge 2022.11\\nPage \\n356\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '55292d97ba9618d02d7eeb81abb7d065'}>,\n",
              "  <Document: {'content': \" \\n       \\nIntroduction\\n: This shows the welcome screen.\\nLicense Agreement\\n: This shows the License Agreement for using the Data Flow Probe. Select \\nI accept the terms\\nof the License Agreement \\nand click \\nNext.\\nSetup Type \\n(For Windows only): Leave the selection on the default option \\nFull Data Flow Probe Installation\\n.\\nInstallation type\\n: Choose \\nNew Installation \\nand click\\n Next.\\nSelect Installation Folder\\n: To choose a different installation path, you can either enter the path manually in the\\ntext field or click \\nChoose…\\n (to open the file explorer). If you want to restore the default installation path,\\nclick \\nRestore Default Folder\\n.\\nProbe Configuration\\n: To connect the Data Flow Probe to OBM:\\n   \\nUnder \\nApplication to report to\\n, select \\nBSM.\\nIn the \\nApplication Server address\\n box, enter the FQDN of the OBM gateway server. If you use a load\\nbalancer, this should be the FQDN of the load balancer.\\nIn the \\nData Flow Probe address box\\n, enter the IP address or FQDN of the machine on which the Data Flow\\nProbe is currently installed, or accept the default.\\nClick \\nNext.\\nIn the \\nCustomer ID \\nbox, leave the value to default (The default value is 1.) and click \\nNext.\\nIs Basic Authentication enabled for the UCMDB server? - \\nSelect \\nNo \\nand click\\n Next.\\nData Flow Probe identifier - \\n This is the display name of the Data Flow Probe. This name is visible in\\nthe OBM GUI and helped to identify it. If you select \\nUse Default UCMDB Domain\\n, the Data Flow Probe gets\\ncreated under the 'DefaultDomain' domain. If you don't select \\nUse Default UCMDB Domain\\n, you must create\\na custom domain name in the next step under \\nProbe Domain Config\\n.\\nNote\\n: If you install the Data Flow Probe on an OBM gateway server, make sure to choose an\\ninstallation path different from the OBM installation path.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n360\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '995e46259db4bbdb9ae2e8f40f6898eb'}>,\n",
              "  <Document: {'content': 'OPTIC Data Lake.\\nOn a system that has the vsql command, such as a Vertica node, run the command:\\n/opt/vertica/bin/vsql -U dbadmin -c \"select node_hint,title,timestamp from mf_shared_provider_default.opr_event where title ilike \\'testEvent\\' limit 10;\"\\nYou are prompted \\nto enter the password for the \\ndbadmin\\n user. You can specify a different user such as the user, \\n<vertica_r\\nouser> \\nthat you created in the \\nPrepare Vertica database\\n section.\\nAfter completing the aforementioned tasks, the events from OBM are sent to OPTIC Data Lake. You will also be able to view the\\nevent reports in BVD (See Event reports for more information). Only the \\'Events by CI report\\' and the \\'Top event flow by CI\\'\\nsection of the Event Executive Summary report won\\'t show any data as these depend on topology. If you want to see these\\nreports, perform the remaining tasks to forward topology to OPTIC Data Lake.\\nTask 4: Install the UD content pack\\nThe Topology based correlation requires UD content pack.\\nFollow the steps to install the UD content pack:\\n1\\n. \\nDownload the installation file \\nUD Content Pack 202x.xx 202x.xx.zip\\n from the \\nMarketplace\\n. \\nAlthough there are newer versions available make sure to download\\n UD Content Pack \\n202x.xx 202x.xx.zip\\n by clicking\\non \"See previous releases\" as seen in the pictures below\\n \\n2\\n. \\nExtract the contents of the \\nCP202x_xx_installation.zip\\n. You will find \\nCP202x.xx.xxx.zip\\n.\\n3\\n. \\nCopy the \\nCP202x.xx.xxx.zip \\nfile on to the OBM DPS:\\nOn Windows: \\n%TOPAZ_HOME%\\\\ucmdb\\\\content\\\\content_packs \\nOn Linux: \\n/opt/HP/BSM/ucmdb/content/content_packs \\n4\\n. \\n(Optional) Download and execute the UCMDB Local Client:\\nNote\\n:\\n Skip tasks 4-6 below if you have already forwarded topology as mentioned in the \\nForward topology\\nfrom classic OBM to OPTIC Data Lake\\n section of this document.\\n\\ue916\\n\\ue916\\nImportant:\\nSkip this task if you are using classic OBM 2021.05.\\nPerform this task only if you are using the classic OBM versions earlier than 2021.05.\\n\\ue91b\\n\\ue91b\\nNote\\n: \\nThe UCMDB Local Client allows you to launch the RTSM application from your local system. Use this option if\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n357\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '42a97b08b9f65cf5e29f256d3050b721'}>,\n",
              "  <Document: {'content': 'Task 2: Download and install the Data flow probe\\nFollow the steps:\\nTask A: Verify the version of RTSM\\nOBM on Linux:\\n1\\n. \\nGo to \\n/opt/HP/BSM/ucmdb\\n2\\n. \\nRun the command: \\ncat version.dat\\n. The version gets displayed.\\nOBM on Windows:\\n1\\n. \\nGo to \\n\\\\HPBSM\\\\ucmdb\\n2\\n. \\nOpen the \\nversion.dat \\nto check the version.\\nTask B: Download the Data Flow Probe installation file\\nThe Data Flow Probe (DFP) installation\\n .zip\\n is a part of the OBM package and it contains installation files for Linux and\\nWindows.\\nDownload the Data Flow Probe installation file from \\nMicro Focus Software Licenses and Downloads\\n. The filename would be\\nOBM_202x.xx_DataFlowProbe.zip\\n. You may get the DFP from \\nhere\\n.\\nExtract the contents and go to:\\nOn Linux:\\n \\nSoftware\\\\Linux\\\\UCMDB_DataFlowProbe-<version>.bin\\nOn Windows: \\nSoftware/Windows/UCMDB_DataFlowProbe-<version>.exe\\nTask C: Install the Data Flow Probe\\nYou can install the Data Flow Probe anywhere but you should establish a connection between the Data Flow Probe and\\nthe OBM gateway server. You can install multiple Data Flow Probes, one on each server. Each Data Flow Probe can connect\\nonly to one OBM gateway server.\\nFollow the steps:\\n1\\n. \\nExecute the installation file:\\nOn Linux:\\n  \\nSoftware/Linux/UCMDB_DataFlowProbe_<version>.bin\\nOn Windows\\n: \\nSoftware\\\\Windows\\\\UCMDB_DataFlowProbe_<version>.exe\\nA splash screen appears.\\n2\\n. \\nChoose a language from the drop-down menu and click \\nOK\\n to continue.\\n3\\n. \\nThe installation wizard appears. For every page, complete the necessary information and click \\nNext\\n to go to the next\\npage. On the last page click \\nInstall\\n.\\nNote\\n:\\n If you are installing the Content Pack on a High Availability system, copy all files on the active UCMDB\\nserver to the same directory on the passive UCMDB server:\\nOn Windows: \\n%TOPAZ_HOME%\\\\runtime\\\\fcmdb\\\\CodeBase\\nOn Linux: \\n/opt/HP/BSM/ucmdb/runtime/fcmdb/CodeBase\\n\\ue916\\n\\ue916\\nImportant: \\nThe version of the Data Flow Probe (DFP) must be the same as the RTSM version\\n.\\nNote\\n:  To install the Data Flow Probe on Red Hat Linux 7, you must first create a symbolic link from \\nlibsasl2.so.3.0.0 \\nto libsasl2.so.2\\n before installing the Data Flow Probe.\\nFor example:\\ncd /usr/lib64\\nln -s ./libsasl2.so.3.0.0 ./libsasl2.so.2\\nNote\\n: To execute the installer, you require a system with GUI. On Linux, enable the \\nX11\\n server display\\nmode.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n359\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f83b20a1ea00251d4086eface994188'}>,\n",
              "  <Document: {'content': '1\\n. \\nOn OBM, go to \\nAdministration\\n > \\nRTSM Administration\\n and click \\nLocal Client\\n.\\n2\\n. \\nIn the \\nLocal Client \\nwindow, based on your operating system, click a link to download the UCMDB Local Client.\\n3\\n. \\nExtract the contents of the UCMDB Local Client zip file (For example, \\nUCMDB_Local_Client_Win\\n) and execute the script.\\n4\\n. \\nIn the UCMDB Local Client window, click \\nAdd\\n. In the \\nAdd/Edit Configuration\\n window, add all the required details: \\n \\n \\nThe required credentials are the same that you use for the Web UI.\\n5\\n. \\nInstall the UD content pack:\\n1\\n. \\nLog in to the UCMDB UI using the UCMDB Local Client. Give the same admin user credentials that you used to log in\\nto OBM.\\n2\\n. \\nGo to \\nAdministration > Package Manager.\\n3\\n. \\nOn the toolbar, click the \\nInstall Content Pack\\n \\n \\nbutton.\\n4\\n. \\nIn the Install Content Pack dialog box that opens, select the version of the content pack, and click \\nInstall\\n. It takes\\nabout 15 minutes to install the content pack\\n6\\n. \\nVerify the installation: Verify that there are no errors in the \\nmam.packaging.log\\n file located at:\\nOn Windows: \\n%TOPAZ_HOME%\\\\ucmdb\\\\\\nruntime\\\\log\\nOn Linux: \\n/opt/HP/BSM/ucmdb\\n/runtime/log\\nVerify that there are no errors in the \\nmam.packaging.log\\n file.\\nOn the UCMDB Local Client, go to \\nData Flow Management > Adapter Management\\n \\nCheck if \\nPulsarPushAdapter\\n is present.\\nyour browser does not support Java Applets.\\nSkip this step if you already have the UCMDB Local Client installed.\\nContainerized Operations Bridge 2022.11\\nPage \\n358\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e200b96369d765b11601803bebd54c4'}>,\n",
              "  <Document: {'content': 'Scan File Upload Config\\n: Create the username (You may leave this to the default option) and password for\\nuploading scan files.\\nPre-Installation Summary\\n: Displays the product name (always: UCMDB Data Flow Probe), the installation folder,\\nand disk space information. Click \\nInstall\\n to start the installation.\\nSelect one of the following options:\\nHigh. Enable CA certificate validation with CRL check.\\nMedium. Enable CA certificate validation without a CRL check.\\nLow. Check for the existence of a certificate.\\nIf you select the \\nHigh\\n or \\nMedium\\n option, you will get the following error: \\'Invalid UCMDB server certificate\\ndetected!\\' Click \\nIgnore\\n.\\nInstalling…\\n: Shows the progress of the installation.\\nYou may choose to remove or keep the Users group access privilege from the Probe installation folder and click\\nNext\\n.\\nYou will get a confirmation message. Click \\nDone\\n to exit the installer wizard.\\nYou can find the install logs at: \\n<DFP install folder>/UninstallerData/Logs/UCMDB_Data_Flow_Probe_Install_<date and time>.lo\\ng\\n4\\n. \\n For Linux only: After the installation, open the \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n file. Verify that the \\nappilog.agent.probe.integrationsOnlyProbe\\n parameter gets set to \\nfalse\\n.\\n5\\n. \\nIf OBM isn\\'t using TLS, edit the \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n file and change the \\nappilog.agent.pro\\nbe.protocol\\n to HTTP\\n6\\n. \\nIf you enable TLS in OBM, enable TLS in the Data Flow Probe:\\n1\\n. \\nOpen the file \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n.\\n2\\n. \\nChange the property \\nserverPortHttps\\n from 8443 to 443.\\n7\\n. \\nIf you enable TLS in OBM, import the corresponding certificates to the \\nJava Keystore\\n of the Data Flow Probe: \\n1\\n. \\nTo get the certificate, run the following command on the OBM system:\\nOBM on Windows: \\n%TOPAZ_HOME%\\\\bin\\\\opr-cert-mgmt.bat -export \"OBM Webserver CA Certificate\" PEM \"C:\\\\certificate.pem\"\\nOBM on Linux:\\necho | openssl s_client -showcerts -servername ${HOSTNAME} -connect ${HOSTNAME}:443 2>/dev/null | sed -n \\'/-----BEGIN CERTIFICATE\\n-----/,/-----END CERTIFICATE-----/p\\' > /root/certificate.pem\\nImportant\\n: \\n{HOSTNAME}\\n would be the FQDN of the OBM server. \\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n362\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1a5a946460a400c1334192932d82923d'}>,\n",
              "  <Document: {'content': \"Probe Domain Config\\n (shown only if \\nUse Default UCMDB Domain\\n isn't selected in the previous step):\\nData Flow Probe domain type\\n: You can leave this on the default option \\nCustomer.\\n \\nData Flow Probe domain\\n: 'DefaultDomain' is the default value. You can keep the default value or change\\nit.\\nProbe Working Mode\\n (Windows only): You may leave this to the default option \\nNo\\n.\\nProbe Memory Size\\n: You may leave this to the default option.\\nAccount Configuration\\n: Create the following passwords:\\nConfigure PostgreSQL Data Flow Probe Account\\n: The Data Flow Probe uses this password to connect to\\nthe embedded PostgreSQL database. The password must contain 8 to 16 characters and must include at least\\none of each of the following four types of characters:\\nUppercase alphabetical characters\\nLowercase alphabetical characters\\nNumeric characters\\nSpecial characters: : / . _ + - [ ]\\nConfigure PostgreSQL Root Account\\n:  You require this password to perform administrative tasks on the\\nembedded PostgreSQL database. \\nSystem Administration (sysadmin)\\n:  This password is for the system administrator (sysadmin) account. \\nSet Up Keystore Password\\n: This password is for the keystore.\\nSet Up Truststore Password\\n: This password is for the truststore.\\nContainerized Operations Bridge 2022.11\\nPage \\n361\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8f8d2d2c186c489e8fe27de8b503b4ac'}>,\n",
              "  <Document: {'content': '1\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Adapter Management >\\nPulsar Push Adapter > Configuration Files > PulsarPushAdapter/settings.json \\nIf you are using the UCMDB Local Client go to \\nData Flow Management > Adapter Management > Pulsar Push\\nAdapter > Configuration Files > PulsarPushAdapter/settings.json.\\n2\\n. \\nTo check if you have configured the Pulsar Push Adapter, change the value of \"\\ntest.connection.topic\\n\" to\\n\"persistent://public/default/mf_shared_cmdb_entity_configuration_item_raw\\n\":\\n3\\n. \\nOn OBM Data Processing Server (DPS), run the following to create a client certificate signed by OBM CA. This certificate\\nenables secure communication between Data Flow Probe and OPTIC Data Lake.\\nOn Linux:\\nrm -f /tmp/dfp_client*\\ncd /opt/OV/bin/\\n./ovcm -issue -file /tmp/dfp_client.p12 -name \"UCMDB Data Flow Probe\" -coreid \"itom-cms-dataflowprobe\" -pass \"dataflowprobe\"\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.crt -nokeys -passin pass:dataflowprobe\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.key -nocerts -passin pass:dataflowprobe -nodes\\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in /tmp/dfp_client.key -out /tmp/dfp_client-key-pk8.pem -nocrypt\\nOn Windows:\\n1\\n. \\nGo to \\nC:\\\\Program Files\\\\HP\\\\HP BTO Software\\\\bin\\\\win64\\n2\\n. \\nRun the following command: \\novcm -issue -file C:\\\\dfp_client.p12 -name \"UCMDB Data Flow Probe\" -coreid \"itom-cms-dataflowprobe\" -pass \"dataflowprobe\"\\n3\\n. \\nManually copy \\ndfp_client.p12\\n certificate from OBM to the \\n/tmp\\n directory on the master (control plane) node.\\n4\\n. \\nLogin into the master (control plane) node and go to the \\n/tmp\\n directory \\n5\\n. \\nRun the following commands: \\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.crt -nokeys -passin pass:dataflowprobe\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.key -nocerts -passin pass:dataflowprobe -nodes\\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in /tmp/dfp_client.key -out /tmp/dfp_client-key-pk8.pem -nocrypt\\n6\\n. \\nManually copy \\ndfp_client.crt\\n, \\ndfp_client-key-pk8.pem\\n certificates from the master (control plane) node to the OBM.\\n4\\n. \\nIf OBM and DFP are on different systems, copy the generated files \\ndfp_client.crt\\n and \\ndfp_client-key-pk8.pem\\n to the DFP\\nmachine and use them to configure the probe. \\n5\\n. \\nCopy the \\nissue_ca.crt\\n to the DFP machine and use it to configure the probe (\\nissue_ca.crt\\n is a part of the OBM integration\\ntools that you procured earlier. See \\nConfigure Classic OBM\\n).\\n6\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Integration Studio \\nto create\\nan integration point.\\nIf you are using the UCMDB Local client, go to \\nData Flow Management > Integration Studio\\n to create an integration\\npoint.\\n1\\n. \\nClick  \\n \\n. The \\nNew Integration Point\\n window appears.\\nTopology integration from an external UCMDB instance into OPTIC DL is currently not supported, but only\\nthrough (classic) OBM/RTSM.\\nThis Pulsar topic exists in OPTIC Data Lake. You will use this later to test the communication from RTSM to\\nPulsar.\\nContainerized Operations Bridge 2022.11\\nPage \\n364\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'deec5479a10f1d3314f5254bf21b25a5'}>,\n",
              "  <Document: {'content': \"2\\n. \\nIf OBM and DFP are on separate systems, copy \\ncertificate.pem \\nto the Data Flow Probe system.\\nFor example: \\nOBM on Linux:\\n \\nscp /root/certificate.pem <DFP Node>:/root\\n \\n3\\n. \\nRun the following command to import the OBM certificate to the Data Flow Probe (DFP) system:\\nOn Linux:\\n/opt/UCMDB/DataFlowProbe/bin/jre/bin/keytool -import -trustcacerts -file /root/certificate.pem -alias obmcert -keystore  /opt/UCMDB/DataF\\nlowProbe/bin/jre/lib/security/cacerts  \\n  \\nOn Windows:\\nC:\\\\UCMDB\\\\DataFlowProbe\\\\bin\\\\jre\\\\bin\\\\keytool.exe -import -trustcacerts -file C:\\\\certificate.pem -alias obm_smperfqa02 -keystore  C:\\\\UCMD\\nB\\\\DataFlowProbe\\\\bin\\\\jre\\\\lib\\\\security\\\\cacerts \\n \\nTask D: Restart the Data Flow Probe\\nFollow the steps:\\nOn Windows:\\n1\\n. \\nGo to \\n<Data Flow Probe install folder>/bin\\n2\\n. \\nRun \\ngateway.bat.\\nFor example: \\n<Data Flow Probe install folder>/bin/gateway.bat restart\\nStarts the Data Flow Probe on a Windows server.\\nOr\\nUse the stop/start menu entries.\\nOn  Linux:\\n1\\n. \\nGo to\\n /opt/UCMDB/DataFlowProbe/bin\\n2\\n. \\nRun \\nProbeGateway.sh\\n with the argument restart.\\nFor example: \\n/opt/UCMDB/DataFlowProbe/bin/ProbeGateway.sh\\n \\nrestart\\nStarts the Data Flow Probe on a Linux server.\\nTask E: Verify the Data Flow Probe Connection\\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Data Flow Probe Setup\\nIf the Data Flow Probe is correctly connected, the domain for which it's created is visible under the \\nDomains and Probes\\n root\\nnode. For example, \\nDefaultDomain (Default)\\n. Under this domain, you will find two nodes: \\nCredentials\\n and \\nData Flow\\nProbes\\n.\\nUnder \\nData Flow Probe\\n, the newly created Data Flow Probe with the given Data Flow Probe name gets displayed.\\nVerify that the status of the probe is 'Connected'.\\nTask 3: Configure topology streaming from RTSM to OPTIC Data Lake \\nFollow the steps to forward topology from OBM's RTSM to OPTIC Data Lake:\\nNote\\n: Set the password to \\nchangeit\\n and when prompted type \\nyes\\n to trust the certificate.\\n\\ue916\\n\\ue916\\nNote\\n:\\n Make sure you use the Internet Explorer browser or the UCMDB Local Client.\\n\\ue916\\n\\ue916\\nImportant:\\nBy default, DFP tries to start on port 80. If DFP and OBM (OBM isn't using TLS) are on the same machine, OBM\\nmay occupy port 80 and you may not see the newly created Data Flow Probe. In such a case, go to \\n<Data Flow Prob\\ne install folder>/conf/DataFlowProbe.properties\\n  and set the value of  \\nappilog.agent.callhome.port\\n to an available port (For\\nexample: \\nappilog.agent.callhome.port = 8081\\n) and restart Data Flow Probe.\\n\\ue91b\\n\\ue91b\\nImportant:\\nUse the Internet Explorer browser or the UCMDB Local Client.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n363\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8ec1f5a233bdcf30186bc0d6a4dc288'}>,\n",
              "  <Document: {'content': 'Add the following:       \\nIntegration Name\\n: Enter a name. For example: RTSM_COSO_Topology_Streaming\\nAdapter\\n: Click \\n \\nSelect Adapter\\n, you will see a list of all adapters. Select \\nPulsar Push Adapter\\n.       \\nIs Integration Activated\\n: Select the check box      \\nHostname/IP\\n: Add hostname. Note that hostname must be in the following format: \\npulsar+ssl://<externalAccessHo\\nst>:31051\\nContainerized Operations Bridge 2022.11\\nPage \\n369\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f7086167b494a8d6a5f238d32da9e9e0'}>,\n",
              "  <Document: {'content': \"Credentials ID\\n: Click  \\n \\n \\nCreate New Credentials\\n. Choose Credentials window appears. Click \\n \\n and add\\nthe following:    \\nProtocol\\n: Choose \\nTWO-WAY-SSL\\n   \\nTrust Cert File Path\\n: Enter the path of the \\nissue_ca.crt \\n(\\nissue_ca.crt\\n is a part of the OBM integration tools\\nthat you procured earlier. See \\nConfigure Classic OBM\\n.)\\nTLS Cert File Path\\n: Enter the path of the \\ndfp_client.crt \\n(You generated the \\ndfp_client.crt\\n in step 3.)\\nTLS Key File Path\\n: Enter the path of the \\ndfp_client-key-pk8.pem \\n(You generated the \\ndfp_client-key-pk8.pem\\n in\\nstep 3.)\\nClick \\nOK.\\n  \\nClick \\nOK\\nData Source Id\\n: Enter an Id. Default Id is UCMDB.           \\nData Flow Probe\\n: Select the name of the Data Flow Probe.       \\n2\\n. \\nClick \\nTest Connection\\n. Here you are testing access to the\\n mf_shared_cmdb_entity_configuration_item_raw\\n  that you\\nspecified earlier. If the connection is successful, click \\nOk\\n. If the connection isn't successful, verify the aforementioned\\nsteps, and test again.\\n7\\n. \\nIn the Integration studio, select the \\nIntegration Point \\n(Pulsar Push Job) created in OBM and click \\n \\nFull\\nSynchronization\\n.\\n8\\n. \\nClick \\nRefresh\\n to see the status of the topology synchronization.\\n9\\n. \\nFollow the steps to schedule delta synchronization:\\n1\\n. \\nIn the Integration studio, select the Integration Point (Pulsar Push Job) and click \\nEdit\\n.\\n2\\n. \\nIn the \\nEdit Integration Job \\nwindow, go to the \\nDelta Synchronization \\ntab, and select the \\nScheduler enabled\\ncheck box.\\n3\\n. \\nSelect a specific interval of your choice and click \\nOK\\n.\\n4\\n. \\nClick \\nRefresh.\\nImportant:\\n In cloud deployments that use private domain names, set the port to 6651.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n370\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '24531ac557952147f2dca366038b1a91'}>,\n",
              "  <Document: {'content': \"Credentials ID\\n: Click  \\n \\n \\nCreate New Credentials\\n. Choose Credentials window appears. Click \\n \\n and add\\nthe following:    \\nProtocol\\n: Choose \\nTWO-WAY-SSL\\n   \\nTrust Cert File Path\\n: Enter the path of the \\nissue_ca.crt \\n(\\nissue_ca.crt\\n is a part of the OBM integration tools\\nthat you procured earlier. See \\nConfigure Classic OBM\\n.)\\nTLS Cert File Path\\n: Enter the path of the \\ndfp_client.crt \\n(You generated the \\ndfp_client.crt\\n in step 3.)\\nTLS Key File Path\\n: Enter the path of the \\ndfp_client-key-pk8.pem \\n(You generated the \\ndfp_client-key-pk8.pem\\n in\\nstep 3.)\\nClick \\nOK.\\n  \\nClick \\nOK\\nData Source Id\\n: Enter an Id. Default Id is UCMDB.           \\nData Flow Probe\\n: Select the name of the Data Flow Probe.       \\n2\\n. \\nClick \\nTest Connection\\n. Here you are testing access to the\\n mf_shared_cmdb_entity_configuration_item_raw\\n  that you\\nspecified earlier. If the connection is successful, click \\nOk\\n. If the connection isn't successful, verify the aforementioned\\nsteps, and test again.\\n7\\n. \\nIn the Integration studio, select the \\nIntegration Point \\n(Pulsar Push Job) created in OBM and click \\n \\nFull\\nSynchronization\\n.\\n8\\n. \\nClick \\nRefresh\\n to see the status of the topology synchronization.\\n9\\n. \\nFollow the steps to schedule delta synchronization:\\n1\\n. \\nIn the Integration studio, select the Integration Point (Pulsar Push Job) and click \\nEdit\\n.\\n2\\n. \\nIn the \\nEdit Integration Job \\nwindow, go to the \\nDelta Synchronization \\ntab, and select the \\nScheduler enabled\\ncheck box.\\n3\\n. \\nSelect a specific interval of your choice and click \\nOK\\n.\\n4\\n. \\nClick \\nRefresh.\\nImportant:\\n In cloud deployments that use private domain names, set the port to 6651.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n366\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '22d97bdc9fd461530792832f60555533'}>,\n",
              "  <Document: {'content': 'Add the following:       \\nIntegration Name\\n: Enter a name. For example: RTSM_COSO_Topology_Streaming\\nAdapter\\n: Click \\n \\nSelect Adapter\\n, you will see a list of all adapters. Select \\nPulsar Push Adapter\\n.       \\nIs Integration Activated\\n: Select the check box      \\nHostname/IP\\n: Add hostname. Note that hostname must be in the following format: \\npulsar+ssl://<externalAccessHo\\nst>:31051\\nContainerized Operations Bridge 2022.11\\nPage \\n365\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4aee8b775b4034def6bb62583a1259e1'}>,\n",
              "  <Document: {'content': \"Related topics\\nFor more information about the UD content pack installation, see \\nContent Pack Installation\\n.\\nTo customize the data retention, see \\nCustomize data retention\\n.\\nTask 5: Download and install the data flow probe\\nTask 6: Configure topology data streaming from RTSM to OPTIC Data Lake\\nFollow the steps to forward topology from OBM's RTSM to OPTIC Data Lake:\\n1\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Adapter Management >\\nPulsar Push Adapter > Configuration Files > PulsarPushAdapter/settings.json \\nIf you are using the UCMDB Local Client go to \\nData Flow Management > Adapter Management > Pulsar Push\\nAdapter > Configuration Files > PulsarPushAdapter/settings.json.\\nNote:\\nFor every full or delta synchronization, the CI details including attribute changes are sent from RTSM to OPTIC\\nData Lake.\\nIf a CI gets deleted from RTSM, the status of the CI gets updated as 'deleted' but the CI isn't deleted from\\nOPTIC Data Lake. \\nImportant:\\nUse the Internet Explorer browser or the UCMDB Local Client.\\nTopology integration from an external UCMDB instance into OPTIC DL is currently not supported, but only\\nthrough (classic) OBM/RTSM.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n367\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd2aaec235eec1654493c0f1e55b1bbe4'}>,\n",
              "  <Document: {'content': '2\\n. \\nTo check if you have configured the Pulsar Push Adapter, change the value of \"\\ntest.connection.topic\\n\" to\\n\"persistent://public/default/mf_shared_cmdb_entity_configuration_item_raw\\n\":\\n3\\n. \\nOn OBM Data Processing Server (DPS), run the following to create a client certificate signed by OBM CA. This certificate\\nenables secure communication between Data Flow Probe and OPTIC Data Lake.\\nOn Linux:\\nrm -f /tmp/dfp_client*\\ncd /opt/OV/bin/\\n./ovcm -issue -file /tmp/dfp_client.p12 -name \"UCMDB Data Flow Probe\" -coreid \"itom-cms-dataflowprobe\" -pass \"dataflowprobe\"\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.crt -nokeys -passin pass:dataflowprobe\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.key -nocerts -passin pass:dataflowprobe -nodes\\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in /tmp/dfp_client.key -out /tmp/dfp_client-key-pk8.pem -nocrypt\\nOn Windows:\\n1\\n. \\nGo to \\nC:\\\\Program Files\\\\HP\\\\HP BTO Software\\\\bin\\\\win64\\n2\\n. \\nRun the following command: \\novcm -issue -file C:\\\\dfp_client.p12 -name \"UCMDB Data Flow Probe\" -coreid \"itom-cms-dataflowprobe\" -pass \"dataflowprobe\"\\n3\\n. \\nManually copy \\ndfp_client.p12\\n certificate from OBM to the \\n/tmp\\n directory on the master (control plane) node.\\n4\\n. \\nLogin into the master (control plane) node and go to the \\n/tmp\\n directory \\n5\\n. \\nRun the following commands: \\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.crt -nokeys -passin pass:dataflowprobe\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.key -nocerts -passin pass:dataflowprobe -nodes\\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in /tmp/dfp_client.key -out /tmp/dfp_client-key-pk8.pem -nocrypt\\n6\\n. \\nManually copy \\ndfp_client.crt\\n, \\ndfp_client-key-pk8.pem\\n certificates from the master (control plane) node to the OBM.\\n4\\n. \\nIf OBM and DFP are on different systems, copy the generated files \\ndfp_client.crt\\n and \\ndfp_client-key-pk8.pem\\n to the DFP\\nmachine and use them to configure the probe. \\n5\\n. \\nCopy the \\nissue_ca.crt\\n to the DFP machine and use it to configure the probe (\\nissue_ca.crt\\n is a part of the OBM integration\\ntools that you procured earlier. See \\nConfigure Classic OBM\\n).\\n6\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Integration Studio \\nto create\\nan integration point.\\nIf you are using the UCMDB Local client, go to \\nData Flow Management > Integration Studio\\n to create an integration\\npoint.\\n1\\n. \\nClick  \\n \\n. The \\nNew Integration Point\\n window appears.\\nThis Pulsar topic exists in OPTIC Data Lake. You will use this later to test the communication from RTSM to\\nPulsar.\\nContainerized Operations Bridge 2022.11\\nPage \\n368\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6f0ee61ce206a4ecd8ee1948ac617144'}>,\n",
              "  <Document: {'content': 'Add the following:       \\nIntegration Name: Enter a name. For example: RTSM_COSO_Topology_Streaming\\nAdapter: Click \\n \\nSelect Adapter\\n, a list of all adapters are displayed. Select \\nPulsar Push Adapter\\n.       \\nIs Integration Activated: Select the check box      \\nHostname/IP: Add hostname. Note that hostname must be in the following format: \\npulsar+ssl://<externalAccessHost\\n>:31051\\nContainerized Operations Bridge 2022.11\\nPage \\n374\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c2b6deca6bba9015e9313d9276192c6a'}>,\n",
              "  <Document: {'content': \"After you complete the configuration steps, events and topology are sent to OPTIC Data Lake. These events are used to\\ngenerate event reports on BVD.\\nRelated topics\\nFor details on the event attributes that are used in out-of-the-box event reports, see \\nOperations Bridge Data Model\\n.\\nTo customize the data retention, see \\nCustomize data retention\\n.\\nNote:\\nFor every full or delta synchronization, the CI details including attribute changes are sent from RTSM to OPTIC\\nData Lake.\\nIf a CI gets deleted from RTSM, the status of the CI gets updated as 'deleted' but the CI isn't deleted from\\nOPTIC Data Lake. \\nNote: \\nFor events, OBM extracts the tenant id information from the CI property of the related CI. If no\\ninformation is available tenant id gets set to “System Default Tenant”. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n371\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cdae73a2797d1b6c7935a6d63e03d072'}>,\n",
              "  <Document: {'content': 'Configure event reports with containerized OBM\\nTo populate the event reports in BVD, you must configure OBM to forward events and topology to OPTIC Data Lake. Event\\nforwarding is automatically configured after you \\nconfigure a secure connection between containerized OBM and OPTIC Data\\nLake\\n.\\nIf you also want to forward topology from OBM to OPTIC Data Lake, perform the following tasks:\\n1\\n. \\nDownload and install the Data Flow Probe\\n2\\n. \\nConfigure topology data streaming from OBM/RTSM to OPTIC Data Lake\\nTo forward topology, each OBM server requires a Data Flow Probe. The Events by CI report and the \"Top event flow by CI\"\\nsection of the Event Executive Summary report show information about topology.\\nPrerequisites\\nOPTIC Reporting\\n capability\\nRun the command on the master (control plane) node to check if you have installed the\\n OPTIC Reporting\\n: \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting:\\nFor example:\\nhelm get values opsb -n opsbs | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true \\nTo add the \\nOPTIC Reporting \\ncapability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page. \\nValidate the connection between BVD and OPTIC Data Lake. See \\nValidate the connection between BVD and OPTIC Data\\nLake\\n.\\nComponents and their supported versions\\nThe components and their supported versions that are required for reporting are listed: \\nComponent\\nSupported version\\nContainerized OBM\\n2021.08 or higher\\nTask 1: Verify event forwarding from containerized OBM\\nFollow the steps to verify the event forwarding from a containerized OBM:\\n1\\n. \\nThe events that occur after you activate the Event Forwarding Rule are sent to OPTIC Data Lake. To verify the\\nconfiguration, run the command on the master (control plane) node to send a test event to OPTIC Data Lake:\\nkubectl exec -it omi-0 -n OpsB-helm -c omi -- bash\\ncd /opt/HP/BSM/opr/support \\n./sendEvent.sh -j -t TestEvent -s normal\\nSample output:\\n2\\n. \\nOn OBM, go to \\nWorkspaces > Operations Console > Event Perspective. \\nCheck that the event is visible in the \\nEvent\\nPerspective\\n and its\\n \\nState\\n is \\nForwarded. \\n3\\n. \\nYou can also check the \\nopr_event\\n table in the \\nmf_shared_provider_default\\n schema to verify if the event has reached\\nOPTIC Data Lake.\\nOn a system that has the vsql command, such as a Vertica node, run the command:\\n/opt/vertica/bin/vsql -U dbadmin -c \"select node_hint,title,timestamp from mf_shared_provider_default.opr_event where title ilike \\'testEvent\\' lim\\nit 10;\"\\nYou are prompted to enter the password for the dbadmin user. You can specify a different user such as the \\n<vertica_rouser\\n> \\nyou created in the \\nPrepare Vertica database\\n.\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n372\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '47f97071d464efe2d1809c50423d98d4'}>,\n",
              "  <Document: {'content': \"1\\n. \\nIn the Integration studio, select the Integration Point (Pulsar Push Job) and click \\nEdit\\n.\\n2\\n. \\nIn the \\nEdit Integration Job \\nwindow, in the \\nDelta Synchronization \\ntab, and select the \\nScheduler enabled\\ncheck box.\\n3\\n. \\nSelect a specific interval of your choice and click \\nOK\\n.\\n4\\n. \\nClick \\nRefresh.\\nAfter you complete the configuration steps, events and topology are sent to OPTIC Data Lake. These events are used to\\ngenerate event reports on BVD.\\nRelated topics\\nFor details on the event attributes that are used in out of the box event reports, see \\nOperations Bridge Data Model\\n.\\nTo customize the data retention, see \\nCustomize data retention\\n.\\nNote:\\nFor every full or delta synchronization, the CI details including attribute changes are sent from RTSM to OPTIC\\nData Lake.\\nIf a CI gets deleted from RTSM, the status of the CI gets updated as 'deleted' but the CI isn't deleted from\\nOPTIC Data Lake. \\nNote: \\nFor events, OBM extracts the tenant id information from the CI property of the related CI. If no\\ninformation is available tenant id gets set to “System Default Tenant”.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n376\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b3cdcbb2a321c87bb52aa080163ef47c'}>,\n",
              "  <Document: {'content': \"Credentials ID: Click  \\n \\n \\nCreate New Credentials\\n. Choose Credentials window appears. Click \\n \\n and add the\\nfollowing:    \\nProtocol: Choose \\nTWO-WAY-SSL\\n   \\nTrust Cert File Path: Enter the path of the \\nissue_ca.crt \\n(issue_ca.crt is a part of the OBM integration tools that\\nyou procured earlier. See \\nConfigure a secure connection between containerized OBM and OPTIC Data Lake\\n.)\\nTLS Cert File Path: Enter the path of the \\ndfp_client.crt \\n(You generated the \\ndfp_client.crt\\n in step 3.)\\nTLS Key File Path: Enter the path of the \\ndfp_client-key-pk8.pem \\n(You generated the \\ndfp_client-key-pk8.pem \\nin\\nstep 3.)\\nClick \\nOK.\\n  \\nClick \\nOK\\nData Source Id: Enter an Id. Default Id is UCMDB.         \\nData Flow Probe: Select the name of the Data Flow Probe.       \\n2\\n. \\nClick \\nTest Connection\\n. Here you are testing access to the \\nmf_shared_cmdb_entity_configuration_item_raw\\n that you\\nspecified earlier. If the connection is successful, click \\nOk\\n. If the connection isn't successful, verify the aforementioned\\nsteps, and test again.\\n5\\n. \\nIn the Integration studio, select the Integration Point (Pulsar Push Job) created in OBM and click \\n \\nFull\\nSynchronization\\n.\\n6\\n. \\nClick \\nRefresh\\n to see the status of the topology synchronization. \\n7\\n. \\nFollow the steps to schedule delta synchronization:\\nImportant:\\n In cloud deployments that use private domain names, set the port to 6651.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n375\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '261662addbe5b35aab56c08addffd1f0'}>,\n",
              "  <Document: {'content': 'After completing the aforementioned tasks, the events from OBM are sent to OPTIC Data Lake. You will also be able to view the\\nevent reports in BVD. Only the \\'Events by CI report\\' and the \\'Top event flow by CI\\' section of the Event Executive Summary\\nreport won\\'t show any data as these depend on topology. If you want to see these reports, perform the remaining tasks to\\nforward topology to OPTIC Data Lake.\\nTask 2: Download and install the Data flow probe\\nTask 3: Configure topology data streaming from RTSM to OPTIC Data Lake \\nFollow the steps to forward topology from OBM\\'s RTSM to OPTIC Data Lake:\\n1\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Adapter Management >\\nPulsar Push Adapter > Configuration Files > PulsarPushAdapter/settings.json  \\nIf you are using the UCMDB Local Client go to \\nData Flow Management > Adapter Management > Pulsar Push\\nAdapter > Configuration Files > PulsarPushAdapter/settings.json.\\n2\\n. \\nTo check if you have configured the Pulsar Push Adapter, change the value of \"\\ntest.connection.topic\\n\"\\nto \\n\"persistent://public/default/mf_shared_cmdb_entity_configuration_item_raw\":\\n3\\n. \\nOn OBM, run the following to create a client certificate signed by OBM CA. This certificate enables secure communication\\nbetween Data Flow Probe and OPTIC Data Lake.\\nTo access the OBM pod, e.g: \\nkubectl exec -it -n $(kubectl get pods -A | awk \\'/ omi-0 / { print $1}\\') omi-0 -c omi -- bash\\nrm -f /tmp/dfp_client*\\ncd /opt/OV/bin/\\n./ovcm -issue -file /tmp/dfp_client.p12 -name \"UCMDB Data Flow Probe\" -coreid \"itom-cms-dataflowprobe\" -pass \"dataflowprobe\"\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.crt -nokeys -passin pass:dataflowprobe\\nopenssl pkcs12 -in /tmp/dfp_client.p12 -out /tmp/dfp_client.key -nocerts -passin pass:dataflowprobe -nodes\\nopenssl pkcs8 -topk8 -inform PEM -outform PEM -in /tmp/dfp_client.key -out /tmp/dfp_client-key-pk8.pem -nocrypt\\n1\\n. \\nOn the master node,  run the following commands to copy the certificate (\\ndfp_client.crt\\n and \\ndfp_client-key-pk8.pem\\n) from\\nOBM to the \\ntmp\\n directory on the master node:\\n \\nkubectl cp omi-0:/tmp/dfp_client.crt /tmp/dfp_client.crt -c omi -n $(kubectl get pods -A | awk \\'/omi-0/ {print $1}\\') \\nkubectl cp omi-0:/tmp/dfp_client-key-pk8.pem /tmp/dfp_client-key-pk8.pem -c omi -n $(kubectl get pods -A | awk \\'/omi-0/ {print $1}\\')\\n2\\n. \\nCopy the generated files \\n/tmp/dfp_client.crt\\n and \\n/tmp/dfp_client-key-pk8.pem\\n to the DFP machine and use them to configure\\nthe probe. \\n3\\n. \\nCopy the \\nissue_ca.crt\\n to the DFP machine and use it to configure the probe (issue_ca.crt is a part of the OBM integration\\ntools that you procured earlier. See \\nConfigure a secure connection between containerized OBM and OPTIC Data Lake\\n). \\n4\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Integration Studio \\nto create\\nan integration point.\\nIf you are using the UCMDB Local client, go to \\nData Flow Management > Integration Studio \\nto create an integration\\npoint.\\n1\\n. \\nClick  \\n \\n. The \\nNew Integration Point\\n window appears.\\nUse the Internet Explorer browser or the UCMDB Local Client.\\nThis Pulsar topic exists in OPTIC Data Lake. You will use this later to test the communication from RTSM to\\nPulsar.\\nContainerized Operations Bridge 2022.11\\nPage \\n373\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b4ff557b4ec5e7e51cedc5fb5553329'}>,\n",
              "  <Document: {'content': \"2\\n. \\nSelect the \\nSend data via Operations\\n \\nAgent\\n check box.\\n3\\n. \\nThe Data receiver URL is automatically populated:\\nhttp://localhost:30005/bsmc/rest/genericdata\\nWhen a BPM script gets executed, a copy of the metric data gets processed by the policies deployed to the Operations\\nAgent and forwarded to OPTIC Data Lake.\\nConfigure existing BPM Instance\\nSelect the instance for which you want to configure the OPTIC Data Lake integration:\\n1\\n. \\nClick the \\nConfiguration\\n tab on the top right corner. \\n2\\n. \\nSelect the \\nSend data via the Operations\\n \\nAgent\\n check box.\\n3\\n. \\nIn the \\nData Push Settings\\n section, select the \\nSend data via the Operations Agent\\n check box.  The Data receiver\\nURL is automatically populated:\\nhttp://localhost:30005/bsmc/rest/genericdata\\nWhen a BPM script gets executed, a copy of the metric data gets processed by the policies deployed to the Operations\\nAgent and forwarded to OPTIC Data Lake.\\n(Optional) Add the tenant id to BPM instances\\nFollow the steps to update the tenant id:\\n1\\n. \\nOn BPM, go to:\\nOn Windows: \\nC:\\\\ProgramData\\\\MF\\\\BPM\\\\config\\nOn Linux:\\n /opt/MF/BPM/config/\\n2\\n. \\nMake sure that you have replaced the \\ncoso_sample_attributes.cfg\\n file with the corresponding file in the\\nBPM9.53_LG2022_Patch\\n.\\n3\\n. \\nOpen the \\ntopaz_agent_ctrl.cfg\\n file. The \\ntopaz_agent_ctrl.cfg\\n file contains a list of all the instances on a BPM.\\n4\\n. \\nFor each instance, add the tenant id in the \\nTenantId\\n parameter. \\nFor example:\\n \\nIn this example, \\nSite19\\n has the \\nTenantId \\n1.\\n5\\n. \\nRestart the BPM service:\\nOn Linux: \\n1\\n. \\nGo to \\n/opt/MF/BPM/bin \\n2\\n. \\nExecute the script to stop BPM service: \\n./stopBpmDaemon.sh\\n file \\n3\\n. \\nExecute the script to start BPM service: \\n./startBpmDaemon.sh\\nYou can use the data in OPTIC Data Lake to generate reports using either BVD or any other Business Intelligence tools of your\\nchoice. You can also use the metrics to generate dashboards using Performance Dashboards. For configuration steps,\\nsee \\nConfigure Performance Dashboards\\n.\\nRelated topics\\nFor details on the metrics collected by BPM, see \\nSynthetic Transaction schema tables\\n.\\nFor details about synthetic transaction reports, see  \\nView Synthetic Transaction Report on BVD\\nThe local client allows you to launch the BPM UI from your local system. Use this option if your browser\\ndoesn't support Java Applets. For details, see \\nHow to Access BPM Admin\\n.\\nNote: \\n To add BPM instances, see the \\nInstance definition page\\n.\\n\\ue916\\n\\ue916\\nNote: \\nThe \\nTenantId\\n can have up to 80 characters and can include both letters and numbers.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n379\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62814cb91479f20370232540359b4f49'}>,\n",
              "  <Document: {'content': \"Configure synthetic transaction reports using BPM\\nSynthetic Transaction Reports give you with information about end user experience, availability, and performance of\\napplications.\\nBusiness Process Monitor (BPM) enables you to run synthetic transactions and collect metrics. This section gives you\\ninformation to send the metrics collected by BPM to OPTIC Data Lake and generate synthetic transaction reports on Business\\nValue Dashboard (BVD).\\nPrerequisites\\nOPTIC Reporting\\n capability\\nRun the command on the master (control plane) node to check if you have installed the \\nOPTIC Reporting \\ncapability:\\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting:\\nFor example:\\nhelm get values opsb -n opsbs | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true\\nTo add the \\nOPTIC Reporting\\n capability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page. \\nOperations Bridge Manager (OBM). For installation steps, see Install.\\nConfigure a secure connection between OBM and OPTIC Data Lake:\\nTo configure classic OBM, see \\nConfigure classic OBM\\nTo configure containerized OBM, see \\nConfigure a secure connection between containerized OBM and OPTIC Data\\nLake\\nValidate the connection between BVD and OPTIC Data Lake. See \\nValidate the connection between BVD and OPTIC Data\\nLake\\n.\\nBusiness Process Monitor. For installation steps, see \\nInstallation tasks\\n.\\nApply the Business Process Monitor patch \\nBPM9.53_LG2022_Patch\\n.\\n \\nFor patch installation instructions, see \\nPatch\\nInstallation Instructions\\n.\\nOBM Management Pack for Business Process Monitor (OBM MP for BPM). Download the OBM MP for BPM from \\nMarket Place\\nand install it. Steps to install are present later in this document.\\nOperations Agent.  Install and integrate Operations Agent on the BPM with OBM.\\nTo stream BPM data into the OPTIC Data Lake, you must integrate the Operations Agent which is on the BPM with OBM.\\nPerform the following steps to check if you have installed Operations Agent:\\n1\\n. \\nLog on to the BPM node.\\n2\\n. \\nRun the following command:\\nOn Linux\\n: \\n/opt/OV/bin/opcagt -version\\nOn Windows\\n: \\nopcagt -version\\nThe version of the Operations Agent appears. Make sure that the version is 12.14 or higher.\\nInstall and integrate Operations Agent with OBM\\nRun the following commands if you want to \\ninstall and integrate\\n Operations Agent (on a BPM) with OBM:\\nOn Linux: \\n./oainstall.sh -i -a -s <OBM load balancer or gateway server>\\nOn Windows: \\ncscript oainstall.vbs -i -a -s <OBM load balancer or gateway server> \\nGrant the Operations Agent certificate request\\nFollow the steps:\\n1\\n. \\nOn OBM, go to \\nAdministration > SETUP AND MAINTENANCE > Certificate Request\\n2\\n. \\nClick \\n \\nto grant the certificate.\\nComponents and their supported versions\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nNote:\\n Aggregate tables of BPM aren't populated for the DI receiver endpoint. In order to populate them, use\\nthe Data Enrichment Service (DES) endpoint.\\n\\ue916\\n\\ue916\\nNote:\\nFor containerized OBM, \\n<OBM load balancer or gateway server> \\nis the  FQDN of the external access host. \\nFor more information, see \\nOperations Agent Install\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n377\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c9c97ef860e3cd95f918c791643b6f9'}>,\n",
              "  <Document: {'content': 'The components and their supported versions that are required for reporting are:\\nComponent\\nSupported version\\nClassic OBM \\n2020.05 and higher \\nContainerized OBM\\n2021.08\\nOperations Agent \\n12.14  and higher \\nBusiness Process Monitor (BPM) with \\nBPM9.53_LG2022_Patch\\n9.53\\nOBM MP for BPM\\n1.02\\nTask 1: Deploy the BPM aspect and configure the Data Receiver endpoint\\nThe BPM aspect is available with the OBM MP for BPM. Download the OBM MP for BPM from the Market Place.\\nInstall the OBM MP for BPM\\nFollow the steps:\\n1\\n. \\nDownload the \\nOBM_MP_for_Business_Process_Monitor_01.00.007.zip\\n from \\nMarket Place\\n and extract its contents.\\n2\\n. \\nIn the OBM UI, go to \\nAdministration \\n>\\n Setup and Maintenance \\n>\\n Content Packs\\n.\\n3\\n. \\nClick \\nImport Content pack Definition and Content\\n. \\n4\\n. \\nGo to the \\nOBM_MP_for_Business_Process_Monitor_01.00.007\\n > \\nopr \\n> \\ncontent\\n > \\nen_US\\n, select the\\nOBM_Management_Pack_for_Businees_Process_Monitor.zip\\n file and click \\nImport\\n.\\nDeploy the BPM aspect and configure the Data Receiver endpoint \\nFollow the steps:\\n1\\n. \\nIn the OBM UI, go to \\nAdministration > Monitoring > Management Templates & Aspects\\n.\\n2\\n. \\nIn the Configurations Folder pane, click \\nBusiness Process Monitor > BPM OPTIC Data Lake Integration\\n.\\n3\\n. \\nIn Management Templates & Aspects pane, select the aspect and then click  \\n \\n \\nAssign and Deploy\\n item.\\n4\\n. \\nIn the Configuration Item tab, click the CI of the BPM node (Operations Agent node) on which you want to deploy the\\nAspect, and then click \\nNext\\n.\\n5\\n. \\nIn the Required Parameters tab, enter the OPTIC Data Lake receiver URL in the following format: \\nhttps://<externalAccessHost\\n>:30001\\nHere, \\nexternalAccessHost\\n is the value that you set in the \\nvalues.yaml\\n file. For more information, see the \\nConfigure\\nValues.yaml\\n page.\\n6\\n. \\nClick \\nNext\\n and then click \\nFinish\\n.\\nVerify the deployment status\\nRun the command on BPM to validate if the BPM policies are installed:\\nOn Linux: \\n/opt/OV/bin/ovpolicy -list\\nOn Windows:\\n ovpolicy -list\\nThe BPM policies are listed as follows:\\nTask 2: Configure BPM to send data to OPTIC Data Lake\\nConfigure new BPM Instance\\nDuring Instance creation, under the Data Push Settings section, enter the following:\\n1\\n. \\nOn BPM UI, select the BPM host machine in the BPM tree and click \\n \\nCreate New Instance\\n button in the Browse tab\\ntoolbar.\\nNote: \\nSkip these steps if you have already installed the BPM Management Pack.\\n\\ue916\\n\\ue916\\nNote:\\n \\nYou can also execute the \\nmpinstall.sh \\n(on Linux) or \\nmpinstall.vbs\\n (on Windows) present in\\nthe \\nOBM_MP_for_Business_Process_Monitor_01.00.007 \\nfile to install the OBM MP for BPM.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n378\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a28e4287f522fe0c63430160b3e38661'}>,\n",
              "  <Document: {'content': 'Related topics\\nFor more information on Event reports, see \\nEvent\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n385\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b4c8ff5e8c8f0c06481d088878167a74'}>,\n",
              "  <Document: {'content': '1\\n. \\nApply the \\nhotfix\\n \\nto populate the tenant_id column in the RUM schema tables.\\n2\\n. \\nGo to \\n<RUM installation directory>\\\\conf\\\\datapublisher\\\\consumers\\n. Open the \\ncoso_integration.xml\\n, scroll down\\nto locate the \\n<tenant_id_value>\\n parameter, and set a value.\\nFor example:\\n \\n \\nRUM metrics that are sent to OPTIC Data Lake are used to generate the real user monitor reports on BVD. To view the real user\\nmonitor reports, see \\nReal User Monitor\\n.\\nRelated topics\\nFor details on the metrics collected by RUM, see \\nReal User Monitor schema tables\\n.\\nNote:\\n While applying the hotfix, make sure that you replace the \\ncoso_integration.xml\\n present in the\\nhotfix with the \\ncoso_integration.xml\\n in the\\n <RUM installation directory>\\\\conf\\\\datapublisher\\nand \\n<RUM installation directory>\\\\conf\\\\datapublisher\\\\consumers. \\n\\ue916\\n\\ue916\\nNote:\\nEach RUM engine can have only one tenant id. \\nThe tenant id can have up to 80 characters and can include both letters and numbers.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n382\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '568bc510302993e3764a3c742a27c3d4'}>,\n",
              "  <Document: {'content': 'Application reports\\nManagement Packs collect application metrics. You can store these metrics in OPTIC Data Lake and then use them to generate\\nreports using either BVD or any other business intelligence tools of your choice. You can also use these metrics to generate\\ndashboards using Performance Dashboards. This topic explains the steps required to set up \\nOPTIC Reporting\\n for\\nManagement Packs (MP).\\nComponents and their supported versions\\nInstall the following classic components in addition to the containerized Operations Bridge:\\nOperations Agent - 12.xx and higher. To install Operations Agent, see \\nInstall\\n.\\nBased on the data source from which you want to collect metrics, you have to install the corresponding Management\\nPacks:\\nDomain\\nData source\\nMP name\\nDescription\\nDatabase\\nMicrosoft\\nSQL Server\\nOBM MP for\\nSQL\\nThe OBM Management Pack for Microsoft SQL Server (OBM MP for\\nMicrosoft SQL Server) works with the Operations Bridge Manager\\n(OBM). It enables you to monitor the Microsoft SQL Server\\ndatabase in your environments and its underlying infrastructure. It\\nincludes Indicators - Health Indicators (HIs), Event Type Indicators\\n(ETIs), and Correlation Rules that analyze the events that occur in\\nthe Microsoft SQL Server databases and report the health status.\\nTo install the OBM MP for Microsoft SQL Server, see \\nInstall\\n.\\nOracle\\nOBM MP for\\nOracle\\nThe OBM Management Pack for Oracle Database works with the\\nOperations Bridge Manager (OBM). It enables you to monitor\\ninfrastructure and Oracle databases running in your environment. It\\nincludes Indicators - Health Indicators (HIs), Event Type Indicators\\n(ETIs), and Correlation Rules that analyze the events that occur in\\nthe Oracle databases and report the health status.\\nTo install the OBM MP for Oracle Database, see \\nInstall\\n.\\nPostgreSQL\\nManagement\\nPack for\\nPostgreSQL\\nThe Management Pack for PostgreSQL (MP for PostgreSQL) works\\nwith Operations Bridge Manager (OBM) and enables you to monitor\\nthe availability and performance of PostgreSQL databases\\noperating in your environment. MP for PostgreSQL enables you to\\nmonitor health with configurable alerts for important events with\\ncustomizable thresholds and monitor key performance metrics for\\nyour PostgreSQL instances.\\nTo install Management Pack for PostgreSQL, see \\nInstall\\n.\\nWeb\\nApplication\\nServer\\nIBM\\nWebSphere\\nApplication\\nServers\\nOBM MP for\\nIBM\\nWebSphere\\nApplication\\nServer\\nThe Management Pack for IBM WebSphere Application Server (MP\\nfor IBM WebSphere Application Server) enables you to monitor IBM\\nWebSphere Application Servers and the underlying infrastructure\\nrunning in your environment. It provides out of the\\nbox management templates and aspects for monitoring the\\navailability, health, and performance of IBM WebSphere Application\\nServers. \\nTo install the OBM MP for IBM WebSphere Application Server,\\nsee \\nInstall\\n.\\nWeblogic\\nApplication\\nServer\\nOBM MP for\\nWeblogic\\nThe OBM Management Pack for  Oracle Weblogic (OBM MP\\nfor Oracle Weblogic) works with the Operations Bridge Manager\\n(OBM). It enables you to monitor your Weblogic Application Server\\nenvironment and underlying infrastructure. The MP includes out of\\nthe box management templates for monitoring the availability,\\nhealth, and performance of the Weblogic Application Servers. \\nTo install the OBM MP for Oracle Weblogic, see \\nInstall\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n386\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ffd7c85990c1c1919e14dc41f9bf8ba8'}>,\n",
              "  <Document: {'content': 'Configure real user monitoring reports using RUM\\nReal User Monitor reports give you information about end user experience, availability, and performance of applications by\\nmonitoring real user traffic.  RUM forwards the following data to OPTIC Data Lake:\\nPages, Actions, Session, Transactions, Crashes, Events, TCP, and Requests.\\nThis section gives you steps to:\\nSend the metrics collected by the Real User Monitor (RUM) probes to OPTIC Data Lake.\\nUse the metrics to generate reports in Business Value Dashboard (BVD).\\nFor information about RUM reports, see \\nReal User Monitor\\n reports.\\nPrerequisites\\nOPTIC Reporting\\n capability\\nRun the command on the master (control plane) node to check if you have installed \\nOPTIC Reporting\\n capability: \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting:\\nFor example:\\nhelm get values opsb -n opsbs | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true \\nTo add the \\nOPTIC Reporting\\n capability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page. \\nOperations Bridge Manager (OBM). For installation steps, see \\nInstall\\n.\\nConfigure a secure connection between OBM and OPTIC Data Lake:\\nTo configure classic OBM, see \\nConfigure classic OBM\\nTo configure containerized OBM, see \\nConfigure a secure connection between containerized OBM and OPTIC Data\\nLake\\nCheck if you have configured the BVD database connection. See \\nValidate the connection between BVD and OPTIC Data\\nLake\\n.\\nReal User Monitor. For installation steps, see \\nInstall\\nApply the\\n \\nhotfix\\n to populate the tenant_id column in the RUM schema tables.\\nOBM Management Pack for Real User Monitor (OBM MP for RUM). Download the OBM MP for RUM from the \\nMarketplace\\nand install it. Steps to install are present later in this document.\\nOperations Agent. Install and integrate Operations Agent on the RUM Engine with OBM.\\nTo stream RUM data into the OPTIC Data Lake, you must integrate the Operations Agent which is on the RUM engine with\\nOBM.\\nPerform the following steps to check if you have installed Operations Agent:\\n1\\n. \\nLog on to the RUM node.\\n2\\n. \\nRun the following command:\\nopcagt -version\\nThe version of the Operations Agent gets displayed. Make sure that the version is 12.14 or higher.\\nInstall and integrate Operations Agent with OBM\\nRun the command if you want to install and integrate Operations Agent (on a RUM engine) with OBM:\\ncscript oainstall.vbs -i -a -s <OBM load balancer or gateway server> \\nGrant the Operations Agent certificate request\\nFollow the steps:\\n1\\n. \\nOn OBM, go to \\nAdministration > SETUP AND MAINTENANCE > Certificate Request\\n2\\n. \\nClick \\n \\nto grant the certificate.\\nComponents and their supported versions\\nThe components and their supported versions that are required for reporting are listed:\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nNote:\\nFor containerized OBM, \\n<OBM load balancer or gateway server> \\nis the  FQDN of the external access host.\\nFor more information, see \\nOperations Agent Install\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n380\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bb57ba209942775573f0535fef17f9a7'}>,\n",
              "  <Document: {'content': 'Multi-language support for Event reports\\nEvent reports are localized in Japanese language. After installing the Operations bridge, you must manually install the\\nJapanese language content package.\\nInstall language content package\\nYou must download the \\nOpsB_Event_Content_JA_<version>.zip\\n file before proceeding with the language content pack installation\\nfrom either of the FTP location:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessPort>/staticfiles/opsb_localization_content/OpsB_Event_Content_JA_<versi\\non>.zip\\nFor example:\\nwget --no-check-certificate https://server.example.net/staticfiles/opsb_localization_content/OpsB_Event_Content_JA_2022.05.111.zip\\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/opsb_localization_content/OpsB_Event_Content_JA_<version>.zip\\nFor example:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/opsb_localization_content/OpsB_Event_Content_JA_2022.05.111.zip\\nFollow the below steps to install the content:\\n1\\n. \\nRun the command to upload OPTIC Data Lake integration content:\\nops-content-ctl upload content -f <content name>_<version>.zip\\nFor example: \\nops-content-ctl upload content -f OpsB_Event_Content_JA_2022.05.xxx.zip\\n2\\n. \\nRun the following commands to install the content:\\nops-content-ctl install content -n <content name> -v <version> \\nFor example:\\nops-content-ctl install content -n OpsB_Event_Content_JA -v 2022.05\\nIt would take a few minutes for the installation to complete. You may run the command to check the status: \\nops-content-ctl l\\nist content\\nSample report\\nNote:\\n To configure \\nops-content-ctl\\n for Windows and Linux, see \\nConfigure CLI tool\\n.\\n\\ue916\\n\\ue916\\nNote:\\n To view the parameter and report names in the localized format (Japanese language), on your local\\nbrowser change the settings to the specific language:\\nGo to \\nSettings > Advanced > Languages > Order languages based on your preference > Add\\nlanguages\\n.\\n \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n384\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b4e0a661ff2ae62a9870440b06feba67'}>,\n",
              "  <Document: {'content': \"Component\\nSupported version\\nClassic OBM \\n2020.05 and higher \\nContainerized OBM\\n2021.08 \\nOperations Agent \\n12.14 and higher \\nReal User Monitor\\n9.52\\nApply the \\nhotfix\\n if you want to add tenant ids.\\nOBM MP for RUM\\n1.03\\nTask 1: Deploy the RUM aspect and configure the Data Receiver endpoint\\nThe RUM aspect is available with the RUM Management Pack. Download the Management Pack from the Market Place.\\nInstall the OBM MP for RUM\\nSkip these steps if you have already installed the RUM Management Pack.\\nFollow the steps:\\n1\\n. \\nDownload the OBM_MP_for_RUM_01.00.009.zip from \\nMarket Place\\n and extract its contents.\\n2\\n. \\nIn the OBM UI, go to \\nAdministration \\n>\\n Setup and Maintenance \\n>\\n Content Packs\\n.\\n3\\n. \\nClick \\nImport Content pack Definition and Content\\n.\\n4\\n. \\nGo to the \\nOBM_MP_for_RUM_01.00.009\\n > \\nopr \\n> \\ncontent\\n > \\nen_US\\n, select the\\nOBM_Management_Pack_for_RUM.zip\\n file and click \\nImport\\n.\\nDeploy the RUM aspect and configure the Data Receiver endpoint\\nFollow the steps:\\n1\\n. \\nOn OBM UI, click \\nAdministration > Monitoring > Management Templates & Aspects\\n.\\n2\\n. \\nIn the Configurations Folder pane, click \\nReal User Monitor > RUM Metric Streaming\\n.\\n3\\n. \\nIn Management Templates & Aspects pane, select the aspect and then click \\n \\n  \\nAssign and Deploy\\n item.\\n4\\n. \\nIn the Configuration Item tab, click the CI of the RUM Engine node (Operations Agent node) on which you want to deploy\\nthe Aspect, and then click \\nNext\\n.\\n5\\n. \\nIn the Required Parameters tab, enter the OPTIC Data Lake receiver URL in the following format: \\nhttps://<externalAccessHost\\n>:30001\\nHere \\nexternalAccessHost\\n is the value that you set in the \\nvalues.yaml\\n file. For more information, see the \\nConfigure\\nValues.yaml\\n page.\\nYou may ignore other optional parameters.\\n6\\n. \\nClick \\nNext \\nand then click \\nFinish\\n.\\nVerify the installation of policies\\n Run the command on the RUM engine to validate if the RUM policies are installed: \\novpolicy -list\\nThe RUM policies are listed as follows:\\nTask 2: Configure RUM Engine node to forward metrics to OPTIC Data Lake\\nCopy the \\ncoso_integration.xml\\n file from \\n<RUM>\\\\conf\\\\datapublisher\\n directory and save in \\n<RUM>\\\\conf\\\\datapublisher\\\\consumers\\n. \\nTask 3: (Optional) Configure tenant id\\nFollow the steps to configure tenant id:\\nNote: \\nYou can also execute the \\nmpinstall.sh\\n (on Linux) or \\nmpinstall.vbs\\n (on Windows) present in the \\nOBM_MP_for_RU\\nM_01.00.009\\n file to install the OBM MP for RUM.\\nNote:\\n  Restart of RUM service isn't needed, as it's a dynamic property.\\nContainerized Operations Bridge 2022.11\\nPage \\n381\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6bd12ae3453bfe416fbb5c43c6742bc8'}>,\n",
              "  <Document: {'content': 'Configure Service Health\\nYou can forward service health data from the Operations Bridge Manager (OBM) to OPTIC DL. To configure service health:\\n1\\n. \\nEnsure \\nOPR-SO-FORWARDER\\n service is running on your Operations Bridge Manager (OBM).\\n2\\n. \\nEnsure to enable the forwarding infrastructure setting: \\nEnable forwarding Downtime/Service Health data to OPTIC\\nDL \\nto\\n TRUE\\n.\\n3\\n. \\nTo store the KPI data for an individual configuration item (CI) on OBM, go to \\nAdministration > Service Health > CI\\nStatus Calculation > CI Customizations\\n and enable the \\nSave KPI Over time data\\n for individual CIs in Service\\nhealth.\\nContainerized Operations Bridge 2022.11\\nPage \\n383\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c9f408ac4c9868a85659ba805ee7bdd5'}>,\n",
              "  <Document: {'content': 'ERP\\nSAP\\nOBM MP for\\nSAP\\nThe OBM Management Pack for SAP works with the Operations\\nBridge Manager (OBM). It enables you to monitor the primary and\\nadvanced components of your SAP environment and the underlying\\ninfrastructure. It includes Health Indicators (HIs) and Event Type\\nIndicators (ETIs) that analyze the events that occur in your\\nSAP Configuration Items (CIs) and report the health status of the\\nSAP environment. \\nTo install the OBM Management Pack for SAP, see \\nInstall\\n.\\nSAP HANA\\nManagement\\nPack for SAP\\nHANA\\nManagement Pack for SAP HANA enables you to monitor SAP HANA\\ndatabase environment. \\nTo install the Management Pack for SAP HANA, see \\nInstall\\n.\\nMicrosoft\\nMicrosoft\\nActive\\nDirectory\\nManagement\\nPack for\\nMicrosoft\\nActive\\nDirectory\\nThe Microsoft Active Directory helps manage your corporate\\nidentities, credentials, information protection, system, and\\napplication settings. The OBM Management Pack for Microsoft\\nActive Directory (MP for Microsoft Active Directory) works with\\nOperations Bridge Manager (OBM) and enables you to monitor\\nActive Directory servers and underlying infrastructure operating in\\nyour environment. It includes indicators - Health Indicators (HIs),\\nEvent Type Indicators (ETIs), and Correlation Rules that categorize\\nand correlate the events based on the type of occurrence and\\nreports the health status of the Active Directory servers.\\nTo install OBM Management Pack for Microsoft Active Directory,\\nsee \\nInstall\\n.\\nMicrosoft\\nExchange\\nServer\\nOBM MP for\\nExchange\\nThe OBM Management Pack for Microsoft Exchange Server works\\nwith the Operations Bridge Manager. It enables you to monitor\\nMicrosoft Exchange Server environments. The MP comprises out of\\nthe box aspects and management templates for monitoring\\nthe Exchange service status, server availability, server\\nperformance, mail flow, transport queues, and the like. It also\\nincludes Health Indicators (HIs), Event Type Indicators ETIs), and\\nCorrelation rules that analyze the events that occur in the Microsoft\\nExchange Servers and report the health status.\\nTo install the OBM MP for Exchange, see \\nInstall\\n.\\nMiddleware\\nJBoss\\nApplication\\nServer\\nManagement\\nPack for\\nJBoss\\nApplication\\nServer\\nManagement Pack for JBoss Application Server enables you to\\nmonitor JBoss Application Servers running in your environment.\\nTo install the Management Pack for JBoss Application Server,\\nsee \\nInstall\\n.\\nOracle\\nWeblogic\\nManagement\\nPack for\\nOracle\\nWeblogic\\nMP for Oracle Weblogic helps you to monitor your Weblogic\\nApplication Server environment and underlying infrastructure. The\\nMP for Oracle Weblogic includes out-of-the-box management\\ntemplates for monitoring the availability, health, and performance\\nof WebLogic Application Server.  As an administrator, you can\\ndeploy management templates to monitor your WebLogic\\nApplication Server. As a Subject Matter Expert (SME) and\\ndeveloper, you can customize management templates to suit\\ndifferent monitoring requirements.\\nTo install Management Pack for Oracle Weblogic, see \\nInstall\\n.\\nDomain\\nData source\\nMP name\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n387\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc74db8244be2ffe8a57a5316bcd014e'}>,\n",
              "  <Document: {'content': 'Create Stakeholder Dashboard\\nBusiness Value Dashboard (BVD) is a data visualization tool. You can create custom, flexible dashboards to visualize\\ninformation from different sources in an informative and appealing way. Business Value Dashboard provides the following\\ncapabilities:\\nReporting\\n: Reporting gives you visual information of historical or recorded data using tables, charts, and widgets. You\\ncan create dashboards in Visio and attach data from data queries, which can retrieve data from any Vertica database\\nusing SQL queries. It is this capability that is used along with the OPTIC Reporting capability to generate the out of the box\\nreports.\\nStakeholder Dashboard\\n: The stakeholder dashboard gives you visual information of live data using tables, charts, and\\nwidgets. Real-time data can be streamed from any data source in JSON format via HTTP post. \\nThis page guides you through the process of creating dashboard in BVD, modifying a Visio file, sending data to the dashboard,\\nand uploading the exported \\nSVG\\n file to BVD.\\nPrerequisite\\nInstall \\nVisio \\non the system where you plan to create dashboards in \\nBVD. \\nYou need administrative privileges on your BVD system to upload dashboards into BVD and to retrieve the API key which allows you to send\\ndata to BVD.\\nStep 1: Download the sample dashboard from BVD.\\n1\\n. \\nLog into BVD.\\n2\\n. \\nAccess side navigation panel, click\\n Administration\\n > \\nDashboards & Reports\\n > \\nStakeholder Dashboards &\\nReports\\n > \\nResources.\\n3\\n. \\nUnder GETTING STARTED DASHBOARD click\\n  \\n \\ndownload.\\n4\\n. \\nEdit the downloaded dashboard in Visio.\\n5\\n. \\nPerform the steps given in the call outs.\\n6\\n. \\nRemove the call outs.\\n7\\n. \\nPress \\nCtrl+A \\nto ensure and select all the elements  in the drawing. \\n8\\n. \\nSave your drawing as an \\nSVG\\n file. Make sure to select the following Visio settings in the save as \\nSVG\\n file dialog.\\n9\\n. \\nSave as type: \\nScalable Vector Graphics (*.svg); \\nSelect: \\nInclude Visio data in the files\\n.\\nTip: \\nTo add your own widget to the dashboard, go to the Resources page. Under \\nVISIO STENCIL AND TOOLS, \\nclick the\\ndownload link to download the zip file and extract the content. See the Widget section for details.\\nStep 2: Send sample data to the dashboard\\n1\\n. \\nAccess side navigation panel, click \\nAdministration\\n > \\nSetup & Configuration \\n> \\nSettings \\n>\\n API key \\nand copy\\nthe \\nAPI Key\\n. BVD includes this key in the data, which the data sender submits to authenticate the BVD receiver. \\n2\\n. \\nAccess side navigation panel, click \\nAdministration\\n > \\nDashboards & Reports\\n > \\nStakeholder Dashboards &\\nNote\\n: If you are using legacy UI, you can find BVD dashboards and other related resources on the upper-right\\nside of the masthead. Use this navigation path: On the upper-right side of the masthead, click   \\n \\nAdministration.\\nNote\\n:  You will use the sample data generated here in your sample dashboard. The API Key page is available only\\nto users with administrator privileges. In the legacy UI, on the upper-right side of the masthead, click\\nAdministration\\n > \\nSettings.\\nContainerized Operations Bridge 2022.11\\nPage \\n392\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e9ce6a04a1e87c3b832a726c62bd185'}>,\n",
              "  <Document: {'content': 'Cloud\\nAmazon\\nWeb\\nServices\\n(AWS)\\nManagement\\nPack for AWS\\nManagement Pack for AWS enables you to discover and monitor\\n(services) provided by AWS. MP for AWS discovers topology and\\ncollects performance metrics to generate events and plot graphs\\nusing Performance Dashboard.\\nMP for AWS contains a Management Template (a group of\\nDiscovery Aspects) and Aspects (a group of policies) that you must\\ndeploy on the managed node from OBM. Each monitored service\\nhas two Aspects: one to discover and the other to monitor (data\\ncollection and event generation). After you deploy the Aspects,\\nrespective services in your AWS environment are discovered and\\nmonitored. \\nTo install the Management Pack for AWS, see \\nInstall\\nConnectors\\nVMware\\nvRealize\\nOperations\\nManager\\n(VROPS)\\nVROPS \\nConnector for VROPS enables you to establish a link between a\\nVMware vRealize Operations Manager (VROPS) environment and\\nOBM. The connector gets deployed on the Operations Agent\\nsystem and uses REST Web Service requests to collect the events,\\nmetrics, and topology from the VROPS environment. The collected\\ndata can then be processed and viewed in OBM. \\nTo install VROPS, see \\nInstall\\nDomain\\nData source\\nMP name\\nDescription\\nClassic OBM - 2021.05 or higher. For installation steps, see \\nInstall\\n.\\nIf you have already installed containerized OBM capability, you don’t need to install classic OBM.\\nFor information about supported integrations with containerized Operations Bridge, see the \\nIntegration metrics\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n388\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34e2a164f69bdd68e0875083756c63fe'}>,\n",
              "  <Document: {'content': 'Reports \\n> \\nResources.\\n    \\n3\\n. \\nUnder \\nData Generator Sample, \\nclick the \\n \\n link. Download the required JSON file and the executable for\\nyour system from the \\ndist\\n folder.\\n4\\n. \\nRun the data generator\\n: \\nbasicGenerator -k <your API key> -a <BVD server hostname:port> -p https -f obmConfig.json\\nExample\\n: \\nbasicGenerator-win.exe -k abc123abc123abc123 -a suite.example.com:19443 -p https -f obmConfig.json\\nThis generates random values and sends them every two seconds to BVD. You will see these values in the dashboard that you\\nwill upload in the next step.\\nStep 3: Upload the \\nSVG\\n file to BVD\\n1\\n. \\nAccess side navigation panel, click \\nAdministration\\n > \\nDashboards & Reports\\n > \\nStakeholder Dashboards &\\nReports \\n> \\nDashboard Management.\\n2\\n. \\nClick \\n+ Add.\\n3\\n. \\nSelect the \\nSVG\\n file, and then click \\nUpload Dashboard\\n. The BVD dashboard editor opens and displays the uploaded\\ndashboard.\\n4\\n. \\nYou can change the properties of the dashboard (for example, the \\nSVG\\n file associated with the dashboard, the title, or\\nthe background color).\\n5\\n. \\nTo edit the properties of a widget, click the widget. When you click the \\nData Channel \\nfield, a drop-down list opens\\nshowing all data streams. Select it, adjust any of the other properties as required, and click \\nSave\\n.\\n6\\n. \\nBy default, newly imported dashboards are visible in the \\nDashboards \\nmenu. To show or hide a dashboard, click the\\n  \\n Show in Menu/Hide from Menu\\n button.\\n7\\n. \\nAccess side navigation panel, click  \\nAdministration\\n > \\nDashboards & Reports\\n > \\nStakeholder Dashboards\\n \\n&\\nReports \\n> \\nDashboard Management.\\n8\\n. \\nSelect the dashboard that you have uploaded in the earlier step.\\nT\\nake a moment to explore the information displayed and watch it updating as new data arrives.\\nNote\\n: In the legacy UI, to view your dashboard, click the \\n \\n  menu.\\nContainerized Operations Bridge 2022.11\\nPage \\n393\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '216d1adcba8005a18bb710183d9b9538'}>,\n",
              "  <Document: {'content': \"Configure application reports\\nManagement Packs collect application metrics. You can store these metrics in OPTIC Data Lake and then use them to generate\\nreports using either BVD or any other business intelligence tools of your choice. You can also use these metrics to generate\\ndashboards using Performance Dashboards. This topic explains the steps required to set up \\nOPTIC Reporting\\n for\\nManagement Packs (MP).\\nComponents and their supported versions\\nInstall the following:\\nOperations Agent - 12.xx and higher. To install Operations Agent, see \\nInstall\\n.\\nMP which sends metrics to OPTIC Data Lake. See, \\nApplication reports\\nClassic OBM - 2021.05 and higher. For installation steps, see \\nInstall\\n.\\nIf you have already installed containerized OBM capability, you don't need to install classic OBM.\\nSynopsis\\nFollow the steps to set up Application reports/ send application data to OPTIC Data Lake:\\n1\\n. \\nCheck for OPTIC Data Lake reporting capability\\n: Make sure that you have installed \\nOPTIC Reporting\\n. Perform this\\ntask only once. See, \\nCheck for OPTIC Data Lake reporting capability\\n.\\n2\\n. \\nVerify targets and credentials\\n: Verify if the targets and credentials are working. See, \\nVerify targets and credentials\\n.\\n3\\n. \\nConfigure TQL\\n: Configure TQL to enable Agent Metric Collector to choose or filter the Operations Agent nodes from\\nwhich you want to collect metrics. See, \\nConfigure TQL\\n.\\n4\\n. \\nConfigure OPTIC Data Lake integration\\n: You have to perform this task each time you want to configure an MP to send\\ndata to OPTIC Data Lake. See, \\nConfigure OPTIC Data Lake integration\\n.\\n5\\n. \\nVisualize the data\\n: You can view data on Business Value Dashboard (BVD), on any business intelligence tool of your\\nchoice, or on the Performance Dashboard (PD). See, \\nVisualize the data\\n.\\nCheck for OPTIC Data Lake reporting capability\\nRun the command on the master node (control plane) to check if you have installed the \\nOPTIC Reporting \\ncapability: \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting: \\nFor example:\\nhelm get values opsb -n opsbs | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true \\nTo add the \\nOPTIC Reporting\\n capability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page.\\nVerify targets and credentials\\nPrerequisites:\\n Download and set up the \\nMonitoring CTL \\ntool (a CLI tool), see \\nSet up monitoring CLI for AMC\\n.\\nVerify if the targets and credentials are working.\\n1\\n. \\nRun the command to verify the targets:\\nops-monitoring-ctl get targets\\nThe output displays \\namc_oa_nodes, amc_obm_rtsm\\n.\\n2\\n. \\nRun the command to verify the credentials:\\nops-monitoring-ctl get credentials\\nThe output displays \\namc_obm_cert_auth\\n, \\namc_obm_basic_auth\\n.\\nConfigure TQL\\nConfigure TQL to enable Agent Metric Collector to choose or filter the Operations Agent nodes from which you want to collect\\nmetrics. \\nFollow the steps:\\n1\\n. \\nGo to \\nhttps://${EXTERNALHOST}:${EXTERNALPORT}/staticfiles/\\nmps-for-coso-integration/tqls/\\n and\\ndownload \\nMP_TQL.zip\\n. You may also download it from the Marketplace.\\n2\\n. \\nDo the following:\\na\\n. \\nIf you are using a browser that supports Java applets:\\ni\\n. \\nOn OBM, go to \\nAdministration \\n> \\nRTSM Administration > Modeling >\\n \\nModeling Studio\\n \\nContainerized Operations Bridge 2022.11\\nPage \\n389\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '731b3ad2f77ab9ca9bd5478f05f33ef3'}>,\n",
              "  <Document: {'content': 'Visualize data\\nAfter you configure the MP to send data to OPTIC Data Lake, you can view the OOTB report on BVD. See, \\nEnterprise\\nApplication\\n.\\nYou can also use these metrics to create custom reports using either the BVD or a business intelligence tool of your choice. To\\ncreate reports using a business intelligence tool of your choice, see \\nGuide to using an external BI tool for reporting on the\\nOperations Bridge data in the OPTIC Data Lake\\n.\\nRelated topics\\nFor details about the metrics collected by Management Packs, see \\nEnterprise application\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n391\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f3d0443ef162ae4d7a408ed6a3b62a6'}>,\n",
              "  <Document: {'content': 'b\\n. \\nIf you are \\nnot\\n using a browser that supports Java applets, you must download the UCMDB local client. Follow the\\nsteps: \\ni\\n. \\nOn OBM, go to \\nAdministration\\n > \\nRTSM Administration\\n and click \\nLocal Client \\nto download.\\nii\\n. \\nLaunch the UCMDB local client and enter the required details.\\n3\\n. \\nLaunch the RTSM UI.  Go to \\nManagers \\n> \\nAdministration\\n > \\nPackage Managers \\n4\\n. \\nClick \\n \\n to open the \\nDeploy Packages to Server \\nwindow.  \\n5\\n. \\nClick  \\n \\n, browse to select the zip file.\\n6\\n. \\nSelect a package zip file (For example: \\nMP_TQL.zip\\n) and click \\nOpen\\n. The package appears in the upper pane and its\\nresources appear in the lower pane.\\n7\\n. \\nClick \\nDeploy\\n. A status message appears indicating that the deployment was successful.\\nConfigure OPTIC Data Lake integration\\nTo configure the Management Packs to send data to OPTIC Data Lake, you must install the OPTIC Data Lake integration\\ncontent.  For all supported Management Packs, there are specific integration contents. These OPTIC Data Lake integration\\ncontents are available with the \\nOpsBridge Content\\nManager \\n(\\nhttps://${EXTERNALHOST}:${EXTERNALPORT}/staticfiles/opsb_content/custom/\\n)\\n. \\nPrerequisites:\\n Download and configure the CLI. For configuration steps, see \\nSet up monitoring CLI for AMC\\n.\\nFollow the steps to configure the Management Packs to send metrics to OPTIC Data Lake:\\n1\\n. \\nInstall the OPTIC Data Lake integration content that\\'s available in the \\nOpsBridge Content Manager.\\n2\\n. \\nRun the command to upload OPTIC Data Lake integration content:\\nops-content-ctl upload content -f <content name>_<version>.zip\\nFor example: \\nops-content-ctl upload content -f OpsB_MsSql_content_2021.11.zip\\n3\\n. \\nRun the commands to install the content:\\nops-content-ctl install content -n <content name> -v <version>\\nFor example:\\nops-content-ctl install content -n OpsB_MsSql_content -v 2021.11\\nIt would take a few minutes for the installation to complete.  You may run the command to check the status: \\nops-content-ctl\\nlist content\\nSample output:\\nNAME                  VERSION      STATUS\\nOpsB_BPM_Content      2020.10.001  Installed\\nOpsB_CMDB_Content     2020.10.057  Installed\\nOpsB_Event_Content    2020.10.057  Installed\\nOpsB_MsSql_content    2021.11      Installed\\nOpsB_RUM_Content      2020.08.001  Installed\\nOpsB_SysInfra_Content 2020.10.057  Installed\\n4\\n. \\nDownload and configure the monitoring CLI. See, \\nSet up monitoring CLI for AMC\\n.\\nFollow the steps to start the collection:\\ni\\n. \\nUnzip the content pack and locate the \\nYAML\\n files for AMC configurations under the\\n opr\\\\integration\\\\collection\\\\oa\\\\\\n<content_name>\\n folder.\\nFor example: \\nopr\\\\integration\\\\collection\\\\oa\\\\ad_collector\\nii\\n. \\nRun the command to create and start the collection:\\nops-monitoring-ctl create -f <path to configuration yaml file>\\nHere, replace <path to configuration yaml file> with the path of the configuration yaml file.\\nFor example: \\nops-monitoring-ctl create –f opr\\\\integration\\\\collection\\\\oa\\\\msadmp\\\\ad_collector.yaml \\niii\\n. \\nRun the command to verify the status of the collection: \\nops-monitoring-ctl get collector-status\\nTip:\\n If the status of installed content is \\nFailed\\n or \\nCompleted with Errors\\n, run the command to check for\\nerrors:\\nops-content-ctl describe content -n <content name> -v <version>.zip\\nFor example:\\nops-content-ctl describe content -n OpsB_MsSql_content -v 2021.11\\n\\ue917\\n\\ue917\\nNote:\\n \\nUse the command \\nops-monitoring-ctl get collector-status -V 2\\n to get the details of the logs. \\nIf the \"\\nops-monitoring-ctl get collector-status\\n\" command returns \"\\nNA\\n\" as the status, it indicates that the\\ncollector has not run yet.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n390\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '34be5e79cbdec29c714e7e02ea1d37f1'}>,\n",
              "  <Document: {'content': 'Deploy UCMDB Views\\nIn OBM, to view the Hyperscale Observability related views in the \\nPerformance Perspective\\n tab, you must deploy the\\nUCMDB views package to RTSM.\\nThis section provides you with steps to:\\nLaunch the RTSM UI of a target OBM server as a desktop application from the OBM UI.\\nDeploy the UCMDB views package to RTSM from your local directory.\\nLaunch RTSM UI as a desktop application from Local Client\\nPerform the following steps:\\n1\\n. \\nGo to \\nAdministration > RTSM Administration\\n and click \\nLocal Client\\n to download the Local Client tool.\\n2\\n. \\nLaunch the Local Client tool.\\n1\\n. \\nExtract the \\nUCMDB_Local_Client.zip\\n package to a location of your choice, for example, the desktop.\\n2\\n. \\nDouble-click \\nUCMDB Local Client.cmd\\n (Windows) or \\nUCMDB Local Client.sh\\n (Mac). The UCMDB Local Client window opens.\\n3\\n. \\nAdd or edit login configuration for the target OBM server that you want to access.\\n1\\n. \\nClick  \\n \\n or  \\n \\n. The Add/Edit Configuration dialog opens.\\n2\\n. \\nEnter the following details:\\nHost/IP\\n: Specify the value provided in the \\nvalues.yaml\\n for \\n<externalAccessHost>\\n.\\nProtocol\\n: Select \\nHTTPS\\n as the protocol from the drop-down list.\\nPort\\n: Specify the value provided in the \\nvalues.yaml\\n for \\n<externalAccessPort>\\n.\\nTarget Env\\n: Select \\nCMS\\n as the target environment from the drop-down list.\\n3\\n. \\nClick \\nOK\\n.\\n4\\n. \\nLaunch RTSM UI from the UCMDB Local Client window.\\n1\\n. \\nIn the \\nUCMDB Local Client\\n window, click the \\nLabel\\n value for the OBM server that you want to access. The \\nLog\\nIn\\n dialog opens.\\n2\\n. \\nIn the \\nLog In\\n dialog, enter your login parameters.\\n3\\n. \\nClick \\nLogin\\n. The RTSM UI opens in a new window.\\nDeploy AWS UCMDB views package to RTSM\\nPerform the following steps:\\n1\\n. \\nDownload AWS UCMDB views from the following location:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessHost>/staticfiles/monitoring-service/Monitoring_Service_AWS_UCMD\\nB_Views.zip\\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/monitoring-service/Monitoring_Service_AWS_UCMDB_Views.zip\\n2\\n. \\nIn the RTSM UI, go to \\nManagers\\n > \\nAdministration\\n > \\nPackage Manager\\n.\\n3\\n. \\nClick the  \\n \\n button to open the Deploy Packages to Server dialog box.\\n4\\n. \\nClick the  \\n \\n button to open the Deploy Packages to Server (from local disk) dialog box.\\n5\\n. \\nSelect the AWS UCMDB views package zip file and click \\nOpen\\n. The package appears in the upper pane of the dialog box\\nand its resources appear in the lower pane.\\n6\\n. \\nSelect the resources from the package that you want to deploy. All the resources are selected by default.\\n7\\n. \\nClick \\nDeploy\\n.\\n8\\n. \\nA status report appears indicating whether the deployment was successful for each resource selected.\\nDeploy Azure UCMDB views package to RTSM\\nPerform the following steps:\\n1\\n. \\nDownload Azure UCMDB views from the following location:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessHost>/staticfiles/monitoring-service/Monitoring_Service_Azure_UCM\\nDB_Views.zip\\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/monitoring-service/Monitoring_Service_Azure_UCMDB_Views.zip\\n2\\n. \\nIn the RTSM UI, go to \\nManagers\\n > \\nAdministration\\n > \\nPackage Manager\\n.\\n3\\n. \\nClick the  \\n \\n button to open the Deploy Packages to Server dialog box.\\n4\\n. \\nClick the  \\n \\n button to open the Deploy Packages to Server (from local disk) dialog box.\\n5\\n. \\nSelect the Azure UCMDB views package zip file and click \\nOpen\\n. The package appears in the upper pane of the dialog box\\nand its resources appear in the lower pane.\\n6\\n. \\nSelect the resources from the package that you want to deploy. All the resources are selected by default.\\n7\\n. \\nClick \\nDeploy\\n.\\n8\\n. \\nA status report appears indicating whether the deployment was successful for each resource selected.\\nDeploy Kubernetes UCMDB views package to RTSM\\nImportant\\n: Perform the steps in this topic if you are integrating Hyperscale Observability with Classic OBM.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n397\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a24db8f1a72dcbf6dcc1e0d3b2ee7865'}>,\n",
              "  <Document: {'content': 'Perform the following steps:\\n1\\n. \\nDownload Kubernetes UCMDB views from the following location:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessHost>/staticfiles/monitoring-service/Monitoring_Service_Kubernetes_\\nUCMDB_Views.zip\\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/monitoring-service/Monitoring_Service_Kubernetes_UCMDB_Views.zip\\n2\\n. \\nIn the RTSM UI, go to \\nManagers\\n > \\nAdministration\\n > \\nPackage Manager\\n.\\n3\\n. \\nClick the  \\n \\n button to open the Deploy Packages to Server dialog box.\\n4\\n. \\nClick the  \\n \\n button to open the Deploy Packages to Server (from local disk) dialog box.\\n5\\n. \\nSelect the Kubernetes UCMDB views package zip file and click \\nOpen\\n. The package appears in the upper pane of the\\ndialog box and its resources appear in the lower pane.\\n6\\n. \\nSelect the resources from the package that you want to deploy. All the resources are selected by default.\\n7\\n. \\nClick \\nDeploy\\n.\\nA status report appears indicating whether the deployment was successful for each resource selected.\\nContainerized Operations Bridge 2022.11\\nPage \\n398\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cce7d51c72685e26bdac2efeb7935b58'}>,\n",
              "  <Document: {'content': \" \\nThe required credentials are the same that you use for the Web UI.\\nVerify that Hyperscale Observability is connected to classic OBM\\n1\\n. \\nOn the UCMDB Local Client, go to \\nData Flow Management > Data Flow \\nProbe Status\\n2\\n. \\nIn the \\nDomain Browser\\n pane, \\nexpand Domains. \\n3\\n. \\nClick the probe name \\n(itom-probe) \\nand verify that the status is '\\nConnected\\n'\\nContainerized Operations Bridge 2022.11\\nPage \\n396\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bd7103f25ea05c642fb9589861617257'}>,\n",
              "  <Document: {'content': 'Import Hyperscale Observability Content Pack into OBM\\nHyperscale Observability content pack includes the following:\\nPerformance Dashboards configuration to graph AWS metrics \\nEvent mapping policy that forwards events generated by Hyperscale Observability to OBM. You must deploy this policy to\\nthe Operations Agent running on the Data Broker Container.\\nPrerequisites\\nMake sure that you have enabled the containerized OBM capability along with Hyperscale Observability capability.\\nImport Hyperscale Observability content pack for AWS into OBM\\nPerform the following steps to import the Hyperscale Observability content pack for AWS into OBM:\\n1\\n. \\nDownload the AWS content pack from the following location:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessHost>/staticfiles/monitoring-service/Monitoring_Service_AWS_Conte\\nnt_Pack_<version>.zip\\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/monitoring-service/Monitoring_Service_AWS_Content_Pack_<version>.zip\\n2\\n. \\nOn OBM user interface, go to \\nAdministration\\n > \\nSETUP AND MAINTENANCE\\n > \\nContent Packs\\n.\\n3\\n. \\nClick \\nImport\\n. The Import Content Pack window appears.\\n4\\n. \\nBrowse to the location where you have saved the AWS content pack and then click \\nImport\\n. The AWS content pack gets\\nimported. Click \\nClose\\n.\\nImport Hyperscale Observability content pack for Azure into OBM\\nPerform the following steps to import the Hyperscale Observability content pack for Azure into OBM:\\n1\\n. \\nDownload the Azure content pack from the following location:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessHost>/staticfiles/monitoring-service/Monitoring_Service_Azure_Cont\\nent_Pack_<version>.zip\\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/monitoring-service/Monitoring_Service_Azure_Content_Pack_<version>.zip\\n2\\n. \\nOn OBM user interface, go to \\nAdministration\\n > \\nSETUP AND MAINTENANCE\\n > \\nContent Packs\\n.\\n3\\n. \\nClick \\nImport\\n. The Import Content Pack window appears.\\n4\\n. \\nBrowse to the location where you have saved the Azure content pack and then click \\nImport\\n. The Azure content pack gets\\nimported. Click \\nClose\\n.\\nImport Hyperscale Observability content for Kubernetes into OBM\\nPerform the following steps to import Kubernetes content pack into OBM:\\n1\\n. \\nDownload the Kubernetes content pack from the following location:\\nOn Linux:\\nwget --no-check-certificate https://<externalAccessHost>:<externalAccessHost>/staticfiles/monitoring-service/Monitoring_Service_Kubernetes\\n_Content_Pack_<version>.zip\\nOn Windows:\\nhttps://<externalAccessHost>:<externalAccessPort>/staticfiles/monitoring-service/Monitoring_Service_Kubernetes_Content_Pack_<version>.zi\\np\\n2\\n. \\nOn OBM, go to \\nAdministration\\n > \\nSETUP AND MAINTENANCE\\n > \\nContent Packs\\n.\\n3\\n. \\nClick \\nImport\\n. The Import Content Pack window appears.\\n4\\n. \\nBrowse to the location where you have saved the Kubernetes content pack and then click \\nImport\\n.\\n5\\n. \\nThe Kubernetes content pack gets imported. Click \\nClose\\n.\\nImportant\\n: Perform the steps in this topic if you are integrating Hyperscale Observability with Classic OBM.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n399\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2a5b23b3d6ba64f2e863b4000227ebe8'}>,\n",
              "  <Document: {'content': \"Verify that Hyperscale Observability is connected to\\nclassic OBM\\nThis page gives you the steps to verify that the\\n Hyperscale Observability\\n is connected to the classic Operations Bridge\\nManager (OBM) after installing the suite.\\nPrerequisites\\nMake sure that \\nitom-cms-gateway\\n and \\nitom-ucmdb-probe\\n are running. \\nValidate the connection on the CMS UI\\nPerform the following:\\n1\\n. \\nEnter the following address on a browser to access the Universal CMDB server:\\nhttps://<hostname of classic OBM>:<port>\\nFor example: \\nhttps://myhost.mycomputer.net:8443\\n2\\n. \\nOn the Universal CMDB home page, click \\nCMS UI\\n and log into the CMS UI.\\n3\\n. \\nClick \\nHome\\n. Expand \\nDiscovery & Integration\\n.\\n4\\n. \\nClick to \\nProbe Setup. \\n5\\n. \\nThe status of the probe is \\n'Connected'.\\nValidate the connection on a UCMDB Local Client\\nPerform the following:\\nDownload and execute the UCMDB Local Client\\n1\\n. \\nEnter the following address on a browser to access the Universal CMDB server:\\nhttps://<hostname of classic OBM>:<port>\\nFor example: \\nhttps://myhost.mycomputer.net:8443\\n2\\n. \\nOn the UCMDB home page, based on your operating system, click a link to download the UCMDB Local Client.\\n3\\n. \\nExtract the contents of the UCMDB Local Client zip file (For example, UCMDB_Local_Client_Win) and execute the script.\\n4\\n. \\nIn the UCMDB Local Client window, click \\nAdd\\n. In the \\nAdd/Edit Configuration\\n window, add all the required details: \\n \\nNote:\\nThe UCMDB Local Client allows you to launch the RTSM application from your local system. Use this option if your\\nbrowser does not support Java Applets.\\nSkip this step if you already have the UCMDB Local Client installed.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n395\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '481214d3bfb9b4ad77f9e3ae5082ade6'}>,\n",
              "  <Document: {'content': 'Configure Hyperscale Observability\\nPerform the following tasks to view Performance Dashboards and events in the Event Perspective of OBM:\\nVerify that Hyperscale Observability is connected to Classic OBM\\nDeploy UCMDB Views\\nImport Hyperscale Observability Content Pack into OBM\\nGrant certificate request on OBM\\nDeploy the event mapping policy\\nSet up CLI and start monitoring\\nContainerized Operations Bridge 2022.11\\nPage \\n394\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7df3b80252e9415afae8a81e4644043a'}>,\n",
              "  <Document: {'content': \"Uninstall\\nYou can uninstall the following:\\u200b\\u200b\\u200b\\u200b\\u200b\\u200b\\nUninstall only Operations Bridge:\\n If you want to retain the cluster setup along with OMT and uninstall only the\\napplication, you can follow the steps mentioned in the following sections:\\nBackup collection configurations before uninstall (if you have used AMC)\\nUninstall application (Applicable if you had installed using CLI or AppHub)\\nDelete AppHub deployment (Applicable if you had installed using AppHub)\\nDelete the application's external Databases (Applicable if you had installed using CLI or AppHub)\\nUninstall everything:\\n You may want to uninstall the Operations Bridge and OMT (along with local storage provisioner)\\nand remove the complete cluster by following these steps:\\nBackup collection configurations before uninstall (if you have used AMC)\\nUninstall application  (Applicable if you had installed using CLI or AppHub)\\nDelete AppHub deployment (Applicable if you had installed using AppHub)\\nDelete the application's external Databases (Applicable if you had installed using CLI or AppHub)\\nUninstall Local storage provisioner,  follow the steps mentioned in \\nUninstall local storage provisioner\\n ((Applicable if\\nyou had installed using CLI or AppHub, \\nNot applicable to OpenShift deployments\\n)\\nUninstall OMT after you uninstall Operations Bridge. To uninstall OMT,  follow the steps mentioned in \\nUninstall OMT\\n of\\nOMT documentation. (Applicable if you had installed using CLI or AppHub)\\nTo uninstall NOM OPTIC Reporting, if deployed in the cluster, see \\nUninstall NOM OPTIC Reporting\\n. \\nBackup collection configurations before uninstall\\nThis is applicable if you have used AMC for collection in OPTIC Reporting capability.\\nBefore uninstalling Operations Bridge, you need to backup custom configurations if any:\\nFollow the steps:\\n1\\n. \\nSet up the monitoring CLI\\n for Agent Metric Collection \\n2\\n. \\nRun the following commands to export all custom configurations.\\na\\n. \\nRun the following command to backup custom credentials, if any: \\n./ops-monitoring-ctl get credentials -n <credential name> -o yaml  -f <file name>\\nExample:\\n./ops-monitoring-ctl get credentials -n custom_amc_obm_basic_auth -o yaml -f custom_amc_obm_basic_auth.yaml\\nb\\n. \\nRun the following command to backup custom targets, if any:\\n ./ops-monitoring-ctl get target -n <target name> -o yaml  -f <file name>\\nExample:\\n./ops-monitoring-ctl get target -n custom_amc_obm_rtsm -o yaml -f custom-amc_obm_rtsm.yaml\\nc\\n. \\nTake a copy of the custom file for \\nnodefilter/proxy/ports/hosts\\n if any. For more information, see \\nModify the collection\\nattributes\\n page.\\nd\\n. \\nRun the following command to backup custom collectors, if any:\\n ./ops-monitoring-ctl get coll -n <collector name> -o yaml  -f <file name>\\nExample:\\n./ops-monitoring-ctl get coll -n custom-agent-collector-sysinfra -o yaml -f custom-agent-collector-sysinfra.yaml\\n                   For more information, see \\nManage Agent Metric collection\\n.\\nNote\\n: In a managed Kubernetes deployment, a reference to the master node in this topic implies the bastion\\nnode.\\n\\ue916\\n\\ue916\\nImportant\\n: Credential files created using the steps mentioned will have the passwords masked.\\nPlease add the passwords before using the file\\n\\ue91b\\n\\ue91b\\nImportant\\n: Remove the \\nCreatedBy\\n and \\nCreatedDate\\n fields before using the credential, target, and\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n402\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd2a08ed8aeab61270cc105295b181273'}>,\n",
              "  <Document: {'content': 'Grant certificate request on OBM\\nYou can configure thresholds for the AWS, Azure, or Kubernetes metrics collected by Hyperscale Observability. If a metric\\nbreaches a threshold, an event is generated and forwarded to OBM using the Data Broker Container (DBC). DBC is registered\\nas an Agent in OBM during the Operations Bridge startup. Hence, you must grant its certificate request on OBM.\\nComponent and the supported version\\nComponent\\nSupported version\\nContainerized OBM\\n2022.05 and above\\nClassic OBM\\n2022.05 and above\\nTo grant the request on OBM, perform the following steps:\\n1\\n. \\nGo to \\nADMINISTRATION\\n > \\nSETUP AND MAINTENANCE\\n > \\nCertificate Requests\\n.\\n2\\n. \\nSelect the certificate request from \\nitom-monitoring-service-data-broker-svc \\nfor a containerized OBM and from the \\n<\\nexternalAccessHost>\\n for a  classic OBM.\\n3\\n. \\nRight-click and select \\nGrant Item\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n400\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a88ad15b630730fe732ce0c31da322e5'}>,\n",
              "  <Document: {'content': 'rm -rf /var/vols/itom/<application pv names>/*\\nFor AWS, run the following commands on the bastion node:\\nrm -rf /mnt/efs/var/vols/itom/<application pv name>/*\\nFor Azure, run the following commands on the bastion node:\\nrm -rf /mnt/nfs/<filesharename>/var/vols/itom/<application pv name>/*\\nrm -rf \\ncommand will remove all the data and configuration files in that specified folder without prompting for\\nconfirmation.  If required, take a backup before executing this command.\\nIn the case of manually set up AWS infrastructure\\n, you must delete the \\nEFS\\n and the bastion stack manually.\\nDelete AppHub deployment\\nIn the \\nActions \\nmenu, select \\nDelete \\nto delete a deployment.  You are prompted to confirm that you want to delete the\\ndeployment.  Click the \\nDelete \\nbutton to confirm.\\nThe operation runs and deletes the deployment from the cluster. This deletes the deployment, and removes all underlying\\nKubernetes objects and the associated Helm release.\\nWhen the delete operation is complete, OPTIC AppHub removes the deployment card from the \\nDeployments \\npage.\\nDelete the application external Databases\\n1\\n. \\nClear the external databases.\\n1\\n. \\nTo delete the application user and databases for PostgreSQL, there are two ways to delete:\\n1\\n. \\nUse \\nRemoveSQL.sql\\n \\ncommand\\n.\\ni\\n. \\nCopy the \\nRemoveSQL.sql\\n to the database server. \\nii\\n. \\nRun the following command if you want to remove the users and databases created by the script:\\npsql -f RemoveSQL.sql\\nNote:\\n This section is applicable only if you have installed the application using AppHub UI.\\n\\ue916\\n\\ue916\\nImportant\\n: To know if AppHub was used at some point in the past, see if \"\\nuserInput.*\\n\" parameters in \"helm\\nget values\" are present. If the values are present, proceed with the steps. If the values are absent and you want to\\nuse AppHub, it\\'s mandatory to onboard the CLI deployment to AppHub following correct procedures. For more\\ninformation, see the \\nOMT documentation\\n. If you skip to onboard the CLI deployment to AppHub and still follow the\\nsteps, there are chances of failure due to missing values.\\n\\ue91b\\n\\ue91b\\nNote: \\nThe \\nhelm uninstall\\n command deletes an application from the specified namespace, but doesn\\'t remove it\\nfrom the OPTIC AppHub database.  This means the deployment will still appear on the \\nDeployments \\npage in\\nOPTIC AppHub.  To remove the deployment from the \\nDeployments \\npage, use the \\nDelete \\naction in OPTIC\\nAppHub.\\n\\ue916\\n\\ue916\\nImportant\\n:\\nRemoveSQL.sql\\n script doesn\\'t remove \\ncdfapiserverdb\\n and \\ncdfidmdb\\n. It will remove application\\nspecific databases: \\nidm, autopass, bvd,  obm_event, obm_mgmt, rtsm, credentialmanager, monitoringadmindb,\\nmonitoringsnfdb, \\nand \\nbtcd\\n (the \\nbtcd\\n database is for metric transformation and applicable only if you want to integrate\\nOperations Bridge and NOM).  CDF databases: \\ncdfapiserverdb\\n and \\ncdfidmdb\\n are removed as part of uninstalling OMT.  For\\nmore information, see \\n\\ue91b\\n\\ue91b\\n \\nUninstall OMT\\n in OMT documentation.  \\nNote\\n: For AWS or Azure deployments, the \\nRemoveSQL.sql\\n script is available in the same directory\\nwhere you had run the  \\n./DBSQLGenerator.sh\\n script while deploying the application. You must run the script\\nfrom the same directory.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n404\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ce387fbf4a526418366aa2a4dd8a5ed'}>,\n",
              "  <Document: {'content': 'Uninstall application\\nYou can uninstall Operations Bridge while retaining the OMT install. You need to perform the following tasks to uninstall\\nOperations Bridge deployment.\\n1\\n. \\nRun the following commands to look up a helm deployment name. \\nFor example,\\n# helm list -n opsb-helm\\nNAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                               APP VERSION\\ndeployment01            opsb-helm       2               2021-05-27 18:22:15.28736276 -0700 PDT  deployed        opsbridge-suite-2021.08.0-71        2021.05.0-71\\n2\\n. \\nUninstall the Operations Bridge deployment with the following command:\\nhelm uninstall <deployment name> -n <application namespace> --no-hooks \\nThis command uninstalls all Kubernetes resources of the application that includes all the capabilities. For example,\\nsecrets, persistent volume claims, \\nconfig maps.\\nFor example, to\\n \\nuninstall or delete a deployment  \\ndeployment01:\\nhelm uninstall deployment01 -n opsb-helm --no-hooks\\n3\\n. \\nVerify whether \\nsis-adapter\\n is terminated:\\nkubectl get pods -n <application namespace>\\n \\n4\\n. \\n Run the following commands:\\nkubectl delete pvc -n <application namespace> --all\\nkubectl delete ns <application namespace>\\n5\\n. \\nRun the following command to delete the application Persistent Volumes:\\nkubectl delete pv <application pv names>\\nIf the PV deletion fails to complete, do the following:\\nkubectl get pv --all-namespaces\\nIf you see that it\\'s  in the \"\\nterminating\\n\" state, execute the following command:\\nkubectl get pv | tail -n+2 | awk \\'{print $1}\\' | xargs -I{} kubectl patch pv {} -p \\'{\"metadata\":{\"finalizers\": null}}\\'\\nAgain try deleting PVs.\\n6\\n. \\nLog on to the NFS server host as \\nroot\\n and  delete the content in the application NFS volume directories: \\n<NFS>/<\\napplication\\n pv name>/*\\nFor example,\\ncollector files.\\n\\ue91b\\n\\ue91b\\nNote\\n: For deployment on Azure, perform this step before you perform the uninstall steps for the application.\\nLog on to the NFS server host as \\nroot\\n and run the following command to give permission to all the application\\ndirectories:\\nsudo chmod 755 -R /mnt/nfs/<filesharename>/var/vols/itom/<application directory name>/*\\n\\ue916\\n\\ue916\\nImportant\\n: \\nSkip steps 4 and 5 in this section while uninstalling\\n OpenShift\\n deployment.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n403\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a23113451188afcbb81c034bee6a461d'}>,\n",
              "  <Document: {'content': \"Deploy the event mapping policy for Hyperscale\\nObservability\\nTo forward the events generated by Hyperscale Observability to OBM, you must deploy the Operations Bridge Manager (OBM)\\nevent mapping policy (\\nMonitoringService_Threshold_Event_Mapper\\n) to the Operations Agent running on the Data Broker\\nContainer ( \\nitom-monitoring-service-data-broker-svc\\n).\\nPerform the following steps to deploy \\nMonitoringService_Threshold_Event_Mapper\\n policy to \\nitom-monitoring-service-\\ndata-broker-svc\\n:\\n1\\n. \\nOn OBM, go to \\nAdministration\\n > \\nMonitoring\\n > \\nPolicy Templates\\n.\\n2\\n. \\nSelect \\nTemplate by Type\\n > \\nEvents\\n > \\nEvent from REST Web Service\\n.\\n3\\n. \\nIn the middle pane, expand \\nMonitoringService_Threshold_Event_Mapper\\n and select the version.\\n4\\n. \\nClick \\n \\n \\nAssign\\n \\nand Deploy\\n. The Assign and Deploy window opens.\\n5\\n. \\nIn the \\nConfiguration Item\\n tab, select the configuration item \\n \\nitom-monitoring-service-data-broker-svc\\n.\\n6\\n. \\nClick \\nAssign\\n.\\nAfter the policy is deployed, you will start seeing the events generated by Hyperscale Observability in OBM Event Browser. \\nRelated topics\\nView AWS events\\nView Azure events\\nView Kubernetes events\\nNote \\nThis procedure applies to all Hyperscale Observability collectors. You must perform it only once even if\\nyou've deployed more than one type of collector. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n401\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1b12fe8dc223875125d8a08251a0f9e2'}>,\n",
              "  <Document: {'content': 'Install Agentless Monitoring in K3s environment\\nYou can install the Agentless Monitoring capability of the Containerized Operations Bridge suite on K3s. This is a low footprint\\nenvironment that requires fewer IT resources as compared to the application install. You can install it on a single node K3s\\nsystem using local storage and an embedded PostgreSQL database.\\nThis topic gives you the steps for installing the \\nAgentless monitoring capability in\\n containerized Operations Bridge capabilities on a\\nK3s environment.\\nSystem requirements\\nSystem requirement \\nSpecification for this installation\\nKubernetes distribution\\nK3s\\nNumber of nodes\\nSingle node with a minimum of 8 CPUs and 16 GB memory\\nStorage\\nK3s local path storage provider. You need to have at least 250 GB of free disk space\\nOperating System\\nLinux system which supports K3s\\nThis topic assumes you have a dedicated K3s installation. You need to consider the following limitations of this installation:\\nYou can\\'t upgrade from a low footprint single node installation to multi-node K3s setup\\nYou can\\'t add Grafana or Prometheus\\nInstallation is CLI based\\nSet up prerequisites\\nStep 1: Activate a Docker Hub account \\nYou need a valid Docker Hub account to download application images from Docker Hub. If you don\\'t already have one, you\\nmust set up the Docker account and then contact \\nMicro Focus\\n with your account details. For more information about how to\\ndo this, see \\nActivate your Docker Hub account\\n.\\nStep 2: Prepare the infrastructure\\n1\\n. \\nInstall K3S environment. For more information, see the \\nK3s documentation\\n.\\n2\\n. \\nRun the following command to create a \\nkube config\\n file:\\n# mkdir ~/.kube\\n# cp /etc/rancher/k3s/k3s.yaml ~/.kube/config\\n3\\n. \\nInstall Helm for K3s. For more information on installing Helm see \\ninstalling Helm on K3s\\n.\\n4\\n. \\nRun the following commands to verify your environment:\\n# kubectl version\\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6+k3s1\", GitCommit:\"418c3fa858b69b12b9cefbcff0526f666a6236b9\", GitTreeState:\"clean\", BuildDate:\"2022-04-28T22:16:18Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6+k3s1\", GitCommit:\"418c3fa858b69b12b9cefbcff0526f666a6236b9\", GitTreeState:\"clean\", BuildDate:\"2022-04-28T22:16:18Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\\n# kubectl get pods --all-namespaces\\nNAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE\\nkube-system   coredns-b96499967-h4j7r                   1/1     Running     0          117s\\nkube-system   local-path-provisioner-7b7dc8d6f5-cmxfc   1/1     Running     0          117s\\nkube-system   svclb-traefik-6e541e4c-bk9tq              2/2     Running     0          103s\\nkube-system   metrics-server-668d979685-qqhpr           1/1     Running     0          117s\\nkube-system   traefik-7cd4fcff68-66mx6                  1/1     Running     0          103s\\n# helm version\\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\\n5\\n. \\nBefore continuing the installation, ensure the DNS name resolution is set up correctly. For Agentless Monitoring the system\\nneeds to be able to resolve its own (external) IP address, localhost must resolve to 127.0.0.1 and you need to be able to resolve\\nthe addresses of all SiteScope servers you want to manage. The SiteScope servers must be able to resolve the system used for\\ninstalling Agentless Monitoring.\\nStep 3: Install OMT tools\\nOPTIC Management Toolkit (OMT) offers the necessary tooling for installing and running containerized ITOM applications on\\nKubernetes.\\nFollow the steps on \\nDownload OMT installation package\\n to download the \"\\nOMT2xxx-xxx-15001-external-K8s.zip\\n\".\\nNotes:\\n \\nNote:\\n The single node K3s installation is only supported for a subset of the capabilities in Operations Bridge.\\nTo install the complete suite, or if you plan to install other capabilities, see the \\nInstall\\n documentation.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n408\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9d6f29b47ec221c99f5d063d21ae9cd6'}>,\n",
              "  <Document: {'content': 'Install use cases\\nThis section provides you with use cases that enable you to get started with deploying a specific deployment scenario for a\\nsubset of the capabilities available in the OpsBridge application.\\nThis section contains:\\nInstall Stakeholder Dashboard in K3s environment \\nContainerized Operations Bridge 2022.11\\nPage \\n407\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd7e85988873a6942ca42ac7856943b81'}>,\n",
              "  <Document: {'content': '2\\n. \\nRun the following SQL statements manually.\\nLog in to a server that can run the \\npsql\\n command and run the following command to connect to the external database. \\npsql -h <external postgresql host or ip> -U <pg_admin_username> -W\\nWhere:\\n<external postgresql host or ip>\\n is the FQDN or IPv4 address of the external PostgreSQL database server.\\n<pg_admin_username>\\n is the database administrator.\\nDelete the databases:\\ndrop database idm;\\ndrop database autopass;\\ndrop database bvd;\\ndrop database obm_mgmt;\\ndrop database obm_event;\\ndrop database rtsm;\\ndrop database credentialmanager;\\ndrop database monitoringadmindb;\\ndrop database monitoringsnfdb;\\n \\ndrop database btcd;\\n2\\n. \\nTo delete the application user and databases for Oracle, there are two ways to delete:\\n1\\n. \\nUse \\nRemoveSQL.sql command. \\ni\\n. \\nCopy the \\nRemoveSQL.sql \\nto the database server.\\nii\\n. \\nRun the following command if you want to remove the schemas created by the script.\\necho exit | sqlplus sys/syspassword as SYSDBA @RemoveSQL.sql\\nFor more information on creating the database, see \"Prepare Oracle or Oracle RAC\" topic. \\n2\\n. \\nRun the following SQL statements manually.\\nLog in to an Oracle database server and run the following command to connect to the external database. Replace the\\nplaceholders with the actual values.\\nsqlplus <admin user>/<admin password>@<oracle server FQDN or IP>:<Port>/<service name or SID>\\nFor example:\\nsqlplus adminuser/mypassword@192.0.2.1:1234/orcl\\nDelete the databases:\\nDROP USER idm CASCADE;\\nDROP USER autopass CASCADE;\\nDROP USER bvd CASCADE;\\nDROP USER obm_event CASCADE; \\nDROP USER obm_mgmt CASCADE;\\nDROP USER rtsm CASCADE;\\nDROP USER credentialmanageruser CASCADE;\\nDROP USER monitoringadminuser CASCADE;\\nDROP USER monitoringsnfuser CASCADE;\\nDROP USER btcduser CASCADE;\\n2\\n. \\nIf you have configured Automatic Event Correlation, log on the Vertica system as dbadmin user, and execute the following query: \\nDROP SCHEMA IF EXISTS itom_analytics_provider_default CASCADE;\\n3\\n. \\nClear the Vertica database  \\n4\\n. \\nIf you have configured Automatic Event Correlation, log on the Vertica system as dbadmin user, and execute the following query: \\nDROP SCHEMA IF EXISTS itom_analytics_provider_default CASCADE;\\n5\\n. \\nClear the Vertica database  \\nFollow these steps to clean up the Vertica database and uninstall the OPTIC DL Vertica Plugin:\\n1\\n. \\nOn the Vertica system, log on as the database administrator user.\\n2\\n. \\nRun the following queries to clean up the schemas:\\nDROP SCHEMA IF EXISTS itom_analytics_provider_default CASCADE;\\nNote\\n: Run the following commands on AWS and Azure environments:\\nFor AWS: \\npsql -h <DB_HOSTNAME> -U <db admin> -d <database name> -f RemoveSQL.sql\\nFor Azure: \\npsql -h <DB_HOSTNAME> -U <db admin>@<DB_HOSTNAME> -d <database name> -f RemoveSQL.sql\\n\\ue916\\n\\ue916\\nNote:\\n In this release containerized OBM isn\\'t supported for OpenShift deployments, you can ignore the\\nOBM database related statements.\\n\\ue916\\n\\ue916\\nTip: \\nIf you have used an existing Shared OPTIC Data Lake from another deployment, skip this section.\\n\\ue917\\n\\ue917\\nTip: \\nIf you have used an existing Shared OPTIC Data Lake from another deployment, skip this section.\\n\\ue917\\n\\ue917\\nDROP SCHEMA IF EXISTS itom_di_configuration_provider_default CASCADE;\\nDROP SCHEMA IF EXISTS itom_di_metadata_provider_default CASCADE;\\nDROP SCHEMA IF EXISTS itom_di_postload_provider_default CASCADE;\\nDROP SCHEMA IF EXISTS itom_di_scheduler_provider_default CASCADE;\\nDROP SCHEMA IF EXISTS mf_shared_provider_default CASCADE;\\nDROP SCHEMA IF EXISTS itom_di_dbinit_provider_default CASCADE;\\nREVOKE USAGE ON RESOURCE POOL itom_di_stream_respool_provider_default FROM <rwuser> CASCADE;\\nContainerized Operations Bridge 2022.11\\nPage \\n405\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3893aca09785b7d4bf05c1f0bc7d450b'}>,\n",
              "  <Document: {'content': \"3\\n. \\nYou can \\ndelete a tenant\\n using \\ndbinit.sh\\n \\nscript:\\ncd /usr/local/itom-di-pulsarudx/bin\\n./dbinit.sh delete\\n4\\n. \\nRun the following queries to drop read-only/read-write users:\\n5\\n. \\nRun the following queries to drop the resource pool:\\nDROP RESOURCE POOL itom_di_express_load_respool_provider_default;\\nDROP RESOURCE POOL itom_di_monitoring_respool_provider_default;   \\nDROP RESOURCE POOL itom_di_postload_respool_provider_default;\\nDROP RESOURCE POOL itom_di_rouser_respool_provider_default;\\n6\\n. \\nRun the following command to drop the \\npulsar\\n \\nudx\\n library:\\n7\\n. \\nRun the following command as the root user to uninstall the OPTIC DL Vertica Plugin RPM\\n:\\n8\\n. \\nRestart the database from the Vertica \\nadmintools\\n.\\nIf you've set up the Azure infrastructure and added Azure components, you must \\ndelete\\n them manually. After you clean the\\ninfrastructure setup manually, run the following commands:\\ncd <cloud-toolkit-location>/azure/tf-itom-sa/\\nterraform destroy -var-file=template.tfvars\\nIf you've set up the Azure infrastructure setup using the ITOM Cloud Deployment Toolkit from MarketPlace, run the \\nterraform des\\ntroy\\n command to delete the Azure infrastructure setup.\\nIf you've set up the AWS infrastructure and added the AWS stacks, you must delete them \\nmanually\\n. You must also delete any\\nadditional resources that you have created. After you clean the infrastructure setup manually, run the following commands:\\ncd\\n \\n<cloud-toolkit-location>\\n/aws/tf-example/\\nterraform destroy -var-file=dev.tfvar\\nIf you've set up the AWS infrastructure using the ITOM Cloud Deployment Toolkit from MarketPlace, run the \\nterraform\\ndestroy\\n command to delete the AWS infrastructure setup.\\nDROP RESOURCE POOL itom_di_stream_respool_provider_default;\\nImportant:\\n If you delete the tenant it will drop read-only/read-write users, resource pools and drop the \\npulsar\\n \\nudx\\n library\\nHence there is no need to perform the steps\\n (4, 5 and 6)\\n unless you want to perform any one of those\\noperations specifically.\\n\\ue91b\\n\\ue91b\\n. \\nDROP USER IF EXISTS <rw_user> CASCADE;\\nDROP USER IF EXISTS <ro_user> CASCADE;\\n/opt/vertica/bin/vsql -U <dbadmin user> -f /usr/local/itom-di-pulsarudx/sql/uninstall.sql\\nrpm -e itom-di-pulsarudx-<version>.x86_64\\napplication\\nNote\\n: Embedded database container gets removed upon \\n\\ue916\\n\\ue916\\n uninstall. Clearing the NFS share will be enough.\\nContainerized Operations Bridge 2022.11\\nPage \\n406\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd3dafe64b0210402ee8b081fb9e710cd'}>,\n",
              "  <Document: {'content': 'Plan Stakeholder Dashboard deployment on K3s\\nThis section provides information required to prepare your environment for installing BVD (Stakeholder Dashboard capability)\\nin K3s environment. Before you begin, ensure that you have the deployment architecture planned and have the server node\\nallocated.\\nYou need to plan the following before deploying Stakeholder Dashboard in K3s:\\nDeployment model\\nThe deployment model depends on the capabilities that you choose to install. To create a deployment plan for installing\\nStakeholder Dashboards (Business Value Dashboard) on a single node in K3s environment, you must consider the following:\\nWorkload distribution model\\n: In this deployment, you can run Kubernetes, OMT, and BVD on a single node. In this\\ncase, the master node (control plane node) also acts as a worker node.\\nNFS server implementation strategy:\\n Not required.\\nDatabase requirements\\n: Embedded PostgreSQL for production in low footprint environments or external PostgreSQL\\n.\\nHigh Availability (HA) requirements\\n - Not required.\\nSizing the deployment\\nWhen you plan the deployment, you need to estimate the size of the environment to monitor the load. \\nThe following is the requirement for Stakeholder Dashboard deployment in a single node:\\nCapability resource requirements (deployed on a single\\nnode)\\nReceive/web\\nserver/quexserv\\nCPUs\\nMemory requirements\\nGB\\nStakeholder Dashboard (BVD)\\nreceiver: 1\\nwebserver:1\\nquexserv: 1\\n4.0\\n8.0\\nDirectory structure and file system requirements\\nThis topic provides a conceptual overview of the directory structure and the storage space requirements to help you set up a\\nserver or a virtual machine (VM) for the installation. You can follow the recommendations below to calculate the requirements\\nbased on your deployment decisions and business needs.\\nAll Linux systems use space under the root \\nfilesystem\\n (“/”). A best practice is to set up a few separate volumes or\\nfilesystems for directory trees such as /tmp or /var. You may follow your own system administration best practices to\\nassign \\nfilesystems\\n as long as they have enough space in the directories specified below. \\nDirectory\\nDescription\\n/ (root\\nfilesystem)\\nThis is the root directory for various Linux operating system functions. The minimum recommended space\\nfor this directory is 10 GB for all OMT systems, if the following sub directories are in other filesystem\\npartitions.\\n/tmp\\nOperating system uses this directory as a temporary location for application files. The minimum\\nrecommended size for this directory is 15 GB on all your systems. You use this directory to copy and\\nunpack the installation files.\\n/opt\\nOperating system uses this directory for various application files: OMT \\nRuntime\\n directory.\\nPort usage\\nThe following ports are used by the installation and depending on the customer environment, you might need to add more\\nports to the firewall configuration:\\n Incoming ports\\n:\\nhttps traffic to BVD (as configured with the option \\nglobal.externalAccessPort\\n during installation)\\nOutgoing ports\\n:\\nOptional: External PostgreSQL as configured with the option \\nglobal.database.port\\n during installation.\\nOptional: External LDAP/SAML server (as configured in IdM UI)\\nOptional: External Vertica access (as configured in BVDs Data Collector UI)\\nSystem requirements\\nNote: If you are scaling BVD receiver, web server, or quexserv, add 0.5 CPU and 2 GB of RAM for each additional\\ninstance.\\nContainerized Operations Bridge 2022.11\\nPage \\n414\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '755f845a45a1e269f828b79b68cf8983'}>,\n",
              "  <Document: {'content': '# tar -C /var/opt/cdf/offline/images_20221122153017 -c . | ctr images import - \\nThe image import will take about five minutes. After successful import, you can delete the downloaded images and the \\nallimages.tar\\nfile to free up some disk space.\\nStep 5: Create a YAML file\\nFor a single node installation, you can save additional resources by using the following \\nlowfootprint.yaml\\n:\\n# vi lowfootprint.yaml\\n# This file includes recommended setting for a single node, low footprint OpsB Agentless Monitoring deployment.\\nglobal:\\n  deployment:\\n    size: lowfootprint\\nidm:\\n  deployment:\\n    replicas: 1\\n    minAvailable: 1\\nitom-ingress-controller:\\n  deployment:\\n    replicas: 1\\nStep 6: Set up CA or trust certificate for connecting to SiteScope\\nAs the Agentless Monitoring UI needs to connect to your SiteScope Servers, you must provide the Certificate(s) to trust your\\nSiteScope servers. Assuming your SiteScope certificates are signed by a CA, you can specify the CA certificate in base64\\ndecoded format. For the following steps, assume the CA certificate is in \\n/root/INSTALL/CA_Base64.cer\\nStep 7: Deploy \\nRun the following helm command to begin the Agentless Monitoring deployment:\\n# helm install agentless --namespace opsbridge \\\\\\n    --set-file \"caCertificates.sitescope-ca\\\\.crt\"=/root/INSTALL/CA_Base64.cer \\\\\\n    -f /root/INSTALL/values_k3s.yaml \\\\\\n    -f /root/INSTALL/lowfootprint.yaml \\\\\\n    /root/INSTALL/opsbridge-suite-chart/charts/opsbridge-suite-<version>.tgz\\nOnce the command runs successfully, the deployment will start up in the background. The initial startup time will take about\\n15 minutes.\\nStep 8: Verify the installation\\n# kubectl get pods -n opsbridge\\nNAME                                                  READY    STATUS      RESTARTS   AGE\\nitom-opsb-db-connection-validator-job-bpmdz            0/1     Completed   0               12m\\nitom-reloader-6c86c8ccd7-z8qcn                         1/1     Running     0               11m\\nitom-opsb-resource-bundle-7488fb98bc-r6sp4             1/1     Running     0               11m\\nitom-prometheus-cert-exporter-5bd999bdb9-9vwln         2/2     Running     0               11m\\nbvd-redis-5649f66c8b-9f8ll                             2/2     Running     0               11m\\nitom-vault-558794455b-fbthr                            1/1     Running     0               11m\\nbvd-quexserv-6c88d8f69f-v6bnn                          2/2     Running     0               11m\\nitom-ingress-controller-5b4cc69946-7r5xt               3/3     Running     0               11m\\nitom-postgresql-5ff656ffb4-dgmvd                       2/2     Running     0               8m45s\\nitom-pg-backup-7d4599444b-6w5f9                        2/2     Running     0               11m\\nitom-opsb-database-init-oa38ppv-mpwrm                  0/1     Completed   0               11m\\ncredential-manager-64ff4c4bbf-w5csc                    2/2     Running     0               11m\\nitom-opsbridge-agentless-monitoring-5496484b4d-ffsvk   2/2     Running     0               11m\\nitom-opsbridge-monitoring-resources-54d576d749-nlzgx   2/2     Running     0               11m\\nbvd-controller-deployment-67f989c949-cppsb             2/2     Running     0               11m\\nwebtopdf-deployment-745ff58b89-ss9jh                   2/2     Running     0               11m\\nbvd-explore-deployment-7c8c865c46-4bxv2                2/2     Running     0               11m\\nbvd-www-deployment-654964d89-5hscz                     2/2     Running     0               11m\\nitom-pgbackup-enabler-job-itxnx5h-m28fz                0/1     Completed   0               11m\\nbvd-receiver-deployment-5979dfbc47-84shk               2/2     Running     0               11m\\nitom-idm-54c9b49d9-phkn4                               2/2     Running     0               11m\\nuif-upload-job-dl4jseg-xp6jc                           0/1     Completed   0               11m\\nitom-autopass-lms-7998d4f5b-cj25w                      2/2     Running     0               11m\\nitom-monitoring-admin-c7b5bc4cd-jtf5p                  2/2     Running     0               11m\\nitom-monitoring-sis-adapter-5479cb84b8-cqnrc           2/2     Running     0               11m\\nOnce the pods are up and running set up users and connect your SiteScope servers. See \\nAdminister Agentless Monitoring\\n.\\nTroubleshooting\\nCrashloop\\n or slow start up of \\nmonitoring-admin\\nProblem\\nContainerized Operations Bridge 2022.11\\nPage \\n411\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '251d7633bf6ef74e40e450ece119a731'}>,\n",
              "  <Document: {'content': \"You observe a slow start up of \\nmonitoring-admin\\n and the container logs show:\\nitom-monitoring-admin 2022-07-10 14:32:00,835 INFO 200-setup-user:setup_user Created user: serviceuser\\nitom-monitoring-admin 2022-07-10 14:32:00,847 INFO 200-setup-user:setup_user Created group: servicegroup\\nitom-monitoring-admin 2022-07-10 14:32:00,873 INFO 999-service:source Running as user 1999: uid=1999(serviceuser) gid=1999(servicegroup) groups=1999(servicegroup)\\nitom-monitoring-admin 2022-07-10 14:32:00,881 WARN 999-service:source Not running with read-only filesystem\\nitom-monitoring-admin 2022-07-10 14:32:00,897 WARN 200-calc-heap:source heap space (773741824) exceeded maximum (500000000) \\nitom-monitoring-admin 2022-07-10 14:32:00,935 INFO utils:importKey Importing key into keystore /tmp/home/secrets/server-keystore: /var/run/secrets/boostport.com/…\\nCause\\nThe reason for the slow key import is a low entropy for generating random numbers on some older systems.\\nSolution\\nInstall and configure \\nhaveged\\n (\\nHardware Volatile Entropy Gathering and Expansion\\n). Follow the steps for CentOS 7.9:\\nyum install haveged\\nsystemctl enable haveged\\nsystemctl start haveged\\nhaveged\\n is part of the Extra Packages for Enterprise Linux (EPEL) repository. If this isn't enabled already, you might need to\\ninstall using:\\nsudo yum install epel-release\\nOnce installed, \\nmonitoring-admin\\n should start up within five minutes.\\n \\nSlow UI response\\nProblem\\nThe low footprint system sizing (8 CPUs, 16 GB) assumes a Linux server system with no other workloads on the system.\\nSolution\\nYou shouldn't run a Linux with a desktop environment enabled unless you add resources. Additionally, ensure your system\\nisn't using swap space. Either disable swap or increase the system memory.\\nMissing container images\\nProblem\\nPods don't startup because of missing images.\\nSolution\\nCheck your disk space. K3s images are stored under \\n/var/\\n and if the disk space used in \\n/var\\n exceeds the eviction threshold\\n(typically 85%), Kubernetes starts deleting the unused images. If this happens during image loading, your containers won't\\nstart up.\\nContainerized Operations Bridge 2022.11\\nPage \\n412\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e84e2579447886cf6f3ec3d32a6ccda1'}>,\n",
              "  <Document: {'content': 'Install Stakeholder Dashboard in K3s environment\\nK3s is a highly available, certified Kubernetes distribution designed for production workloads. K3s is a single less than 40 MB\\nbinary that reduces the dependencies and steps needed to install, run and auto update a production Kubernetes cluster.\\nThis section provides information required to prepare your environment for installing BVD (Stakeholder Dashboard capability)\\nin K3s environment. Before you begin, ensure that you have the deployment architecture planned and have the server node\\nallocated and perform the following tasks:\\n1\\n. \\nPlan Stakeholder Dashboard deployment\\n2\\n. \\nDeploy Stakeholder Dashboard\\nNote\\n: Containerized OpsBridge supports embedded PostgreSQL for production in low footprint environments\\n. \\nContainerized Operations Bridge 2022.11\\nPage \\n413\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc70641188d4b6ee14923fa7236fda5e'}>,\n",
              "  <Document: {'content': 'The OMT documentation distinguishes between Control plane / Bastion / Worker nodes. For the low footprint, single node\\ninstallation, all these terms refer to different roles of the same single system.\\nThis topic assumes the installation packages are downloaded and unzipped in \\n~root/INSTALL/\\nThis topic assumes installation as the root user. For installing as non-root you may adjust the commands to use sudo.\\nRun the following commands to install the OMT tools:\\n# cd ~/INSTALL/OMT_External_K8s_202x.xx-xxx\\n# ./install --capabilities Tools=true,Monitoring=false,LogCollection=false,DeploymentManagement=false,ClusterManagement=false\\nWarning: tools will be copied to /root/cdf.\\nWarning: All the files under the following folders will be removed: /root/cdf/bin /root/cdf/scripts /root/cdf/tools.\\nAre you sure to continue? (Y/N): y\\nTool copy done! Tools are copied to /root/cdf.\\nAfter installing the OMT tool reload your environment or log off and on to ensure the OMT path settings are active. For\\nadditional details, see \\nEnable OMT tools\\n.\\nDeploy \\nStep 1: Create a deployment \\n# cdfctl deployment create -t helm -d opsbsuite -n opsbridge\\n2022-05-30T11:45:51+02:00 WRN Secret registrypullsecret doesn\\'t exist. The deployment will be created without image pull secret\\n2022-05-30T11:45:51+02:00 INF Creating deployment ... name=opsbsuite namespace=opsbridge\\n2022-05-30T11:45:51+02:00 INF Created namespace \"opsbridge\" ...\\n2022-05-30T11:45:51+02:00 INF Successfully created deployment \"opsbsuite\" uuid=66094403-4fb0-4c13-9845-7b6ef18a5298\\nStep 2: Download the installation chart\\nFollow the \\ndocumentation for downloading the Operations Bridge Chart\\n and download into ~/INSTALL. For this installation, you\\nwill only need to download the Operations Bridge Helm chart (\\nopsbridge-suite-chart-202x.xx.0.zip\\n).\\nStep 3: Configure the deployment options\\nIn this step, edit the default \\nvalues.yaml\\n file for installing Agentless Monitoring in a low footprint, single node environment. You\\nmust copy the sample file included in the samples directory:\\n# cd ~/INSTALL\\n# cp  opsbridge-suite-chart/samples/values.yaml values_k3s.yaml\\n Edit the file and apply the following changes. Make sure to include or update the lines with the exact indentation. Make sure\\nto keep the indentation intact. Don\\'t use \"tabs\".\\n# vi values_k3s.yaml\\n--------- SEARCH THE FOLLOWING LINES AND MODIFY THEM ACCORDINGLY ----------\\nacceptEula: true\\nglobal:\\n  externalAccessHost: <YOUR_SERVERS_FQDN>\\n  externalAccessPort: 30443\\n  services:\\n    agentlessMonitoring:\\n      deploy: true\\n  persistence:\\n    enabled: true\\n    storageClasses:\\n      default-rwx: local-path    \\n      default-rwo: local-path   \\n    accessMode: ReadWriteOnce\\n  docker:\\n    registry: docker.io\\n    orgName: hpeswitom\\n  database:\\n    internal: true\\n  cluster:\\n    k8sProvider: generic       # k8s Provider\\nsecrets:\\nNote:\\n The configuration below will use port 30443 for the external access port. For installing using another\\nport you might need to adjust the K3s installation (disable traefik and/or extend the K3s port range).\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n409\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '74f723a5b9950df85e1a5bffc272aeb7'}>,\n",
              "  <Document: {'content': '  idm_opsbridge_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  bvd_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  IDM_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  AUTOPASS_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  BVD_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  MA_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  CM_DB_PASSWD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  idm_admin_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  sys_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\nThe password requires 8-32 characters and requires an upper and lower case character, a number, and a special character. If\\nyour password doesn\\'t meet the password policy the installation will fail.\\nYou must specify the password in a base64 encoded format. When using special characters, be sure that your shell doesn\\'t\\nreplace these. To test whether your password encodes correctly, you can use the following example:\\n# echo -n ChangeMe2$ | base64 | base64 -d ; echo\\nChangeMe2$\\n#\\nIf the password returned differs from the original, you might need to escape control characters.\\nFollow these steps to create a secrets file. Keep the indentation and don\\'t use tabs. Replace all occurrences of \\n<YOUR_BASE64_E\\nNCODED_PASSWORD>\\n.\\n# echo -n  My_secret_passw0rd$ | base64\\nTXlfc2VjcmV0X3Bhc3N3MHJkJA==\\nStep 4: Load the required Container Images\\nDownload the required container images. OMT provides a utility for downloading just the required set of images. Use the \\n-C\\nand \\n-H\\n parameters to ensure \\nimage-set.json\\n contains only the images needed by the selected capabilities. For additional details\\nrefer to \\nDownload the application images\\n.\\n# /root/cdf/tools/generate-download/generate_download_bundle.sh \\\\\\n     --chart /root/INSTALL/opsbridge-suite-chart/charts/opsbridge-suite-<version>.tgz \\\\\\n     -H /root/INSTALL/values_k3s.yaml -S -o hpeswitom -d /tmp/\\nThis will generate an \"\\noffline-download.zip\\n\" bundle. Copy this to a Linux system, which has access to the internet (\\ndocker.hub\\n). If\\nthe system you are installing on has internet access, then you can continue to use the same system. Unzip the bundle in the \\n/t\\nmp\\n directory and download the required images:\\n# unzip offline_download.zip\\n# cd offline-download/\\n# chmod u+x downloadimages.sh\\n# ./downloadimages.sh -r registry.hub.docker.com -o hpeswitom -u <YOUR DOCKER ACCOUNT>\\nContacting Registry: https://registry.hub.docker.com ../[Failed]                                                                                                        Retrying contacting https://registry.hub.docker.com. please make sure your user name, password and network/proxy configuration are correct.\\nPassword:\\nContacting Registry: https://registry.hub.docker.com ..[OK]\\nStart downloading ...\\n! Warning: Please check suite sizing documentation and make sure you have enough disk space for downloading suite images.\\nContinue?[Y/N]?y\\nDownloading image [1/24] hpeswitom/itom-agentless-monitoring:1.2.0-354 ...[OK]\\nDownloading image [2/24] hpeswitom/itom-autopass-lms:2022.11-2022102615 ...[OK]\\nDownloading image [3/24] hpeswitom/itom-bvd:11.10.22 ...[OK]\\nDownloading image [4/24] hpeswitom/itom-credential-manager:1.19.0.5 ...[OK]\\nDownloading image [5/24] hpeswitom/itom-idm:1.36.1-661 ...[OK]\\nDownloading image [6/24] hpeswitom/itom-k8s-sidecar:1.1.0-0020 ...[OK]\\nDownloading image [7/24] hpeswitom/itom-monitoring-admin:2022.11-262 ...[OK]\\nDownloading image [8/24] hpeswitom/itom-monitoring-resources:1.2.0-120 ...[OK]\\nDownloading image [9/24] hpeswitom/itom-monitoring-sis-adapter:1.3.0-167 ...[OK]\\nDownloading image [10/24] hpeswitom/itom-nginx-ingress:0.23.0-0050 ...[OK]\\nDownloading image [11/24] hpeswitom/itom-opsbridge-database-init:1.1.0-11 ...[OK]\\nDownloading image [12/24] hpeswitom/itom-opsbridge-dbvalidator:2.3.0-13 ...[OK]\\nDownloading image [13/24] hpeswitom/itom-pg-backup:12.1.0 ...[OK]\\nDownloading image [14/24] hpeswitom/itom-pgbackup-enabler:1.0.0-16.commit-7a89ccd ...[OK]\\nDownloading image [15/24] hpeswitom/itom-postgresql:14-00119 ...[OK]\\nDownloading image [16/24] hpeswitom/itom-prometheus-exporter-cert:1.9.0-00101 ...[OK]\\nDownloading image [17/24] hpeswitom/itom-redis:11.10.22 ...[OK]\\nDownloading image [18/24] hpeswitom/itom-reloader:1.1.0-0080 ...[OK]\\nDownloading image [19/24] hpeswitom/itom-static-files-provider:2.4.0-3427 ...[OK]\\nDownloading image [20/24] hpeswitom/itom-stunnel:11.10.0-0036 ...[OK]\\nDownloading image [21/24] hpeswitom/itom-tools-base:1.2.0-0031 ...[OK]\\nDownloading image [22/24] hpeswitom/kubernetes-vault-init:0.17.0-0042 ...[OK]\\nDownloading image [23/24] hpeswitom/kubernetes-vault-renew:0.17.0-0042 ...[OK]\\nDownloading image [24/24] hpeswitom/vault:0.21.0-00103 ...[OK]\\nDownload completed in 257 seconds.\\nDownload-process successfully completed.\\nSuccessfully downloaded the images  to /var/opt/cdf/offline/images_20221122153017/.\\nPlease refer to /var/opt/cdf/offline/images_20221122153017/downloadimages-20221122152554.log for more detail\\nLoad the images into the container. Run the following command to tar and import the images:\\nContainerized Operations Bridge 2022.11\\nPage \\n410\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '68a7bd5f44abd9940812cf3065864794'}>,\n",
              "  <Document: {'content': \"Prepare external Postgres for Stakeholder Dashboard\\nExternal PostgreSQL\\nThis section provides you information to create PostgreSQL databases for the following:\\nidm\\nautopass\\nbvd\\nRole\\nLocation\\nPrivileges required\\nDatabase administrator\\nDatabase server\\nDatabase administrator\\nInstall an external PostgreSQL instance. For details, see the \\nPostgreSQL documentation\\n.\\nMake sure to install \\npostgres-contrib\\n or \\npostgresql<version>-contrib\\n \\npackage on the database server depending on the PostgreSQL\\nversion you are using.\\nExample:\\nyum install postgresql11-contrib -y\\nor \\nyum install postgresql12-contrib -y\\nTo create the databases, you can either use the DB script or follow the manual steps.\\nNote\\n: When you create entities like database, schema, users, etc in PostgreSQL, specify them in lowercase only.\\nCreate the databases using the DB script\\nThe \\nDBSQLGenerator.sh \\nenables you to generate a SQL script that you can run to create the databases (\\nidm, autopass,\\nbvd\\n). The suite zip (\\nopsbridge-suite-chart-2022.05.0.zip\\n) contains the \\nDBSQLGenerator.sh \\nfile under \\nscripts \\ndirectory.\\nFollow the steps:\\n1\\n. \\nGo to the \\nscripts \\ndirectory and execute the \\nDBSQLGenerator.sh\\n:\\n./DBSQLGenerator.sh \\nThe script will prompt you to enter the required values. For all the queries in the script, to use the default value for the\\nparameter, press the enter key without entering any value, \\nThis script generates \\nCreateSQL.sql\\n and \\nRemoveSQL.sql\\n, in the current working directory.\\n2\\n. \\nCopy the \\nCreateSQL.sql\\n to the database server. Execute \\nCreateSQL.sql\\n with database admin privileges: \\nRun the following commands on the PostgreSQL server to create the users and databases:\\nsu - postgres\\npsql -f <Path where script is copied>/CreateSQL.sql\\nExample:\\n psql -f /root/script/CreateSQL.sql\\nImportant: \\nCreateSQL.sql \\ncontains the specified \\npassword in plain text\\n, based on your security policies you can delete\\nthis script after execution or create the databases manually.\\nCreate the databases manually\\nLog in to the PostgreSQL server and then run the following command to connect to the PostgreSQL instance: \\npsql -U <postgres admin>\\nCreate a database for idm (OMT IdM)\\nRun the following queries:\\nCREATE USER <idmuser> login PASSWORD '<idm_user_password>';\\nGRANT <idmuser> TO <postgres admin>;\\nCREATE DATABASE <idmdb> WITH owner=<idmuser>;\\n\\\\c <idmdb>;\\nALTER SCHEMA public OWNER TO <idmuser>;\\nALTER SCHEMA public RENAME TO <idmschema>;\\nREVOKE ALL ON SCHEMA <idmschema> from public;\\nGRANT ALL ON SCHEMA <idmschema> to <idmuser>;\\nALTER USER <idmuser> SET search_path TO <idmschema>;\\nCreate a database for autopassdb\\nContainerized Operations Bridge 2022.11\\nPage \\n421\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a10497de7c615d33c0a40ce7c347adb'}>,\n",
              "  <Document: {'content': \"7\\nDeploy Stakeholder Dashboard Capability \\n \\nOn server\\nDeploy\\nStakeholder\\nDashboard\\nCapability \\n8\\nVerify installation \\nCheck Pod status to find out whether the deployment was successful.\\nOn server\\nVerify installation \\nS/N\\nTask\\nWhere to perform\\nHow to perform\\nPost-installation task\\nCreate stakeholder dashboard in BVD\\nThe stakeholder dashboard gives you visual information of live data using tables, charts, and widgets. You can stream Real\\ntime data from any data source in JSON format via HTTP post. \\nCreate stakeholder dashboard in BVD\\nUninstall BVD in K3s\\nIf you want to uninstall BVD deployed in K3s run the following command:\\nhelm uninstall <deployment name> -n <suite namespace> --no-hooks\\nFor example:\\nhelm uninstall bvd -n bvd-helm --no-hooks\\nMigrate on-premises Stakeholder Dashboard in K3s environment to full suite\\ninstallation\\nAn on premise Stakeholder Dashboard in K3s environment can't get upgraded in place into a full suite installation. The\\nenvironment needs to get migrated to a full installation to install other capabilities. This is only possible for production\\ninstallations with external PostgreSQL databases.\\nThe migration to a full suite installation comprises the following steps:\\nBackup database\\nInstall containerized OpsBridge Suite with Stakeholder Dashboard capability\\nThe migration will contain only the BVD configuration. For example, The migration does'nt include IdM configuration.\\nBack up the BVD database\\nRun the following command to bring down the K3s based installation:\\n/opt/cdf/scripts/cdfctl.sh runlevel set -l DB\\nBack up the \\nbvd\\n database. See the vendor documentation of PostgreSQL for details.\\nInstall \\nContainerized Operations Bridge\\n with BVD capability\\nEnsure the following while installing containerized Operations Bridge:\\nDon't create new \\nbvd\\n database. Ensure that the BVD K3s based installation is down.\\nWhen specifying values in values.yaml or using the \\nOpsBridgeInstallScript.sh\\n script, point the installer to the existing BVD\\ndatabase for the Stakeholder Dashboard capability.\\nFollow the instructions to install containerized Operations Bridge. See Install.\\nContainerized Operations Bridge 2022.11\\nPage \\n417\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cac85fa4fb7db1664b4492502a14e01d'}>,\n",
              "  <Document: {'content': 'Install BVD in K3s\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator \\nRoot\\nFollow the documentation on the Rancher website \\nhttps://rancher.com/docs/k3s/latest/en/installation/install-options/\\n to install\\nK3s on your system.\\nContainerized Operations Bridge 2022.11\\nPage \\n419\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e0f9297c648b58e54cf734b90006608'}>,\n",
              "  <Document: {'content': \"Deploy Stakeholder Dashboard on-premises in K3s\\nenvironment\\nThis topic provides a checklist for preparation, deployment, and installation tasks related to the deployment of \\nStakeholder\\nDashboard\\n capability\\n.\\nPlan your deployment\\nBefore you begin, plan your deployment model and prepare the environment. Make sure that you have the deployment size, architecture,\\nand control plane (master) node allocated by referring to the sizing page.\\nThe tables in the following sections gives a list of tasks that you must complete in the given order. To view the detailed\\nprocedure for each task, click on the links in the \\nHow to perform\\n column.\\nPrepare\\nComplete the tasks in the given order to deploy the application:\\nInstall k3s\\nA system administrator performs the following tasks:\\nS/N\\nTask\\nWhere to perform\\nHow to perform\\n1\\nInstall K3s\\nOn server\\nInstall K3s\\nInstall and configure OMT\\nS/N\\nTask\\nWhere to perform\\nHow to perform\\n1\\nActivate your Docker Hub account\\nYou need a valid Docker Hub account to download Micro Focus\\nproduct images from Docker Hub.\\nYou can skip this section if you already have an active Docker Hub\\naccount that Micro Focus has authorized.\\nAny browser and email\\nclient that has internet\\nconnectivity\\nActivate your\\nDocker Hub account\\n2\\nDownload the required installation packages\\nDownload and extract the ITOM OPTIC Management Toolkit (OMT)\\npackage.\\nOn server\\nDownload the\\nrequired installation\\npackages\\n3\\nEmbedded PostgreSQL is supported for this deployment.\\nOptional step\\n:\\nExecute the following two steps only if you want to have external\\nPostgreSQL. Otherwise you can skip these steps and proceed to\\nthe next row.\\n1\\n. \\nCreate all required databases \\n(\\ncdfapi\\nserverdb,cdfidmdb,idm,\\nautopassdb,\\nand \\nbvd\\n) using PostgreSQL. \\n2\\n. \\nEnable TLS in PostgreSQL \\nOn server\\nPrepare external\\nPostgreSQL\\nEnable TLS in\\nPostgreSQL\\n4\\nInstall OMT\\nInstall the ITOM OPTIC Management Toolkit (OMT).\\nOn server\\nInstall OMT \\nInstall and configure OpsBridge application\\nS/N\\nTask\\nWhere to perform\\nHow to perform\\n1\\nDownload the installer\\nOn server\\nDownload the\\ninstaller\\n2\\nEdit environment variables\\nOn server\\nEdit environment\\nvariables\\n3\\nConfigure K3s environment\\nOn server\\nConfigure K3s\\nenvironment\\n4\\nLoad the required container images\\nExecute this step\\n, \\nonly if you're installing on a system that has \\nno\\ninternet access\\n or if you don't want to give kubernetes access to your\\ndocker account.\\nOn server and on a\\nsystem that has\\ninternet access\\nLoad Container\\nimages\\n5\\nConfigure bvd.yaml\\nUpdate all your deployment configuration values in the \\nbvd.yaml\\n file.\\nOn server\\nConfigure\\nbvd.yaml\\n6\\nUpdate secrets\\nConfigure the passwords or update the passwords in \\nbvd.yaml\\n file.\\nOn server\\nUpdate secrets\\nContainerized Operations Bridge 2022.11\\nPage \\n416\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8da21703c942d5ea3f9ae96349dd726a'}>,\n",
              "  <Document: {'content': 'The following operating systems are supported:\\n Operating System\\nArchitecture Type\\nVersions\\nRed Hat Enterprise Linux\\nx86_64\\n7.x where (x>=6) and 8.x where (x>=3)\\nCentOS\\nx86_64\\n7.x where (x>=6) and 8.x where (x>=3)\\nOracle Enterprise Linux\\nx86_64\\n7.x where (x>=6) and 8.x where (x>=3)\\nRed Hat Enterprise Linux, CentOS and Oracle Enterprise Linux supports SELinux.\\nAny version containing Kubernetes 1.20.x. K3S versions are supported.\\nContainerized Operations Bridge 2022.11\\nPage \\n415\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6695fe48a02e0f56659ba71637644fac'}>,\n",
              "  <Document: {'content': \"Download the installer\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nDownload and extract the contents of the OpsBridge application package\\nTo download and verify the Operations Bridge suite installation packages, perform the following steps:\\n1\\n. \\nDownload the following installation package from the \\nMicro Focus Software \\n website:\\nThe Operations Bridge application Composition Chart: \\nopsbridge-suite-chart-2022.05.0.zip\\n2\\n. \\nChoose a directory as your <\\nInstall temporary directory\\n>\\n \\nsuch as /tmp on server .\\n \\nThis directory must have at least\\n15 GB free space. Copy the packages and the public keys to this directory.\\n3\\n. \\nRun the following command to unzip the \\nopsbridge-suite-chart-2022.05.0.zip\\n file.\\n4\\n. \\nunzip opsbridge-suite-chart-2022.05.0.zip\\nThe unzipped file will have these directories and files under \\nopsbridge-suite-chart\\n: \\nDirectories/files\\nDescription\\ncharts\\nSuite install chart (\\nopsbridge-suite-2022.05.0.tgz\\n). Don't extract this.\\nsamples\\nvalues.yaml\\nVerify suite package\\nVerify the Operations Bridge suite installation package. Skip this step if you don't want to verify the package:\\n1\\n. \\nUsing a web browser, visit the \\nMicro Focus Software Licenses and Downloads\\n website. Agree to the terms and conditions,\\nand then download the \\nMF_public_keys.tar.gz\\n package to a local directory.\\n2\\n. \\nRun the following commands to extract the public keys from the package:\\ngunzip MF_public_keys.tar.gz\\ntar -xvf MF_public_keys.tar\\n3\\n. \\nCopy the public key \\nhelm-public-key.asc\\n to a local folder.\\n4\\n. \\nAdd the public key to the \\nGPG\\n keyring:\\ngpg --import helm-public-key.asc\\nOn machines that use \\nkbx\\n format, you may get an error as follows:\\ngpg: no valid OpenPGP data found.\\ngpg: Total number processed: 0\\nIf you get this error, export the key as \\npubring.gpg\\n:\\ngpg --export > ~/.gnupg/pubring.gpg\\n5\\n. \\nNavigate to the\\n $HOME/opsbridge-suite-chart/charts\\n directory:\\ncd $HOME/opsbridge-suite-chart/charts\\n6\\n. \\nRun the following command to verify the signature of the helm chart files:\\nhelm verify <chart-name>.tgz\\n7\\n. \\nYou will get a message similar to the following indicating that successful verification:\\nSigned by: Micro Focus Group Limited (GPG Key for Helm Chart Signing) xxxxx@microfocus.com\\n  Using Key With Fingerprint: xxxxx\\n  Chart Hash Verified: sha256:xxxxx\\nContainerized Operations Bridge 2022.11\\nPage \\n420\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9bc9c8c025b7ceb3448e35674486f546'}>,\n",
              "  <Document: {'content': 'Reference topics for BVD in K3s\\nContainerized Operations Bridge 2022.11\\nPage \\n418\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3048751fab663a41a46295aa64f70ed1'}>,\n",
              "  <Document: {'content': 'Enable TLS PostgreSQL\\nTo enable TLS for encrypted communication with PostgreSQL, you can use CA signed certificate or a self-signed certificate.\\nGenerate a CA signed certificate\\nTo enable TLS you need a CA signed certificate for PostgreSQL database. The Subject Alternative Name (SAN) of the server\\ncertificate must include the FQDN of all the PostgreSQL high availability nodes. Otherwise, use a wildcard in the Common\\nName (CN) that matches the FQDN of the PostgreSQL high availability nodes. \\nThe certificate files must be in .crt format (for example, server_signed.crt). The trust chain must include the root\\ncertificate and intermediate certificates (if any). \\nGenerate a self-signed certificate\\nThis section provides an example to enable TLS for encrypted communication with PostgreSQL database server on Linux using\\nself-signed certificate. \\nPerform the following steps as a Postgres user.\\nNote\\n: The paths shown in the examples are with respect to PostgreSQL 10, you need to change the paths according to your\\nPostgreSQL version.\\n1\\n. \\nCreate a self-signed certificate.\\n1\\n. \\nCreate the certificate and the key\\ncd /var/lib/pgsql/10/data/\\nopenssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout server.key -out server.crt -subj \"/CN=${HOST}\"\\nReplace the \\n${HOST}\\n with the hostname of the machine. For example: \\nmydatabase.myhost.com\\n2\\n. \\nSet permissions for the files\\nchmod 400 server.*;chown postgres:postgres server.*\\n2\\n. \\nChange \\npostgresql.conf\\ncd /var/lib/pgsql/10/data/\\nEdit \\npostgresql.conf\\n and search for\\n ssl\\n.\\nYou will see the following property commented out and with a value of off. Uncomment the property and set the value to\\non.\\n \\nUncomment the properties for \\nssl_cert_file\\n and \\nssl_key_file\\n.\\nNote: \\nEnsure the file names values match the private key and certificate created in the previous step.\\n3\\n. \\nChange \\npg_hba.conf\\ncd /var/lib/pgsql/10/data/\\nEdit \\npg_hba.conf\\n and add the following line at the end\\nhostssl all all 0.0.0.0/0 md5\\nEdit the following line and change the value of \\nident\\n to trust\\nhost    all             all             127.0.0.1/32            ident\\nIf you want the database user \\npostgres\\n to use SSL, edit \\npg_hba.conf\\n and specify \\npostgres\\n instead of \\nall\\n for the \\nUSER\\nvalue for \\nTYPE\\n \\nhostssl\\n.\\n4\\n. \\nAs root, restart PostgreSQL server\\nsystemctl restart postgresql-10.service\\nsystemctl status postgresql-10.service\\n5\\n. \\nEnsure to enable \\npsql -h `hostname` -U postgres\\n in SSL and you should see something like below:\\n6\\n. \\nOptional.\\n Create a user with \\ndba\\n privileges for access\\ncreate user cdfUser with superuser password \\'SomePassword123#\\';\\nContainerized Operations Bridge 2022.11\\nPage \\n423\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bb26874f9fa920cf4c868913a0435c46'}>,\n",
              "  <Document: {'content': 'Run the following queries:\\nCREATE USER <autopassuser> login PASSWORD \\'<autopass_user_password>\\';\\nGRANT <autopassuser> TO <postgres admin>; \\nCREATE DATABASE <autopassdb> OWNER <autopassuser>;\\n\\\\c <autopassdb>;\\nGRANT ALL PRIVILEGES ON DATABASE <autopassdb> TO <autopassuser>;\\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\\nALTER SCHEMA public OWNER TO <autopassuser>;\\nALTER SCHEMA public RENAME TO <autopassschema>;\\nREVOKE ALL ON SCHEMA <autopassschema> from public;\\nGRANT ALL ON SCHEMA <autopassschema> to <autopassuser>;\\nGRANT ALL PRIVILEGES ON DATABASE <autopassdb> TO <autopassuser>;\\nALTER USER <autopassuser> SET search_path TO <autopassschema>;\\nCreate a database for bvd\\nRun the following queries:\\nCREATE USER <bvduser> login PASSWORD \\'<bvd_user_password>\\';\\nGRANT <bvduser> TO <postgres admin>; \\nCREATE DATABASE <bvddb> OWNER <bvduser>;\\n\\\\c <bvddb>;\\nGRANT ALL PRIVILEGES ON DATABASE <bvddb> TO <bvduser>;\\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\\nALTER SCHEMA public OWNER TO <bvduser>;\\nALTER SCHEMA public RENAME TO <bvdschema>;\\nREVOKE ALL ON SCHEMA <bvdschema> from public;\\nGRANT ALL ON SCHEMA <bvdschema> to <bvduser>;\\nGRANT ALL PRIVILEGES ON DATABASE <bvddb> TO <bvduser>;\\nALTER USER <bvduser> SET search_path TO <bvdschema>;\\nChecklist\\nMake a note of the following information after creating all the databases (\\nidm, autopassdb, bvd\\n). You will need this\\ninformation during the installation.\\nDatabase host\\nDatabase port\\nDatabase names\\nDatabase login users/passwords\\nNote\\n: Suggesting you to enable TLS, for more information see, Enable TLS for PostgreSQL.\\nContainerized Operations Bridge 2022.11\\nPage \\n422\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ebf15fed722d3d9e0556fb1158f4526'}>,\n",
              "  <Document: {'content': 'Configure K3s environment\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nAfter installing OMT, you must create a new Kubernetes namespace and tag the node.\\n1\\n. \\nRun the following command to get a list of all nodes (there should be only one node) in your K3s environment:\\nkubectl get nodes\\n2\\n. \\nRun the following command to label the node for BVD. Replace \\n<nodename>\\n with the name of the node returned by the\\nprevious command.\\nkubectl label node <nodename> Worker=label\\n3\\n. \\nCreate a namespace for BVD (Stakeholder Dashboard capability) deployment. This documentation will use \"\\nbvd-helm\\n\" as\\nthe namespace name. You can choose any valid name for namespace except \"\\nkube-system\\n\" which is already in use.\\nkubectl create ns bvd-helm\\nContainerized Operations Bridge 2022.11\\nPage \\n426\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6644b94ff6b67eafe5dacab86bd4dce8'}>,\n",
              "  <Document: {'content': 'Install OMT\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nTo install ITOM OPTIC Management Toolkit (OMT), follow these steps:\\n1\\n. \\nLog on to the node as a root user or sudo user.\\n2\\n. \\nNavigate to OMT_External_K8s_20XX.xx\\n-xxx\\n sub directory inside the <\\nInstall temporary directory\\n>.\\n3\\n. \\nRun the following command to install the OMT tools:\\n./install --k8s-provider generic --capabilities Tools=true,Monitoring=false,LogCollection=false,DeploymentManagement=false,ClusterManagement=false --cdf-home /opt/cdf-home\\nIn the following command, the following parameters are mandatory:\\n--k8s-provider:\\n The Kubernetes provider. Generic in this case\\n--capabilities:\\n \\nA list of capabilities to install. For this use case, need to install only the tools. Following are the values for the\\nparameter \"\\ncapabilites\"\\nTools\\n=true\\nMonitoring\\n=false,\\nLogCollection\\n=false\\nDeploymentManagement\\n=false\\nClusterManagement\\n=false\\n--cdf-home\\n \\nThe path where the OMT tools should get installed. Example: \\n--cdf-home /opt/cdf-home\\nFor a full list of options for this command, run the following command:\\n ./install -h\\nIf you receive error messages when you run the installation command, refer to the installation log in the \\n$TMP_FOLDER\\ndirectory.\\nWhen you have fixed the errors, run the installation command again to continue the OMT installation. Note that you must run\\nthe installation command together with the same options and configurations.\\nContainerized Operations Bridge 2022.11\\nPage \\n424\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cede70a1c62b021b64e9ce3682ee2362'}>,\n",
              "  <Document: {'content': \"Generate secrets for Stakeholder Dashboard\\nAdministrator passwords are set during the deployment using a secrets file. \\nAll the password must be specified in a base64 encoded format. When using special characters, be sure that these aren't\\nreplaced by your shell. To test whether your password encodes correctly, you can use the following example:\\n# echo -n ChangeMe2$ | base64 | base64 -d ; echo\\nChangeMe2$\\n#\\nIf the password returned differs from the original, you might need to escape control characters.\\nFollow these steps to create a secrets file. Please keep the indentation and don't use tabs. Replace all occurrences\\nof <YOUR_BASE64_ENCODED_PASSWORD>.\\n# echo -n  My_secret_passw0rd$ | base64\\nTXlfc2VjcmV0X3Bhc3N3MHJkJA==\\n# vi secrets.yaml\\nsecrets:\\n  idm_opsbridge_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  bvd_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  IDM_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  AUTOPASS_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  BVD_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  idm_admin_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\nNote\\n: The passwords must have a minimum of 8 and a maximum of 32 characters in length. The passwords\\nshould include an upper, lower case character, a number, and a special character. If any of the passwords don't\\nmeet the password policy the installation will fail. \\nRecommendation\\n: Create a different password for each of the keys.\\nNote\\n: If you're using external postgres database for the deployment, you need to update the secrets.yaml file\\nwith the passwords\\n \\nthat are created for \\nidm, autopassdb, bvd \\nin the\\n \\nPrepare\\n \\nExternal Postgres\\n step\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n427\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fc151a1f776a04a18912bc0a0558c096'}>,\n",
              "  <Document: {'content': 'Edit environment variables\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nIn case you need to use a proxy to access the internet, set the HTTP proxy environment variables before running the\\ninstallation script.\\nFor example:\\nexport http_proxy=http://web-proxy.example.com:8080\\nexport https_proxy=https://web-proxy.example.com:8080\\nTo be able to use the CDF tools during the installation, you need to specify the following environment variables:\\nexport CDF_HOME=/opt/cdf-home\\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml\\nexport PATH=$PATH:$CDF_HOME/bin:$CDF_HOME/scripts\\nNote\\n : Add the environment variables mentioned in the earlier step to your \\n.bashrc\\n or similar file, to have them\\navailable at your next login.\\nContainerized Operations Bridge 2022.11\\nPage \\n425\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a03a616e4ed3d6c0e10970f06cc61369'}>,\n",
              "  <Document: {'content': \"Update Secrets for K3S\\nThis topic gives information on the parameters that you must configure to update secrets. Configure only the paramaters\\nrequired to deploy Stakeholder Dashboard in K3S.\\n\\u200bA secret is an object that has sensitive data such as a password, a token, or a key.  Based on the capability that you have\\nplanned to deploy, you must update the password for all the applicable secrets. The secrets are already existing, but you must\\nupdate the password for all the required secrets in base64 format in the \\nsamples/values.yaml\\n or \\nsamples/aws/values.yaml\\n or \\nsample\\ns/azure/values.yaml\\n or \\nsamples/openshift/values.yaml\\n or \\nsamples/generic/values.yaml \\nfile depending on the Kubernetes\\ndistribution\\n \\nthat you have selected.\\nThe following table lists all the available secrets and the capability that uses them:\\nPassword \\nDescription\\nITOMDI_DB\\nA_PASSWO\\nRD_KEY\\nPassword of the Vertica database user with read and write permission. The application will give the\\nusername later in \\nvalues.yaml\\n file under \\nglobal.vertica.rwuser.\\nOPTIC\\nReporting,\\nAutomatic\\nEvent\\nCorrelation\\nITOMDI_RO\\n_USER_PAS\\nSWORD_KE\\nY\\nPassword of the Vertica database user with read-only permission. The application will give the username later in \\nv\\nalues.yaml\\n file under \\nglobal.vertica.rouser\\nOPTIC\\nReporting,\\nAutomatic\\nEvent\\nCorrelation\\nidm_opsbri\\ndge_admin\\n_password\\nPassword for the Operations Bridge admin user. This creates a user called admin with this password for access to\\nOperations Bridge IdM UI. The password must be 8 to 32 characters and must contain at least one lowercase, one\\nuppercase, one digit, and one special character.\\nThis is different from the admin credential entered during CDF installation. \\nAll\\nIDM_DB_U\\nSER_PASS\\nWORD_KEY\\nPassword for IdM database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under \\nd\\neployment.database.user\\n of IdM.\\nAll\\nAUTOPASS\\n_DB_USER_\\nPASSWORD\\n_KEY\\nPassword for \\nAUTOPASS\\n database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\nunder \\ndeployment.database.user\\n of \\nAUTOPASS\\n.\\nAll\\nBVD_DB_U\\nSER_PASS\\nWORD_KEY\\nPassword for BVD database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under \\nd\\neployment.database.user\\n of BVD.\\nStakeholder\\nDashboard\\nOBM_MGM\\nT_DB_USER\\n_PASSWOR\\nD_KEY\\nPassword for \\nMGMT\\n database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under\\ndeployment.mgmtDatabase.user\\n of OBM.\\nOBM\\nOBM_EVEN\\nT_DB_USER\\n_PASSWOR\\nD_KEY\\nPassword for \\nEVENT\\n database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under\\ndeployment.eventDatabase.user \\nof OBM.\\nOBM\\nRTSM_DB_\\nUSER_PASS\\nWORD_KEY\\nPassword for RTSM database (PostgreSQL) user. The application will give the username in Helm \\nvalues.yaml\\n under \\ndeployment.database.user\\n of UCMDB.\\nOBM\\nMA_DB_US\\nER_PASSW\\nORD_KEY\\nPassword for monitoring admin database (PostgreSQL) user. The username is in \\nvalues.yaml\\n under \\ndeployment.datab\\nase.user\\n of monitoring admin.\\nHyperscale\\nObservability\\nCM_DB_PA\\nSSWD_KEY\\nPassword for PostgreSQL database for credentials manager.\\nHyperscale\\nObservability\\nOBM_RTSM\\n_PASSWOR\\nD\\nPassword for External OBM RTSM users. It's the password that you set for the 'Agent Metric Collector integration\\nuser' (See \\nCreate an Agent Metric Collector integration user\\n). If you didn't create this user, press enter. The\\nv generates a random password. The application will give the username later in \\nvalues.yaml \\nfile under \\nglobal.amc.rts\\nmUsername \\nNote:\\n You need not set this password if you are using containerized OBM (OBM capability).\\nOPTIC\\nReporting\\nsys_admin\\n_password\\nYou can use this Password in combination with the username sysadmin to log in to UCMDB’s JMX console.\\nOBM\\nucmdb_ma\\nster_key\\nSets the value for the master key of the UCMDB service that's used to encrypt all UCMDB keys.\\nOBM\\nUD_USER_\\nPASSWOR\\nD\\nPassword for External Universal Discovery user.The username will be provided in Helm values.yaml under\\nglobal.cms.udUsername\\nContainerized Operations Bridge 2022.11\\nPage \\n428\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff5d57a8460e221e59a593b88e3d6a85'}>,\n",
              "  <Document: {'content': \"OBM_USER\\n_PASSWOR\\nD_KEY\\nPassword for External OBMs user with Content Packs upload permissions. The username will be provided in Helm\\nvalues.yaml under global.monitoringService.obmUsername\\nSNF_DB_U\\nSER_PASS\\nWORD_KEY\\nPassword for temporary storage PostgreSQL database for Hyperscale Observability.\\nHyperscale\\nObservability\\nschedule_\\nmail_passw\\nord_key\\nPassword for scheduling the reports to an email for BVD.\\nOPTIC\\nReporting\\nBTCD_DB_\\nPASSWD_K\\nEY\\nPassword for NOM metric transformation and Hyperscale Observability.\\nHyperscale\\nObservability\\nOPTIC_DAT\\nALAKE_INT\\nEGRATION_\\nPASSWOR\\nD\\nIDM Integration user password of the providing deployment when using Shared OPTIC DL. \\nIf you plan to use Shared OPTIC DL, retrieve the password as mentioned below and pass the same in\\nthe secrets section of values.yaml.\\nTo retrieve the decoded password, execute the command:\\nkubectl -n <namespace of the providing application> get secret <providing_suite_secret> --template={{.data.idm_integration_admin_password}}\\nExample:\\nkubectl -n <nom-helm> get secret <nom-secret> --template={{.data.idm_integration_admin_password}}\\nShared\\nOPTIC\\nReporting\\nIf you want to install Stakeholder Dashboard only, then the Vertica, OBM RTSM, and OPTIC Data Lake Health Insights secrets\\naren't applicable. \\nIf you want to install Automatic Event Correlation without OPTIC Reporting, the OBM RTSM secret isn't applicable. For\\nAutomatic Event Correlation, the application configures the OBM credentials later.\\nUpdate the passwords in Base64 format\\nThe samples/values.yaml\\n or \\nsamples/<kubernetes distribution>/values.yaml\\n file will have all the required secret keys. You must update\\nrespective passwords in the base64 format.\\nExample:  \\n   \\nIf the password for \\nidm_opsbridge_admin_password\\n is Testing@123, follow the below example to update the password in base64\\nformat:          \\necho –n Testing@123 | base64 \\nCopy the output value which needs to be updated in the \\nvalues.yaml\\n  \\nfor the same key\\n idm_opsbridge_admin_password.\\nExample:\\nidm_opsbridge_admin_password: VGVzdGluZ0AxMjM=\\nNote:\\n If you change the password for\\n \\nidm_opsbridge_admin_password\\n, then you need to update the same password\\nfor \\nbvd_admin_password\\n \\nand \\nidm_admin_admin_password\\n keys also.\\nContainerized Operations Bridge 2022.11\\nPage \\n429\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c2dc5ad9d6892538b9c391f46e0c03af'}>,\n",
              "  <Document: {'content': 'Load Container images\\nNow download the required container images. OMT provides a utility for downloading just the required set of images. \\n# /root/cdf/tools/generate-download/generate_download_bundle.sh \\\\\\n     --chart /root/INSTALL/opsbridge-suite-chart/charts/opsbridge-suite-2.3.0+20220500.528.tgz \\\\\\n     -H /root/INSTALL/values_k3s.yaml -S -o hpeswitom -d /tmp/\\nThis will generate an \"offline download.zip\" bundle. Copy this to a Linux system, which has access to the internet (docker.hub).\\nIf the system you are installing on has internet access, then you can continue with the same system. Unzip the bundle in the\\n/tmp directory and download the required images:\\n# unzip offline_download.zip\\n# cd offline-download/\\n# chmod u+x downloadimages.sh\\n# ./downloadimages.sh -r registry.hub.docker.com -o hpeswitom -u <YOUR DOCKER ACCOUNT>\\nContacting Registry: https://registry.hub.docker.com ../[Failed] \\nRetrying contacting https://registry.hub.docker.com. please make sure your user name, password and network/proxy configuration are correct.\\nPassword:\\nContacting Registry: https://registry.hub.docker.com ..[OK]\\nStart downloading ...\\n! Warning: Please check suite sizing documentation and make sure you have enough disk space for downloading suite images.\\nContinue?[Y/N]?y\\nDownloading image [1/23] hpeswitom/itom-autopass-lms:2022.05-2022042615 ...[OK]\\nDownloading image [2/23] hpeswitom/itom-bvd:11.9.17 ...[OK]\\nDownloading image [3/23] hpeswitom/itom-cacert-bundler:0.0.1-1.commit-8db3723 ...[OK]\\nDownloading image [4/23] hpeswitom/itom-credential-manager:1.17.0.13 ...[OK]\\nDownloading image [5/23] hpeswitom/itom-data-migrator:0.8.0-5.commit-9c313e2 ...[OK]\\nDownloading image [6/23] hpeswitom/itom-idm:1.35.0-690 ...[OK]\\nDownloading image [7/23] hpeswitom/itom-k8s-sidecar:1.1.0-0020 ...[OK]\\nDownloading image [8/23] hpeswitom/itom-monitoring-admin:1.2.0-164 ...[OK]\\nDownloading image [9/23] hpeswitom/itom-monitoring-gateway:1.1.0-494 ...[OK]\\nDownloading image [10/23] hpeswitom/itom-monitoring-sis-adapter:1.2.0-104 ...[OK]\\nDownloading image [11/23] hpeswitom/itom-nginx-ingress:0.22.0-0038 ...[OK]\\nDownloading image [12/23] hpeswitom/itom-opsbridge-database-init:1.1.0-11 ...[OK]\\nDownloading image [13/23] hpeswitom/itom-opsbridge-dbvalidator:2.3.0-13 ...[OK]\\nDownloading image [14/23] hpeswitom/itom-pg-backup:1.1.101 ...[OK]\\nDownloading image [15/23] hpeswitom/itom-postgresql:12-00106 ...[OK]\\nDownloading image [16/23] hpeswitom/itom-redis:11.9.17 ...[OK]\\nDownloading image [17/23] hpeswitom/itom-reloader:1.0.0-0044 ...[OK]\\nDownloading image [18/23] hpeswitom/itom-static-files-provider:2.3.0-2151 ...[OK]\\nDownloading image [19/23] hpeswitom/itom-tools-base:1.1.0-0029 ...[OK]\\nDownloading image [20/23] hpeswitom/itom-web-to-pdf:11.9.17 ...[OK]\\nDownloading image [21/23] hpeswitom/kubernetes-vault-init:0.16.0-0043 ...[OK]\\nDownloading image [22/23] hpeswitom/kubernetes-vault-renew:0.16.0-0043 ...[OK]\\nDownloading image [23/23] hpeswitom/vault:0.20.0-0065 ...[OK]\\nDownload completed in 226 seconds.\\nDownload-process successfully completed.\\nSuccessfully downloaded the images  to /var/opt/cdf/offline/images_20220603091229/.\\nPlease refer to /var/opt/cdf/offline/images_20220603091229/downloadimages-20220603090714.log for more detail\\nIn case you did the download on a separate system, copy the images folder to the system you want to install Stakeholder\\nDashboard on.\\nThen import the images using (adjust the image directory to the location returned be the previous command):\\n# tar -C /var/opt/cdf/offline/images_20220603091229/ -c . |  ctr image import -\\nunpacking docker.io/hpeswitom/itom-autopass-lms:2022.05-2022042615 (sha256:d2fd7f72c5b62a0facf0afa2777c2828f382d667d9a01b9a54c0e40b44191889)...done\\nunpacking docker.io/hpeswitom/itom-bvd:11.9.17 (sha256:888aa38a1d89b330ad7de8d78677a32238d3f3ffde50e0c7cd290083a311fc49)...done\\nunpacking docker.io/hpeswitom/itom-cacert-bundler:0.0.1-1.commit-8db3723 (sha256:37d5f63d94dd59f7c13e0944466a7ae8531d6112975596219c62c3fea73ac4dc)...done\\n...\\n#\\nThe image import will take around 5 minutes. After successful import you can delete the downloaded images to free up some\\ndisk space.\\nNote\\n: Execute this step\\n, \\nonly if you\\'re installing on a system that has \\nno internet access\\n or if you don\\'t want\\nto give kubernetes access to your docker account.\\nContainerized Operations Bridge 2022.11\\nPage \\n430\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd59bb93d3e0bc95e93e8063256bca7d3'}>,\n",
              "  <Document: {'content': 'Configure BVD yaml file\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nDon\\'t change any indentation in the YAML file. Update the required values and keep the YAML syntax. Don\\'t change the\\nparameters which have explicit comment \\n[DON\\'T CHANGE]\\n in the values.yaml file.\\nNavigate to <\\nInstall temporary directory\\n> and run the following command to copy values.yaml file to \\nbvd.yaml\\n: \\ncp opsbridge-suite-chart/samples/values.yaml bvd.yaml\\nEdit the file \\nbvd.yaml\\n file and adjust the following values:\\nEnd User License Agreement\\n (EULA)\\nYou must accept the End User License Agreement (EULA) to deploy BVD. By default, the value of the \\nacceptEula\\n is false, set it\\nto true.\\nParameter\\ngroup\\nValues\\nDescription\\nacceptEula\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\n false\\nAccept \\nMicro Focus\\n \\nEULA\\n to proceed further.\\nYou can find the End User License Agreement (\\nEULA\\n) at \\nSoftware\\nLicense\\n.\\nServices\\nAll the Capabilities deployment is disabled by default, you must remove the \\'\\'\\n#\\n\" character to remove the comment for the\\nspecific line to deploy. Each tag represents an Operations Bridge suite capability. Set \\ntags.stakeholderDashboard \\nvalue to true to\\nenable the capability. Set all other capabilities value to false.\\nParameter group\\nValues\\nDescription\\ntags.stakeholderDashboard\\nPossible\\nvalues: \\ntrue/false\\nDefault value: \\ntrue\\nEnable/disable deployment of the Stakeholder Dashboard\\ncapability. \\nInstalls the Stakeholder Dashboard capability with the BVD\\ncomponent.\\nTo deploy, you must delete the \\'\\'\\n#\\n\" character for this\\nparameter.\\nExternal access host\\nThe installation fails without these mandatory parameter values. Each deployment has unique values.\\nParameter\\ngroup\\nValues\\nDescription\\nglobal.external\\nAccessHost\\nSet it to the FQDN of the machine where you\\'re installing BVD\\nThe hostname/FQDN that you\\ncan access externally. \\nglobal.external\\nAccessPort\\nChoose a port which is later used to access BVD. In a default installation,\\nK3s allows only ports between 30000 and 32767\\nThe port you can access\\nexternally.\\nDocker repository\\nConfigure these parameters only, if you didn\\'t download the container images in \\nLoad the required container images\\n step. \\nParameter\\ngroup\\nValues\\nDescription\\nglobal.dock\\ner.registry\\nlocalhost:5000\\nThe Docker registry URL.\\nglobal.dock\\ner.orgName\\nDefault value:\\nhpeswitom\\nDocker registry orgName.\\nContainerized Operations Bridge 2022.11\\nPage \\n431\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6b3cc2cefc42b49edc4937745ce3e841'}>,\n",
              "  <Document: {'content': \"Verify installation for Stakeholder Dashboard in K3s\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nRun the following command to see the pod status in a namespace:\\nkubectl get pods -n <suite namespace> \\nAfter all pods are up and running, you can access BVD with the following URL:\\nhttps://<externalAccessHost>:<externalAccessPort>/ui\\nYou can use the IDM UI to add users or authentication providers. It's available at this URL:\\nhttps://<externalAccessHost>:<externalAccessPort>/idm-admin\\nSee IDM documentation for details.\\nContainerized Operations Bridge 2022.11\\nPage \\n436\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2c053f6f710f195d1d044cacea757455'}>,\n",
              "  <Document: {'content': 'bvd.smtpServer\\n.user\\nGive the email id for the \\nsmtpServer\\nUser name for authentication with the SMTP server.\\nbvd.smtpServer\\n.passwordKey\\nThis is set while running gen_secrets.sh\\nscript and refers to the mail password\\nThe credentials for that SMTP server user account need to be\\nset as described in the suite documentation.\\nbvd.smtpServer\\n.from\\n \\nEmail address of the sender\\nUser email address from where the emails are initiated.\\nParameter\\ngroup\\nValues\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n434\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'da6b0a8fd7fc44aa37332ea6e75ed062'}>,\n",
              "  <Document: {'content': 'Deploy Stakeholder Dashboard\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nDeployment with external PostgreSQL database with TLS enabled\\nTo deploy BVD, run the following command:\\nhelm install <helm deployment name> -n <suite namespace> -f <values.yaml> <chart> --set-file \"caCertificates.postgres\\\\.crt\"=<relational database certificate file> -f <secrets.yaml>\\nWhere:\\n<helm deployment name>:\\n Deployment name you want to create. \\n<suite namespace>:\\n Namespace where BVD will get deployed. \\n<values.yaml>:\\n Give the full path to the \\nbvd.yaml\\n file. \\n<chart>:\\n The absolute path to the suite chart package. Example: \\nopsbridge-suite-2021.05.0.tgz.\\n Chart file is available under \\ncharts\\n directory mentioned in the topic \\nDownload the required installation packages.\\n<relational database certificate file>:\\n Give the full path to the PostgreSQL database TLS certificate file. For steps to enable TLS\\nfor PostgreSQL, see \\nEnable TLS in PostgreSQL\\nsecrets.yaml\\n: You have created the secrets using the \\ngenerate_secrets\\n script. This file contains all the secrets.\\nExample:\\nhelm install bvd -n bvd-helm -f bvd.yaml opsbridge-suite-chart/charts/opsbridge-suite-2021.05.0.tgz \\n--set-file \"caCertificates.postgres\\\\.crt\"=/dbStuffs/server.crt -f mysecretsfile.yaml\\nDeployment with internal PostgreSQL database and TLS not enabled\\nTo deploy BVD, run the following command:\\nhelm install <deployment name> -n <suite namespace> -f <values.yaml> <chart> -f <secrets.yaml>\\nWhere:\\n<helm deployment name>:\\n Deployment name you want to create. \\n<suite namespace>:\\n Namespace where BVD will get deployed. \\n<values.yaml>:\\n Give the full path to the \\nbvd.yaml\\n file. \\n<chart>:\\n The absolute path to the suite chart package. Example: \\nopsbridge-suite-2021.05.0.tgz.\\n Chart file is available under \\ncharts\\n directory mentioned in the topic \\nDownload the required installation packages.\\nsecrets.yaml\\n: You have created the secrets using the \\ngenerate_secrets\\n script. This file contains all the secrets.\\nExample:\\nhelm install bvd -n bvd-helm -f bvd.yaml opsbridge-suite-chart/charts/opsbridge-suite-2021.05.0.tgz -f mysecretsfile.yaml\\nUnsuccessful deployment\\nIf the deployment isn\\'t successful and displays any error message, identify the issue and resolve it. Before redeploying\\nStakeholder Dashboard, you need to uninstall and install it again.\\nRun the following command to uninstall the BVD deployment:\\nhelm uninstall <deployment name> -n <namespace> --no-hooks\\nExample:\\nhelm uninstall bvd -n bvd-helm --no-hooks\\nContainerized Operations Bridge 2022.11\\nPage \\n435\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7771015852a2bc7abbe4a610cfe472d4'}>,\n",
              "  <Document: {'content': 'global.dock\\ner.imagePul\\nlSecret\\nNot defined\\nName of the secret to login to the docker registry.\\nFor example:\\nCreate a secret \\nregistrypullsecret:\\nkubectl create secret docker-registry registrypullsecret -n <namespace> --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-password> --docker-email=<your-email>\\nwhere:\\n<your-registry-server>\\n is your Private Docker Registry FQDN. Use https://index.docker.io/v2/ for DockerHub.\\n<your-name>\\n is your Docker username.\\n<your-password>\\n is your Docker password.\\n<your-email>\\n is your Docker email.\\nYou have successfully set your Docker credentials in the cluster as a Secret called \\nregistrypullsecret\\nImagepullsecret\\n is a secret that holds the username/password of a docker registry (internal or external).\\nFor the local cluster registry, you can access images without a username/password. Hence you can leave this blank.\\nIf you have configured an external registry and want to use it directly (without doing a download/upload of images), you can specify the registry and give the\\nusername/password in this secret.\\nglobal.dock\\ner.imagePul\\nlPolicy\\nDefault value:\\nIfNotPresent\\nDocker image pull policy.\\nParameter\\ngroup\\nValues\\nDescription\\nVertica database details\\nPlease refer \\nConfigure values.yaml\\n file section under installation in suite documentation for Vertica database details.\\nRelational database details\\nEdit this section only in case of external relational databases. The default value for \\nglobal.database.internal\\n is false. If you want to\\nuse embedded PostgreSQL you would need to set \\nglobal.database.internal \\nto true, you don\\'t need to change any other\\nparameters in this section. \\nParameter group\\nValues\\nDescription\\nglobal.database.internal\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\nfalse\\nEdit this section only for external relational\\ndatabases. The default for \\nglobal.database.inte\\nrnal\\n is \"\\nfalse\\n\". If you want to use embedded\\nPostgreSQL you would need to set \\nglobal.data\\nbase.internal \\nto \"\\ntrue\\n\", you don\\'t need to\\nchange any other parameters in this\\nsection.\\nSet \\ndatabase.internal\\n to \\nfalse\\n to use an\\nexternal PostgreSQL.\\nSet \\ndatabase.internal\\n to \\ntrue\\n to use the\\nembedded PostgreSQL database. \\nglobal.database.host\\nNot defined\\nExternal PostgreSQL host name.\\nglobal.database.port\\nNot defined\\nDB Port. Required for PostgreSQL.\\nglobal.database.tlsEnabled\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\ntrue\\nIf TLS is enabled, the \\nPostgreSQLServer\\nCertificate\\n must be provided later while\\nrunning the install.\\nPersistent volume \\nIf you don\\'t find this parameter, you need to create and set the value. Set this parameter value to \\nReadWriteOnce\\n in the YAML\\nfile. \\nParameter group\\nValues\\nDescription\\nglobal.persistence.accessMode\\nReadWriteOnce\\nSet persistence access mode.\\nDatabase parameters for idm, autopass, and bvd\\nContainerized Operations Bridge 2022.11\\nPage \\n432\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6d37c30d5d0bc552b432b8fe4605fb5b'}>,\n",
              "  <Document: {'content': 'This section deals with specific parameters for IDM, Autopass, and BVD. For embedded PostgreSQL you need not change any\\nvalue for these parameters. Set value for these parameters only, if you\\'re using external PostgreSQL. See, \\nPrepare external\\npostgres\\n page for the required database and credentials.\\nParameter group\\nValues\\nDescription\\nidm.deployment.database.db\\nName\\nDefault value: \\nidm\\n \\nGive database name for \\nidm\\n.\\nidm.deployment.database.us\\ner\\nDefault value: \\nidm\\nGive the user name for \\nidm \\ndatabase.\\nidm.deployment.database.us\\nerPasswordKey\\nDefault value:\\nIDM_DB_USER_PASSWORD_KEY\\nRefers to the password for \\nidm \\ndatabase that is\\ncreated in \"Generate secrets\" step. \\nautopass.deployment.databa\\nse.dbName\\nDefault value: \\nautopass\\nGive database name for \\nautopass\\n.\\nautopass.deployment.databa\\nse.user\\nDefault value: \\nautopass\\nGive the user name for \\nautopass \\ndatabase.\\nautopass.deployment.databa\\nse.schema\\nDefault value: \\nautopassschema\\nSchema name should be what has been set in\\nduring the PostgreSQL prepare step for \\nautopassd\\nb\\n.\\nIf Databases are created using the script, then\\ndon\\'t change the value.\\nautopass.deployment.databa\\nse.userPasswordKey\\nDefault value:\\nAUTOPASS_DB_USER_PASSWORD_KEY\\nRefers to the password for \\nautopass\\ndatabase that is created in \"Generate secrets\"\\nstep.\\nbvd.deployment.database.db\\nName\\nDefault value: \\nbvd\\nGive \\nbvd \\ndatabase name.\\nbvd.deployment.database.us\\ner\\nDefault value: \\nbvd\\nGive the user name for \\nbvd\\n database.\\nbvd.deployment.database.us\\nerPasswordKey\\nDefault value:\\nBVD_DB_USER_PASSWORD_KEY\\nRefers to the password for \\nbvd\\n database that is\\ncreated in \"Generate secrets\" step.\\nSecrets\\nYou must provide all the required secrets password in \\nBase64 encoded \\nformat.\\nParameter group\\nValues\\nDescription\\nsecrets.idm_opsbridge_ad\\nmin_password\\nAdmin Password for IDM admin user. This password will be used to log into IDM UI.\\nThe password must meet the following requirements:\\nLength must be at least 8 characters\\nLength cannot exceed 64 characters\\nMust contain 1 or more upper case characters\\nMust contain 1 or more lower case characters\\nMust contain 1 or more digit (0-9) characters\\nMust contain 1 or more special characters in: -+\"?/.,<>:;[]{}`~!@#%^&*()_=|$\\nsecrets.AUTOPASS_DB_US\\nER_PASSWORD_KEY\\nPassword for \\nAUTOPASS\\n database (PostgreSQL) user. The application will give the\\nusername in Helm \\nvalues.yaml\\n under \\ndeployment.database.user\\n of \\nAUTOPASS\\n.\\nsecrets.BVD_DB_USER_PA\\nSSWORD_KEY\\nPassword for BVD database (PostgreSQL) user. The application will give the username\\nin Helm \\nvalues.yaml\\n under \\ndeployment.database.user\\n of BVD.\\nsecrets.CM_DB_PASSWD_K\\nEY\\nPassword for PostgreSQL database for credentials manager.\\nsecrets.sys_admin_passwo\\nrd\\nYou can use this Password in combination with the username sysadmin to log in to\\nUCMDB’s JMX console.\\nsecrets.IDM_DB_USER_PAS\\nSWORD_KEY\\nPassword for IdM database (PostgreSQL) user. The application will give the username in\\nHelm \\nvalues.yaml\\n under \\ndeployment.database.user\\n of IdM.\\nsecrets.schedule_mail_pas\\nsword_key\\nPassword for scheduling the reports to an email for BVD.\\nSMTP server configuration\\nYou need to configure mail server which is used by the Web-to-PDF service to send emails for scheduled jobs. \\nParameter\\ngroup\\nValues\\nDescription\\nbvd.smtpServer\\n.host\\nThe host name of the SMTP server\\nYou must give these parameters to schedule the reports to a\\nspecified email id. Give the \\nsmtpServer\\n host name.\\nbvd.smtpServer\\n.port\\nGive the \\nsmtpServer\\n port\\nPort of the SMTP server used for sending emails.\\nbvd.smtpServer\\n.security\\nGive the value as \\nTLS\\n or \\nSTARTTLS\\nThe security used by the Web to PDF service to talk to the\\nSMTP server. TLS and STARTTLS are supported.\\nContainerized Operations Bridge 2022.11\\nPage \\n433\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bb112a13ba81cbb169f69b658b65daa'}>,\n",
              "  <Document: {'content': 'Create a network load balancer\\nAs a prerequisite, you must make sure to allow an L3 Bi directional connection between the remote system and AWS setup to\\naccess Operations Bridge services. You can do this using a VPN connection or Direct Connect. If L3 communication isn\\'t\\nenabled between the remote system and AWS setup, you must create an environment in the same VPC to access Operations\\nBridge services.\\nOperations Bridge install uses an external access host for some services. You must make sure to complete the Network Load\\nBalancer configuration before you deploy Operations Bridge.\\nYou must log on to the AWS console and copy the VPC ID. You will use this ID to configure the network load balancer.\\nCreate an external network load balancer\\nYou must perform this step only once for a cluster. If the network load balancer is already created, then copy the \\nARN\\n of the\\nload balancer and use it for \\nNLB_ARN\\n parameter.\\nNLB_ARN=$(aws elbv2 create-load-balancer --name <NLB_NAME> --subnets <private_subnet> --scheme internal --type network --ip-address-type ipv4 --query \"LoadBalancers[].LoadBalancerArn\" --output text )\\nConfigure target groups and listeners for OMT\\nListener Port\\nTarget Port\\nProtocol\\nComponent\\n5443\\n5443\\nTLS\\nOMT AppHub\\nPerform the following steps to configure the target groups and listeners for the \\ncore\\n namespace. You can list all the load\\nbalancers in the namespace using the command: \\nkubectl get svc -n <namespace> | grep -i loadbalancer\\nYou must perform these steps for the \\nportal-ingress-controller-svc\\n load balancer:\\n1\\n. \\nRun the following command to create a Target group for Port: \\nTG_ARN_5443=$(aws elbv2 create-target-group --name <Target_Group_Name> --protocol TLS --port <Target_Port_Number> --target-type ip --vpc-id <VPC_ID> --query \"TargetGroups[].TargetGroupArn\" --output text)\\n where, the \\nTarget_Port_Number\\n for which you will create the Target group is available in the table. The \\nVPC_ID\\n is the ID you\\nnoted down from the AWS console.\\n2\\n. \\nRun the following command to get the load balancer with the service for which the target group is created: \\nloadBalancer_5443=$(kubectl get svc <SERVICE_NAME> -n <NAMESPACE> | grep -v EXTERNAL-IP | awk \\'{print $4}\\')\\nYou can refer to the table in this topic for port information. \\n3\\n. \\n Run the following command to associate target with the load balancer:\\nNetInt_5443=$(aws elbv2 describe-load-balancers --query \"LoadBalancers[?DNSName==\\\\`$loadBalancer_5443\\\\`].LoadBalancerName\" --output text)\\nPPIPs_5443=$(aws ec2 describe-network-interfaces --filters \"Name=vpc-id,Values=<VPC_ID>\" \"Name=description,Values=*${NetInt_5443}*\" --query \"NetworkInterfaces[].PrivateIpAddresses[?Primary==\\\\`true\\\\`].PrivateIpAddress\" --output text)\\n4\\n. \\nRun the following command to format the target:\\n  Targets_5443=\"\"\\n      for ip_5443 in $PPIPs_5443; do\\n      Targets_5443=$Targets_5443\" Id=$ip_5443\"\\n  done\\n5\\n. \\nRun the following command to register Target to the Target group created in step 1:\\naws elbv2 register-targets --target-group-arn $TG_ARN_5443 --targets $Targets_5443 \\n6\\n. \\nFrom the \\nvalues.yaml\\n, get the certificate ARN for the FQDN given as external Access host name.\\n7\\n. \\nRun the following command to get the Certificate ARN:\\nCert_ARN=$(aws acm list-certificates --query \"CertificateSummaryList[?DomainName==\\\\`<CERTIFICATE_DOMAIN_NAME>\\\\`].CertificateArn\" --output text)\\n You may also get the Certificate ARN from the \\nAWS Console\\n > \\nACM\\n.\\n8\\n. \\nRun the following command to create a listener for the network load balancer that you created in step 1:\\nTip\\n: If you have deployed your AWS environment using the ITOM Cloud Deployment Toolkit, skip the following\\nsections and run the command on the bastion node: \\n/home/ec2-user/bin/load-balancers.sh create core\\n<SUITE_PREFIX>\\n. You must use the same \\nSUITE_PREFIX\\n you used during OMT installation. You can ignore the\\n\"\\nservices \\n<service name>\\n not found\\n\" error that you may see while running the script\\nContainerized Operations Bridge 2022.11\\nPage \\n443\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bb74ea1d6ae7f67682f6e52004dc88b8'}>,\n",
              "  <Document: {'content': '5\\n. \\nRestart the \\npostgres\\n server.\\n \\nUpdate Certificate\\nEdit \\npostgresql.conf\\n and search for \\nssl\\n.\\nYou will see the following property commented out and with a value of off.  Uncomment the property and set the value to\\non.\\nUncomment the properties for \\nssl_cert_file\\n and \\nssl_key_file\\n.\\nView the generated \\nserver.crt\\nContainerized Operations Bridge 2022.11\\nPage \\n442\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9578eaf67c94785a46a01931803e4f76'}>,\n",
              "  <Document: {'content': 'Reference\\nContainerized Operations Bridge 2022.11\\nPage \\n437\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6a8fae2c4b6864e9241119dbf14fe7f5'}>,\n",
              "  <Document: {'content': \"openssl genrsa -out ca.key 4096\\nopenssl req -new -key ca.key -out ca.csr -config req.conf.ca -extensions 'v3_req_ca'\\nopenssl x509 -sha512 -signkey ca.key -in ca.csr -req -days 3650 -out ca.pem\\nopenssl genrsa -out server.key 2048\\nopenssl req -nodes -new -sha512 -key server.key -out server.csr -config req.conf.serv -extensions 'v3_req_serv'\\nopenssl x509 -req -in server.csr -days 3650 -sha512 -CAcreateserial -CA ca.pem -CAkey ca.key -out server.crt -extensions 'v3_req_serv' -extfile req.conf.serv\\nopenssl verify -verbose -CAfile ca.pem server.crt\\nopenssl x509 -in server.crt -text -noout\\nExample output: \\nCertificate:\\n    Data:\\n        Version: 3 (0x2)\\n        Serial Number:\\n            c4:eb:da:9a:58:61:c5:ae\\n    Signature Algorithm: sha512WithRSAEncryption\\n        Issuer: C=IN, ST=KA, L=Bangalore, O=MF, OU=ITOM, CN=Microfiocus\\n        Validity\\n            Not Before: Apr 18 07:13:47 2022 GMT\\n            Not After : Apr 15 07:13:47 2032 GMT\\n        Subject: C=IN, ST=KA, L=Bangalore, O=MF, OU=ITOM, CN=examplehost.net\\n        Subject Public Key Info:\\n            Public Key Algorithm: rsaEncryption\\n                Public-Key: (2048 bit)\\n                Modulus:\\n                    00:e2:92:6e:76:ba:70:5c:38:5e:f8:c3:0a:90:c5:\\n                    fa:eb:85:0b:e5:92:02:9c:2b:01:f0:6e:6f:be:09:\\n                    33:01:7b:d8:10:6e:7a:c0:4e:65:dd:f4:44:7a:ce:\\n                    76:c7:0d:20:b5:6b:a7:fb:ba:f2:b7:ba:af:c8:7b:\\n                    db:cd:23:9b:5c:4b:9a:39:19:16:b6:26:fc:2c:f6:\\n                    36:bd:11:b7:72:67:34:af:20:8b:c4:d1:2b:61:21:\\n                    f1:e8:27:f8:03:0d:0a:ce:13:ec:81:d5:1d:c2:a9:\\n                    28:24:59:98:61:9a:86:40:c2:fa:eb:9a:ef:c9:c9:\\n                    da:e2:8c:be:ef:12:37:ed:9d:e3:05:44:72:b6:e1:\\n                    b1:d3:c2:07:86:61:ec:04:7c:91:e3:64:8b:6a:72:\\n                    eb:2f:95:20:c2:ef:d9:c8:7b:6d:cb:da:80:8c:e3:\\n                    b4:57:a6:a0:69:66:e3:25:16:42:b6:f1:c0:0b:d3:\\n                    fe:a9:c2:bd:57:39:2b:3d:28:8f:50:c8:60:e2:e9:\\n                    5a:79:a1:65:cc:b5:4e:a9:03:93:5c:45:f4:da:a8:\\n                    84:b9:dd:10:5b:e9:bd:30:df:a7:6a:72:a1:d8:0a:\\n                    ab:bf:ab:0d:73:87:fc:82:d3:54:13:da:6f:7b:1b:\\n                    c2:63:b1:2e:52:69:08:ef:cf:d6:06:a5:ed:1b:f3:\\n                    82:35\\n                Exponent: 65537 (0x10001)\\n        X509v3 extensions:\\n            X509v3 Key Usage:\\n                Digital Signature, Key Encipherment, Data Encipherment\\n            X509v3 Extended Key Usage:\\n                TLS Web Server Authentication\\n            X509v3 Subject Alternative Name:\\n                DNS:sac-vms00063.swinfra.net\\n    Signature Algorithm: sha512WithRSAEncryption\\n         90:26:c9:9e:94:ae:e0:e0:24:57:22:97:7c:d6:1f:d9:1e:b5:\\n         88:d5:48:17:42:a1:1c:65:90:a7:34:87:f4:a1:30:71:4e:7e:\\n         af:81:13:f5:d2:41:31:58:c9:c7:a0:b5:6c:2d:6f:cf:d5:12:\\n         4d:ad:6a:80:1f:df:83:90:7e:a1:94:bb:c5:69:9c:a6:52:69:\\n         2b:54:a4:46:74:0a:8b:be:8c:83:72:b2:ff:b7:65:89:d9:f0:\\n         41:0a:cc:7c:13:a7:58:70:76:84:c9:b8:a7:cd:8b:4d:1b:f1:\\n         71:3c:cc:23:b5:32:a6:03:d1:9b:b3:a9:83:7f:86:5c:8c:3a:\\n         a2:74:21:14:ab:21:96:6b:3b:79:f8:41:d6:31:2c:ab:e1:01:\\n         f7:f1:d5:e5:0b:e3:d4:23:89:45:a6:af:04:4e:48:d5:8d:19:\\n         6b:36:93:ad:ef:24:f2:c0:d9:a5:16:6b:1f:9e:20:06:2c:f3:\\n         24:74:0c:05:6a:fd:d4:40:b8:76:c7:49:6d:34:f1:52:71:1e:\\n         18:42:34:c9:6f:73:f6:f3:79:a9:4d:8f:5f:c6:f6:34:8e:6f:\\n         8a:3d:3c:68:16:9f:ae:be:4b:04:5f:4f:ee:2d:25:bd:2e:d0:\\n         bb:75:d6:fe:48:6c:ac:a9:25:69:d5:93:28:68:bb:fb:4a:20:\\n         fa:38:25:92:5f:f2:73:56:44:b4:ed:c2:d9:7d:bc:8c:be:2d:\\n         fd:75:d3:9c:22:22:53:aa:53:da:c1:f9:81:8f:55:60:d2:99:\\n         36:23:c2:95:f9:e4:22:c2:af:05:42:99:d4:2f:e4:08:fe:e0:\\n         43:40:bb:56:94:e8:87:c1:0a:ae:7d:d3:f4:c4:5d:ef:52:7b:\\n         d7:ea:f0:fd:35:4f:4f:7f:cf:a0:de:95:f0:c7:e9:e2:66:b2:\\n         39:5a:f7:ee:ff:19:bf:1f:36:21:3f:e3:47:cb:b3:81:19:4b:\\n         b5:11:a8:c1:95:f1:86:ba:55:ae:ca:a2:77:32:04:fc:96:47:\\n         84:cc:8c:3a:ba:98:54:c8:7d:9c:89:ae:e8:84:5d:cc:6b:51:\\n         f5:4a:75:5a:dc:ec:12:0b:62:1a:17:9b:90:e3:54:8d:e0:77:\\n         73:52:73:c7:42:a5:79:46:ea:e6:92:37:33:25:0d:a0:4c:c5:\\n         84:4f:bb:4e:5a:5d:78:e0:f3:54:7b:b4:1a:d3:08:35:c2:29:\\n         98:d5:5f:47:4e:f6:79:53:dc:69:50:cb:01:04:a7:8d:c2:22:\\n         22:fc:f9:e5:b5:08:0c:6c:f9:29:3e:41:25:72:fc:ad:17:b7:\\n         13:9e:b0:32:85:ed:ee:15:c5:38:66:75:6c:13:8f:c0:ba:3e:\\n         3c:c5:ea:60:a8:1c:f9:dc\\n4\\n. \\nRun the following commands to change permissions for server certificate:\\nchmod 400 server.* ca.*\\nContainerized Operations Bridge 2022.11\\nPage \\n441\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e9279cefcb1dad00433ddb17de3d51da'}>,\n",
              "  <Document: {'content': 'If your organization runs a managed PKI, you can use any tool to create CSR. Since you are generating only a CSR and not the\\ncertificate. The \\nIssue certificate utility\\n can be used in the production environment for generating CSR.\\n./issuecert.sh csr <ca_name which reflects issuerDN> <server_name which reflects subjectDN> [<subject alternative names>]\\nCertificates issued via IssueCert (not for production)\\nUse the Issue certificate utility to generate a CA and Server certificate.  All the requirements are handled automatically\\nthrough the tool. See  \\nIssue certificate utility\\n for more information on usage.\\nTo start in \\nSSL\\n mode, files containing the server certificate and private key must exist. By default, these files are expected to\\nbe named \\nserver.crt\\n and \\nserver.key\\n, respectively, in the server\\'s data directory.\\n./issuecert.sh certificate <ca_name which reflects issuerDN> <server_name which reflects subjectDN> [<subject alternative names>]\\nSelf-signed (not for production)\\nA certificate is self-issued if the same DN (Distinguished Name) appears in the subject and issuer fields. It\\'s not recommended\\nto use self-signed certificates. However, for non-production environments, you can use tools like OpenSSL to issue self-signed\\ncertificates.\\nRelated topics\\nSee \\nHow to collect the certificate data and monitor the certificate expiration using \\nnode-export-cert\\n.  \\n \\nThe application doesn\\'t recommend using Key Exchange Algorithms that are based on encrypting the TLS Premaster\\nSecret Message (see \\nhttps://datatracker.ietf.org/doc/html/rfc5246#section-7.4.7.1\\n for more information).\\nSee https://datatracker.ietf.org/doc/html/rfc5280#section-4.2.1.12.\\nAfter \"Not After\" has passed, clients typically consider the certificate to be invalid. Connections are then no longer\\naccepted by the client and the certificate needs to be renewed.\\nSee \\nhttps://datatracker.ietf.org/doc/html/rfc5280#section-3.3\\n for more information about certificate revocation.\\nSee \\nhttps://datatracker.ietf.org/doc/html/rfc5280#section-4.2.1.6\\n for details on dNSName and subjectAlternativeName\\nextension.\\nSee \\nhttps://datatracker.ietf.org/doc/html/rfc6125#section-6.4.4\\n for more details on \"Checking of Common Names\".\\nImportant: \\nIf you want to execute the\\n issuecert.sh\\n on the database server, you must copy the entire \\nissuecert\\ndirectory onto the database server and then execute the script.\\nissuecert\\nissuecert.sh\\nREADME.md\\nutility\\nstep\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n439\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '15c43da9fc853fba377161340b4d5348'}>,\n",
              "  <Document: {'content': 'Certificate requirements for TLS connection\\nThis topic briefs you about the authentication modes supported by the application for TLS connection, types of certificates, and\\nprerequisite to configure server authentication certificates.  \\nSupported authentication modes\\nTLS supports multiple authentication modes from which application only supports:\\nServer authentication - The server must send an \\nX.509v3\\n certificate to the client.\\nClient and server authentication\\nFor setting up server authentication, you must configure the server to return a valid server certificate to the client.\\nSupported Certificates\\nCertificate Authority signed certificate\\nCertificate generated by \\nIssue certificate utility\\nPrerequisites for configuring server authentication\\nThe certificate must fulfill the following requirements to act as a server certificate which is commonly accepted by TLS clients:\\nThe certificate type must be \\nX.509v3\\n.\\nThe certificate\\'s public key must be of type \\nRSA\\n.\\nThe signature hash algorithm used while signing the server certificate must be one of \\nSHA-256,\\n \\nSHA-384,\\n \\nSHA-512\\n.\\nSee \\nhttps://datatracker.ietf.org/doc/html/rfc4055#section-5\\n for more details.\\nThe minimum length of the server certificate\\'s \\nRSA\\n key must be 2048 bits.\\nIf \\nkeyUsage\\n is present,\\nYou must set the\\n keyEncipherment bit\\n to any RSA Key Exchange Algorithm.\\nYou must set the \\ndigitalSignature\\n \\nbit\\n to \\nECDHE_RSA, DHE_RSA \\nbased Key Exchange Algorithms.\\nYou must set the \\nkeyAgreement bit\\n to any \\nDH_RSA\\n Key Exchange Algorithms.\\nExtended Key Usage: If the field is present it must contain \"\\nTLS WWW server authentication\\n\".\\nThe validity period must have started already, which means, the value of the \"\\nNot Before\\n\" field must represent a\\ntimestamp in the past.\\nThe validity period mustn\\'t have ended already, which means, the value of the \"\\nNot After\\n\" must represent a timestamp\\nin the future.\\nIf an intermediate CA has issued the server certificate, make sure that it includes the intermediate CA certificate chain,\\nstarting with the certificate of the CA that issued the resource\\'s server certificate, followed by other intermediate\\ncertificates (if any) and with the \\nrootCA\\n certificate at the end. \\nYou must follow the same sequence for a successful\\nTLS setup.\\nIn addition to the above requirements, the server certificate must contain a\\n dNSName\\n field in the\\nsubjectAlternativeName\\n extension of the server certificate that matches the hostname that the client uses to connect to\\nthe server. The \\ndNSName\\n field of the certificate may instead or in addition also contain one or more wildcard characters\\n(\\'*\\').  See \\nhttps://datatracker.ietf.org/doc/html/rfc6125#section-6.2.1\\n for more information about wildcard certificates and their\\nmatching rules.\\nSupported Certificates\\nYou can create certificates by using any one of the following options:\\nPKI (recommended for production)\\nTip:\\n If you are using a certificate issued by a well known Certificate Authority such as a Certificate Authority\\nwhich is a member of the CA/Browser Forum (For example, \\nLet\\n\\'\\ns encrypt, Verisign, DigiCert\\n), all mentioned\\nrequirements for a certificate are already fulfilled and you can skip reading the remaining section.\\n\\ue917\\n\\ue917\\nImportant\\n:\\n Although all the mentioned requirements are met, a server certificate can still be invalid in case\\nit\\'s revoked by the certificate issuer. Certificate revocation status information may be provided using the Online\\nCertificate Status Protocol (OCSP) or Certificate Revocation List (CRL).\\n\\ue91b\\n\\ue91b\\nImportant:\\n \\nThe usage of server certificates without a \\nsubjectAlternativeName\\n extension isn\\'t supported.\\nMatching the hostname relying on checking the \\nCommon Name\\n field isn\\'t supported.\\nThe value for the\\n Common Name\\n of a server certificate must be\\n unique\\n within the server certificates chain.\\nNone of the \\nCommon Name\\n fields within the server certificates chain must be empty.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n438\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3552a2e3d82bf2aa68ca387ae8745f08'}>,\n",
              "  <Document: {'content': 'Enable TLS in PostgreSQL\\nTo enable TLS for encrypted communication with PostgreSQL, you can use CA signed certificate or a self-signed certificate.\\nGenerate a CA signed certificate\\nTo enable TLS you need a CA signed certificate for PostgreSQL database. The Subject Alternative Name (SAN) of the server\\ncertificate must include the FQDN of all the PostgreSQL high availability nodes. Instead, you can use a wildcard in the Common\\nName (CN) that matches the FQDN of the PostgreSQL high availability nodes. \\nThe certificate files must be in .crt format (for example, server_signed.crt). The trust chain must include the root\\ncertificate and intermediate certificates (if any). \\nGenerate a self-signed certificate\\nThis section provides an example to enable TLS for encrypted communication with PostgreSQL database server on Linux using\\nself-signed certificate. \\nPerform the following steps as a \\npostgres\\n user to create the certificate and the key:\\n1\\n. \\ncd /var/lib/pgsql/10/data/\\n2\\n. \\nCreate the configuration file\\n \\nreq.conf.ca\\n and \\nreq.conf.serv. \\nReplace \\nall values in <> with appropriate values from your\\norganization:\\n#cat req.conf.ca \\ndistinguished_name = itomPostgreSQL\\nx509_req_extensions = v3_req_ca\\nprompt = no\\n[itomPostgreSQL]\\nC = <Country_name> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = <OU>\\nCN = <some string other than PostgreSQL FQDN> \\n[v3_req_ca]\\nsubjectKeyIdentifier=hash\\nauthorityKeyIdentifier=keyid:always,issuer\\nbasicConstraints = critical,CA:true\\n \\n$ cat req.conf.serv\\ndistinguished_name = itomPostgreSQL\\nx509_req_extensions = v3_req_serv\\nprompt = no\\n[itomPostgreSQL]\\nC = <Country_name> (2 letter code)\\nST = <State>\\nL = <Locality>\\nO = <Organization>\\nOU = ITOM  <OU>\\nCN = <PostgreSQL server FQDN> \\n[v3_req_serv]\\nkeyUsage = keyEncipherment, dataEncipherment, digitalSignature\\nextendedKeyUsage = serverAuth\\nsubjectAltName = @alt_names\\n[alt_names]\\nDNS.1 = <PostgreSQL FQDN>\\n3\\n. \\nRun the following commands to generate the\\n server.key\\n and \\nserver.crt\\n:\\n \\nNote:\\n You may skip this topic, if you are using enterprise certificates and if you are aware of configuring TLS\\non PostgreSQL server.\\nIf you are upgrading from OpsB 2021.11 to 2022.05 and your database server certificate is not having the \\nsubjectAl\\nternativeName(SAN)\\n extension, then you must recreate your database certificate before upgrading to 2022.05 and\\nthe same must be used during the upgrade.\\n\\ue916\\n\\ue916\\nImportant:\\n The paths shown in the examples are with respect to PostgreSQL 10, you need to change the\\npaths according to your PostgreSQL version.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n440\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '636eee2787091f37a938557165cdf37a'}>,\n",
              "  <Document: {'content': \"Set up the database\\nFollow these steps in a single session on the Vertica node where you have installed the RPM: \\n1\\n. \\nLog on to the Vertica node as the \\nroot\\n user.\\n2\\n. \\nGo to the location \\n/usr/local/itom-di-pulsarudx/conf\\n and ensure the \\ndbinit_conf.yaml\\n file is available in the location. You may edit\\nthe \\ndbinit_conf.yaml\\n file, change the values and make sure that the values are as required for your deployment. If the file\\nisn't available, you must run the script \\n./dbinit.sh genconfig <options>\\n. For more information, see \\nCreate the variables file\\n.\\n3\\n. \\nGo to the location \\n/usr/local/itom-di-pulsarudx/bin.\\n4\\n. \\nRun the following command to perform the user creation, grants, install, and create the resource pool:\\n./dbinit.sh <option>\\nYou can retain the same values as prompted or change the values and press \\nenter\\n. You must type the database\\nadministrator password when prompted. For the \\nreadWriteUser\\n and \\nreadOnlyUser,\\n \\ndbinit.sh\\n prompts for passwords unless\\nyou use the \\n--silent\\n option.\\nOptions usage\\n: \\ndbinit.sh [-h|-?|--help] [-c|--tlscrt] [-k|--tlskey] [-e|--tlsenforce] [-t|--tlsonly] [-s|--silent|--suppress] [-w|--dbapass [password]]\\n [-v|--verbose] [-n|--nochange]\\nOption\\nDescription\\n-h\\n, \\n-?\\n, \\n--h\\nelp\\nDisplay the script usage or help.\\n-s\\n, \\n--\\nsilent\\n, \\n--su\\nppress\\nOption to run the script without prompting for variable input. The passwords gets pulled from\\nenvironment variables: \\nVERTICA_DBA_PASS, VERTICA_RW_PASSWD, VERTICA_RO_PASSWD\\n.\\n-w\\n, \\n--dbap\\nass\\nWith this option, you may specify the database administrator password on the command line. \\nIf you use \\n-w\\n without a password and \\nVERTICA_DBA_PASS\\n is already given in the environment variable,\\nthen the \\ndbinit\\n script uses the environment variable. \\n-c\\n, \\n--tlscrt\\nYou must type the path of the file that contains the base64 encoded server certificate along with the\\ncertificate file name. If you don't give this option, the Vertica TLS remains as configured earlier.\\nYou must use this option with \\n--tlskey\\n, to enable TLS communication with Vertica.\\n-k\\n, \\n--\\ntlskey\\nYou must type the path of the file that contains the base64 encoded server key certificate along with\\nthe key file name. \\n-e\\n, \\n--tlsenf\\norce\\nOption to enforce TLS. The default is \\ntrue\\n. When you set \\ntlsenforce\\n to \\ntrue\\n, Vertica gets configured to\\nreject non TLS communication. When set to \\nfalse\\n, then Vertica will accept non TLS communication. \\nYou can use this option without the \\n--tlscrt\\n and \\n--tlskey\\n options to disable or enable TLS enforcement.\\n-t\\n, \\n--tlsonl\\ny\\nOption to apply only the TLS changes to Vertica. You must use \\n--tlscrt\\n and \\n--tlskey \\noptions or \\n--tlsenforce\\noption with this option.\\nThe script performs only the TLS configurations if you use the \\ntlsonly\\n option and the other settings in\\nVertica remain the same. \\n-v,--verbos\\ne\\nOption to view the detailed output on the console that includes all the SQL queries invoked.\\n-n,--nochan\\nge\\nOption to view the SQL queries that are invoked when the script runs. This option doesn't change\\nVertica. It displays only the queries. \\nFor example:\\nTo configure the server key and certificate for TLS communication along with other configurations:\\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce true\\nNote that the \\n--tlsonly \\noption isn't required in the command while you perform other configurations using the script.\\nTo configure the server key and certificate for TLS communication and to disable non TLS communication with Vertica: \\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce true --tlsonly\\nTo configure the server key and certificate for TLS communication and to allow non TLS communication with Vertica: \\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce false --tlsonly\\nIf you have a Vertica system already configured for TLS communication, to disable non TLS connections to Vertica: \\n./dbinit.sh --tlsenforce true --tlsonly\\n5\\n. \\nTo apply the TLS settings, make sure to restart the Vertica database.\\nView the configuration file history\\nThe database configuration tool stores its history in the database. You can use the \\ndbinit history\\n command to retrieve the\\nhistory or save the content of the last \\ndbinit_conf.yaml\\n file used.\\n./dbinit.sh history <options>\\nNote\\n: It isn't recommended to set the  \\ntlsenforce\\n to \\nfalse\\n.\\nImportant\\n: If you want to use a new CA certificate, you must regenerate the certificate and key files and\\nupdate the configuration in Vertica. To update the configuration, run the \\ndbinit\\n script again with the new\\ncertificate and key file using the \\n--tlscert\\n and \\n--tlskey\\n options.\\nContainerized Operations Bridge 2022.11\\nPage \\n445\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e773131cac10e6b096c939f26291d009'}>,\n",
              "  <Document: {'content': 'aws elbv2 create-listener --load-balancer-arn $NLB_ARN --protocol TLS --port <Listener_port_number> --ssl-policy ELBSecurityPolicy-2016-08 --certificates CertificateArn=${Cert_ARN} --default-actions Type=forward,TargetGroupArn=${TG_ARN_5443}\\nUpdate record\\nYou must create a DNS record in the hosted zone to access Operations Bridge UI. Perform the following steps to configure and\\nupdate the DNS record:\\n1\\n. \\nTo create a DNS record, go to \\nAWS Console\\n > \\nRoute 53\\n > \\nHosted Zone\\n and click on your hosted zone. For\\ninformation to create a private hosted zone, see \\nAmazon documentation\\n.\\n2\\n. \\nClick \\nCreate Record\\n. You must make sure to give the same DNS record name for the \\nexternalAccessHost\\n parameter in \\nval\\nues.yaml\\n and \\nconfig.json\\n.\\nFor example: If the \\nHosted zone\\n is \\nopsb.itombyok.com\\n and \\nEXTERNAL_ACCESS_HOST\\n is \\ndemo.opsb.itombyok.com\\n, then\\nthe \\nRecord name\\n while creating DNS record will be \\ndemo.opsb\\n.\\n3\\n. \\nSelect the \\nRecord type\\n as \\nA\\n and in \\nValue\\n enable the toggle to \\nAlias\\n. In \\nChoose Endpoint\\n drop-down, select \\nAlias\\nto Network Load Balancer\\n. In \\nChoose Region\\n drop-down, select your deployment region. In \\nChoose network load\\nbalancer\\n, search or type the NLB created during the load configuration. Click \\nCreate records\\n.\\n4\\n. \\nIf you plan to use the OPTIC Reporting or AEC capabilities, you must make sure to include the \\nglobal.di.cloud.externalDNS.ena\\nbled\\n parameter with value \\nFalse\\n in the \\nvalues.yaml\\n file.\\nContainerized Operations Bridge 2022.11\\nPage \\n444\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '927bbd052e0f69d73dbb0da349883356'}>,\n",
              "  <Document: {'content': 'Options usage\\n: \\ndbinit.sh history [-h|-?|--help] [-c|--config] [--p <path to prefix>|--prefix=<path to prefix>] [-s|--sql] [-w <password>] [-n <numb\\ner>|--number <number>] [-z|--zip]\\nOption\\nDescription\\n-h\\n, \\n-?\\n, \\n--help\\nDisplay the script usage or help.\\n-c\\n, \\n--config\\nDisplay the last configuration used on the screen.\\nUsage:\\n \\ndbinit.sh history --config > /tmp/my_conf.yaml\\n-p <path to prefix>\\n,\\n--prefix=<path to\\nprefix>\\nOption to specify the path to save the history. The history file(s) will use that prefix\\nwith\\n _<n>.yaml\\n appended to it. \\nUsage:\\n \\ndbinit.sh history -p /tmp/my_history\\ndbinit.sh history --prefix=/tmp/my_history\\n-s\\n, \\n--sql\\nOption to include the SQL commands in the dump. The \\ndbinit\\n script saves all the SQL commands\\nused to modify the database. If you want to include these SQL commands to the history dumps, you\\ncan use this option.\\nUsage:\\n \\ndbinit.sh history -s\\ndbinit.sh history --sql\\n-w <password>\\nOption to specify the Vertica administrator password. Or, you can set \\nVERTICA_DBA_PASS\\n in the\\nenvironment, while you run the \\ndbinit.sh\\n script. \\nUsage:\\n \\nVERTICA_DBA_PASS=<password> dbinit.sh history -w\\n-n <number>\\n, \\n--nu\\nmber <number>\\nOption to specify how many history items you want to save. By default, the last script run gets\\nupdated in the history. You can use this option to include more history.\\nUsage:\\n to save the history for the last three runs of \\ndbinit.sh\\n: \\ndbinit.sh history -n 3\\nto save the entire history: \\ndbinit.sh history -n 0\\n-z\\n, \\n--zip\\nOption to zip all the history files into one file. The \\ndbinit.sh\\n script saves history items into separate\\nfiles. This option will zip all the files into a file \\n<prefix>.zip\\n.\\nUsage:\\n \\ndbinit.sh history -n 2 -p /tmp/my_history -z\\nThis will save the last two histories into \\n/tmp/my_history_1.zip\\n and \\n/tmp/my_history_2.zip\\n. These files\\ngets zipped into \\n/tmp/my_history.zip\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n446\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a4d41097850c05c4f44820530948e804'}>,\n",
              "  <Document: {'content': \"Issue certificate utility\\nThis topic lists the prerequisites and the steps required for generating CA certificates using a wrapper script. This script uses\\nan open source CLI tool and requires minimal input from the user. See \\nBasic Crypto Operations\\n for more information.\\nIssue certificate utility (\\nissuecert\\n) script for generating server certificates which are signed by a CA for all resources requiring a\\ncertificate generation by the user. For example, database certificates (Oracle, PostgreSQL, Vertica), ingress certificates, and\\nKubernetes cluster certificates.\\nThis utility generates server certificates which are signed by a CA.  This CA is self-issued when you run issuecert command. If\\nyou trust this CA, all the certificates created by issuecert with respect to this CA will get trusted. \\nThe utility can generate a reusable CA (to issue multiple certificates), and a \\nCSR\\n to get it signed by a well known Certificate\\nAuthority such as a Certificate Authority which is a member of the CA/Browser Forum.\\nThe application zip (\\n<suite-chart>-20xx.xx.0.zip\\n)\\n contains the \\nissuecert.sh\\n \\nfile under \\n<directory where you unzipped the <suite-chart>-\\n<version>.zip>scripts/issuecert/\\n directory\\n.\\n \\nPrerequisites \\nCertificate name \\nFQDN of the server\\nUsage\\nRun the script with  \\n--help\\n option to view the commands:\\ncd <directory where you unzipped the <suite-chart>-<version>.zip>/scripts/issuecert/\\n$ ./issuecert.sh --help\\nUsage:\\n------\\n  ./issuecert.sh <command> <certificate name> [<subject alternative names>]\\n  Commands:\\n  ---------\\n  certificate     - To create a Root and Server Certificate\\n  csr             - To create a Certificate Signing Request\\nIssue certificates\\n$ ./issuecert.sh certificate <certName> <host1.com> <host2.com>\\nCreating Root and Sever Certificates\\nYour certificate has been saved in <.certName_ca.crt>\\nYour certificate has been saved in <certName_server.crt>\\nYour private key has been saved in <certName_server.key>\\nThe command \\ncertificate\\n will create the CA and the server certificates.\\nThe CA key gets deleted after it's used to sign the leaf/end-entity certificate.\\nImportant: \\nThis script doesn't generate intermediate certificates, it creates only the root and \\nserver(leaf)\\n certificates.\\nYou can use this tool for a non-production setup. However, in a production environment, you can use this tool\\nat your own risk. By using this tool, you understand and agree to assume all associated risks and hold\\n \\nMicro\\nFocus harmless for the same. It remains always your sole responsibility to assess your own regulatory and\\nbusiness requirements. Micro Focus doesn't represent or warrant that its products comply with any specific\\nlegal or regulatory standards applicable in conducting your business.\\nImportant: \\nIf you want to execute the\\n issuecert.sh\\n on the database server, you must copy the entire \\nissuecert\\ndirectory onto the database server and then execute the script.\\nissuecert\\nissuecert.sh\\nREADME.md\\nutility\\nstep\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n447\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd8b105fed14c5d4c16d7e8543a3ae120'}>,\n",
              "  <Document: {'content': \"Usage:\\n./issuecert.sh certificate <ca_name which reflects issuerDN> <server_name which reflects subjectDN> [<subject alternative names>]\\nExample for Postgres:\\n./issuecert.sh certificate postgres mypostgres.host1.net\\nExample for Oracle:\\n./issuecert.sh certificate oracle myoracle.host1.net\\nExample for Vertica:\\n./issuecert.sh certificate vertica myvertica.clusterhost1.net myvertica.clusterhost2.net myvertica.clusterhost3.net\\nIssue certificate signing request\\nThe Issue certificate utility can be used in the production environment for generating CSR.\\nThe command \\ncsr\\n will create the \\ncsr\\n and a private key, which is ready to get signed by a well known Certificate Authority such\\nas a Certificate Authority which is a member of the CA/Browser Forum.\\n$ ./issuecert.sh csr <csrName>\\nCreating Certificate Signing Request\\nYour certificate signing request has been saved in <csrName.csr>.\\nYour private key has been saved in <csrName.key>.\\nYou can't use the certificate signing request multiple times, as in each time for a separate Vertica node.\\n\\u200bExample: \\n./issuecert.sh csr <ca_name which reflects issuerDN> <server_name which reflects subjectDN> [<subject alternative names>]\\nExample for Postgres:\\n./issuecert.sh csr postgres mypostgres.host1.net\\nExample for Oracle:\\n./issuecert.sh csr oracle myoracle.host1.net\\nExample for Vertica:\\n./issuecert.sh csr vertica myvertica.clusterhost1.net myvertica.clusterhost2.net myvertica.clusterhost3.net\\nTip: \\nYou must copy the <certName_server.crt> and  <certName_server.key> to the database\\ndirectory. \\nPostgreSQL: \\nYou can copy to \\n/var/lib/pgsql/<version>/data\\n.\\nOracle:\\n You can copy to \\n/home/oracle/.\\nVertica: \\nYou can copy to \\n/home/dbadmin.\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n448\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '13a3f548dc193897f9debb20ab8b0ca1'}>,\n",
              "  <Document: {'content': \"Update the load balancer configuration after application\\ninstall\\nAfter application installation is completed with OpenShift, your Load Balancer needs to be updated to direct traffic to each of\\nthe workers for web browser access, inbound data sources such as classic OBM, Metric Streaming Agents, etc. and external\\nsystems such as Vertica.\\nCreate a listener for the external access port that's passed during application install, for the service\\n itom-ingress-controller-sv\\nc\\n.\\nConfigure \\npulsar proxy, des node port, data broker svc, di receiver, di admin \\nand \\ndata access\\n which correspond to \\n*:31051, *:30010, *:\\n31383 \\n(Use the port passed in values file under \\nglobal.amc.dataBrokerNodePort\\n), \\n*:30001, *:30004, *:30003\\n respectively. \\nIf you have enabled OBM, update your load balancer with the omi-bbc service NodePort 30383. \\n To find the configured ingress port run the following command\\n:\\nhelm get values $(helm list -A | grep opsbridge-suite | awk '{print $1}') -n $(helm list -A | grep opsbridge-suite | awk '{print $2}') | grep externalAccessPort\\n   externalAccessPort: 30443\\nUse the table below for reference when configuring the Load Balancer for web browser access:\\nDescription\\nSource Port\\nDestination\\nPort\\nProtocol\\nUpstream\\nselection\\nalgorithm\\n(persistency)\\nService\\nhealth\\ncheck\\nLoad\\nbalancing\\ntype/load\\nbalancing\\nlayer\\nDestination\\nComment\\nHTTPS end user traffic\\n(typically through\\nbrowser access)\\nexternal access port\\n/itom-ingress-controller\\n30443 or as\\nconfigured\\nin \\nglobal.exte\\nrnalAccessPor\\nt\\n30443 or as\\nconfigured\\nin \\nglobal.exte\\nrnalAccessPor\\nt\\nTCP/HTTPS\\nLeast\\nconnection\\nor client IP\\nhash\\nTCP\\nhealth\\ncheck\\non the\\nsame\\nport\\nL4 or L7\\nAll worker\\nnodes in\\nthe\\nOpenShift\\ncluster.\\nPort is\\nconfigurable\\nat helm\\nparameter \\nglobal.extern\\nalAccessPort\\nYou will need to edit your load balancer configuration for other inbound data sources. Use the table below for reference:\\nDescription\\nSource\\nPort\\nDestination\\nPort\\nProtocol\\nUpstream\\nselection\\nalgorithm\\n(persistency)\\nService\\nhealth\\ncheck\\nLoad\\nbalancing\\ntype/load\\nbalancing\\nlayer\\nDestination\\nComment\\nOPTIC DL pulsar\\nproxy\\n31051\\n31051\\nTCP\\nLeast\\nconnection or\\nclient IP hash\\nTCP health\\ncheck on\\nthe same\\nport\\nL4\\nAll worker\\nnodes in the\\nOpenShift\\ncluster\\nOPTIC DL data\\naccess service\\n30003\\n30003\\nTCP\\nLeast\\nconnection or\\nclient IP hash\\nTCP health\\ncheck on\\nthe same\\nport\\nL4\\nAll worker\\nnodes in the\\nOpenShift\\ncluster\\ndes node port\\n30010\\n30010\\nTCP\\nLeast\\nconnection or\\nclient IP hash\\nTCP health\\ncheck on\\nthe same\\nport\\nL4\\nAll worker\\nnodes in the\\nOpenShift\\ncluster\\ndata broker\\ncomponent of the\\nagent metric\\ncollector\\n31383\\n31383\\nTCP\\nLeast\\nconnection or\\nclient IP hash\\nTCP health\\ncheck on\\nthe same\\nport\\nL4\\nAll worker\\nnodes in the\\nOpenShift\\ncluster\\nglobal.amc\\n.dataBroke\\nrNodePort\\nOPTIC DL admin\\nservice\\n30004\\n30004\\nTCP\\nLeast\\nconnection or\\nclient IP hash\\nTCP health\\ncheck on\\nthe same\\nport\\nL4\\nAll worker\\nnodes in the\\nOpenShift\\ncluster\\nContainerized Operations Bridge 2022.11\\nPage \\n450\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56bfd2c31be26cf965fa3afca98ba241'}>,\n",
              "  <Document: {'content': \"Parameter tuning\\nThe table below describes the recommended values for a number of PostgreSQL database initialization parameters. However,\\ndepending on how large your OBM deployment is, you might need to increase some values. For example, when using several\\ngateway servers, you might also need to increase the maximum number of client connections allowed. \\nFor the steps to configure these parameters on cloud setup, see:\\nAWS \\n- \\nConfigure AWS RDS Postgres parameters\\nAzure \\n- \\nConfigure Azure Postgres parameters\\nParameter name\\nEvaluation\\nMedium\\nLarge\\nshared_buffers\\n512 MB\\n1024 MB\\n4096 MB\\nwork_mem\\n25 MB\\n50 MB\\n50 MB\\nmaintenance_work_mem\\n128 MB\\n256 MB\\n340 MB\\neffective_cache_size\\n1024 MB\\n4096 MB\\n8192 MB\\nmax_wal_size\\n1536 MB\\n2048 MB\\n3072 MB\\nmax_connections\\n475\\n475\\n475\\ncheckpoint_timeout\\n10min\\n or '\\n10 min\\n'\\n20min\\n or '\\n20 min\\n'\\n30min \\nor '\\n30 min\\n'\\nIf there is a space between the value and the unit, you should include them in single\\nquotes.\\ncheckpoint_completion_target\\n0.9\\n0.9\\n0.9\\nautovacuum_vacuum_threshold\\n5000\\n5000\\n5000\\nautovacuum_analyze_threshold\\n5000\\n5000\\n5000\\nautovacuum_analyze_scale_facto\\nr\\n0.1\\n0.2\\n0.2\\nlog_min_duration_statement\\n3000\\n3000\\n3000\\ncommit_delay\\n500\\n500\\n500\\nmax_locks_per_transaction\\n512\\n512\\n512\\nContainerized Operations Bridge 2022.11\\nPage \\n449\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c711250c9da6ced104feec306ae603fe'}>,\n",
              "  <Document: {'content': 'OPTIC DL data\\nreceiver\\n30001\\n30001\\nTCP\\nLeast\\nconnection or\\nclient IP hash\\nTCP health\\ncheck on\\nthe same\\nport\\nL4\\nAll worker\\nnodes in the\\nOpenShift\\ncluster\\nOMI-BBC Port\\n30383\\n30383\\nTCP\\nLeast\\nconnection or\\nclient IP hash\\nTCP health\\ncheck on\\nthe same\\nport\\nL4\\nobm.bbc.port\\nDescription\\nSource\\nPort\\nDestination\\nPort\\nProtocol\\nUpstream\\nselection\\nalgorithm\\n(persistency)\\nService\\nhealth\\ncheck\\nLoad\\nbalancing\\ntype/load\\nbalancing\\nlayer\\nDestination\\nComment\\nApply similar rules for all the items in the table above. After you have made updates to your load balancer configuration,\\nrestart the balancer services if required. You can proceed with the additional required p post install steps.\\nContainerized Operations Bridge 2022.11\\nPage \\n451\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5efa3eaeccf999bc4559be8c9e4b979b'}>,\n",
              "  <Document: {'content': \"If you are deploying any capability that requires the OPTIC Data Lake component, for example OPTIC Reporting, you will need\\nto use block storage and shared filesystem storage, for different volumes. Block storage is used by Message Bus and shared\\nfile system storage is used for persisting logs. If you don't have block volume provisioner in your Kubernetes cluster, then you\\ncan use local volumes. For example, a block volume provisioner isn't available with embedded Kubernetes. Therefore, you\\nmust add local disks to each of the worker nodes and then use the local volume provisioner chart to provision local volumes.\\nThe local volume provisioner creates local volumes by detecting the disks mounted in /mnt/disks folder on the Kubernetes\\nworker nodes.\\nStorage requirements for Message Bus\\nOne of the important feature of OPTIC Data Lake is the ability to ingest data at high scale. The Message Bus component within\\nOPTIC Data Lake plays a key role in achieving high data ingestion rate. Message Bus depends heavily on the filesystem for\\nstoring ingested messages. Hence, the storage configured for Message Bus determines the throughput of the ingestion to a\\nlarge extent. The recommended storage to provision volumes for Message Bus is block storage.\\nMessages ingested to OPTIC Data Lake are synced to storage before sending response to clients. To ensure low latency to\\nclients, multiple volumes are used by OPTIC Data Lake Message Bus. There are three volumes that are used by OPTIC Data\\nLake:\\nJournal: \\nJournal volume is used to achieve durability. Sequential writes are performed on journal volume when a\\nmessage is received by Message Bus.\\nLedger:\\n Data is kept in ledger volumes till applications consume the messages. Writes to ledger volume happens in the\\nbackground and data is indexed and stored in this volume. Fast random I/O operations happen on ledger volumes.\\nZookeeper:\\n Zookeeper stores metadata information. The requests sent to Zookeeper are persisted to volume attached\\nto every instance of Zookeeper. Low latency disk writes are essential for good performance of Zookeeper. Configure\\nstorage to provision volumes that meet these needs.\\nIn some deployments you may not have block storage provisioned. In such deployments it's recommended to use local\\nstorage. If there are difficulties in provisioning disks for local storage you can use NAS/NFS volumes. However, there are some\\nlimitations that you must consider before using the NAS/NFS volumes. NAS usually displays wide variation in latencies which\\nmakes it difficult to provide a consistent performance on production deployments. Also, most of the NFS file systems aren't\\nPortable Operating System Interface for UNIX (POSIX) compliant. Using non-POSIX compliant filesystems may result in some\\ndata loss. \\nAll other OPTIC Data Lake services require volumes for logging. It's recommended to use shared file system storage so that\\nlogs are available in a single storage for all the components.\\nMapping of all the capabilities and storage selectors\\nThe following table provides a mapping of capabilities and storage selectors for all the capabilities available in Operations\\nBridge, if you are deploying on an Embedded Kubernetes provided by Optic Management Toolkit (OMT). If you want to use the\\nstorage class applicable in your environment, for example if you have installed the NFS provisioner provided by OMT, set the\\nstorage class to \\ncdf-nfs\\n.\\nPVC\\nStorage class selector helm\\nparameter\\nDefault\\nvalue\\nAccess\\nmode\\nCapabilities\\nconfigvolumeclaim\\nglobal.persistence.storageClasse\\ns.default-rwx\\nNo\\nvalue\\nReadWr\\niteMany\\nAll capabilities\\ndatavolumeclaim\\nglobal.persistence.storageClasse\\ns.default-rwx\\nNo\\nvalue\\nReadWr\\niteMany\\nAll capabilities\\ndbvolumeclaim\\nglobal.persistence.storageClasse\\ns.default-rwx\\nNo\\nvalue\\nReadWr\\niteMany\\nAll capabilities\\nlogvolumeclaim\\nglobal.persistence.storageClasse\\ns.default-rwx\\nNo\\nvalue\\nReadWr\\niteMany\\nAll capabilities\\nomi-artemis-pvc\\nglobal.persistence.storageClasse\\ns.default-rwo\\nNo\\nvalue\\nReadWr\\niteOnce\\nOnly if OBM is selected\\npvc-omi-0, pvc-omi-1*\\nglobal.persistence.storageClasse\\ns.default-rwo\\nNo\\nvalue\\nReadWr\\niteOnce\\nOnly if OBM is selected\\nitomdipulsar-bookkeeper-journal-ito\\nmdipulsar-bookkeeper-n\\nitomdipulsar.bookkeeper.volume\\ns.journal.storageClassName\\nfast-\\ndisks\\nReadWr\\niteOnce\\nOnly if AEC or Reporting is selected\\n(Capabilities that use OPTIC Data Lake)\\nitomdipulsar-bookkeeper-ledgers-ito\\nmdipulsar-bookkeeper-n\\nitomdipulsar.bookkeeper.volume\\ns.ledgers.storageClassName\\nfast-dis\\nks\\nReadWr\\niteOnce\\nOnly if AEC or Reporting is\\nselected (Capabilities that use OPTIC\\nData Lake)\\nitomdipulsar-zookeeper-zookeeper-\\ndata-itomdipulsar-zookeeper-n\\nitomdipulsar.zookeeper.volumes.\\ndata.storageClassName\\nfast-dis\\nks\\nReadWr\\niteOnce\\nOnly if AEC or Reporting is selected\\n(Capabilities that use OPTIC Data Lake)\\nBased on the above requirements, make sure that you have the required storage set up.\\nContainerized Operations Bridge 2022.11\\nPage \\n453\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c44e11c2c832002f86e5dc6c9165f341'}>,\n",
              "  <Document: {'content': 'Storage concepts\\nThis topic provides you a background on the Kubernetes storage concepts and the storage requirements of Operations Bridge. \\nPersistent volume and persistent volume claim\\nIf a container within cluster stops or restarts, all changes made inside the container are lost. To save information such as\\nconfiguration files and databases, the container must store the information in an external storage. This external storage is\\nrepresented in Kubernetes as a Persistent Volume (PV).\\nThe cluster components (say a Pod) access a storage volume in the following flow:\\nWhen you create a PV, the mapped physical storage volume is available in the Kubernetes cluster. An application requests for\\na particular storage in the form of a PVC. Kubernetes determines a corresponding available PV that matches the requested\\nstorage of the PVC and makes it available to the pod.\\nKubernetes supports different types of volumes including awsElasticBlockStore, Azure Disk, iSCSI, local storage, NFS. In\\nOperations Bridge, few volumes are based on block storage. They can be mounted to only one pod at a time. This is known as\\naccess mode \\nReadWriteOnce\\n (\\nRWO\\n). A few volumes are based on shared filesystem storage. This can be mounted and accessed\\non multiple pods at the same time. This is known as access mode \\nReadWriteMany\\n (\\nRWX\\n). For more information on the access\\nmodes, see \\nKubernetes documentation\\n.\\nThe following table gives a mapping of supported storage types and access mode in Operations Bridge:\\nSupported storage types\\nAccess Mode\\nNFS, Ceph RBD, EBS, Azure Disk\\nReadWriteOnce\\nNFS, Cephfs, Azure Files, EFS\\nReadWriteMany\\nYou must create the PVs manually before deploying the application, or they can be dynamically created through a storage\\nprovisioner. If you choose to use a storage provisioner, Kubernetes uses the provisioner to create PVs on demand based on\\nrequested PVCs that are created while deploying the application.\\nDynamic provisioning with storage provisioner\\nDynamic volume provisioning creates storage volumes (the PVs) on demand. For more information about dynamic volume\\nprovisioning, see \\nKubernetes documentation\\n. \\nYou can also have multiple storage provisioners, for example, one storage provisioner which is able to create PVs with access\\nmode \\nReadWriteOnce\\n and another one with access mode \\nReadWriteMany\\n,\\n \\nor for different performance characteristics. Refer to\\nthe documentation of your Kubernetes environment to see what type of storage classes are available. For example, while\\ndeploying Operations Bridge on AWS, \\nio2\\n storage class represents fast throughput volumes and \\ngp3\\n storage class represents\\nother volumes. If you are using OMT Embedded Kubernetes, you can use the NFS storage provisioner with the storage class as \\ncdf-nfs\\n.\\nWhen you use a storage provisioner, the application requests storage by stating the corresponding storage class in its PVC\\nrequest. The dynamic storage provisioner responsible for a specific storage class creates a new storage volume and the\\ncorresponding PV that represents the storage. The PV is bound to PVC and the PVC can be used by application.\\nThe following diagram illustrates how the provisoning of a PV happens through a storage class. \\nFor detailed information on storage concepts in Kubernetes see \\nStorage\\n.\\nStorage requirements for OPTIC Data Lake\\nContainerized Operations Bridge 2022.11\\nPage \\n452\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f91d942e087ed34478f0425f35bc163b'}>,\n",
              "  <Document: {'content': 'Deploy on Azure\\nAfter you complete the Azure infrastructure setup using the ITOM Cloud Deployment Toolkit, perform these tasks to complete\\nthe OMT and Operations Bridge deployment. Ensure that you have extracted the Operations Bridge installer package on the\\nbastion node to access the scripts and proceed with the following tasks. \\nComplete the external database and user creation\\nCreate the required PostgreSQL databases and users listed in the following table for the application:\\nCapability\\nDescription\\nExample\\nDatabase\\nName\\nExample\\nuser\\nname\\nOMT\\nRequired for OMT API server database.\\ncdfapiserv\\nerdb\\ncdfapiser\\nverdb\\nOMT\\nRequired for managing OMT user/authentication.\\ncdfidmdb\\ncdfidmd\\nb\\nAll capabilities\\nRequired for managing application user/authentication. You\\nmust create a separate database for the application. This is\\ndifferent from the one created during the OMT installation\\nidm\\nidm\\nAll capabilities\\nRequired\\n by autopass\\n for managing licenses.\\nautopass\\nautopass\\nOPTIC Reporting, Stakeholder\\nDashboards, Agentless\\nMonitoring, Hyperscale\\nObservability, Automatic Event\\nCorrelation\\nRequired for OPTIC Reporting or Stakeholder Dashboard\\ncapabilities.\\nbvd\\nbvd\\nOperations Bridge Manager\\nOBM requires a management database for storing system wide\\nand management related metadata of the OBM environment. \\nobm_mgm\\nt\\nobm_mg\\nmt\\nOperations Bridge Manager\\nRequired for storing events and related data, such as\\nannotations, configuration data, and event correlation rules.\\nobm_even\\nt\\nobm_eve\\nnt\\nOperations Bridge Manager\\nThe run time service model database is required for storing\\nconfiguration information gathered from the various OBM and\\nthird party applications and tools. The application uses this\\ninformation when building OBM views.\\nrtsm\\nrtsm\\nHyperscale Observability, OPTIC\\nReporting, Agentless Monitoring\\nHyperscale Observability uses this database for storing\\nHyperscale Observability specific configurations.\\nmonitorin\\ngadmindb\\nmonitorin\\ngadminus\\ner\\nHyperscale Observability, OPTIC\\nReporting\\nHyperscale Observability uses this database for storing data\\ntemporarily.\\nmonitorin\\ngsnfdb\\nmonitorin\\ngsnfuser\\nHyperscale Observability, OPTIC\\nReporting, Agentless Monitoring\\nCredential manager service uses this database in Hyperscale\\nObservability.\\ncredential\\nmanager\\ncredentia\\nlmanager\\nuser\\nHyperscale Observability, OPTIC\\nReporting\\nThis database stores all baseline and threshold computations\\nand the metric transform service uses these data. Create this\\ndatabase if you are deploying the Hyperscale Observability.\\nbtcd\\nbtcd\\nFor information on creating databases using a utility see \\nCreating a database\\n.\\nYou can find the certificate to connect to an Azure Database for PostgreSQL server at \\nBaltimoreCyberTrustRoot.crt\\n. You must\\nsave this certificate in the bastion node in a folder that you will give in the install command later. If there is more than one file,\\nseparate them by using commas. For example: \\n/home/centos/system-setup/BaltimoreCyberTrustRoot.crt.pem\\nComplete the Vertica database configuration\\nYou will need Vertica database to deploy OPTIC Reporting, Hyperscale Observability, AEC capabilities, or an external OPTIC\\nData Lake scenario for a provider application. The ITOM Cloud Deployment toolkit creates the Vertica database and the users.\\nAdditionally, you must perform the following to complete the Vertica database configurations:\\n1\\n. \\nCreate the variables file\\n. This file helps you configure the variables to create objects within Vertica while executing the\\ndatabase configuration tool (\\ndbinit.sh\\n).  If you plan to share the OPTIC DL with NOM, you must set the Express Load\\nresource pool in the variables file and configure the Express Load buckets in step 3.\\n2\\n. \\nCreate certificates to enable TLS for Vertica\\n3\\n. \\nConfigure Vertica database to enable TLS\\n. To enable the TLS configurations.\\n4\\n. \\nVerify if Vertica is TLS enabled\\n.\\nRun the auto configuration script\\nThe \\nAutoConfigAzure.sh\\n script uses the ITOM Cloud Deployment Toolkit output and automatically creates the required files for\\nthe OMT installation. The script creates the following files:\\nconfig.json \\n- the JSON file for the OMT installation.\\nContainerized Operations Bridge 2022.11\\nPage \\n454\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6d59159533a8c1ae23e251efae9cd743'}>,\n",
              "  <Document: {'content': '    }\\n}\\nSample \\ncdf-pv.yaml\\n file\\n[root@abc-hvm tf-itom-sa]# cat cdf-pv.yaml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: itom-vol\\nspec:\\n  accessModes:\\n  - ReadWriteMany\\n  azureFile:\\n    secretName: azure-secret\\n    secretNamespace: core\\n    shareName: itomfileshare/var/vols/itom/core\\n  capacity:\\n    storage: 5Gi\\n  claimRef:\\n    apiVersion: v1\\n    kind: PersistentVolumeClaim\\n    name: itom-vol-claim\\n    namespace: core\\n  mountOptions:\\n  - dir_mode=0775\\n  - uid=1999\\n  - gid=1999\\n  - mfsymlinks\\n  persistentVolumeReclaimPolicy: Retain\\n  storageClassName: cdf-default\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: itom-logging\\nspec:\\n  accessModes:\\n  - ReadWriteMany\\n  capacity:\\n    storage: 5Gi\\n  claimRef:\\n    apiVersion: v1\\n    kind: PersistentVolumeClaim\\n    name: itom-logging-vol\\n    namespace: core\\n  azureFile:\\n    secretName: azure-secret\\n    shareName: itomfileshare/var/vols/itom/itom-logging-vol\\n    secretNamespace: core\\n  mountOptions:\\n  - dir_mode=0775\\n  - uid=1999\\n  - gid=1999\\n  - mfsymlinks\\n  persistentVolumeReclaimPolicy: Retain\\n  storageClassName: cdf-default\\n  volumeMode: Filesystem\\nSample \\ninstall.txt\\n file\\n[root@abc-hvm tf-itom-sa]# cat install.txt\\n./install  --registry-url toolkitcontainerregistry9xxxxxx.azurecr.io -c <path of the config.json file>/config.json --registry-username ToolkitContainerRegistry9xxxxxx --registry-password /eAFBd/ZZvXXXXXdDsDrJRn/xxVK --registry-orgname hpeswitom --external-access-host opsb.itombyok.internal --k8s-provider  AZURE --system-user-id 1999 --system-group-id 1999 --db-user cdfapiuser@itom-toolkit-8bxxxxxx-db-postgres --db-password password@123 --db-url jdbc:postgresql://itom-toolkit-8bxxxxxx-db-postgres.postgres.database.azure.com:5432/cdfapiserverdb --db-crt /home/centos/system-setup/BaltimoreCyberTrustRoot.crt.pem --deployment-name cdf --loadbalancer-info \"azure-load-balancer-internal=true\"\\nThe \\nAutoConfigAzure.sh\\n script on a successful run automatically creates the \\ninstall.txt\\n file. This text file has the OMT installation\\ncommand and the required parameters with the values.\\nRun the following command to create the volumes for OMT:\\nkubectl create -f cdf-pv.yaml\\nThis command creates the volumes as follows:\\npersistentvolume/itom-vol\\npersistentvolume/itom-logging\\nCopy the complete install text with the values to your bastion node command prompt, update the \\nconfig.json\\n file path, and run\\nthe command as follows to deploy OMT:\\n./install --registry-url toolkitcontainerregistry9xxxxxx.azurecr.io -c <path of the config.json file>/config.json --registry-username ToolkitContainerRegistry9xxxxxx --registry-password /eAFBd/ZZvXXXXXdDsDrJRn/xxVK --registry-orgname hpeswitom --external-access-host opsb.itombyok.internal --k8s-provider AZURE --system-user-id 1999 --system-group-id 1999 --db-user cdfapiuser@itom-toolkit-8bxxxxxx-db-postgres --db-password password@123 --db-url jdbc:postgresql://itom-toolkit-8bxxxxxx-db-postgres.postgres.database.azure.com:5432/cdfapiserverdb --db-crt /home/centos/system-setup/BaltimoreCyberTrustRoot.crt.pem --deployment-name cdf --loadbalancer-info \"azure-load-balancer-internal=true\"\\nAfter you deploy the OMT, you must make sure to complete to add the DNS records to access the external services.\\nFollow the steps in \\nAzure documentation\\n to add a DNS record to the private DNS zone. For more information, see the \\nlist of\\ncommands\\n. \\nContainerized Operations Bridge 2022.11\\nPage \\n456\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1581a214a2b5b9b039946095c07b722d'}>,\n",
              "  <Document: {'content': 'cdf-pv.yaml \\n- the YAML file with the persistent volumes for the OMT installation.\\ninstall.txt\\n  - the file with the command line for the OMT installation. You can copy the install text in this file onto the\\ncommand prompt and run the OMT installation.\\nWhile the script runs, it prompts for any additional values required in the files. You must copy the \\nAutoConfigAzure.sh\\n script from\\nthe \\n<unzipped opsbridge-suite-chart folder>/scripts\\n folder to the \\nitom-cloud-toolkit-202x.xx.xx-XX\\\\azure\\\\tf-itom-sa\\n folder.\\nFollow these steps to run the script:\\n1\\n. \\nLog on to the system where you have run the toolkit.\\n2\\n. \\nFrom the system where you have downloaded the application packages, go to the \\n<unzipped opsbridge-suite-chart folder>/scrip\\nts\\n copy the \\nAutoConfigAzure.sh\\n script to the \\nitom-cloud-toolkit-202x.xx.xx-XX\\\\azure\\\\tf-itom-sa\\n folder of the toolkit.\\n3\\n. \\nGo to the \\ntf-itom-sa\\n folder location and run the following command:\\n./AutoConfigAzure.sh\\n4\\n. \\nWhile the script runs, it prompts for the additional parameter values. Give the values for the following: \\nFQDN external access hostname\\n: The FQDN or IPv4 address of the external host you have assigned for the application\\nfor the private DNS record. This is used to access the entire cluster. The format for the external host name is \\n<APPLICATION\\n_PREFIX>.<Hosted Zone>\\n where: \\nAPPLICATION_PREFIX\\n can be any prefix you want as long as it\\'s valid as part of a URL. For\\nexample: If the \\nHosted zone\\n is \\nitombyok.com\\n the \\nexternalHostname\\n is \\nopsb.itombyok.com\\n.\\ndeployment name:\\n Give the deployment name you want to create.\\norganization name of the registry\\n: The organization in the image registry that contains the images (the default value\\nis \\n<hpeswitom>\\n).\\ncdfapiserverdb\\n database name\\n: The database name of the \\ncdfapiserverdb\\n  database.\\ncdfapiserverdb\\n user name\\n: The username of the \\ncdfapiserverdb\\n  database user.\\npassword for \\ncdfapiserverdb\\n user\\n: The password of the \\ncdfapiserverdb\\n database user.\\nCDF IdM database name\\n: The database name of the \\ncdfidmdb\\n  database. \\nCDF IdM user name\\n: The user name of the \\ncdfidmdb\\n  database user. \\npassword for IdM user\\n: The password of the \\ncdfidmdb\\n database user.\\nOnce the script runs successfully it creates the \\nconfig.json\\n, \\ncdf-pv.yaml\\n, and \\ninstall.txt\\n files.\\n5\\n. \\nYou must copy all these files to the bastion node from where you will deploy OMT and application.\\n6\\n. \\nMake sure you have extracted the OMT installer package on the bastion node.\\nFollowing is the sample output of the script and the files it creates:\\n[root@abc-hvm tf-itom-sa]# ./AutoConfigAzure.sh\\nPlease provide the fqdn external access hostname: opsb.itombyok.internal\\nPlease provide a deployment name [cdf]:\\nEnter the organization name of the registry [hpeswitom]:\\nEnter the cdfapiserverdb database name [cdfapiserverdb]:\\nEnter the cdfapiserverdb user name [cdfapiuser]:\\nEnter the password for above user:\\nEnter the CDF idm database name [cdfidmdb]:\\nEnter the CDF idm user name [cdfidmuser]:\\nEnter the password for above user:\\nWorking ...\\nCreated cdf-pv.yaml\\nCreated config.json\\nCreated install.txt\\nFinished.\\nSample \\nconfig.json\\n file\\n[root@abc-hvm tf-itom-sa]# cat config.json\\n{\\n        \"licenseAgreement\": {\\n                \"eula\": true,\\n                \"callHome\": false\\n        },\\n        \"connection\": {\\n                \"externalHostname\": \"opsb.itombyok.internal\",\\n                \"port\": \"443\"\\n        },\\n        \"volumes\": [\\n        {\\n            \"name\": \"itom-logging-vol\",\\n            \"type\": \"NFS\",\\n            \"path\": \"itomfileshare/var/vols/itom/itom-logging-vol\"\\n        }\\n                   ],\\n        \"database\": {\\n        \"type\": \"extpostgres\",\\n        \"param\": {\\n            \"dbHost\": \"itom-toolkit-8bxxxxxx-db-postgres.postgres.database.azure.com\",\\n            \"dbPort\": \"5432\",\\n            \"dbName\": \"cdfidmdb\",\\n            \"dbUser\": \"cdfidmuser@itom-toolkit-8bxxxxxx-db-postgres\",\\n            \"dbPassword\": \"password@123\",\\n            \"dbCert\": \"/home/centos/system-setup/BaltimoreCyberTrustRoot.crt.pem\"\\n        }\\nContainerized Operations Bridge 2022.11\\nPage \\n455\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f5c3bedec110aeb50acc054ef81e476'}>,\n",
              "  <Document: {'content': 'To get the external IP of the OMT deployment, run the following command:\\nkubectl get svc portal-ingress-controller-svc -n core -o jsonpath=\\'{.status.loadBalancer.ingress[*].ip}{\"\\\\n\"}\\'\\nDeploy Operations Bridge\\n1\\n. \\nTo install an application, you need to upload a Helm chart file to the cluster. For more information, see \\nUpload application\\nchart\\n.\\n2\\n. \\nOperations Bridge requires persistent volumes before it\\'s deployed. For more information to create the persistent\\nvolumes, see \\nCreate persistent volumes\\n.\\n3\\n. \\nYou can now \\ndeploy Operations Bridge\\n.\\n4\\n. \\nAfter you complete the install, you can \\nverify\\n and perform the required \\npost install tasks\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n457\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8759f85bec6bb8ab997030f883775134'}>,\n",
              "  <Document: {'content': 'volumeBindingMode: WaitForFirstConsumer\\n\\'\\nAfter you install Operations Bridge, these volumes will be visible in your chosen region in the \\nEC2 Service\\n under \\nElastic\\nBlock Storage\\n > \\nVolumes\\n.\\n3\\n. \\nYou can now \\ndeploy Operations Bridge\\n.\\n4\\n. \\nIf you have used any other \\nExternal Access Port\\n other than 30443, you must update the port in \\nrules.sh\\n. Open the file\\nin edit mode from the location \\n/home/ec2-user/bin/rules.sh\\n. For the \\nitom-ingress-controller-svc-internal\\n, update the same port you\\nused as an external access port while deploying Operations Bridge. Save the file.\\n5\\n. \\nRun the command following on the bastion node to configure the listeners and target groups for the application:\\n/home/ec2-user/bin/load-balancers.sh create <OpsB_NAMESPACE> <APPLICATION_PREFIX>\\n \\n<APPLICATION_NAME>\\nYou must use the same \\nAPPLICATION_PREFIX\\n you used during OMT installation. The \\nAPPLICATION_NAME\\n is the\\napplication name that you will deploy in small alphabets.\\nFor example: \\n/home/ec2-user/bin/load-balancers.sh create opsbtest opsb opsb\\nYou can ignore the \"\\nTried to create resource set but it already exists\\n\" error that you may see while running the script.\\n6\\n. \\nAfter you complete the installation, you can \\nverify\\n and perform the required \\npost install tasks\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n461\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ad081d1ccb05b0608b1b2f31999f889'}>,\n",
              "  <Document: {'content': \"Deploy on AWS\\nAfter you complete the AWS infrastructure setup using the ITOM Cloud Deployment Toolkit, perform these tasks to complete\\nthe OMT and Operations Bridge deployment. Ensure that you have extracted the Operations Bridge installer package on the\\nbastion node to access the scripts and proceed with the following tasks. \\nComplete the external database and user creation\\nCreate the required PostgreSQL databases and users listed in the following table for the application:\\nCapability\\nDescription\\nExample\\nDatabase\\nName\\nExample\\nuser\\nname\\nOMT\\nRequired for OMT API server database.\\ncdfapiserv\\nerdb\\ncdfapiser\\nverdb\\nOMT\\nRequired for managing OMT user/authentication.\\ncdfidmdb\\ncdfidmd\\nb\\nAll capabilities\\nRequired for managing application user/authentication. You\\nmust create a separate database for the application. This is\\ndifferent from the one created during the OMT installation\\nidm\\nidm\\nAll capabilities\\nRequired\\n by autopass\\n for managing licenses.\\nautopass\\nautopass\\nOPTIC Reporting, Stakeholder\\nDashboards, Agentless\\nMonitoring, Hyperscale\\nObservability, Automatic Event\\nCorrelation\\nRequired for OPTIC Reporting or Stakeholder Dashboard\\ncapabilities.\\nbvd\\nbvd\\nOperations Bridge Manager\\nOBM requires a management database for storing system wide\\nand management related metadata of the OBM environment. \\nobm_mgm\\nt\\nobm_mg\\nmt\\nOperations Bridge Manager\\nRequired for storing events and related data, such as\\nannotations, configuration data, and event correlation rules.\\nobm_even\\nt\\nobm_eve\\nnt\\nOperations Bridge Manager\\nThe run time service model database is required for storing\\nconfiguration information gathered from the various OBM and\\nthird party applications and tools. The application uses this\\ninformation when building OBM views.\\nrtsm\\nrtsm\\nHyperscale Observability, OPTIC\\nReporting, Agentless Monitoring\\nHyperscale Observability uses this database for storing\\nHyperscale Observability specific configurations.\\nmonitorin\\ngadmindb\\nmonitorin\\ngadminus\\ner\\nHyperscale Observability, OPTIC\\nReporting\\nHyperscale Observability uses this database for storing data\\ntemporarily.\\nmonitorin\\ngsnfdb\\nmonitorin\\ngsnfuser\\nHyperscale Observability, OPTIC\\nReporting, Agentless Monitoring\\nCredential manager service uses this database in Hyperscale\\nObservability.\\ncredential\\nmanager\\ncredentia\\nlmanager\\nuser\\nHyperscale Observability, OPTIC\\nReporting\\nThis database stores all baseline and threshold computations\\nand the metric transform service uses these data. Create this\\ndatabase if you are deploying the Hyperscale Observability.\\nbtcd\\nbtcd\\nFor information on creating databases using a utility see \\nCreating a database\\n.\\nYou must download the AWS RDS certificate - \\nrds-ca-2019-root.pem\\n that's available in the \\nAWS Documentation\\n. Save this\\ncertificate in the bastion node in a folder that you will give in the install command later. For example: \\n/home/ec2-user/system-setu\\np/rds-ca-2019-root.pem\\n.\\nComplete the Vertica database configuration\\nYou will need Vertica database to deploy OPTIC Reporting, Hyperscale Observability, AEC capabilities, or an external OPTIC\\nData Lake scenario for a provider application. The ITOM Cloud Deployment toolkit creates the Vertica database and the users.\\nAdditionally, you must perform the following to complete the Vertica database configurations:\\n1\\n. \\nCreate certificates to enable TLS for Vertica\\n2\\n. \\nConfigure Vertica database to enable TLS\\n. To enable the TLS configurations.\\n3\\n. \\nVerify if Vertica is TLS enabled\\n.\\nRun the auto configuration script\\nThe \\nAutoConfigAWS.sh\\n script uses the ITOM Cloud Deployment Toolkit output and automatically creates the required files for the\\nOMT installation. The script creates the following files:\\nconfig.json \\n- the JSON file for the OMT installation.\\ninstall.txt\\n  - the file with the command line for the OMT installation.\\nWhile the script runs, it prompts for any additional values required in the files. You must copy the \\nAutoConfigAWS.sh\\n script from\\nContainerized Operations Bridge 2022.11\\nPage \\n458\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9b3337ac36e5a19d438f45832a1d5214'}>,\n",
              "  <Document: {'content': 'the \\n<unzipped opsbridge-suite-chart folder>/scripts\\n folder to the \\nitom-cloud-toolkit-202x.xx.xx-XX\\\\aws\\\\tf-itom-sa\\n folder.\\nFollow these steps to run the script:\\n1\\n. \\nLog on to the system where you have run the toolkit.\\n2\\n. \\nOn the system where you have downloaded the application packages, go to the \\n<unzipped opsbridge-suite-chart\\nfolder>/scripts\\n, copy the \\nAutoConfigAWS.sh\\n script to the \\nitom-cloud-toolkit-202x.xx.xx-XX\\\\aws\\\\tf-itom-sa\\n folder of the toolkit.\\n3\\n. \\nGo to the \\ntf-itom-sa\\n folder location and run the following command:\\n./AutoConfigAWS.sh\\n4\\n. \\nWhile the script runs, it prompts for the additional parameter values. Give the values for the following:\\nFQDN external access hostname\\n: The FQDN or IPv4 address of the external host you have assigned for the\\napplication. This is used to access the entire cluster. It’s used for the requests to the \\ncdf-apiserver\\n server. The value\\ndepends on Terraform\\'s configuration and must follow the format: \\n<APPLICATION_PREFIX>.<TERRAFORM_ENVIRONMENT_PREFIX>.\\n<TERRAFORM_PRIVATE_DOMAIN>\\n, where, \\nAPPLICATION_PREFIX\\n can be any prefix you want as long as it\\'s valid as part of a URL. \\nTERRAFORM_ENVIRONMENT_PREFIX\\n corresponds to \\nenvironment-prefix\\n in Terraforms\\' \\ntemplate.tfvars\\n. \\nTERRAFORM_PRIVATE_DOMAI\\nN\\n corresponds to \\nprivate-domain\\n in Terraforms\\' \\nvariables.tf\\n.\\nserverKey\\n certificate name\\n: The path and file name of the external access host server key in the bastion host. If you\\nhave used a custom CA certificate during bastion configuration, you must give that certificate key. \\nserverCrt\\n certificate name\\n: The path and file name of the external access host server certificate in the bastion host. If\\nyou have used a custom CA certificate during bastion configuration, you must give that certificate. \\nrootCrt\\n certificate name\\n: The path and file name of the external access host root certificate in the bastion host.\\ndeployment name:\\n Give the deployment name you want to create.\\norganization name of the registry\\n: The organization in the image registry that contains the images (the default value\\nis \\n<hpeswitom>\\n).\\ncdfapiserverdb\\n database name\\n: The database name of the \\ncdfapiserverdb\\n  database.\\ncdfapiserverdb\\n user name\\n: The username of the \\ncdfapiserverdb\\n  database user.\\npassword for \\ncdfapiserverdb\\n user\\n: The password of the \\ncdfapiserverdb\\n database user.\\nCDF IdM database name\\n: The database name of the \\ncdfidmdb\\n  database. \\nCDF IdM user name\\n: The user name of the \\ncdfidmdb\\n  database user. \\npassword for IdM user\\n: The password of the \\ncdfidmdb\\n database user.\\nAWS REGION\\n: Give the AWS region.\\nAWS ACCOUNT ID\\n: Give the AWS account ID.\\nOnce the script runs successfully it creates the \\nconfig.json\\n and \\ninstall.txt\\n files.\\n5\\n. \\nCopy these files to the bastion node from where you will deploy OMT and the application.\\n6\\n. \\nMake sure you have extracted the OMT installer package on the bastion node.\\nFollowing is the sample output of the script and the files it creates:\\n[root@abc-hvm tf-itom-sa]# ./AutoConfigAWS.sh\\nPlease provide the fqdn external access hostname: aws.opsbmf.itombyok.internal\\nserverKey certificate name including path (on the bastion node) for external access host: /home/ec2-user/opsbmf-lb-cert.key\\nserverCrt certificate name including path (on the bastion node) for external access host: /home/ec2-user/opsbmf-lb-cert.pem\\nrootCrt certificate name including path (on the bastion node) for external access host: /home/ec2-user/opsbmf-lb-ca-cert.pem\\nPlease provide a deployment name [cdf]:\\nEnter the organization name of the registry [hpeswitom]:\\nEnter the cdfapiserverdb database name [cdfapiserverdb]:\\nEnter the cdfapiserverdb user name [cdfapiuser]:\\nEnter the password for above user:\\nEnter the CDF idm database name [cdfidmdb]:\\nEnter the CDF idm user name [cdfidmuser]:\\nEnter the password for above user:\\nPlease provide the AWS REGION (e.g. us-east-1): ap-south-1\\nPlease provide the AWS ACCOUNT ID: 50xxx6xx00xx\\nWorking ...\\nCreated config.json\\nCreated install.txt\\nFinished.\\nSample \\nconfig.json\\n file\\n[root@abc-hvm tf-itom-sa]# cat config.json\\n{\\n        \"connection\": {\\n                \"externalHostname\": \"aws.opsbmf.itombyok.internal\",\\n                \"port\": \"443\",\\n                \"serverKey\": \"/home/ec2-user/opsbmf-lb-cert.key\",\\n                \"serverCrt\": \"/home/ec2-user/opsbmf-lb-cert.pem\",\\n                \"rootCrt\": \"/home/ec2-user/opsbmf-lb-ca-cert.pem\"\\n        },\\n                \"licenseAgreement\": {\\n                \"eula\": true,\\n                \"callHome\": false\\n        },\\n        \"database\": {\\n        \"type\": \"extpostgres\",\\n        \"param\": {\\n            \"dbHost\": \"opsbmf-postgres-mydb.cimxxxx4wmi.ap-south-1.rds.amazonaws.com\",\\nContainerized Operations Bridge 2022.11\\nPage \\n459\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a63ba831dab0d8f312c47aad74ed5530'}>,\n",
              "  <Document: {'content': 'Uninstall local storage provisioner\\nFollow these steps to uninstall the local storage provisioner and clear the residual files of the OPTIC DL Message Bus on all the\\nnodes of the cluster where you have deployed the application namespace:\\n1\\n. \\nRun the following command to uninstall the local storage provisioner:\\nhelm uninstall <helm_deployment_name> -n core \\nwhere:\\n<helm_deployment_name> \\nis the name with which you have deployed the local storage provisioner.\\nExample:\\nhelm uninstall lpv -n core\\n2\\n. \\nRun the following commands to delete the local storage provisioner PVCs:\\nkubectl get pvc -n <application namespace> | grep fast-disks \\nkubectl delete pvc <pvc name of the local storage provisioner> -n <application namespace>\\nExample:\\n# kubectl get pvc -n opsb-helm| grep fast-disk\\nitomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-0        Bound    local-pv-a306c635   29Gi       RWO            fast-disk                           s       20h\\nitomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-1        Bound    local-pv-e644ca     97Gi       RWO            fast-disk                           s       20h\\nitomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-2        Bound    local-pv-f03deeb    29Gi       RWO            fast-disk                           s       20h\\nitomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-0        Bound    local-pv-c61e642    29Gi       RWO            fast-disk                           s       20h\\nitomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-1        Bound    local-pv-6b090b1b   29Gi       RWO            fast-disk                           s       20h\\nitomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-2        Bound    local-pv-8199a5b1   97Gi       RWO            fast-disk                           s       20h\\nitomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-0   Bound    local-pv-5b1599ea   29Gi       RWO            fast-disk                           s       20h\\nitomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-1   Bound    local-pv-faf9cc75   97Gi       RWO            fast-disk                           s       20h\\nitomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-2   Bound    local-pv-a019bb9c   29Gi       RWO            fast-disk                           s       20h\\n# kubectl delete pvc  itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-0 -n opsb-helm                           \\n# kubectl delete pvc itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-1 -n opsb-helm        \\n# kubectl delete pvc itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-2 -n opsb-helm        \\n# kubectl delete pvc itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-0 -n opsb-helm        \\n# kubectl  delete pvc itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-1 -n opsb-helm       \\n# kubectl delete pvc itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-2 -n opsb-helm       \\n# kubectl delete pvc itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-0 -n opsb-helm\\n# kubectl  delete pvc itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-1 -n opsb-helm   \\n# kubectl delete pvc itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-2 -n opsb-helm \\n3\\n. \\nRun the following commands to delete the local storage provisioner PV:\\nkubectl get pv | grep fast-disks\\n \\nkubectl delete pv <pv name of the local storage provisioner>\\nExample:\\n# kubectl get pv | grep fast-disks\\n#kubectl delete pv local-pv-5b1599ea  local-pv-6b090b1b  local-pv-8199a5b1 local-pv-a019bb9c  local-pv-a306c635 local-pv-c61e642  local-pv-e644ca  local-pv-f03deeb  local-pv-faf9cc75 \\n4\\n. \\nGo to the location where the local storage volumes are mounted (the default location is \\n/mnt/disks\\n). Go to each virtual\\ndisk, unmount the directories (mount path), and then delete the \\nlpv\\n directory. Perform this on all the worker nodes.\\nExample:\\ncd /mnt/disks\\nfor lpv in lpv1 lpv2 lpv3 \\ndo\\n    rm -rf /mnt/disks/$lpv/*\\ndone\\nNote:\\n Ignore this page if you had installed only OBM or Stakeholder Dashboards or both OBM and Stakeholder\\nDashboards or uninstalling OpenShift deployment.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n462\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bfc1c0427bf31a4a99b42f2ce546837b'}>,\n",
              "  <Document: {'content': '            \"dbPort\": \"5432\",\\n            \"dbName\": \"cdfidmdb\",\\n            \"dbUser\": \"cdfidmuser\",\\n            \"dbPassword\": \"password_123\",\\n            \"dbCert\": \"/home/ec2-user/system-setup/rds-ca-2019-root.pem\"\\n        }\\n    }\\nSample \\ninstall.txt\\n file\\n[root@abc-hvm tf-itom-sa]# cat install.txt\\n./install --deployment-name cdf -c <path of the config.json file>/config.json --k8s-provider aws --loadbalancer-info \"aws-load-balancer-type=nlb;aws-load-balancer-internal=true\" --nfsprov-server fs-0cxxxexxxcadxxxx.efs.ap-south-1.amazonaws.com --nfsprov-folder /var/vols/itom/core --system-user-id 1999 --system-group-id 1999 --registry-url \"50xxx6xx00xx.dkr.ecr.ap-south-1.amazonaws.com\" --registry-username \"AWS\" --registry-password \"eyJwYXlsb2FkIjoia1V1azQ4ZENjSEJsdXB2cGl1cGhLdHl4aDcwTExLUitTZG5CNFYyWEVGUllPaVlwUE5wcnNrTjJzMEF3L3lxYW1HYnd1WDNaSE5NUzlHc2I5aTRmeGtWMm1KU2M4QTd4UFBHQ1FrMkZaTzM2VEdweStOb09PNnliN2xGL3pabExvZndrc09nLzhGblhDakdzRllGRE9iMkJEMmVONVZTV3F4UTNta29MRkVjclFkTVVamRPai93V3NNQXF2ZGpJd09uaXljNExNNllCSFVMK3J2dzBldW1YcG4DlmeHdVdUs5VEtnOUhyMU9WcHEzNTJpVUTlGcUVtVjI2QmR3d0hiUGlvUU1wS295NWpJWW5wZFFZTHpBQUFBZmpCOEJna3Foa2lHOXcwQkJ3YWaXJhdGlvbiI6MTY0MjA5NTA4MX0=\" --registry-orgname hpeswitom --external-access-host aws.opsbmf.itombyok.internal --db-user cdfapiuser --db-password password_123 --db-url \"jdbc:postgresql://opsbmf-postgres-mydb.cimxxxx4wmi.ap-south-1.rds.amazonaws.com:5432/cdfapiserverdb\" --db-crt /home/ec2-user/system-setup/rds-ca-2019-root.pem --capabilities \"Tools=true,Monitoring=true,MonitoringContent=true,LogCollection=true,DeploymentManagement=true,ClusterManagement=true,NfsProvisioner=true\"\\nThe \\nAutoConfigAWS.sh\\n script on a successful run automatically creates the \\ninstall.txt\\n file. This text file has the OMT installation\\ncommand and the required parameters with the values.\\nCopy the complete install text with the values from the \\ninstall.txt\\n file to the same bastion node command prompt. Update the\\npath of the \\nconfig.json\\n file with its path in bastion node and then run the command to deploy OMT as follows:\\n./install --deployment-name cdf -c <path of the config.json file>/config.json --k8s-provider aws --loadbalancer-info \"aws-load-balancer-type=nlb;aws-load-balancer-internal=true\" --nfsprov-server fs-0cxxxexxxcadxxxx.efs.ap-south-1.amazonaws.com --nfsprov-folder /var/vols/itom/core --system-user-id 1999 --system-group-id 1999 --registry-url \"50xxx6xx00xx.dkr.ecr.ap-south-1.amazonaws.com\" --registry-username \"AWS\" --registry-password \"eyJwYXlsb2FkIjoia1V1azQ4ZENjSEJsdXB2cGl1cGhLdHl4aDcwTExLUitTZG5CNFYyWEVGUllPaVlwUE5wcnNrTjJzMEF3L3lxYW1HYnd1WDNaSE5NUzlHc2I5aTRmeGtWMm1KU2M4QTd4UFBHQ1FrMkZaTzM2VEdweStOb09PNnliN2xGL3pabExvZndrc09nLzhGblhDakdzRllGRE9iMkJEMmVONVZTV3F4UTNta29MRkVjclFkTVVamRPai93V3NNQXF2ZGpJd09uaXljNExNNllCSFVMK3J2dzBldW1YcG4DlmeHdVdUs5VEtnOUhyMU9WcHEzNTJpVUTlGcUVtVjI2QmR3d0hiUGlvUU1wS295NWpJWW5wZFFZTHpBQUFBZmpCOEJna3Foa2lHOXcwQkJ3YWaXJhdGlvbiI6MTY0MjA5NTA4MX0=\" --registry-orgname hpeswitom --external-access-host aws.opsbmf.itombyok.internal --db-user cdfapiuser --db-password password_123 --db-url \"jdbc:postgresql://opsbmf-postgres-mydb.cimxxxx4wmi.ap-south-1.rds.amazonaws.com:5432/cdfapiserverdb\" --db-crt /home/ec2-user/system-setup/rds-ca-2019-root.pem --capabilities \"Tools=true,Monitoring=true,MonitoringContent=true,LogCollection=true,DeploymentManagement=true,ClusterManagement=true,NfsProvisioner=true\"\\n--deployment-name\\n:  Type the deployment name for the OMT installation. You can modify it if desired.\\n--nfsprov-server\\n: is the \\nefs-dns-names\\n. \\n--nfsprov-folder\\n: The path where of the \\ncore\\n volumes. For example: \\n/var/vols/itom/core\\n--system-user-id\\n: The default value is 1999.\\n--system-group-id\\n: The default value is 1999.\\n--registry-orgname\\n: is the one you used while you run the toolkit.\\n--external-access-host\\n:\\n \\nthe external access host name allocated for the application. The format for the external host name\\nis \\n<Application prefix>.<Hosted Zone>\\n. For example: If the \\nHosted zone\\n is \\nitombyok.com\\n the \\nEXTERNAL_ACCESS_HOST\\n is \\nopsb.itombyok.com\\n.  \\n--db-user\\n and \\n--db-password:\\n The values you have passed while running the \\nAutoConfigAWS.sh\\n script for \\ncdfapiserver\\n and \\ncdfid\\nm\\n user.\\n--db-url:\\n The connection URL of the external database. This is with the \\ndb-address\\n value from the toolkit output.\\n--db-crt\\n: The path where you have saved the certificate.\\n-c\\n The path to the \\nconfig.json\\n file that the \\nAutoConfigAWS.sh\\n script creates.\\n--capabilities\\n: Options to enable the ITOM monitoring dashboards on Grafana. The \\nNfsProvisioner\\n option when set\\nto \\ntrue\\n, creates the PV for the application PVC creation request.\\nAfter you deploy the OMT, you must make sure to complete the Network Load Balancer configuration to enable routing of the\\napplication\\'s web services before you deploy Operations Bridge. Run the command on the bastion node: \\n/home/ec2-user/bin/load-balancers.sh create <OMT_NAMESPACE> <APPLICATION_PREFIX> <APPLICATION_NAME>\\nYou must use the same \\nAPPLICATION_PREFIX\\n and \\nOMT_NAMESPACE \\nyou used during OMT installation through the script.  The \\nAPPLI\\nCATION_NAME\\n is \\ncore\\n.\\nDeploy Operations Bridge\\n1\\n. \\nTo install an application, you need to upload a Helm chart file to the cluster. For more information, see \\nUpload application\\nchart\\n.\\n2\\n. \\nIf you plan to use the capabilities that require OPTIC DL (For example, OPTIC Reporting, AEC, Hyperscale Observability),\\nyou must create storage classes for the dynamic provisioning of volumes. Run the following commands to create required\\nstorage classes:\\nkubectl apply -f - <<<\\'\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: gp3\\n  annotations:\\n    storageclass.kubernetes.io/is-default-class: \"false\"\\nprovisioner: ebs.csi.aws.com\\nparameters:\\n  type: gp3\\n  fsType: ext4\\nvolumeBindingMode: WaitForFirstConsumer\\n---\\nkind: StorageClass\\napiVersion: storage.k8s.io/v1\\nmetadata:\\n  name: io2\\n  annotations:\\n    storageclass.kubernetes.io/is-default-class: \"false\"\\nprovisioner: ebs.csi.aws.com\\nparameters:\\n  type: io2\\n  fsType: ext4\\n  iopsPerGB: \"50\"\\nTip\\n: You can ignore the \"\\nservices \\n<service name>\\n not found\\n\" error that you may see while running the script.\\nContainerized Operations Bridge 2022.11\\nPage \\n460\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6ea00677327d306c8b5f648a8d42098d'}>,\n",
              "  <Document: {'content': 'The OMT documentation distinguishes between Control plane / Bastion / Worker nodes. For the low footprint, single node\\ninstallation, all these terms refer to different roles of the same single system.\\nThis topic assumes the installation packages are downloaded and unzipped in \\n~root/INSTALL/\\nThis topic assumes installation as the root user. For installing as non-root you may adjust the commands to use sudo.\\nRun the following commands to install the OMT tools:\\n# cd ~/INSTALL/OMT_External_K8s_202x.xx-xxx\\n# ./install --capabilities Tools=true,Monitoring=false,LogCollection=false,DeploymentManagement=false,ClusterManagement=false\\nWarning: tools will be copied to /root/cdf.\\nWarning: All the files under the following folders will be removed: /root/cdf/bin /root/cdf/scripts /root/cdf/tools.\\nAre you sure to continue? (Y/N): y\\nTool copy done! Tools are copied to /root/cdf.\\nAfter installing the OMT tool reload your environment or log off and on to ensure the OMT path settings are active. For\\nadditional details, see \\nEnable OMT tools\\n.\\nDeploy \\nStep 1: Create a deployment \\n# cdfctl deployment create -t helm -d opsbsuite -n opsbridge\\n2022-05-30T11:45:51+02:00 WRN Secret registrypullsecret doesn\\'t exist. The deployment will be created without image pull secret\\n2022-05-30T11:45:51+02:00 INF Creating deployment ... name=opsbsuite namespace=opsbridge\\n2022-05-30T11:45:51+02:00 INF Created namespace \"opsbridge\" ...\\n2022-05-30T11:45:51+02:00 INF Successfully created deployment \"opsbsuite\" uuid=66094403-4fb0-4c13-9845-7b6ef18a5298\\nStep 2: Download the installation chart\\nFollow the \\ndocumentation for downloading the Operations Bridge Chart\\n and download into ~/INSTALL. For this installation, you\\nwill only need to download the Operations Bridge Helm chart (\\nopsbridge-suite-chart-202x.xx.0.zip\\n).\\nStep 3: Configure the deployment options\\nIn this step, edit the default \\nvalues.yaml\\n file for installing Agentless Monitoring in a low footprint, single node environment. You\\nmust copy the sample file included in the samples directory:\\n# cd ~/INSTALL\\n# cp  opsbridge-suite-chart/samples/values.yaml values_k3s.yaml\\n Edit the file and apply the following changes. Make sure to include or update the lines with the exact indentation. Make sure\\nto keep the indentation intact. Don\\'t use \"tabs\".\\n# vi values_k3s.yaml\\n--------- SEARCH THE FOLLOWING LINES AND MODIFY THEM ACCORDINGLY ----------\\nacceptEula: true\\nglobal:\\n  externalAccessHost: <YOUR_SERVERS_FQDN>\\n  externalAccessPort: 30443\\n  services:\\n    agentlessMonitoring:\\n      deploy: true\\n  persistence:\\n    enabled: true\\n    storageClasses:\\n      default-rwx: local-path    \\n      default-rwo: local-path   \\n    accessMode: ReadWriteOnce\\n  docker:\\n    registry: docker.io\\n    orgName: hpeswitom\\n  database:\\n    internal: true\\n  cluster:\\n    k8sProvider: generic       # k8s Provider\\nsecrets:\\nNote:\\n The configuration below will use port 30443 for the external access port. For installing using another\\nport you might need to adjust the K3s installation (disable traefik and/or extend the K3s port range).\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n464\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1309e17b9a367e15cde2ad5ada568f3d'}>,\n",
              "  <Document: {'content': '  idm_opsbridge_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  bvd_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  IDM_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  AUTOPASS_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  BVD_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  MA_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  CM_DB_PASSWD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  idm_admin_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  sys_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\nThe password requires 8-32 characters and requires an upper and lower case character, a number, and a special character. If\\nyour password doesn\\'t meet the password policy the installation will fail.\\nYou must specify the password in a base64 encoded format. When using special characters, be sure that your shell doesn\\'t\\nreplace these. To test whether your password encodes correctly, you can use the following example:\\n# echo -n ChangeMe2$ | base64 | base64 -d ; echo\\nChangeMe2$\\n#\\nIf the password returned differs from the original, you might need to escape control characters.\\nFollow these steps to create a secrets file. Keep the indentation and don\\'t use tabs. Replace all occurrences of \\n<YOUR_BASE64_E\\nNCODED_PASSWORD>\\n.\\n# echo -n  My_secret_passw0rd$ | base64\\nTXlfc2VjcmV0X3Bhc3N3MHJkJA==\\nStep 4: Load the required Container Images\\nDownload the required container images. OMT provides a utility for downloading just the required set of images. Use the \\n-C\\nand \\n-H\\n parameters to ensure \\nimage-set.json\\n contains only the images needed by the selected capabilities. For additional details\\nrefer to \\nDownload the application images\\n.\\n# /root/cdf/tools/generate-download/generate_download_bundle.sh \\\\\\n     --chart /root/INSTALL/opsbridge-suite-chart/charts/opsbridge-suite-<version>.tgz \\\\\\n     -H /root/INSTALL/values_k3s.yaml -S -o hpeswitom -d /tmp/\\nThis will generate an \"\\noffline-download.zip\\n\" bundle. Copy this to a Linux system, which has access to the internet (\\ndocker.hub\\n). If\\nthe system you are installing on has internet access, then you can continue to use the same system. Unzip the bundle in the \\n/t\\nmp\\n directory and download the required images:\\n# unzip offline_download.zip\\n# cd offline-download/\\n# chmod u+x downloadimages.sh\\n# ./downloadimages.sh -r registry.hub.docker.com -o hpeswitom -u <YOUR DOCKER ACCOUNT>\\nContacting Registry: https://registry.hub.docker.com ../[Failed]                                                                                                        Retrying contacting https://registry.hub.docker.com. please make sure your user name, password and network/proxy configuration are correct.\\nPassword:\\nContacting Registry: https://registry.hub.docker.com ..[OK]\\nStart downloading ...\\n! Warning: Please check suite sizing documentation and make sure you have enough disk space for downloading suite images.\\nContinue?[Y/N]?y\\nDownloading image [1/24] hpeswitom/itom-agentless-monitoring:1.2.0-354 ...[OK]\\nDownloading image [2/24] hpeswitom/itom-autopass-lms:2022.11-2022102615 ...[OK]\\nDownloading image [3/24] hpeswitom/itom-bvd:11.10.22 ...[OK]\\nDownloading image [4/24] hpeswitom/itom-credential-manager:1.19.0.5 ...[OK]\\nDownloading image [5/24] hpeswitom/itom-idm:1.36.1-661 ...[OK]\\nDownloading image [6/24] hpeswitom/itom-k8s-sidecar:1.1.0-0020 ...[OK]\\nDownloading image [7/24] hpeswitom/itom-monitoring-admin:2022.11-262 ...[OK]\\nDownloading image [8/24] hpeswitom/itom-monitoring-resources:1.2.0-120 ...[OK]\\nDownloading image [9/24] hpeswitom/itom-monitoring-sis-adapter:1.3.0-167 ...[OK]\\nDownloading image [10/24] hpeswitom/itom-nginx-ingress:0.23.0-0050 ...[OK]\\nDownloading image [11/24] hpeswitom/itom-opsbridge-database-init:1.1.0-11 ...[OK]\\nDownloading image [12/24] hpeswitom/itom-opsbridge-dbvalidator:2.3.0-13 ...[OK]\\nDownloading image [13/24] hpeswitom/itom-pg-backup:12.1.0 ...[OK]\\nDownloading image [14/24] hpeswitom/itom-pgbackup-enabler:1.0.0-16.commit-7a89ccd ...[OK]\\nDownloading image [15/24] hpeswitom/itom-postgresql:14-00119 ...[OK]\\nDownloading image [16/24] hpeswitom/itom-prometheus-exporter-cert:1.9.0-00101 ...[OK]\\nDownloading image [17/24] hpeswitom/itom-redis:11.10.22 ...[OK]\\nDownloading image [18/24] hpeswitom/itom-reloader:1.1.0-0080 ...[OK]\\nDownloading image [19/24] hpeswitom/itom-static-files-provider:2.4.0-3427 ...[OK]\\nDownloading image [20/24] hpeswitom/itom-stunnel:11.10.0-0036 ...[OK]\\nDownloading image [21/24] hpeswitom/itom-tools-base:1.2.0-0031 ...[OK]\\nDownloading image [22/24] hpeswitom/kubernetes-vault-init:0.17.0-0042 ...[OK]\\nDownloading image [23/24] hpeswitom/kubernetes-vault-renew:0.17.0-0042 ...[OK]\\nDownloading image [24/24] hpeswitom/vault:0.21.0-00103 ...[OK]\\nDownload completed in 257 seconds.\\nDownload-process successfully completed.\\nSuccessfully downloaded the images  to /var/opt/cdf/offline/images_20221122153017/.\\nPlease refer to /var/opt/cdf/offline/images_20221122153017/downloadimages-20221122152554.log for more detail\\nLoad the images into the container. Run the following command to tar and import the images:\\nContainerized Operations Bridge 2022.11\\nPage \\n465\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '76df9a60ace3838358ebc6b9eda01050'}>,\n",
              "  <Document: {'content': 'Install Agentless Monitoring in K3s environment\\nYou can install the Agentless Monitoring capability of the Containerized Operations Bridge suite on K3s. This is a low footprint\\nenvironment that requires fewer IT resources as compared to the application install. You can install it on a single node K3s\\nsystem using local storage and an embedded PostgreSQL database.\\nThis topic gives you the steps for installing the \\nAgentless monitoring capability in\\n containerized Operations Bridge capabilities on a\\nK3s environment.\\nSystem requirements\\nSystem requirement \\nSpecification for this installation\\nKubernetes distribution\\nK3s\\nNumber of nodes\\nSingle node with a minimum of 8 CPUs and 16 GB memory\\nStorage\\nK3s local path storage provider. You need to have at least 250 GB of free disk space\\nOperating System\\nLinux system which supports K3s\\nThis topic assumes you have a dedicated K3s installation. You need to consider the following limitations of this installation:\\nYou can\\'t upgrade from a low footprint single node installation to multi-node K3s setup\\nYou can\\'t add Grafana or Prometheus\\nInstallation is CLI based\\nSet up prerequisites\\nStep 1: Activate a Docker Hub account \\nYou need a valid Docker Hub account to download application images from Docker Hub. If you don\\'t already have one, you\\nmust set up the Docker account and then contact \\nMicro Focus\\n with your account details. For more information about how to\\ndo this, see \\nActivate your Docker Hub account\\n.\\nStep 2: Prepare the infrastructure\\n1\\n. \\nInstall K3S environment. For more information, see the \\nK3s documentation\\n.\\n2\\n. \\nRun the following command to create a \\nkube config\\n file:\\n# mkdir ~/.kube\\n# cp /etc/rancher/k3s/k3s.yaml ~/.kube/config\\n3\\n. \\nInstall Helm for K3s. For more information on installing Helm see \\ninstalling Helm on K3s\\n.\\n4\\n. \\nRun the following commands to verify your environment:\\n# kubectl version\\nClient Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6+k3s1\", GitCommit:\"418c3fa858b69b12b9cefbcff0526f666a6236b9\", GitTreeState:\"clean\", BuildDate:\"2022-04-28T22:16:18Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6+k3s1\", GitCommit:\"418c3fa858b69b12b9cefbcff0526f666a6236b9\", GitTreeState:\"clean\", BuildDate:\"2022-04-28T22:16:18Z\", GoVersion:\"go1.17.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\\n# kubectl get pods --all-namespaces\\nNAMESPACE     NAME                                      READY   STATUS    RESTARTS   AGE\\nkube-system   coredns-b96499967-h4j7r                   1/1     Running     0          117s\\nkube-system   local-path-provisioner-7b7dc8d6f5-cmxfc   1/1     Running     0          117s\\nkube-system   svclb-traefik-6e541e4c-bk9tq              2/2     Running     0          103s\\nkube-system   metrics-server-668d979685-qqhpr           1/1     Running     0          117s\\nkube-system   traefik-7cd4fcff68-66mx6                  1/1     Running     0          103s\\n# helm version\\nversion.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\\n5\\n. \\nBefore continuing the installation, ensure the DNS name resolution is set up correctly. For Agentless Monitoring the system\\nneeds to be able to resolve its own (external) IP address, localhost must resolve to 127.0.0.1 and you need to be able to resolve\\nthe addresses of all SiteScope servers you want to manage. The SiteScope servers must be able to resolve the system used for\\ninstalling Agentless Monitoring.\\nStep 3: Install OMT tools\\nOPTIC Management Toolkit (OMT) offers the necessary tooling for installing and running containerized ITOM applications on\\nKubernetes.\\nFollow the steps on \\nDownload OMT installation package\\n to download the \"\\nOMT2xxx-xxx-15001-external-K8s.zip\\n\".\\nNotes:\\n \\nNote:\\n The single node K3s installation is only supported for a subset of the capabilities in Operations Bridge.\\nTo install the complete suite, or if you plan to install other capabilities, see the \\nInstall\\n documentation.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n463\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '448bf407ec13bd8cf5c6424824c123d7'}>,\n",
              "  <Document: {'content': '# tar -C /var/opt/cdf/offline/images_20221122153017 -c . | ctr images import - \\nThe image import will take about five minutes. After successful import, you can delete the downloaded images and the \\nallimages.tar\\nfile to free up some disk space.\\nStep 5: Create a YAML file\\nFor a single node installation, you can save additional resources by using the following \\nlowfootprint.yaml\\n:\\n# vi lowfootprint.yaml\\n# This file includes recommended setting for a single node, low footprint OpsB Agentless Monitoring deployment.\\nglobal:\\n  deployment:\\n    size: lowfootprint\\nidm:\\n  deployment:\\n    replicas: 1\\n    minAvailable: 1\\nitom-ingress-controller:\\n  deployment:\\n    replicas: 1\\nStep 6: Set up CA or trust certificate for connecting to SiteScope\\nAs the Agentless Monitoring UI needs to connect to your SiteScope Servers, you must provide the Certificate(s) to trust your\\nSiteScope servers. Assuming your SiteScope certificates are signed by a CA, you can specify the CA certificate in base64\\ndecoded format. For the following steps, assume the CA certificate is in \\n/root/INSTALL/CA_Base64.cer\\nStep 7: Deploy \\nRun the following helm command to begin the Agentless Monitoring deployment:\\n# helm install agentless --namespace opsbridge \\\\\\n    --set-file \"caCertificates.sitescope-ca\\\\.crt\"=/root/INSTALL/CA_Base64.cer \\\\\\n    -f /root/INSTALL/values_k3s.yaml \\\\\\n    -f /root/INSTALL/lowfootprint.yaml \\\\\\n    /root/INSTALL/opsbridge-suite-chart/charts/opsbridge-suite-<version>.tgz\\nOnce the command runs successfully, the deployment will start up in the background. The initial startup time will take about\\n15 minutes.\\nStep 8: Verify the installation\\n# kubectl get pods -n opsbridge\\nNAME                                                  READY    STATUS      RESTARTS   AGE\\nitom-opsb-db-connection-validator-job-bpmdz            0/1     Completed   0               12m\\nitom-reloader-6c86c8ccd7-z8qcn                         1/1     Running     0               11m\\nitom-opsb-resource-bundle-7488fb98bc-r6sp4             1/1     Running     0               11m\\nitom-prometheus-cert-exporter-5bd999bdb9-9vwln         2/2     Running     0               11m\\nbvd-redis-5649f66c8b-9f8ll                             2/2     Running     0               11m\\nitom-vault-558794455b-fbthr                            1/1     Running     0               11m\\nbvd-quexserv-6c88d8f69f-v6bnn                          2/2     Running     0               11m\\nitom-ingress-controller-5b4cc69946-7r5xt               3/3     Running     0               11m\\nitom-postgresql-5ff656ffb4-dgmvd                       2/2     Running     0               8m45s\\nitom-pg-backup-7d4599444b-6w5f9                        2/2     Running     0               11m\\nitom-opsb-database-init-oa38ppv-mpwrm                  0/1     Completed   0               11m\\ncredential-manager-64ff4c4bbf-w5csc                    2/2     Running     0               11m\\nitom-opsbridge-agentless-monitoring-5496484b4d-ffsvk   2/2     Running     0               11m\\nitom-opsbridge-monitoring-resources-54d576d749-nlzgx   2/2     Running     0               11m\\nbvd-controller-deployment-67f989c949-cppsb             2/2     Running     0               11m\\nwebtopdf-deployment-745ff58b89-ss9jh                   2/2     Running     0               11m\\nbvd-explore-deployment-7c8c865c46-4bxv2                2/2     Running     0               11m\\nbvd-www-deployment-654964d89-5hscz                     2/2     Running     0               11m\\nitom-pgbackup-enabler-job-itxnx5h-m28fz                0/1     Completed   0               11m\\nbvd-receiver-deployment-5979dfbc47-84shk               2/2     Running     0               11m\\nitom-idm-54c9b49d9-phkn4                               2/2     Running     0               11m\\nuif-upload-job-dl4jseg-xp6jc                           0/1     Completed   0               11m\\nitom-autopass-lms-7998d4f5b-cj25w                      2/2     Running     0               11m\\nitom-monitoring-admin-c7b5bc4cd-jtf5p                  2/2     Running     0               11m\\nitom-monitoring-sis-adapter-5479cb84b8-cqnrc           2/2     Running     0               11m\\nOnce the pods are up and running set up users and connect your SiteScope servers. See \\nAdminister Agentless Monitoring\\n.\\nTroubleshooting\\nCrashloop\\n or slow start up of \\nmonitoring-admin\\nProblem\\nContainerized Operations Bridge 2022.11\\nPage \\n466\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '69a1b8183afb833763ab1c28e6342fd1'}>,\n",
              "  <Document: {'content': \"Deploy Stakeholder Dashboard on-premises in K3s\\nenvironment\\nThis topic provides a checklist for preparation, deployment, and installation tasks related to the deployment of \\nStakeholder\\nDashboard\\n capability\\n.\\nPlan your deployment\\nBefore you begin, plan your deployment model and prepare the environment. Make sure that you have the deployment size, architecture,\\nand control plane (master) node allocated by referring to the sizing page.\\nThe tables in the following sections gives a list of tasks that you must complete in the given order. To view the detailed\\nprocedure for each task, click on the links in the \\nHow to perform\\n column.\\nPrepare\\nComplete the tasks in the given order to deploy the application:\\nInstall k3s\\nA system administrator performs the following tasks:\\nS/N\\nTask\\nWhere to perform\\nHow to perform\\n1\\nInstall K3s\\nOn server\\nInstall K3s\\nInstall and configure OMT\\nS/N\\nTask\\nWhere to perform\\nHow to perform\\n1\\nActivate your Docker Hub account\\nYou need a valid Docker Hub account to download Micro Focus\\nproduct images from Docker Hub.\\nYou can skip this section if you already have an active Docker Hub\\naccount that Micro Focus has authorized.\\nAny browser and email\\nclient that has internet\\nconnectivity\\nActivate your\\nDocker Hub account\\n2\\nDownload the required installation packages\\nDownload and extract the ITOM OPTIC Management Toolkit (OMT)\\npackage.\\nOn server\\nDownload the\\nrequired installation\\npackages\\n3\\nEmbedded PostgreSQL is supported for this deployment.\\nOptional step\\n:\\nExecute the following two steps only if you want to have external\\nPostgreSQL. Otherwise you can skip these steps and proceed to\\nthe next row.\\n1\\n. \\nCreate all required databases \\n(\\ncdfapi\\nserverdb,cdfidmdb,idm,\\nautopassdb,\\nand \\nbvd\\n) using PostgreSQL. \\n2\\n. \\nEnable TLS in PostgreSQL \\nOn server\\nPrepare external\\nPostgreSQL\\nEnable TLS in\\nPostgreSQL\\n4\\nInstall OMT\\nInstall the ITOM OPTIC Management Toolkit (OMT).\\nOn server\\nInstall OMT \\nInstall and configure OpsBridge application\\nS/N\\nTask\\nWhere to perform\\nHow to perform\\n1\\nDownload the installer\\nOn server\\nDownload the\\ninstaller\\n2\\nEdit environment variables\\nOn server\\nEdit environment\\nvariables\\n3\\nConfigure K3s environment\\nOn server\\nConfigure K3s\\nenvironment\\n4\\nLoad the required container images\\nExecute this step\\n, \\nonly if you're installing on a system that has \\nno\\ninternet access\\n or if you don't want to give kubernetes access to your\\ndocker account.\\nOn server and on a\\nsystem that has\\ninternet access\\nLoad Container\\nimages\\n5\\nConfigure bvd.yaml\\nUpdate all your deployment configuration values in the \\nbvd.yaml\\n file.\\nOn server\\nConfigure\\nbvd.yaml\\n6\\nUpdate secrets\\nConfigure the passwords or update the passwords in \\nbvd.yaml\\n file.\\nOn server\\nUpdate secrets\\nContainerized Operations Bridge 2022.11\\nPage \\n469\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb63718bdb286c61ec400e9883ae92c7'}>,\n",
              "  <Document: {'content': \"You observe a slow start up of \\nmonitoring-admin\\n and the container logs show:\\nitom-monitoring-admin 2022-07-10 14:32:00,835 INFO 200-setup-user:setup_user Created user: serviceuser\\nitom-monitoring-admin 2022-07-10 14:32:00,847 INFO 200-setup-user:setup_user Created group: servicegroup\\nitom-monitoring-admin 2022-07-10 14:32:00,873 INFO 999-service:source Running as user 1999: uid=1999(serviceuser) gid=1999(servicegroup) groups=1999(servicegroup)\\nitom-monitoring-admin 2022-07-10 14:32:00,881 WARN 999-service:source Not running with read-only filesystem\\nitom-monitoring-admin 2022-07-10 14:32:00,897 WARN 200-calc-heap:source heap space (773741824) exceeded maximum (500000000) \\nitom-monitoring-admin 2022-07-10 14:32:00,935 INFO utils:importKey Importing key into keystore /tmp/home/secrets/server-keystore: /var/run/secrets/boostport.com/…\\nCause\\nThe reason for the slow key import is a low entropy for generating random numbers on some older systems.\\nSolution\\nInstall and configure \\nhaveged\\n (\\nHardware Volatile Entropy Gathering and Expansion\\n). Follow the steps for CentOS 7.9:\\nyum install haveged\\nsystemctl enable haveged\\nsystemctl start haveged\\nhaveged\\n is part of the Extra Packages for Enterprise Linux (EPEL) repository. If this isn't enabled already, you might need to\\ninstall using:\\nsudo yum install epel-release\\nOnce installed, \\nmonitoring-admin\\n should start up within five minutes.\\n \\nSlow UI response\\nProblem\\nThe low footprint system sizing (8 CPUs, 16 GB) assumes a Linux server system with no other workloads on the system.\\nSolution\\nYou shouldn't run a Linux with a desktop environment enabled unless you add resources. Additionally, ensure your system\\nisn't using swap space. Either disable swap or increase the system memory.\\nMissing container images\\nProblem\\nPods don't startup because of missing images.\\nSolution\\nCheck your disk space. K3s images are stored under \\n/var/\\n and if the disk space used in \\n/var\\n exceeds the eviction threshold\\n(typically 85%), Kubernetes starts deleting the unused images. If this happens during image loading, your containers won't\\nstart up.\\nContainerized Operations Bridge 2022.11\\nPage \\n467\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f6436008614b9d0a7035ebd06cf4cfda'}>,\n",
              "  <Document: {'content': 'Install Stakeholder Dashboard in K3s environment\\nK3s is a highly available, certified Kubernetes distribution designed for production workloads. K3s is a single less than 40 MB\\nbinary that reduces the dependencies and steps needed to install, run and auto update a production Kubernetes cluster.\\nThis section provides information required to prepare your environment for installing BVD (Stakeholder Dashboard capability)\\nin K3s environment. Before you begin, ensure that you have the deployment architecture planned and have the server node\\nallocated and perform the following tasks:\\n1\\n. \\nPlan Stakeholder Dashboard deployment\\n2\\n. \\nDeploy Stakeholder Dashboard\\nNote\\n: Containerized OpsBridge supports embedded PostgreSQL for production in low footprint environments\\n. \\nContainerized Operations Bridge 2022.11\\nPage \\n468\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd043e9c950d5ca7898b65c3d6898dc7e'}>,\n",
              "  <Document: {'content': \"7\\nDeploy Stakeholder Dashboard Capability \\n \\nOn server\\nDeploy\\nStakeholder\\nDashboard\\nCapability \\n8\\nVerify installation \\nCheck Pod status to find out whether the deployment was successful.\\nOn server\\nVerify installation \\nS/N\\nTask\\nWhere to perform\\nHow to perform\\nPost-installation task\\nCreate stakeholder dashboard in BVD\\nThe stakeholder dashboard gives you visual information of live data using tables, charts, and widgets. You can stream Real\\ntime data from any data source in JSON format via HTTP post. \\nCreate stakeholder dashboard in BVD\\nUninstall BVD in K3s\\nIf you want to uninstall BVD deployed in K3s run the following command:\\nhelm uninstall <deployment name> -n <suite namespace> --no-hooks\\nFor example:\\nhelm uninstall bvd -n bvd-helm --no-hooks\\nMigrate on-premises Stakeholder Dashboard in K3s environment to full suite\\ninstallation\\nAn on premise Stakeholder Dashboard in K3s environment can't get upgraded in place into a full suite installation. The\\nenvironment needs to get migrated to a full installation to install other capabilities. This is only possible for production\\ninstallations with external PostgreSQL databases.\\nThe migration to a full suite installation comprises the following steps:\\nBackup database\\nInstall containerized OpsBridge Suite with Stakeholder Dashboard capability\\nThe migration will contain only the BVD configuration. For example, The migration does'nt include IdM configuration.\\nBack up the BVD database\\nRun the following command to bring down the K3s based installation:\\n/opt/cdf/scripts/cdfctl.sh runlevel set -l DB\\nBack up the \\nbvd\\n database. See the vendor documentation of PostgreSQL for details.\\nInstall \\nContainerized Operations Bridge\\n with BVD capability\\nEnsure the following while installing containerized Operations Bridge:\\nDon't create new \\nbvd\\n database. Ensure that the BVD K3s based installation is down.\\nWhen specifying values in values.yaml or using the \\nOpsBridgeInstallScript.sh\\n script, point the installer to the existing BVD\\ndatabase for the Stakeholder Dashboard capability.\\nFollow the instructions to install containerized Operations Bridge. See Install.\\nContainerized Operations Bridge 2022.11\\nPage \\n470\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fbfc458f4c469d5947c86352f4496746'}>,\n",
              "  <Document: {'content': 'Install BVD in K3s\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator \\nRoot\\nFollow the documentation on the Rancher website \\nhttps://rancher.com/docs/k3s/latest/en/installation/install-options/\\n to install\\nK3s on your system.\\nContainerized Operations Bridge 2022.11\\nPage \\n471\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e82779aa20a68e943a7dc7ea559389cf'}>,\n",
              "  <Document: {'content': 'Enable TLS PostgreSQL\\nTo enable TLS for encrypted communication with PostgreSQL, you can use CA signed certificate or a self-signed certificate.\\nGenerate a CA signed certificate\\nTo enable TLS you need a CA signed certificate for PostgreSQL database. The Subject Alternative Name (SAN) of the server\\ncertificate must include the FQDN of all the PostgreSQL high availability nodes. Otherwise, use a wildcard in the Common\\nName (CN) that matches the FQDN of the PostgreSQL high availability nodes. \\nThe certificate files must be in .crt format (for example, server_signed.crt). The trust chain must include the root\\ncertificate and intermediate certificates (if any). \\nGenerate a self-signed certificate\\nThis section provides an example to enable TLS for encrypted communication with PostgreSQL database server on Linux using\\nself-signed certificate. \\nPerform the following steps as a Postgres user.\\nNote\\n: The paths shown in the examples are with respect to PostgreSQL 10, you need to change the paths according to your\\nPostgreSQL version.\\n1\\n. \\nCreate a self-signed certificate.\\n1\\n. \\nCreate the certificate and the key\\ncd /var/lib/pgsql/10/data/\\nopenssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout server.key -out server.crt -subj \"/CN=${HOST}\"\\nReplace the \\n${HOST}\\n with the hostname of the machine. For example: \\nmydatabase.myhost.com\\n2\\n. \\nSet permissions for the files\\nchmod 400 server.*;chown postgres:postgres server.*\\n2\\n. \\nChange \\npostgresql.conf\\ncd /var/lib/pgsql/10/data/\\nEdit \\npostgresql.conf\\n and search for\\n ssl\\n.\\nYou will see the following property commented out and with a value of off. Uncomment the property and set the value to\\non.\\n \\nUncomment the properties for \\nssl_cert_file\\n and \\nssl_key_file\\n.\\nNote: \\nEnsure the file names values match the private key and certificate created in the previous step.\\n3\\n. \\nChange \\npg_hba.conf\\ncd /var/lib/pgsql/10/data/\\nEdit \\npg_hba.conf\\n and add the following line at the end\\nhostssl all all 0.0.0.0/0 md5\\nEdit the following line and change the value of \\nident\\n to trust\\nhost    all             all             127.0.0.1/32            ident\\nIf you want the database user \\npostgres\\n to use SSL, edit \\npg_hba.conf\\n and specify \\npostgres\\n instead of \\nall\\n for the \\nUSER\\nvalue for \\nTYPE\\n \\nhostssl\\n.\\n4\\n. \\nAs root, restart PostgreSQL server\\nsystemctl restart postgresql-10.service\\nsystemctl status postgresql-10.service\\n5\\n. \\nEnsure to enable \\npsql -h `hostname` -U postgres\\n in SSL and you should see something like below:\\n6\\n. \\nOptional.\\n Create a user with \\ndba\\n privileges for access\\ncreate user cdfUser with superuser password \\'SomePassword123#\\';\\nContainerized Operations Bridge 2022.11\\nPage \\n475\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd01dbdd72188852e08e1628317905c02'}>,\n",
              "  <Document: {'content': 'Install OMT\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nTo install ITOM OPTIC Management Toolkit (OMT), follow these steps:\\n1\\n. \\nLog on to the node as a root user or sudo user.\\n2\\n. \\nNavigate to OMT_External_K8s_20XX.xx\\n-xxx\\n sub directory inside the <\\nInstall temporary directory\\n>.\\n3\\n. \\nRun the following command to install the OMT tools:\\n./install --k8s-provider generic --capabilities Tools=true,Monitoring=false,LogCollection=false,DeploymentManagement=false,ClusterManagement=false --cdf-home /opt/cdf-home\\nIn the following command, the following parameters are mandatory:\\n--k8s-provider:\\n The Kubernetes provider. Generic in this case\\n--capabilities:\\n \\nA list of capabilities to install. For this use case, need to install only the tools. Following are the values for the\\nparameter \"\\ncapabilites\"\\nTools\\n=true\\nMonitoring\\n=false,\\nLogCollection\\n=false\\nDeploymentManagement\\n=false\\nClusterManagement\\n=false\\n--cdf-home\\n \\nThe path where the OMT tools should get installed. Example: \\n--cdf-home /opt/cdf-home\\nFor a full list of options for this command, run the following command:\\n ./install -h\\nIf you receive error messages when you run the installation command, refer to the installation log in the \\n$TMP_FOLDER\\ndirectory.\\nWhen you have fixed the errors, run the installation command again to continue the OMT installation. Note that you must run\\nthe installation command together with the same options and configurations.\\nContainerized Operations Bridge 2022.11\\nPage \\n476\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5a6fef3673e079e3c88a3bca581744b7'}>,\n",
              "  <Document: {'content': \"Download the installer\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nDownload and extract the contents of the OpsBridge application package\\nTo download and verify the Operations Bridge suite installation packages, perform the following steps:\\n1\\n. \\nDownload the following installation package from the \\nMicro Focus Software \\n website:\\nThe Operations Bridge application Composition Chart: \\nopsbridge-suite-chart-2022.05.0.zip\\n2\\n. \\nChoose a directory as your <\\nInstall temporary directory\\n>\\n \\nsuch as /tmp on server .\\n \\nThis directory must have at least\\n15 GB free space. Copy the packages and the public keys to this directory.\\n3\\n. \\nRun the following command to unzip the \\nopsbridge-suite-chart-2022.05.0.zip\\n file.\\n4\\n. \\nunzip opsbridge-suite-chart-2022.05.0.zip\\nThe unzipped file will have these directories and files under \\nopsbridge-suite-chart\\n: \\nDirectories/files\\nDescription\\ncharts\\nSuite install chart (\\nopsbridge-suite-2022.05.0.tgz\\n). Don't extract this.\\nsamples\\nvalues.yaml\\nVerify suite package\\nVerify the Operations Bridge suite installation package. Skip this step if you don't want to verify the package:\\n1\\n. \\nUsing a web browser, visit the \\nMicro Focus Software Licenses and Downloads\\n website. Agree to the terms and conditions,\\nand then download the \\nMF_public_keys.tar.gz\\n package to a local directory.\\n2\\n. \\nRun the following commands to extract the public keys from the package:\\ngunzip MF_public_keys.tar.gz\\ntar -xvf MF_public_keys.tar\\n3\\n. \\nCopy the public key \\nhelm-public-key.asc\\n to a local folder.\\n4\\n. \\nAdd the public key to the \\nGPG\\n keyring:\\ngpg --import helm-public-key.asc\\nOn machines that use \\nkbx\\n format, you may get an error as follows:\\ngpg: no valid OpenPGP data found.\\ngpg: Total number processed: 0\\nIf you get this error, export the key as \\npubring.gpg\\n:\\ngpg --export > ~/.gnupg/pubring.gpg\\n5\\n. \\nNavigate to the\\n $HOME/opsbridge-suite-chart/charts\\n directory:\\ncd $HOME/opsbridge-suite-chart/charts\\n6\\n. \\nRun the following command to verify the signature of the helm chart files:\\nhelm verify <chart-name>.tgz\\n7\\n. \\nYou will get a message similar to the following indicating that successful verification:\\nSigned by: Micro Focus Group Limited (GPG Key for Helm Chart Signing) xxxxx@microfocus.com\\n  Using Key With Fingerprint: xxxxx\\n  Chart Hash Verified: sha256:xxxxx\\nContainerized Operations Bridge 2022.11\\nPage \\n472\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6a5681240dba4556caf54beb6c71a6ce'}>,\n",
              "  <Document: {'content': 'Configure K3s environment\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nAfter installing OMT, you must create a new Kubernetes namespace and tag the node.\\n1\\n. \\nRun the following command to get a list of all nodes (there should be only one node) in your K3s environment:\\nkubectl get nodes\\n2\\n. \\nRun the following command to label the node for BVD. Replace \\n<nodename>\\n with the name of the node returned by the\\nprevious command.\\nkubectl label node <nodename> Worker=label\\n3\\n. \\nCreate a namespace for BVD (Stakeholder Dashboard capability) deployment. This documentation will use \"\\nbvd-helm\\n\" as\\nthe namespace name. You can choose any valid name for namespace except \"\\nkube-system\\n\" which is already in use.\\nkubectl create ns bvd-helm\\nContainerized Operations Bridge 2022.11\\nPage \\n478\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '924bfcb2f4ff7655743f59dd64941fd'}>,\n",
              "  <Document: {'content': \"Prepare external Postgres for Stakeholder Dashboard\\nExternal PostgreSQL\\nThis section provides you information to create PostgreSQL databases for the following:\\nidm\\nautopass\\nbvd\\nRole\\nLocation\\nPrivileges required\\nDatabase administrator\\nDatabase server\\nDatabase administrator\\nInstall an external PostgreSQL instance. For details, see the \\nPostgreSQL documentation\\n.\\nMake sure to install \\npostgres-contrib\\n or \\npostgresql<version>-contrib\\n \\npackage on the database server depending on the PostgreSQL\\nversion you are using.\\nExample:\\nyum install postgresql11-contrib -y\\nor \\nyum install postgresql12-contrib -y\\nTo create the databases, you can either use the DB script or follow the manual steps.\\nNote\\n: When you create entities like database, schema, users, etc in PostgreSQL, specify them in lowercase only.\\nCreate the databases using the DB script\\nThe \\nDBSQLGenerator.sh \\nenables you to generate a SQL script that you can run to create the databases (\\nidm, autopass,\\nbvd\\n). The suite zip (\\nopsbridge-suite-chart-2022.05.0.zip\\n) contains the \\nDBSQLGenerator.sh \\nfile under \\nscripts \\ndirectory.\\nFollow the steps:\\n1\\n. \\nGo to the \\nscripts \\ndirectory and execute the \\nDBSQLGenerator.sh\\n:\\n./DBSQLGenerator.sh \\nThe script will prompt you to enter the required values. For all the queries in the script, to use the default value for the\\nparameter, press the enter key without entering any value, \\nThis script generates \\nCreateSQL.sql\\n and \\nRemoveSQL.sql\\n, in the current working directory.\\n2\\n. \\nCopy the \\nCreateSQL.sql\\n to the database server. Execute \\nCreateSQL.sql\\n with database admin privileges: \\nRun the following commands on the PostgreSQL server to create the users and databases:\\nsu - postgres\\npsql -f <Path where script is copied>/CreateSQL.sql\\nExample:\\n psql -f /root/script/CreateSQL.sql\\nImportant: \\nCreateSQL.sql \\ncontains the specified \\npassword in plain text\\n, based on your security policies you can delete\\nthis script after execution or create the databases manually.\\nCreate the databases manually\\nLog in to the PostgreSQL server and then run the following command to connect to the PostgreSQL instance: \\npsql -U <postgres admin>\\nCreate a database for idm (OMT IdM)\\nRun the following queries:\\nCREATE USER <idmuser> login PASSWORD '<idm_user_password>';\\nGRANT <idmuser> TO <postgres admin>;\\nCREATE DATABASE <idmdb> WITH owner=<idmuser>;\\n\\\\c <idmdb>;\\nALTER SCHEMA public OWNER TO <idmuser>;\\nALTER SCHEMA public RENAME TO <idmschema>;\\nREVOKE ALL ON SCHEMA <idmschema> from public;\\nGRANT ALL ON SCHEMA <idmschema> to <idmuser>;\\nALTER USER <idmuser> SET search_path TO <idmschema>;\\nCreate a database for autopassdb\\nContainerized Operations Bridge 2022.11\\nPage \\n473\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '91f515028f6136124873c6279b9cf916'}>,\n",
              "  <Document: {'content': 'Edit environment variables\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nIn case you need to use a proxy to access the internet, set the HTTP proxy environment variables before running the\\ninstallation script.\\nFor example:\\nexport http_proxy=http://web-proxy.example.com:8080\\nexport https_proxy=https://web-proxy.example.com:8080\\nTo be able to use the CDF tools during the installation, you need to specify the following environment variables:\\nexport CDF_HOME=/opt/cdf-home\\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml\\nexport PATH=$PATH:$CDF_HOME/bin:$CDF_HOME/scripts\\nNote\\n : Add the environment variables mentioned in the earlier step to your \\n.bashrc\\n or similar file, to have them\\navailable at your next login.\\nContainerized Operations Bridge 2022.11\\nPage \\n477\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '41030bb4fd8b3460f82ed4a57e8ea080'}>,\n",
              "  <Document: {'content': 'Run the following queries:\\nCREATE USER <autopassuser> login PASSWORD \\'<autopass_user_password>\\';\\nGRANT <autopassuser> TO <postgres admin>; \\nCREATE DATABASE <autopassdb> OWNER <autopassuser>;\\n\\\\c <autopassdb>;\\nGRANT ALL PRIVILEGES ON DATABASE <autopassdb> TO <autopassuser>;\\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\\nALTER SCHEMA public OWNER TO <autopassuser>;\\nALTER SCHEMA public RENAME TO <autopassschema>;\\nREVOKE ALL ON SCHEMA <autopassschema> from public;\\nGRANT ALL ON SCHEMA <autopassschema> to <autopassuser>;\\nGRANT ALL PRIVILEGES ON DATABASE <autopassdb> TO <autopassuser>;\\nALTER USER <autopassuser> SET search_path TO <autopassschema>;\\nCreate a database for bvd\\nRun the following queries:\\nCREATE USER <bvduser> login PASSWORD \\'<bvd_user_password>\\';\\nGRANT <bvduser> TO <postgres admin>; \\nCREATE DATABASE <bvddb> OWNER <bvduser>;\\n\\\\c <bvddb>;\\nGRANT ALL PRIVILEGES ON DATABASE <bvddb> TO <bvduser>;\\nCREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\\nALTER SCHEMA public OWNER TO <bvduser>;\\nALTER SCHEMA public RENAME TO <bvdschema>;\\nREVOKE ALL ON SCHEMA <bvdschema> from public;\\nGRANT ALL ON SCHEMA <bvdschema> to <bvduser>;\\nGRANT ALL PRIVILEGES ON DATABASE <bvddb> TO <bvduser>;\\nALTER USER <bvduser> SET search_path TO <bvdschema>;\\nChecklist\\nMake a note of the following information after creating all the databases (\\nidm, autopassdb, bvd\\n). You will need this\\ninformation during the installation.\\nDatabase host\\nDatabase port\\nDatabase names\\nDatabase login users/passwords\\nNote\\n: Suggesting you to enable TLS, for more information see, Enable TLS for PostgreSQL.\\nContainerized Operations Bridge 2022.11\\nPage \\n474\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4dbfa454959f6d4aac9a1eb902d66251'}>,\n",
              "  <Document: {'content': 'global.dock\\ner.imagePul\\nlSecret\\nNot defined\\nName of the secret to login to the docker registry.\\nFor example:\\nCreate a secret \\nregistrypullsecret:\\nkubectl create secret docker-registry registrypullsecret -n <namespace> --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-password> --docker-email=<your-email>\\nwhere:\\n<your-registry-server>\\n is your Private Docker Registry FQDN. Use https://index.docker.io/v2/ for DockerHub.\\n<your-name>\\n is your Docker username.\\n<your-password>\\n is your Docker password.\\n<your-email>\\n is your Docker email.\\nYou have successfully set your Docker credentials in the cluster as a Secret called \\nregistrypullsecret\\nImagepullsecret\\n is a secret that holds the username/password of a docker registry (internal or external).\\nFor the local cluster registry, you can access images without a username/password. Hence you can leave this blank.\\nIf you have configured an external registry and want to use it directly (without doing a download/upload of images), you can specify the registry and give the\\nusername/password in this secret.\\nglobal.dock\\ner.imagePul\\nlPolicy\\nDefault value:\\nIfNotPresent\\nDocker image pull policy.\\nParameter\\ngroup\\nValues\\nDescription\\nVertica database details\\nPlease refer \\nConfigure values.yaml\\n file section under installation in suite documentation for Vertica database details.\\nRelational database details\\nEdit this section only in case of external relational databases. The default value for \\nglobal.database.internal\\n is false. If you want to\\nuse embedded PostgreSQL you would need to set \\nglobal.database.internal \\nto true, you don\\'t need to change any other\\nparameters in this section. \\nParameter group\\nValues\\nDescription\\nglobal.database.internal\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\nfalse\\nEdit this section only for external relational\\ndatabases. The default for \\nglobal.database.inte\\nrnal\\n is \"\\nfalse\\n\". If you want to use embedded\\nPostgreSQL you would need to set \\nglobal.data\\nbase.internal \\nto \"\\ntrue\\n\", you don\\'t need to\\nchange any other parameters in this\\nsection.\\nSet \\ndatabase.internal\\n to \\nfalse\\n to use an\\nexternal PostgreSQL.\\nSet \\ndatabase.internal\\n to \\ntrue\\n to use the\\nembedded PostgreSQL database. \\nglobal.database.host\\nNot defined\\nExternal PostgreSQL host name.\\nglobal.database.port\\nNot defined\\nDB Port. Required for PostgreSQL.\\nglobal.database.tlsEnabled\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\ntrue\\nIf TLS is enabled, the \\nPostgreSQLServer\\nCertificate\\n must be provided later while\\nrunning the install.\\nPersistent volume \\nIf you don\\'t find this parameter, you need to create and set the value. Set this parameter value to \\nReadWriteOnce\\n in the YAML\\nfile. \\nParameter group\\nValues\\nDescription\\nglobal.persistence.accessMode\\nReadWriteOnce\\nSet persistence access mode.\\nDatabase parameters for idm, autopass, and bvd\\nContainerized Operations Bridge 2022.11\\nPage \\n482\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2955c05dd5f032d1ca246550844bed7a'}>,\n",
              "  <Document: {'content': 'Load Container images\\nNow download the required container images. OMT provides a utility for downloading just the required set of images. \\n# /root/cdf/tools/generate-download/generate_download_bundle.sh \\\\\\n     --chart /root/INSTALL/opsbridge-suite-chart/charts/opsbridge-suite-2.3.0+20220500.528.tgz \\\\\\n     -H /root/INSTALL/values_k3s.yaml -S -o hpeswitom -d /tmp/\\nThis will generate an \"offline download.zip\" bundle. Copy this to a Linux system, which has access to the internet (docker.hub).\\nIf the system you are installing on has internet access, then you can continue with the same system. Unzip the bundle in the\\n/tmp directory and download the required images:\\n# unzip offline_download.zip\\n# cd offline-download/\\n# chmod u+x downloadimages.sh\\n# ./downloadimages.sh -r registry.hub.docker.com -o hpeswitom -u <YOUR DOCKER ACCOUNT>\\nContacting Registry: https://registry.hub.docker.com ../[Failed] \\nRetrying contacting https://registry.hub.docker.com. please make sure your user name, password and network/proxy configuration are correct.\\nPassword:\\nContacting Registry: https://registry.hub.docker.com ..[OK]\\nStart downloading ...\\n! Warning: Please check suite sizing documentation and make sure you have enough disk space for downloading suite images.\\nContinue?[Y/N]?y\\nDownloading image [1/23] hpeswitom/itom-autopass-lms:2022.05-2022042615 ...[OK]\\nDownloading image [2/23] hpeswitom/itom-bvd:11.9.17 ...[OK]\\nDownloading image [3/23] hpeswitom/itom-cacert-bundler:0.0.1-1.commit-8db3723 ...[OK]\\nDownloading image [4/23] hpeswitom/itom-credential-manager:1.17.0.13 ...[OK]\\nDownloading image [5/23] hpeswitom/itom-data-migrator:0.8.0-5.commit-9c313e2 ...[OK]\\nDownloading image [6/23] hpeswitom/itom-idm:1.35.0-690 ...[OK]\\nDownloading image [7/23] hpeswitom/itom-k8s-sidecar:1.1.0-0020 ...[OK]\\nDownloading image [8/23] hpeswitom/itom-monitoring-admin:1.2.0-164 ...[OK]\\nDownloading image [9/23] hpeswitom/itom-monitoring-gateway:1.1.0-494 ...[OK]\\nDownloading image [10/23] hpeswitom/itom-monitoring-sis-adapter:1.2.0-104 ...[OK]\\nDownloading image [11/23] hpeswitom/itom-nginx-ingress:0.22.0-0038 ...[OK]\\nDownloading image [12/23] hpeswitom/itom-opsbridge-database-init:1.1.0-11 ...[OK]\\nDownloading image [13/23] hpeswitom/itom-opsbridge-dbvalidator:2.3.0-13 ...[OK]\\nDownloading image [14/23] hpeswitom/itom-pg-backup:1.1.101 ...[OK]\\nDownloading image [15/23] hpeswitom/itom-postgresql:12-00106 ...[OK]\\nDownloading image [16/23] hpeswitom/itom-redis:11.9.17 ...[OK]\\nDownloading image [17/23] hpeswitom/itom-reloader:1.0.0-0044 ...[OK]\\nDownloading image [18/23] hpeswitom/itom-static-files-provider:2.3.0-2151 ...[OK]\\nDownloading image [19/23] hpeswitom/itom-tools-base:1.1.0-0029 ...[OK]\\nDownloading image [20/23] hpeswitom/itom-web-to-pdf:11.9.17 ...[OK]\\nDownloading image [21/23] hpeswitom/kubernetes-vault-init:0.16.0-0043 ...[OK]\\nDownloading image [22/23] hpeswitom/kubernetes-vault-renew:0.16.0-0043 ...[OK]\\nDownloading image [23/23] hpeswitom/vault:0.20.0-0065 ...[OK]\\nDownload completed in 226 seconds.\\nDownload-process successfully completed.\\nSuccessfully downloaded the images  to /var/opt/cdf/offline/images_20220603091229/.\\nPlease refer to /var/opt/cdf/offline/images_20220603091229/downloadimages-20220603090714.log for more detail\\nIn case you did the download on a separate system, copy the images folder to the system you want to install Stakeholder\\nDashboard on.\\nThen import the images using (adjust the image directory to the location returned be the previous command):\\n# tar -C /var/opt/cdf/offline/images_20220603091229/ -c . |  ctr image import -\\nunpacking docker.io/hpeswitom/itom-autopass-lms:2022.05-2022042615 (sha256:d2fd7f72c5b62a0facf0afa2777c2828f382d667d9a01b9a54c0e40b44191889)...done\\nunpacking docker.io/hpeswitom/itom-bvd:11.9.17 (sha256:888aa38a1d89b330ad7de8d78677a32238d3f3ffde50e0c7cd290083a311fc49)...done\\nunpacking docker.io/hpeswitom/itom-cacert-bundler:0.0.1-1.commit-8db3723 (sha256:37d5f63d94dd59f7c13e0944466a7ae8531d6112975596219c62c3fea73ac4dc)...done\\n...\\n#\\nThe image import will take around 5 minutes. After successful import you can delete the downloaded images to free up some\\ndisk space.\\nNote\\n: Execute this step\\n, \\nonly if you\\'re installing on a system that has \\nno internet access\\n or if you don\\'t want\\nto give kubernetes access to your docker account.\\nContainerized Operations Bridge 2022.11\\nPage \\n480\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '369206576718b32c500cee177539c23e'}>,\n",
              "  <Document: {'content': \"Generate secrets for Stakeholder Dashboard\\nAdministrator passwords are set during the deployment using a secrets file. \\nAll the password must be specified in a base64 encoded format. When using special characters, be sure that these aren't\\nreplaced by your shell. To test whether your password encodes correctly, you can use the following example:\\n# echo -n ChangeMe2$ | base64 | base64 -d ; echo\\nChangeMe2$\\n#\\nIf the password returned differs from the original, you might need to escape control characters.\\nFollow these steps to create a secrets file. Please keep the indentation and don't use tabs. Replace all occurrences\\nof <YOUR_BASE64_ENCODED_PASSWORD>.\\n# echo -n  My_secret_passw0rd$ | base64\\nTXlfc2VjcmV0X3Bhc3N3MHJkJA==\\n# vi secrets.yaml\\nsecrets:\\n  idm_opsbridge_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  bvd_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  IDM_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  AUTOPASS_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  BVD_DB_USER_PASSWORD_KEY:  <YOUR_BASE64_ENCODED_PASSWORD>\\n  idm_admin_admin_password:  <YOUR_BASE64_ENCODED_PASSWORD>\\nNote\\n: The passwords must have a minimum of 8 and a maximum of 32 characters in length. The passwords\\nshould include an upper, lower case character, a number, and a special character. If any of the passwords don't\\nmeet the password policy the installation will fail. \\nRecommendation\\n: Create a different password for each of the keys.\\nNote\\n: If you're using external postgres database for the deployment, you need to update the secrets.yaml file\\nwith the passwords\\n \\nthat are created for \\nidm, autopassdb, bvd \\nin the\\n \\nPrepare\\n \\nExternal Postgres\\n step\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n479\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad1bfc3ff31e7066bd907481d6b7103b'}>,\n",
              "  <Document: {'content': 'Configure BVD yaml file\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nDon\\'t change any indentation in the YAML file. Update the required values and keep the YAML syntax. Don\\'t change the\\nparameters which have explicit comment \\n[DON\\'T CHANGE]\\n in the values.yaml file.\\nNavigate to <\\nInstall temporary directory\\n> and run the following command to copy values.yaml file to \\nbvd.yaml\\n: \\ncp opsbridge-suite-chart/samples/values.yaml bvd.yaml\\nEdit the file \\nbvd.yaml\\n file and adjust the following values:\\nEnd User License Agreement\\n (EULA)\\nYou must accept the End User License Agreement (EULA) to deploy BVD. By default, the value of the \\nacceptEula\\n is false, set it\\nto true.\\nParameter\\ngroup\\nValues\\nDescription\\nacceptEula\\nPossible\\nvalues: \\ntrue\\n/\\nfalse\\nDefault value:\\n false\\nAccept \\nMicro Focus\\n \\nEULA\\n to proceed further.\\nYou can find the End User License Agreement (\\nEULA\\n) at \\nSoftware\\nLicense\\n.\\nServices\\nAll the Capabilities deployment is disabled by default, you must remove the \\'\\'\\n#\\n\" character to remove the comment for the\\nspecific line to deploy. Each tag represents an Operations Bridge suite capability. Set \\ntags.stakeholderDashboard \\nvalue to true to\\nenable the capability. Set all other capabilities value to false.\\nParameter group\\nValues\\nDescription\\ntags.stakeholderDashboard\\nPossible\\nvalues: \\ntrue/false\\nDefault value: \\ntrue\\nEnable/disable deployment of the Stakeholder Dashboard\\ncapability. \\nInstalls the Stakeholder Dashboard capability with the BVD\\ncomponent.\\nTo deploy, you must delete the \\'\\'\\n#\\n\" character for this\\nparameter.\\nExternal access host\\nThe installation fails without these mandatory parameter values. Each deployment has unique values.\\nParameter\\ngroup\\nValues\\nDescription\\nglobal.external\\nAccessHost\\nSet it to the FQDN of the machine where you\\'re installing BVD\\nThe hostname/FQDN that you\\ncan access externally. \\nglobal.external\\nAccessPort\\nChoose a port which is later used to access BVD. In a default installation,\\nK3s allows only ports between 30000 and 32767\\nThe port you can access\\nexternally.\\nDocker repository\\nConfigure these parameters only, if you didn\\'t download the container images in \\nLoad the required container images\\n step. \\nParameter\\ngroup\\nValues\\nDescription\\nglobal.dock\\ner.registry\\nlocalhost:5000\\nThe Docker registry URL.\\nglobal.dock\\ner.orgName\\nDefault value:\\nhpeswitom\\nDocker registry orgName.\\nContainerized Operations Bridge 2022.11\\nPage \\n481\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c5bcb0a4bbb56c0a750d9f5a9beaf124'}>,\n",
              "  <Document: {'content': 'This section deals with specific parameters for IDM, Autopass, and BVD. For embedded PostgreSQL you need not change any\\nvalue for these parameters. Set value for these parameters only, if you\\'re using external PostgreSQL. See, \\nPrepare external\\npostgres\\n page for the required database and credentials.\\nParameter group\\nValues\\nDescription\\nidm.deployment.database.db\\nName\\nDefault value: \\nidm\\n \\nGive database name for \\nidm\\n.\\nidm.deployment.database.us\\ner\\nDefault value: \\nidm\\nGive the user name for \\nidm \\ndatabase.\\nidm.deployment.database.us\\nerPasswordKey\\nDefault value:\\nIDM_DB_USER_PASSWORD_KEY\\nRefers to the password for \\nidm \\ndatabase that is\\ncreated in \"Generate secrets\" step. \\nautopass.deployment.databa\\nse.dbName\\nDefault value: \\nautopass\\nGive database name for \\nautopass\\n.\\nautopass.deployment.databa\\nse.user\\nDefault value: \\nautopass\\nGive the user name for \\nautopass \\ndatabase.\\nautopass.deployment.databa\\nse.schema\\nDefault value: \\nautopassschema\\nSchema name should be what has been set in\\nduring the PostgreSQL prepare step for \\nautopassd\\nb\\n.\\nIf Databases are created using the script, then\\ndon\\'t change the value.\\nautopass.deployment.databa\\nse.userPasswordKey\\nDefault value:\\nAUTOPASS_DB_USER_PASSWORD_KEY\\nRefers to the password for \\nautopass\\ndatabase that is created in \"Generate secrets\"\\nstep.\\nbvd.deployment.database.db\\nName\\nDefault value: \\nbvd\\nGive \\nbvd \\ndatabase name.\\nbvd.deployment.database.us\\ner\\nDefault value: \\nbvd\\nGive the user name for \\nbvd\\n database.\\nbvd.deployment.database.us\\nerPasswordKey\\nDefault value:\\nBVD_DB_USER_PASSWORD_KEY\\nRefers to the password for \\nbvd\\n database that is\\ncreated in \"Generate secrets\" step.\\nSecrets\\nYou must provide all the required secrets password in \\nBase64 encoded \\nformat.\\nParameter group\\nValues\\nDescription\\nsecrets.idm_opsbridge_ad\\nmin_password\\nAdmin Password for IDM admin user. This password will be used to log into IDM UI.\\nThe password must meet the following requirements:\\nLength must be at least 8 characters\\nLength cannot exceed 64 characters\\nMust contain 1 or more upper case characters\\nMust contain 1 or more lower case characters\\nMust contain 1 or more digit (0-9) characters\\nMust contain 1 or more special characters in: -+\"?/.,<>:;[]{}`~!@#%^&*()_=|$\\nsecrets.AUTOPASS_DB_US\\nER_PASSWORD_KEY\\nPassword for \\nAUTOPASS\\n database (PostgreSQL) user. The application will give the\\nusername in Helm \\nvalues.yaml\\n under \\ndeployment.database.user\\n of \\nAUTOPASS\\n.\\nsecrets.BVD_DB_USER_PA\\nSSWORD_KEY\\nPassword for BVD database (PostgreSQL) user. The application will give the username\\nin Helm \\nvalues.yaml\\n under \\ndeployment.database.user\\n of BVD.\\nsecrets.CM_DB_PASSWD_K\\nEY\\nPassword for PostgreSQL database for credentials manager.\\nsecrets.sys_admin_passwo\\nrd\\nYou can use this Password in combination with the username sysadmin to log in to\\nUCMDB’s JMX console.\\nsecrets.IDM_DB_USER_PAS\\nSWORD_KEY\\nPassword for IdM database (PostgreSQL) user. The application will give the username in\\nHelm \\nvalues.yaml\\n under \\ndeployment.database.user\\n of IdM.\\nsecrets.schedule_mail_pas\\nsword_key\\nPassword for scheduling the reports to an email for BVD.\\nSMTP server configuration\\nYou need to configure mail server which is used by the Web-to-PDF service to send emails for scheduled jobs. \\nParameter\\ngroup\\nValues\\nDescription\\nbvd.smtpServer\\n.host\\nThe host name of the SMTP server\\nYou must give these parameters to schedule the reports to a\\nspecified email id. Give the \\nsmtpServer\\n host name.\\nbvd.smtpServer\\n.port\\nGive the \\nsmtpServer\\n port\\nPort of the SMTP server used for sending emails.\\nbvd.smtpServer\\n.security\\nGive the value as \\nTLS\\n or \\nSTARTTLS\\nThe security used by the Web to PDF service to talk to the\\nSMTP server. TLS and STARTTLS are supported.\\nContainerized Operations Bridge 2022.11\\nPage \\n483\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e84dfbe06abf97183241160d2181b8'}>,\n",
              "  <Document: {'content': \"Plan for the upgrade\\nTo create an upgrade plan, review the following parameters:\\nYour upgrade sequence should include the components and the databases existing in your environment. You can upgrade the\\nfollowing containerized components: \\nCapability\\nComponents\\nStakeholder Dashboard\\nBusiness Value Dashboard (BVD)\\nOPTIC Reporting\\nOPTIC Data Lake, Collection Services, BVD\\nAutomatic Event Correlation\\nOPTIC Data Lake, AEC services, BVD\\nOperations Bridge Manager\\nOBM\\nHyperscale Observability (Cloud Monitoring)\\nOBM, OPTIC Data Lake, BVD\\nAgentless Monitoring\\nBVD\\nYour environment might also have other non containerized components. You need to plan the upgrade sequence for these\\ncomponents in your environment. You may not need to upgrade certain components if backward compatibility is available with\\nthe upgraded components.\\nDecide on the upgrade path\\nYou can use the upgrade matrix to decide the versions that you can upgrade to this version of containerized Operations\\nBridge.\\nBase version\\nSupported upgrade path\\nRollback\\n2022.05\\nUpgrade to 2022.11\\nNot\\nsupported\\n2021.11\\nUpgrade to 2022.11\\nIf containerized OpsB 2021.11 uses embedded PostgreSQL, then directly upgrading to\\n2022.11 isn't supported.\\n \\nYou must upgrade OpsB to 2022.05 and then to 2022.11.\\nNot\\nsupported\\n2021.11\\nUpgrade to 2022.05 or upgrade to 2022.11\\nNot\\nsupported\\n2021.08 or\\nprevious\\nversions\\nUpgrade to 2021.11 and then to 2022.05\\nNot\\nsupported\\nLower than\\n2019.11\\nPerform a parallel upgrade to 2019.11. Then perform a parallel upgrade to 2020.10 \\nNot\\nsupported\\nOther upgrade scenarios\\nUpgrade network reports integration on Shared OPTIC Data Lake\\nContainerized Operations Bridge supports integration with Network Reports. Supported product versions:\\nContainerized Operations Bridge version\\nNetwork Reports version\\n2022.11\\n2022.11, 2022.05 \\n2022.05\\n2022.05, 2021.11\\n2021.11\\n2021.11, 2021.08\\n2021.08\\n2021.08, 2021.05\\n2021.05\\n2021.05, 2020.11\\n2020.10.P2\\n2020.11\\nScenario 1: Containerized Operations Bridge and Network Reports are integrated\\nNote: \\nIf your earlier deployment has OMT deployed, you must upgrade OMT. For more information, see \\nOPTIC\\nManagement Toolkit (OMT) documentation\\n.\\n\\ue916\\n\\ue916\\nImportant:\\n If you have deployed Hyperscale Observability and configured classic OBM in your 2021.11\\nenvironment, you must \\nupgrade classic OBM to 2022.11\\n before upgrading Operations Bridge to 2022.11.\\nTo use the Agentless Monitoring capability, upgrade Operations Bridge to 2022.11 and then upgrade Sitescope or\\napply the required patch or hotfix for Sitescope 2022.05.\\nContact the support team to get the patch details.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n488\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '49fe06e61fedba814fe72a799dfba617'}>,\n",
              "  <Document: {'content': 'Upgrade\\nThis section provides the information required to upgrade the containerized Operations Bridge.\\nUpgrade matrix\\nYou can use this upgrade matrix to decide the versions that you can upgrade to this version of containerized Operations\\nBridge.\\nBase version\\nSupported upgrade path\\nRollback\\n2022.05\\nUpgrade to 2022.11\\nNot\\nsupported\\n2021.11\\nUpgrade to 2022.11, follow the steps to Upgrade from 2021.11\\nNot\\nsupported\\n2021.11\\nUpgrade to 2022.05 or upgrade to 2022.11\\nNot\\nsupported\\n2021.08 or previous\\nversions\\nUpgrade to 2021.11 and then to 2022.05\\nNot\\nsupported\\nLower than 2019.11\\nPerform a parallel upgrade to 2019.11. Then perform a parallel upgrade to\\n2020.10 \\nNot\\nsupported\\nIf your earlier deployment has OMT deployed, you must upgrade OMT. For more information, see \\nOPTIC Management Toolkit\\n(OMT) documentation\\n.\\nRelated topics\\nTo know more about new features, fixes, and known issues in this release, see \\nRelease notes\\nTo install a new instance of containerized Operations Bridge, see \\nInstall\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n487\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '217dd1bd0e23536349275e70eedca9ef'}>,\n",
              "  <Document: {'content': 'Deploy Stakeholder Dashboard\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nDeployment with external PostgreSQL database with TLS enabled\\nTo deploy BVD, run the following command:\\nhelm install <helm deployment name> -n <suite namespace> -f <values.yaml> <chart> --set-file \"caCertificates.postgres\\\\.crt\"=<relational database certificate file> -f <secrets.yaml>\\nWhere:\\n<helm deployment name>:\\n Deployment name you want to create. \\n<suite namespace>:\\n Namespace where BVD will get deployed. \\n<values.yaml>:\\n Give the full path to the \\nbvd.yaml\\n file. \\n<chart>:\\n The absolute path to the suite chart package. Example: \\nopsbridge-suite-2021.05.0.tgz.\\n Chart file is available under \\ncharts\\n directory mentioned in the topic \\nDownload the required installation packages.\\n<relational database certificate file>:\\n Give the full path to the PostgreSQL database TLS certificate file. For steps to enable TLS\\nfor PostgreSQL, see \\nEnable TLS in PostgreSQL\\nsecrets.yaml\\n: You have created the secrets using the \\ngenerate_secrets\\n script. This file contains all the secrets.\\nExample:\\nhelm install bvd -n bvd-helm -f bvd.yaml opsbridge-suite-chart/charts/opsbridge-suite-2021.05.0.tgz \\n--set-file \"caCertificates.postgres\\\\.crt\"=/dbStuffs/server.crt -f mysecretsfile.yaml\\nDeployment with internal PostgreSQL database and TLS not enabled\\nTo deploy BVD, run the following command:\\nhelm install <deployment name> -n <suite namespace> -f <values.yaml> <chart> -f <secrets.yaml>\\nWhere:\\n<helm deployment name>:\\n Deployment name you want to create. \\n<suite namespace>:\\n Namespace where BVD will get deployed. \\n<values.yaml>:\\n Give the full path to the \\nbvd.yaml\\n file. \\n<chart>:\\n The absolute path to the suite chart package. Example: \\nopsbridge-suite-2021.05.0.tgz.\\n Chart file is available under \\ncharts\\n directory mentioned in the topic \\nDownload the required installation packages.\\nsecrets.yaml\\n: You have created the secrets using the \\ngenerate_secrets\\n script. This file contains all the secrets.\\nExample:\\nhelm install bvd -n bvd-helm -f bvd.yaml opsbridge-suite-chart/charts/opsbridge-suite-2021.05.0.tgz -f mysecretsfile.yaml\\nUnsuccessful deployment\\nIf the deployment isn\\'t successful and displays any error message, identify the issue and resolve it. Before redeploying\\nStakeholder Dashboard, you need to uninstall and install it again.\\nRun the following command to uninstall the BVD deployment:\\nhelm uninstall <deployment name> -n <namespace> --no-hooks\\nExample:\\nhelm uninstall bvd -n bvd-helm --no-hooks\\nContainerized Operations Bridge 2022.11\\nPage \\n485\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a518c0c0ad10b5dce3cfe7b5a38f1fe'}>,\n",
              "  <Document: {'content': 'bvd.smtpServer\\n.user\\nGive the email id for the \\nsmtpServer\\nUser name for authentication with the SMTP server.\\nbvd.smtpServer\\n.passwordKey\\nThis is set while running gen_secrets.sh\\nscript and refers to the mail password\\nThe credentials for that SMTP server user account need to be\\nset as described in the suite documentation.\\nbvd.smtpServer\\n.from\\n \\nEmail address of the sender\\nUser email address from where the emails are initiated.\\nParameter\\ngroup\\nValues\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n484\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2d1232ed826c5577f5e7095edc7edb20'}>,\n",
              "  <Document: {'content': \"Verify installation for Stakeholder Dashboard in K3s\\nWhere to perform this task\\nWho should do it\\nAccess permissions needed\\nServer\\nSystem administrator\\nRoot\\nRun the following command to see the pod status in a namespace:\\nkubectl get pods -n <suite namespace> \\nAfter all pods are up and running, you can access BVD with the following URL:\\nhttps://<externalAccessHost>:<externalAccessPort>/ui\\nYou can use the IDM UI to add users or authentication providers. It's available at this URL:\\nhttps://<externalAccessHost>:<externalAccessPort>/idm-admin\\nSee IDM documentation for details.\\nContainerized Operations Bridge 2022.11\\nPage \\n486\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '77e45bbee21b16fc99bb4428fab33eff'}>,\n",
              "  <Document: {'content': \"System requirements\\nBefore you upgrade Operations Bridge, see the \\nSystem requirements\\n and make sure you have the supported version of\\ndatabases and software available. \\nDecide on extra space requirements\\nFor external Kubernetes\\nYou must re-evaluate your deployment before upgrading using the sizing calculator and then resize the deployment manually\\nif required.\\nFor embedded Kubernetes\\nTo install containerized components, you would have created the folder structure with recommended disk space during the\\ninstallation. There is no extra CPU and RAM requirement for the upgrade. However, you will need to make sure there is enough\\ndisk space.\\nDirectory to contain the offline download of the Docker images\\nTo upgrade you can:\\ndownload the Docker images to the first master (control plane) node.\\ndownload them on a separate Docker system or any other system with enough space and copy them to the first master\\n(control plane) node.\\nYou can specify a directory for the download images or use the default directory \\n/var/opt/cdf/offline\\n. The images require a\\nminimum of 30 GB of free space in the target directory. The first master (control plane) node requires this space and isn't\\nneeded on any other node. You can remove the images after you complete the install. If you have any downloaded images\\nfrom the earlier installation, then you can remove them to free up space.\\nFoundation temporary directory\\nYou need a directory to copy and unpack the upgrade files. You can choose any directory as the foundation temporary\\ndirectory. Make sure that you have a minimum of 20 GB of storage space for this directory. It's recommended that you choose\\na directory under /var, for example /var/tmp or /var/cdf.\\nIf you are doing an \\nautoUpgrade\\n, then this directory can be on any system including one outside of the CDF cluster.\\nIf you are doing a manual upgrade, then this directory must exist on every master (control plane) and worker node.\\n/tmp\\nThe upgrade process requires ~2 GB free disk space in /tmp folder on all master (control plane) nodes to run the upgrade.sh\\ncommand.\\nRelated Topics \\nIn case you've met disk issue, refer to the OMT topic \\nFree disk space for control plane nodes and worker nodes\\n for solutions. \\nNote:\\n This disk space doesn't include your backup requirement, you will have to plan for the required disk space\\nfor backing up your data based on your data size.\\nContainerized Operations Bridge 2022.11\\nPage \\n490\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3439f72075f5756d3f61e9f0cc032158'}>,\n",
              "  <Document: {'content': \"Download the packages to upgrade\\nBefore upgrading, check the system requirements and plan your deployment. \\nThis topic provides you with the steps to download and extract the application installer package, Verica database installer rpm,\\nverify the public key for helm chart signing and validate the chart signature.\\nIt also lists the extracted installer package directory structure\\u200b\\u200b\\u200b\\u200b\\u200b.\\nDownload and extract the application installer package\\n1\\n. \\nGo to the \\nSoftware Licenses and Downloads\\n website\\n2\\n. \\nDownload the Operations Bridge Helm chart, with binary name \\nopsbridge-suite-chart-<version>.zip.\\n3\\n. \\nYou must accept the terms and conditions to download the application package from the website. \\n4\\n. \\nRun the following command to unzip the \\nopsbridge-suite-chart-\\n<version>\\n.zip\\n file\\n:\\nunzip opsbridge-suite-chart-<version>.zip\\nVerify application package\\nVerify Operations Bridge application installation and upgrade package. Skip this step if you don't want to verify the package:\\n1\\n. \\nUsing a web browser, visit the \\nMicro Focus Software Licenses and Downloads\\n website. Agree to the terms and conditions,\\nand then download the \\nMF_public_keys.tar.gz\\n package to a local directory.\\n2\\n. \\nRun the following commands to extract the public keys from the package:\\ngunzip MF_public_keys.tar.gz\\ntar -xvf MF_public_keys.tar\\n3\\n. \\nCopy the public key \\nhelm-public-key.asc\\n to a local folder.\\n4\\n. \\nAdd the public key to the \\nGPG\\n keyring:\\ngpg --import helm-public-key.asc\\nOn machines that use \\nkbx\\n format, you may get an error as follows:\\ngpg: no valid OpenPGP data found.\\ngpg: Total number processed: 0\\nIf you get the following error, export the key as \\npubring.gpg\\n:\\ngpg --export > ~/.gnupg/pubring.gpg\\n5\\n. \\nNavigate to the\\n $HOME/opsbridge-suite-chart/charts\\n directory:\\ncd $HOME/opsbridge-suite-chart/charts\\n6\\n. \\nRun the following command to verify the signature of the helm chart files:\\nhelm verify <chart-name>.tgz\\n7\\n. \\nYou will get a message similar to the following message indicating successful verification:\\nSigned by: Micro Focus Group Limited (GPG Key for Helm Chart Signing) xxxxx@microfocus.com\\n  Using Key With Fingerprint: xxxxx\\n  Chart Hash Verified: sha256:xxxxx\\nOPTIC AppHub validates Helm charts to ensure that they're digitally signed and aren't tampered with or corrupted.  If a chart\\nfails signature validation, OPTIC AppHub displays a warning on the application tile. If there is no warning on the tile, the chart's\\nsignature validation is successful and you can deploy it.\\nDigital verification failed\\nIf a chart is digitally signed, but the chart's \\nSHA\\n checksum doesn't match the checksum in the chart’s provenance file,\\nsomeone may have tampered with the chart. In this scenario, OPTIC AppHub displays the \\nDigital verification failed\\n message. This\\nmeans OPTIC AppHub can't guarantee the authenticity of the chart and you won’t be able to deploy the application.  \\nIt's recommended that you obtain a new copy of the chart before proceeding.\\nMissing digital signature\\nIf a chart’s provenance file is missing, OPTIC AppHub can’t validate the digital signature. In this scenario, OPTIC AppHub\\ndisplays the \\nMissing Digital Signature\\n message. This means OPTIC AppHub couldn’t find the provenance file in the chart directory\\nContainerized Operations Bridge 2022.11\\nPage \\n491\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5e9f1d251488cc64bcd8f3b127a873d1'}>,\n",
              "  <Document: {'content': 'scripts\\nAutoConfigAWS.sh\\nAutoConfigAzure.sh\\nbyok\\ncreate_aws_repositories.py\\nimage-transfer.py\\nissuecert\\nissuecert.sh\\nREADME.md\\nutility\\nstep\\nDBSQLGenerator.sh\\nfind-current-pod-logs.sh\\nguided-install\\nOpsbridgeGuidedInstall\\nresources\\nConfigureFirewallSettingsOmt.sh\\nConfigureFirewallSettings.sh\\nConfigureStorageProvisioner.sh\\nInstallPostgresInstance.sh\\nInstallVerticaInstance.sh\\nomt-db.sql\\npgdg-redhat-repo-latest.noarch.rpm\\nsilent-Config.json\\nSyncTime.sh\\ncustom-input.properties\\nleast-input.properties\\nitom-backup-restore-scripts\\nbackup.sh\\nrestore.sh\\ntools\\namc-benchmark-tool.zip\\nitom-di-pulsarudx-<version>.x86_64.rpm\\nDirectories/files\\nDescription\\nContainerized Operations Bridge 2022.11\\nPage \\n493\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '74b459fbd394980f4033e0761ef81e8d'}>,\n",
              "  <Document: {'content': \"1\\n. \\nUpgrade containerized Operations Bridge. Perform the steps mentioned in the Upgrade section topics.\\n2\\n. \\nUpgrade Network Reports. For steps, see \\nUpgrade Network Reports within an existing integration\\n.\\nScenario 2: Containerized Operations Bridge and Network Reports aren't integrated\\n1\\n. \\nIf you are on an earlier version of the containerized Operations Bridge, upgrade to the latest version. Perform the steps\\nmentioned in the Upgrade section topics.\\n2\\n. \\nInstall Network Reports. You can either choose to install the Network Reports version \\n2022.11\\n or \\n2022.05\\n. It's\\nrecommended that you install the latest version of Network Reports.\\n3\\n. \\nIntegrate containerized Operations Bridge with Network Reports. For integration steps, see \\nIntegrate Network Reports\\n.\\nUpgrade DCA integration on shared OPTIC Data Lake\\nContainerized Operations Bridge supports integration with Data Center Automation (DCA). Supported product versions:\\nContainerized Operations Bridge version\\nDCA version\\nContainerized Operations Bridge 2022.11\\nDCA 2022.11\\nContainerized Operations Bridge 2022.05\\nDCA 2022.05\\nContainerized Operations Bridge 2021.11\\nDCA 2021.11\\nContainerized Operations Bridge 2021.08\\nDCA 2021.08\\nContainerized Operations Bridge 2021.05\\nDCA 2021.05\\nContainerized Operations Bridge 2021.05\\nDCA 2020.11\\nContainerized Operations Bridge 2020.10\\nDCA 2020.11\\nContainerized Operations Bridge 2020.10\\nDCA 2020.08\\nContainerized Operations Bridge 2020.08\\nDCA 2020.08\\nScenario 1: Containerized Operations Bridge and DCA are integrated\\nFollow the steps:\\n1\\n. \\nUpgrade containerized Operations Bridge. Perform the steps mentioned in the Upgrade section topics.\\n2\\n. \\nUpgrade DCA. For upgrade steps, see \\nUpgrade DCA\\n.\\nNote:\\n DCA supports upgrades only from n to n+1 version. \\n3\\n. \\nIntegrate containerized Operations Bridge with DCA. For integration steps, see \\nIntegrate DCA\\n.\\nScenario 2: Containerized Operations Bridge and DCA aren't integrated\\n1\\n. \\nIf you are on an earlier version of the containerized Operations Bridge, upgrade to the latest version. Perform the steps\\nmentioned in the Upgrade section topics.\\n2\\n. \\nInstall DCA. You can either choose to install the DCA version \\n2022.11\\n or \\n2022.05\\n. It's recommend that you install the\\nlatest version of DCA.\\n3\\n. \\nIntegrate containerized Operations Bridge with DCA. For integration steps, see \\nIntegrate DCA\\n.\\nUpgrade Stakeholder dashboards from 2019.11\\nIf you have deployed containerized Operations Bridge 2019.11 in your environment, you can install containerized Operations\\nBridge 2022.05 with Stakeholder dashboard capability. Then migrate the BVD workload to the 2022.11. For details about BVD\\nmigration, see \\nMigrate Stakeholder dashboards from older versions\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n489\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '10c1d31e0b8f12baf874a27c8ccfee0f'}>,\n",
              "  <Document: {'content': \"because it's not an official chart or it may have been removed to circumvent the authenticity check. You can still deploy an\\nunsigned chart, but you must acknowledge that you accept the risks associated with deploying a chart that isn't verified.\\nIt's recommended that you obtain a new copy of the chart before proceeding.\\nDownload and extract the Vertica package\\nYou will need Vertica database if you plan to use the OPTIC Reporting, AEC, and Hyperscale Observability capabilities. If you\\nhaven't installed Vertica, follow these steps to download the installer files.\\n1\\n. \\nDownload the \\nVertica installer\\n \\n.\\nrpm\\n file \\nand \\nlicense file \\nfrom the \\nSoftware Licenses and\\nDownloads\\n website (requires Passport credentials), if not downloaded already. \\nYou will get the Vertica license key when you request a key for the \\nContainerized Operations Bridge\\nPremium/Ultimate.\\n2\\n. \\nCopy the installer and the license to a temporary directory, such as \\n/tmp\\n, on the system where you will install Vertica.\\nDirectory structure of the \\u200b\\u200b\\u200b\\u200b\\u200bapplication installer package \\nThe unzipped file contains the following directories and files under \\nopsbridge-suite-chart\\n:\\nDirectories/files\\nDescription\\ncharts\\nApplication install charts (\\nDO NOT extract\\n):  \\nopsbridge-suite-<version>.tgz\\nopsbridge-suite-<version>.tgz.prov\\ndeployment\\nExtra-Large-Deployment.yaml\\nLarge-Deployment.yaml\\n  \\nMedium-Deployment.yaml\\n  \\nEvaluation-Deployment.yaml\\nLow-Footprint-OpsB-Reporting.yaml\\nsamples\\naws\\nvalues.yaml\\nazure\\nvalues.yaml\\nopenshift\\nvalues.yaml\\ngeneric\\nvalues.yaml\\ngenericPVC.yaml\\ngenericPV.yaml\\nMultiDeploymentSample.yaml\\nvalues.yaml\\nitom-opsb-scc.yaml\\nFilled samples for:\\nExternalOracleSample.yaml\\nExternalPostgresSample.yaml\\nEvaluationSample.yaml\\nContainerized Operations Bridge 2022.11\\nPage \\n492\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b0b28b4e54681c7ebf65316bc0247d84'}>,\n",
              "  <Document: {'content': 'Upgrade using AppHub\\nThis page mentions the steps required to upgrade Operations Bridge to 2022.11. Some of these steps are specific to the base\\nversion from which you are upgrading and also the capability deployed in your current deployment. \\n1\\n. \\nTo upgrade an application, you need to upload a Helm chart file to the cluster. For more information, see \\nUpload\\napplication chart\\n.\\n2\\n. \\nIf you are upgrading CLI deployment in AppHub follow the steps mentioned in\\n Onboard a CLI deployment to\\nAppHub\\n. \\n3\\n. \\nBefore you upgrade the application, you must download the required application images from Docker Hub or other\\nregistries and then upload the images to your registry.\\nFor AWS\\n, Ensure that the containers used by the application must be available in the Elastic Container Registry (ECR) for\\nthe desired region. For more information and steps, see \\nAWS ECR\\n topics.\\nFor Azure\\n, Ensure that the containers used by the application must be available in the Azure Container Registry (ACR)\\nfor the desired region. For more information and steps, see \\nAzure ACR\\n topics.\\nFor other K8s providers\\n, see \\nDownload and Upload installation images\\n.\\nYou can do one of the following to download and upload installation images: \\nDownload OMT and application images from Docker Hub to a private image repository server in a\\ncustomer network:\\n Use this private image repository server to download images during OMT and application\\ninstallation as required and it can serve as a “cache” for all users in that network. \\nDownload OMT and application images from Docker Hub to a private machine in the customer network\\n:\\nDownload OMT and application images directly from Docker Hub to a local machine using the download images script\\nprovided by OMT.\\nUpload these images to a Micro Focus embedded Kubernetes cluster using the upload images script provided by\\nOMT.\\nDownload OMT and application images from Docker Hub to a  local machine (\\nlocalhost:5000\\n) using\\nscripts provided by OMT:\\n The images are stored within the cluster.\\n4\\n. \\nIf you are upgrading from Operations Bridge 2021.11\\n and\\n \\nhave containerized OBM capability deployed\\n,\\nupgrade fails due to \\nitom-omi-aec-integration-watcher\\n, perform the \\nworkaround\\n before proceeding with the upgrade.\\n \\n5\\n. \\nLog into AppHub\\n \\n6\\n. \\nGo to \\nDeployments.\\n The Operations Bridge deployment will have a highlighter indicating an upgrade is available.\\n7\\n. \\nClick on the actions menu and select \\nUpgrade\\n.\\n8\\n. \\nFrom the drop-down select the version of the chart to which you want to upgrade.\\n9\\n. \\nClick \\nConfigure\\n.\\n10\\n. \\nYou will see the values configured in the previous deployment in\\nthe \\nGeneral\\n, \\nCapabilities\\n, \\nSecurity\\n, \\nDatabases\\n, and \\nAdvanced \\ntabs are retained. \\n11\\n. \\nIf you are upgrading from Operations Bridge 2021.11\\n and\\n \\nhave containerized OBM capability with HA deployed\\n,\\ndisable OBM HA\\n on the AppHub UI using the toggle button for \\nEnable OBM HA.\\n12\\n. \\nIf you have installed an OBM CA certificate in your current deployment, ensure that you have followed the steps\\nmentioned in the section \\nInstall OBM CA certificate in Operations Bridge using AppHub\\n of Install an OBM CA certificate\\ntopic.\\nImportant:\\n \\n If you want to download only the required images for your upgrade configuration, you must\\nconfigure the \\nvalues.YAML\\n following \\nUpdate the values file\\n and pass it with \\n-H\\n option while running the \\ngenerat\\ne_download_bundle.sh\\n script.  You can run the script without \\n-H\\n \\nvalues.YAML\\n to download all the images that are\\nlisted in the specified chart. \\n\\ue91b\\n\\ue91b\\nTip: \\nIf you are upgrading from Operations Bridge 2022.05 and you don\\'t have OBM capability installed in\\nyour current deployment, then skip steps  \\n4,11 \\nand\\n 20\\n.\\n\\ue917\\n\\ue917\\nImportant:\\nMake sure that the \\nAutomatically create required databases\\n toggle is disabled. The auto creation\\nof databases through AppHub isn\\'t supported during the upgrade.\\nWhile upgrading\\n, \\ndon\\'t\\n enable any new capability, retain the default options as it\\'s for new\\nservices. You can reconfigure the existing installation after a successful upgrade by\\nfollowing \\nAdd/Remove capabilities\\n. \\n\\ue91b\\n\\ue91b\\nNote:\\n You will see an \"\\nX\\n\" marked for the \\nDatabases \\ntab\\n.\\n Go to the databases tab and validate all the\\ndatabases connections by clicking on \\nVerify \\nbutton to proceed with\\n Upgrade. \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n498\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '90b653c3bff5f58ea8262c0875e9500f'}>,\n",
              "  <Document: {'content': 'If you are upgrading from Operations Bridge 2021.11 to 2022.11, you must perform the additional pre upgrade tasks\\nmentioned in this section.\\nUpdate external PostgreSQL database server certificate after OMT upgrade\\nThe usage of server certificates without subjectAlternativeName(SAN) extension is not supported in 2022.11. Matching the\\nhostname by relying on checking the “Common Name” field is not supported. Any certificates not having a SAN need to be\\nupdated or recreated before upgrading to 2022.05 and and later releases and the same must be used during the upgrade. For\\nmore details on the certificate requirement details, See \\nEnable TLS in PostgreSQL\\n. \\nTo validate whether the server certificate has the\\nsubjectAlternativeName(SAN)\\n extension, execute the command \\nopenssl x509 -in \\nserver.crt -text -noout\\n \\non your PostgreSQL server and verify the output for Subject Alternative Name.\\nModify the configuration information of external databases\\nPerform this step only if you have updated the external PostgreSQL database server certificate after OMT upgrade.\\nWhen you change the configuration of OMT\\'s external databases, you must also update the database configuration information\\nin OMT to make sure the deployment runs well. OMT provides the \\nupdateExternalDbInfo\\n script to do this. See \\nModify the\\nconfiguration information of external databases\\n in OMT documentation.\\nBack up the external certificates\\nPerform this task if you have imported external certificates into OPTIC Data Lake with the idl_config.sh script, for example as\\npart of configuring OBM or to establish trust between OPTIC Data Lake’s Pulsar and OBM’s Data Flow Probe to sync topology.\\nFor more information, see \\nBack up external certificates\\n. \\nCreate the additional PostgreSQL database for Hyperscale Observability (Cloud Monitoring in 2021.11) and OPTIC\\nReporting\\nYou must create the additional database that is required for the capability Hyperscale Observability (Cloud Monitoring in\\n2021.11) due to the design updates in 2022.05.\\nRun the following postgreSQL queries to create \\nbtcd\\n database: \\nCREATE USER btcd with encrypted password \\'password\\';\\nGRANT btcd to postgres;\\nCREATE DATABASE btcd;\\n\\\\c btcd;\\nGRANT ALL PRIVILEGES ON DATABASE btcd TO btcd;\\nALTER SCHEMA public OWNER TO btcd;\\nALTER SCHEMA public RENAME TO baselinedbschema;\\nREVOKE ALL ON SCHEMA baselinedbschema from public;\\nGRANT ALL ON SCHEMA baselinedbschema to btcd;\\nGRANT ALL PRIVILEGES ON DATABASE btcd TO btcd;\\nALTER USER btcd SET search_path TO baselinedbschema;\\nRun the following Oracle queries if you are using Oracle:\\nCREATE USER btcd IDENTIFIED BY \"btcd\" DEFAULT TABLESPACE USERS TEMPORARY TABLESPACE TEMP QUOTA UNLIMITED ON USERS;\\nGRANT CONNECT TO btcd;\\nGRANT CREATE SEQUENCE TO btcd;\\nGRANT CREATE TABLE TO btcd;\\nGRANT CREATE TRIGGER TO btcd;\\nGRANT CREATE VIEW TO btcd;\\nGRANT CREATE PROCEDURE TO btcd;\\nGRANT CREATE TYPE TO btcd;\\nALTER USER btcd DEFAULT ROLE ALL;\\nUpdate storage class name\\nPerform this step only if you have deployed Operations Bridge with containerized OBM capability.\\nWhen OpsB 2021.11 was installed you would have created a couple of PVs. This includes 2 PVs with storage\\nclass \\nomistatefulset\\n. If you want to enable OBM after upgrading to 2022.05, the \\nomistatefulset \\nPVs will not get bound. OBM in\\n2022.05 requires PVs with default storage class\\n.\\n A similar issue would arise if you have enabled OBM in non-HA mode and\\nwant to enable HA after upgrading to 2022.11.\\nRun the below command to get the PV names that are having \\nstorageClassName\\n set to \\nomistatefulset\\n:\\nkubectl get pv | grep omistatefulset | awk \\'{ print $1 }\\'\\nNote:\\n In the case of shared OPTIC DL with NOM, NOM will continue to use the btcd database. If you plan to\\nuse the Hyperscale Observability capability, you must create another database with the name \\nopsbbtcd.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n495\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e9cd73450cb9f08c6d250c35cf9dfa3e'}>,\n",
              "  <Document: {'content': 'Upgrade\\nYou can configure the application using the AppHub UI or update the \\nvalues.yaml\\n file before upgrade.\\nBefore you configure the settings,\\n1\\n. \\nMake sure you have allocated the required resources based on the sizing guidelines, you have set up the infrastructure\\nprerequisites and the environment is ready for installation.\\n2\\n. \\nIf you are configuring using AppHub, make sure that you have the required \\nOMT tools and utilities\\n.\\n3\\n. \\nUpload and download the suite images to the registry.\\nYou can upgrade Operations Bridge either using the \\nAppHub UI\\n or the \\nCLI\\n.  \\nContainerized Operations Bridge 2022.11\\nPage \\n497\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2056b6a3af6cc4494566cbd33d400e1e'}>,\n",
              "  <Document: {'content': \"Example output:\\n# kubectl get pv | grep omistatefulset | awk '{ print $1 }'\\nopsbvol5\\nopsbvol6\\nEdit each of the PVs and update the field  \\nstorageClassName\\n by running the following command:\\nkubectl edit pv <pv-name>\\nUpdate the field storageClassName to change it from omistatefulset to empty value.\\nCreate a persistent volume for OBM\\nPerform this step only if you have deployed Operations Bridge with containerized OBM capability.\\nFor more information, see \\nCreate Artemis PV for OBM\\n.\\nAdd Access to SDK role to the configured User Role\\nEnsure that the Access to SDK role is configured. If this role is missing then when you upgrade to 2022.11, the \\nitom-opsbridge-de\\ns-cicache-builder\\n pod will enter into \\nCrashLoopBackOff\\n. As a workaround, add \\nAccess to SDK \\nrole to the configured User Role.\\nFor more information and steps, see \\nCreate an Agent Metric Collector integration user\\n topic.\\nTip:\\n This task is not applicable for upgrade on Red Hat OpenShift. \\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n496\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1395be49a09de4e8bef89cdeb9f0d859'}>,\n",
              "  <Document: {'content': \"Pre upgrade tasks\\nThis topic lists all the pre upgrade tasks that are required before proceeding with upgrading Operations Bridge.\\nYou must perform the tasks listed under \\nPre upgrade tasks for upgrading Operations Bridge to 2022.11\\n section if\\nyou are upgrading from base version 2021.11 or 2022.05. \\nYou must perform the tasks listed under \\nAdditional pre upgrade tasks for upgrading Operations Bridge 2021.11\\nto 2022.11\\n section if you are upgrading from 2021.11.\\nPre upgrade tasks for upgrading Operations Bridge to 2022.11\\nBack up the data\\nFollow the backup instructions mentioned in the Administer section > \\nBack up and restore\\n topics according to your Kubernetes\\nplatform to back up your existing deployment and data before the upgrade.\\nUpgrade OMT\\nYou must upgrade OMT before proceeding with the application upgrade.\\nPoints to note:\\nFor embedded Kubernetes, use the \\nautoupgrade\\n script to upgrade OMT from 2022.05 or 2021.11.\\nFor external Kubernetes, use the \\nupgrade.sh\\n script to upgrade OMT from 2021.11 to 2022.05 and then to 2022.05 to\\n2022.11.\\nYou must upgrade OMT by following the steps mentioned in the OMT documentation. See \\nupgrade OMT\\n.\\nUpdate Security context constraints (SCCs) for application on Red Hat OpenShift\\nUpdate security context constraints (\\nSCC\\ns) to allow administrators to control permissions for pods. For more information, see\\nUpdate Security context constraints (SCCs) for application\\n.\\nUpgrade Storage Provisioner chart\\nIf you've used OPTIC DL in your existing installation, you must upgrade the \\nlvp\\n chart. See \\nUpgrade storage provisioner chart\\n.\\nUpgrade Vertica and plugin\\nIf you have deployed OPTIC Reporting, or an external OPTIC Data Lake scenario for a provider application you must make sure\\nyou have the required Vertica version and the plugin. \\nEnsure that you have the supported Vertica version. For more information, see \\nSystem requirements\\n. To upgrade\\nVertica, follow the steps in \\nUpgrade Vertica\\n.\\nIf you are upgrading from the Operations Bridge 2021.11 version, you must \\nUpgrade OPTIC DL Vertica Plugin\\n . You may\\nskip this task if you are upgrading from 2022.05.\\nEnable or disable data migration \\nThe 2022.11 release includes an update to the \\nopr_event\\n dataset. For more information, see \\nopr_event\\n. Using the helm\\nvariable \\nValues.global.oprEventFlex\\n, you can choose to either enable or disable history data migration from backup to \\nopr_event\\n.\\nFor more information, see \\nEnable or Disable Data Migration\\n. Also, the \\nopr_event_temp\\n table stores old data as a backup. \\nAdditional pre upgrade tasks for upgrading Operations Bridge 2021.11 to 2022.11\\nNote:\\n \\nThis is in addition to the tasks listed under \\nPre upgrade tasks for upgrading Operations\\nBridge to 2022.11\\n section.\\n\\ue916\\n\\ue916\\nImportant:\\n \\nYou must ensure that, when you download the OMT installation packages, they must all be at\\nthe same level in the temporary directory. For example, if you are upgrading from 2021.11 to 2022.11, you would\\nsave the following unzipped packages in the temporary directory:\\n/temporary_directory/OMT_Embedded_K8s_2022.11-xxx\\n/temporary_directory/OMT_Embedded_K8s_2022.05.xxx\\n\\ue91b\\n\\ue91b\\nImportant: \\nWhen you start to upgrade the OPTIC DL Vertica Plugin, the data ingestion into the OPTIC DL will\\nbreak because there is a version mismatch between the itom-di-scheduler-udx pod and the plugin. The \\nitom-di-sche\\nduler-udx\\n pod restarts after the application upgrade is complete. After the application upgrade, the Vertica RPM\\nand the \\nitom-di-scheduler-udx\\n pod versions align and then the data ingestion will continue to work. Hence, ensure\\nyou have completed the steps in \\nDisable the Agent Metric Collector\\n.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n494\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3f8e8f9aa49ea1ad422b9d9f1d512a74'}>,\n",
              "  <Document: {'content': '13\\n. \\nIf you are upgrading from Operations Bridge 2021.11 \\nand\\n you have created new certificates\\n, then\\n \\nupload the\\nnewly configured external PostgreSQL certificate after OMT upgrade, in the\\n Security\\n tab.\\n \\nSee \\nUpdate external\\nPostgreSQL database server certificate after OMT upgrade\\n.  \\nOnce you have all the configurations in place the sidebar for all the tabs will indicate a \"✔\" mark.\\n14\\n. \\nIf you are upgrading from Operations Bridge 2021.11\\n and you have a \\nhotfix or patch installed\\n, you must check\\nand then if present remove any hard coded \\nimage Tags\\n from your 2021.11 deployment before upgrading to 2022.11. \\nSee the topic \\nRemove hard coded image Tags from 2021.11 hotfixes before upgrade.\\n15\\n. \\nIf you are upgrading an\\n \\nAzure deployment with OBM\\n capability installed, open the YAML editor, add the following\\nsection and provide the load balancer IP value.\\n # Required only for Azure. Private IP for Azure Load Balancer configuration\\n global:\\n   loadBalancer:\\n     ip:\\n16\\n. \\nIf you are upgrading an environment deployed using K3S \\nand  if the \\nDefault S\\ntorage\\n \\nClass\\n section parameters\\nare\\n empty\\n or \\nnull\\n then, update the \\nDefault S\\ntorage\\n \\nClass(\\nRWX) \\nand\\n Default S\\ntorage\\n \\nClass(\\nRWO)\\n parameters to \\n\"loc\\nal-path\"\\n before proceeding with the upgrade.\\n17\\n. \\nClick \\nUpgrade\\n.\\n18\\n. \\nCheck the \\nMicro Focus End User License Agreement \\nand then click \\nOK\\n.\\nIf you see a warning similar to \"\\nUpgrading this deployment will invalidate your current draft\\n\", ignore it and\\nproceed.\\n19\\n. \\nYou must wait until the deployment status moves to \\nDeployed\\n. Once the deployment is successful, check the pod status\\nin CLI.\\n20\\n. \\nIf you are upgrading from Operations Bridge 2021.11 and you have the OBM capability with HA\\n and \\nyou\\nhave already performed step 11,\\n then after a successful upgrade\\nGo to\\n \\nAppHub UI\\n > Deployments > Edit > Capabilities > \\nUse\\n “Enable OBM HA” \\ntoggle button to enable HA mode\\n> Redeploy\\nImportant:\\n If you have installed an OBM CA certificate in your current deployment and you had not\\nuploaded the certificate into AppHub following the section \\nInstall OBM CA certificate in Operations\\nBridge using the AppHub\\n, the integration post upgrade will fail since AppHub would be missing that\\ncertificate.\\n\\ue91b\\n\\ue91b\\nNote:\\n You can check the deployment status from the View details tab.\\n\\ue916\\n\\ue916\\nImportant\\n:\\n If the upgrade fails, don\\'t roll back. Continue to use the product and contact Software\\nSupport for help.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n499\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '529e4b30b48570f5545ddbb9d600402f'}>,\n",
              "  <Document: {'content': \"Upgrade using CLI\\nThis topic mentions the steps required to upgrade Operations Bridge to 2022.11.\\n1\\n. \\nIf you are upgrading from Operations Bridge 2021.11\\n,\\n \\nyou must \\nupdate secrets\\n.\\n2\\n. \\nIf you are upgrading from Operations Bridge 2021.11\\n, and you have a hotfix or patch installed, you must check and then if\\npresent remove any hard coded \\nimage Tags\\n from your 2021.11 deployment before upgrading to 2022.11.\\n See the topic \\nRemove\\nhard coded image Tags from 2021.11 hotfixes before upgrade.\\n.\\n3\\n. \\nIf you are upgrading from Operations Bridge 2021.11\\n, the upgrade fails due to \\nitom-omi-aec-integration-watcher\\n,\\nperform the \\nworkaround\\n before proceeding with the upgrade.\\n4\\n. \\nIf you are upgrading from Operations Bridge 2021.11\\n and\\n \\nhave containerized OBM capability with HA\\ndeployed, \\n scale down \\nucmdbserver\\n by setting the \\nucmdbserver.deployment.replicaCount\\n to 1 in the \\nvalues.yaml.\\nucmdbserver:\\n  deployment:\\n    replicaCount: 1                                  # If OBM HA is disabled, Update this value to 1.\\n    database:\\n      dbName: rtsm\\n      user: rtsm\\n      userPasswordKey: RTSM_DB_USER_PASSWORD_KEY           # [DO NOT CHANGE]\\n5\\n. \\nUpdate the values file\\n \\n6\\n. \\nBefore you upgrade the application, you must download the required application images from Docker Hub or other\\nregistries and then upload the images to your registry.\\nFor AWS\\n, Ensure that the containers used by the application must be available in the Elastic Container Registry (ECR) for\\nthe desired region. For more information and steps, see \\nAWS ECR\\n topics.\\nFor Azure\\n, Ensure that the containers used by the application must be available in the Azure Container Registry (ACR)\\nfor the desired region. For more information and steps, see \\nAzure ACR\\n topics.\\nFor other K8s providers\\n, see \\nDownload and Upload installation images\\n.\\n7\\n. \\nUpgrade using CLI\\n8\\n. \\nIf you are upgrading from Operations Bridge 2021.11, you have the OBM capability with HA deployed \\nand\\nyou have already performed step 4,\\n then after a successful upgrade scale up \\nucmdbserver\\n using the command: \\nhelm upgrade <helm deployment name> <chart> -n <application namespace> --reuse-values --set ucmdbserver.deployment.replicaCount=2\\n9\\n. \\nOptional\\n. You must perform \\nUpload application chart\\n steps if you have used the CLI to upgrade the application and you\\nwant to use the AppHub UI for configuring the application.\\nTip:\\n \\nIf you are upgrading from Operations Bridge 2022.05 and you don't have OBM capability installed in\\nyour current deployment, then skip steps \\n3,\\n \\n4\\n and\\n 8\\n.\\n\\ue917\\n\\ue917\\nImportant:\\n \\n If you want to download only the required images for your upgrade configuration, you must\\nconfigure the \\nvalues.YAML\\n and pass it with \\n-H\\n option while running the \\ngenerate_download_bundle.sh\\n script.  You\\ncan run the script without \\n-H\\n \\nvalues.YAML\\n to download all the images that are listed in the specified chart. \\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n500\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1cf453eddd5055383dd1f09fd63eca6b'}>,\n",
              "  <Document: {'content': '4\\n. \\nRun the following command in the folder containing the \\nops-content-ctl\\n tool to upload the custom collection configuration:\\nOn Linux:\\n./ops-content-ctl upload collection -f <custom collection configuration file>\\nHere, replace  \\n<custom collection configuration file>\\n with the custom collection configuration file name. For example:\\n./ops-content-ctl upload collection -f AgentMetricCollection_regular_custom.json\\nOn Windows:\\nops-content-ctl.exe upload collection -f <custom collection configuration file>\\nHere, replace  \\n<custom collection configuration file>\\n with the custom collection configuration file name. For example:\\nops-content-ctl.exe upload collection -f AgentMetricCollection_regular_custom.json\\n5\\n. \\nRun the command to stop the collection:\\nOn Linux:\\n./ops-content-ctl stop collection -n <content name> -v <version> -c <name of the collection configuration>\\nHere, replace <content name> with the name of the content, <version> with version of the content, and <name of the\\ncollection configuration> with the collection configuration name. For example:\\n./ops-content-ctl stop collection -n OpsB_SysInfra_Content -v 2021.05.00 -c AgentMetricCollection_regular\\nOn Windows:\\nops-content-ctl.exe stop collection -n <content name> -v <version> -c <name of the collection configuration>\\nHere, replace <content name> with the name of the content, <version> with version of the content, and <name of the\\ncollection configuration> with the collection configuration name. For example:\\nops-content-ctl.exe stop collection -n OpsB_SysInfra_Content -v 2021.05.00 -c AgentMetricCollection_regular\\nVerify that the collection has stopped\\nRun the following command in the folder containing the \\nops-content-ctl\\n tool to verify the status collection configurations:\\nOn Linux:\\n./ops-content-ctl list collection\\nOn Window:\\nops-content-ctl.exe list collection\\nFor example: \\n# ops-content-ctl.exe list collection\\nPassword:\\nNAME                             TYPE       STATUS                   CONTENT NAME            CONTENT VERSION\\nAgentMetricCollection_historic   historic   collection not started   OpsB_SysInfra_Content   2021.05.00\\nAgentMetricCollection_regular    regular    collection not started   OpsB_SysInfra_Content   2021.05.00\\nContainerized Operations Bridge 2022.11\\nPage \\n505\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '347baf23b423e59fca459ec8523a4cd7'}>,\n",
              "  <Document: {'content': \"ops-monitoring-ctl get collector -n agent-collector-sysinfra -o yaml -f <file name>\\nExample:\\nops-monitoring-ctl get collector -n  agent-collector-sysinfra -o yaml -f agent-collector-sysinfra.yaml\\n2\\n. \\nCompare the configurations with the latest \\nsample yaml file\\n. You can find the changes highlighted on the sample yaml file\\nfor your reference. If there are any differences, edit the yaml file and make the necessary changes. \\n3\\n. \\nRun the following command to update the collection configuration:\\nops-monitoring-ctl update -f agent-collector-sysinfra.yaml\\nScenario 2: \\nIf you've modified the\\n OOTB and renamed the metadata name \\nfor agent-collector-sysinfra configuration file.\\n1\\n. \\nRun the following command to view the configured collector:\\nops-monitoring-ctl get collector\\nYou can find default and customized OOTB collector configuration files listed out:\\nExample:\\nagent-collector-sysinfra_customized \\nagent-collector-sysinfra\\n2\\n. \\nRun the following command to view the configured collector:\\nops-monitoring-ctl get collector -n agent-collector-sysinfra -o yaml -f <file name>\\nExample:\\nops-monitoring-ctl get collector -n agent-collector-sysinfra -o yaml -f agent-collector-sysinfra\\n3\\n. \\nRepeat the above command for the customized OOTB configuration by replacing \\nagent-collector-sysinfra \\nwith \\nagent-collector-s\\nysinfra_customized.\\n4\\n. \\nCompare the differences in both the files and make the necessary update on the customized file. In this case in the \\nagent-\\ncollector-sysinfra_customized\\n configuration file.\\n5\\n. \\nRun the following command to save the updated collection configuration:\\nops-monitoring-ctl update -f <file name>\\nExample:\\nops-monitoring-ctl update -f agent-collector-sysinfra_customized.yaml\\n6\\n. \\nTo avoid the duplicate configurations, run the following command to delete the OOTB \\nagent-collector-sysinfra \\nconfiguration\\nfile:\\nops-monitoring-ctl delete -f <file name>\\nExample:\\nops-monitoring-ctl delete -f agent-collector-sysinfra.yaml\\nEnable Agent Metric Collector\\nDuring an upgrade, the\\n \\nAgent Metric Collector (AMC) configuration files - Credential, Target, and Collector configuration files,\\nget created and deployed. The custom collection configuration, if present, also gets configured. \\nFollow the steps to enable the Agent Metric Collector: \\n1\\n. \\nRun the following command in the folder containing the \\nops-monitoring-ctl\\n tool to check the status of the collection\\nconfigurations: \\nops-monitoring-ctl get collector-status -o yaml -n agent-collector-sysinfra\\nSample output:\\nContainerized Operations Bridge 2022.11\\nPage \\n507\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f357f8ffafdb96b814c624377588f404'}>,\n",
              "  <Document: {'content': 'Reference\\nContainerized Operations Bridge 2022.11\\nPage \\n503\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2dbf40d07fa400520798e119256e6336'}>,\n",
              "  <Document: {'content': 'Enable the Agent Metric Collector\\nThis section explains the prerequisites required to enable Agent Metric Collector (AMC) and how to enable it.\\nWhen you enable AMC, regardless of how long the downtime lasted, the Agent Metric Collector retrieves the metric for the\\nprevious six hours (360 minutes). The \\nmax_history_mins\\n parameter is defined in the regular collection configuration file. The\\ndefault value of this parameter is 360 minutes. For more information, see \\nCollection Attributes\\n page.\\nPrerequisites to start AMC\\nVerify  OBM connections\\nVerify AMC status\\nValidate Autoconfigure job\\nVerify OBM connections\\nFor classic OBM: Make sure that you have assigned the correct roles to the Agent Metric Collector integration user. The\\nAgent Metric Collector won\\'t list any nodes or start metrics collection if you assign the wrong roles. To add and assign\\nroles, see \\nCreate an Agent Metric Collector integration user\\n.\\nFor containerized OBM: Make sure that the\\n omi-0\\n (and \\nomi-1\\n in HA) pod is running.\\nRun the following command to see the pod status in a namespace:\\nkubectl get pods --namespace <suite namespace>\\nExample:\\n \\n kubectl get pods -n opsbridge1\\nValidate AMC status\\nThe value of the following two parameters controls the functionality of Agent metric collection:\\n1\\n. \\nglobal.autoStartAgentMetricCollector. \\nThe default value of the parameter is true. \\n2\\n. \\nisAgentMetricCollectorEnabled\\n  The default value of this parameter is false. \\nRun the following command to validate if AMC is enabled during suite installation.\\nhelm get values $(helm list -A | awk \\'/opsbridge-suite-/ { print $1}\\') -n $( helm list -A | awk \\'/opsbridge-suite-/ { print $2}\\') | grep isAgentMetricCollectorEnabled\\nOutput:\\nisAgentMetricCollectorEnabled: true   #This means AMC is enabled during the upgrade. But collection is not started yet.\\n Validate autoconfigure job \\nAfter the \\nautoConfigure \\npod is in running or completed state, run the command to verify the creation and deployment of the\\nAMC configurations: \\nkubectl logs  -n $(kubectl get pod -A | awk \\'/autoconfigure/ {print $1, $2}\\')  > autoconfig.log\\nOpen the \\nautoconfig.log \\ncreated in the previous step and check the status of the Agent Metric Collector (AMC) configuration\\nfiles: \\nCredential,Target,\\nand \\nCollector\\n \\nconfiguration\\n files. Check the status doesn\\'t refer to a failure (Example: \"Create Failed\") and\\nthe collector Enabled value is \\'true\\'\\n \\nto proceed further as shown in the sample output below:\\nSample output:\\nVerify collector configuration update\\nScenario 1: \\nIf you have updated the content\\n without changing the metadata name \\nfor OOTB agent-collector-sysinfra\\nconfiguration file.\\n1\\n. \\nRun the following command to view the configured collector:\\nNote:\\n Skip this page if you haven\\'t configured Agent Metric Collector in your environment.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n506\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58703bb91f121a449ebad956fbe25c16'}>,\n",
              "  <Document: {'content': 'Verify the upgrade\\nRun the following command to verify if the pods are in a \"running\" or \"completed\" state:\\nkubectl get pods -n <suite namespace> \\nYou will see the list of pods according to your deployment and the status. \\nFollowing is a sample output:\\n# kubectl get pods -n opsb-helm\\nNAME                                                              READY   STATUS      RESTARTS   AGE\\nbvd-ap-bridge-55bd98c76c-zcz6t                                    2/2     Running     0          114m\\nbvd-controller-deployment-769cdc5c85-wmlm6                        2/2     Running     0          114m\\nbvd-explore-deployment-7bcf6794b-f9qkf                            2/2     Running     3          114m\\nbvd-quexserv-69776d44b9-t9fsh                                     2/2     Running     0          114m\\nbvd-receiver-deployment-55446bbbc9-dx9gs                          2/2     Running     0          114m\\nbvd-redis-75484f6b54-92mqq                                        3/3     Running     0          114m\\nbvd-www-deployment-78b7f5c495-v6hql                               2/2     Running     2          114m\\nips-deployment-5b78b97c8f-584vq                                   2/2     Running     0          114m\\nitom-analytics-auto-event-correlation-job-1606488000-cctdk        0/1     Completed   0          27m\\nitom-analytics-auto-event-correlation-job-1606488600-mx5dx        0/1     Completed   0          17m\\nitom-analytics-auto-event-correlation-job-1606489200-7bkx7        0/1     Completed   0          7m23s\\nitom-analytics-datasource-registry-5f5dc8765b-6hz9f               2/2     Running     0          114m\\nitom-analytics-ea-config-b54865c75-rl47d                          2/2     Running     0          114m\\nitom-analytics-event-attribute-reader-84b744b6b9-k2n4p            2/2     Running     0          114m\\nitom-analytics-opsbridge-notification-65f565895b-wpl47            2/2     Running     0          114m\\nitom-autopass-lms-777c645d5d-v6z8j                                2/2     Running     0          114m\\nitom-cdf-deployer-2020.08-1.2-2.2-7hdt4                           0/1     Completed   0          7h20m\\nitom-cdf-upgrade-deployer-202011-928h7                            0/1     Completed   0          4h38m\\nitom-opsb-amc-collector-1-7c7f46f9cb-clhzn                        2/2     Running     0          19m\\nitom-collect-once-collection-manager-87dd9ffd7-scp7f              2/2     Running     0          114m\\nitom-collect-once-data-broker-75ddf6df44-wzblf                    2/2     Running     0          114m\\nitom-di-administration-784766f897-s54c5                           2/2     Running     0          114m\\nitom-di-data-access-dpl-5f7ffb57fb-phpns                          2/2     Running     0          114m\\nitom-di-dp-job-submitter-dpl-6ccc97ddf7-wv2wt                     2/2     Running     0          114m\\nitom-di-dp-master-dpl-847c847f5f-cntxz                            2/2     Running     0          114m\\nitom-di-dp-worker-dpl-646cb9c4c5-rzc8c                            2/2     Running     0          114m\\nitom-di-metadata-server-6c66df958-24pzs                           2/2     Running     0          114m\\nitom-di-postload-taskcontroller-76cd8b8889-6ffxg                  2/2     Running     0          114m\\nitom-di-postload-taskexecutor-b8c5bd78d-6vvxg                     2/2     Running     0          114m\\nitom-di-receiver-dpl-785c7dfc6c-6wz5h                             2/2     Running     0          110m\\nitom-di-receiver-dpl-785c7dfc6c-xrhfr                             2/2     Running     0          114m\\nitom-di-scheduler-udx-5849b9c5bd-fbkfr                            2/2     Running     0          114m\\nitom-idm-66c8f598d4-stlgt                                         2/2     Running     0          114m\\nitom-idm-66c8f598d4-vdf8h                                         2/2     Running     0          109m\\nitom-ingress-controller-78ff59f77f-6qztl                          2/2     Running     0          114m\\nitom-ingress-controller-78ff59f77f-bz5qb                          2/2     Running     0          113m\\nitom-migrate-vault-secrets-to-k8s-job-c658h                       0/1     Completed   0          115m\\nitom-omi-aec-integration-watcher-1606488000-h76lb                 0/1     Completed   0          27m\\nitom-omi-aec-integration-watcher-1606488600-rms2f                 0/1     Completed   0          17m\\nitom-omi-aec-integration-watcher-1606489200-kfwbl                 0/1     Completed   0          7m23s\\nitom-omi-aec-integration-wi4za-f6988                              0/1     Completed   0          114m\\nitom-omi-di-integration-p5l6g-x6rzp                               0/1     Completed   0          114m\\nitom-opsb-content-administration-7649d4b5dd-k8554                 2/2     Running     0          114m\\nitom-opsb-content-administration-job-a0qop-k9sw4                  0/1     Completed   0          114m\\nitom-opsb-db-connection-validator-job-dr8zf                       0/1     Completed   0          114m\\nitom-opsb-node-resolver-55b6bcd44c-w72kh                          2/2     Running     0          114m\\nitom-opsb-resource-bundle-758c66b866-tvx8z                        1/1     Running     0          114m\\nitom-vault-b8b48ff49-nmh8w                                        1/1     Running     0          114m\\nitomdimonitoring-grafana-97bb8b57b-ztzxn                          2/2     Running     0          114m\\nitomdimonitoring-prometheus-cf4d5c46-wlkjv                        2/2     Running     0          114m\\nitomdimonitoring-prometheus-kube-state-metrics-5f5786db8d-w5vv7   2/2     Running     0          114m\\nitomdimonitoring-prometheus-node-exporter-4lcnz                   1/1     Running     0          108m\\nitomdimonitoring-prometheus-node-exporter-cw6dv                   1/1     Running     0          114m\\nitomdimonitoring-prometheus-node-exporter-j6dns                   1/1     Running     0          108m\\nitomdimonitoring-prometheus-node-exporter-jkx7h                   1/1     Running     0          107m\\nitomdimonitoring-prometheus-node-exporter-l2kmk                   1/1     Running     0          113m\\nitomdimonitoring-prometheus-node-exporter-rfxkm                   1/1     Running     0          107m\\nitomdimonitoring-verticapromexporter-59df7d5785-7drnt             2/2     Running     0          114m\\nitomdipulsar-autorecovery-0                                       1/1     Running     1          114m\\nitomdipulsar-bastion-0                                            2/2     Running     0          114m\\nitomdipulsar-bookkeeper-0                                         2/2     Running     0          109m\\nitomdipulsar-bookkeeper-1                                         2/2     Running     0          111m\\nitomdipulsar-bookkeeper-2                                         2/2     Running     0          114m\\nitomdipulsar-bookkeeper-init-2uppzqu-5m667                        0/1     Completed   0          114m\\nitomdipulsar-broker-5d9457d796-4rdnq                              2/2     Running     0          109m\\nitomdipulsar-broker-5d9457d796-9f5hv                              2/2     Running     0          114m\\nitomdipulsar-broker-5d9457d796-fqf8p                              2/2     Running     0          114m\\nitomdipulsar-proxy-575957cc44-5ltd6                               2/2     Running     0          114m\\nitomdipulsar-proxy-575957cc44-tx422                               2/2     Running     0          114m\\nitomdipulsar-zookeeper-0                                          2/2     Running     0          109m\\nitomdipulsar-zookeeper-1                                          2/2     Running     0          110m\\nitomdipulsar-zookeeper-2                                          2/2     Running     0          114m\\nitomdipulsar-zookeeper-metadata-n7e4b8w-44sgm                     0/1     Completed   0          114m\\nomi-0                                                             2/2     Running     0          2d9h\\nomi-artemis-766fdf6564-sp5jm                                      2/2     Running     0          2d9h\\nContainerized Operations Bridge 2022.11\\nPage \\n501\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3526aecf83c3e1767074099c6dd59a13'}>,\n",
              "  <Document: {'content': \"Stop AMC\\nFollow the steps to prepare Agent Metric Collector for an upgrade: \\nStop out of the box collection\\n1\\n. \\nRun the following command in the folder containing the \\nops-content-ctl\\n tool to list all the collection configurations:\\nOn Linux:\\n./ops-content-ctl list collection\\nOn Window:\\nops-content-ctl.exe list collection\\n2\\n. \\nRun the command to stop the out of the box collection:\\nOn Linux:\\n./ops-content-ctl stop collection -n <content name> -v <version> -c <name of the collection configuration>\\nHere, replace <content name> with the name of the content, <version> with version of the content, and <name of the\\ncollection configuration> with the collection configuration name. For example:\\n./ops-content-ctl stop collection -n OpsB_SysInfra_Content -v 2021.05.00 -c AgentMetricCollection_regular\\nOn Windows:\\nops-content-ctl.exe stop collection -n <content name> -v <version> -c <name of the collection configuration>\\nHere, replace <content name> with the name of the content, <version> with version of the content, and <name of the\\ncollection configuration> with the collection configuration name. For example:\\nops-content-ctl.exe stop collection -n OpsB_SysInfra_Content -v 2021.05.00 -c AgentMetricCollection_regular\\nStop custom collection\\nFollow the steps only if you have customized the collection:\\n1\\n. \\nRun the following command in the folder containing the \\nops-content-ctl\\n tool to list all the collection configurations:\\nOn Linux:\\n./ops-content-ctl list collection\\nOn Window:\\nops-content-ctl.exe list collection\\n2\\n. \\nRun the following command in the folder containing the \\nops-content-ctl\\n tool to download the collection configuration:\\nOn Linux:\\n./ops-content-ctl download collection -n <content name> -v <version> -c <name of the collection configuration>\\nHere, replace <content name> with the name of the content, <version> with version of the content, and <name of the\\ncollection configuration> with the collection configuration name. For example:\\n./ops-content-ctl download collection -n OpsB_SysInfra_Content -v 2021.05.00 -c AgentMetricCollection_regular\\nOn Windows:\\nops-content-ctl.exe download collection -n <content name> -v <version> -c <name of the collection configuration>\\nHere, replace <content name> with the name of the content, <version> with version of the content, and <name of the\\ncollection configuration> with the collection configuration name. For example:\\nops-content-ctl.exe download collection -n OpsB_SysInfra_Content -v 2021.05.00 -c AgentMetricCollection_regular\\n3\\n. \\nModify the collection configuration name and then save the file.\\nImportant:\\n If you don't change the collection configuration name, the custom content will be overwritten\\nwith \\nOpsB_SysInfra_Content\\n during the next upgrade.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n504\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '744a33270c97043499d0e90de17dcef'}>,\n",
              "  <Document: {'content': '1\\n. \\nRun the following command for each \\ndisabled\\n collector that you want to enable in the folder containing the \\nops-monitoring\\n-ctl\\n:\\nOn Linux\\n:\\nOn Linux:\\n./ops-monitoring-ctl enable collector -n <name of the collection configuration>\\nOn Windows:\\nops-monitoring-ctl.exe enable collector -n <name of the collection configuration>\\nHere, replace <collection configuration> with the name of the collection. For example:\\n./ops-monitoring-ctl enable collector -n agent-collector-sysinfra_new_custom\\nSample outptut:\\nUpdating collector [agent-collector-sysinfra_new_custom]...\\nSuccessful\\nCollection enabled successfully.\\nVerify status\\nYou may run the following command to view the status of the configured collector:\\nops-monitoring-ctl get collector-status\\nSample output:\\nYou can also run this command to check the status of the configured collector:\\n ./ops-monitoring-ctl get collector-status -o yaml\\nNote\\n: If the \\nops-monitoring-ctl get collector-status\\n  command returns \"NA\" as the status, it indicates that the collector\\nhasn\\'t run yet.\\nContainerized Operations Bridge 2022.11\\nPage \\n508\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '63897ffb77dbfb085d73c3cd5ce8946a'}>,\n",
              "  <Document: {'content': 'Post-upgrade configurations\\nUpdate the load balancer configuration after suite install for RedHat OpenShift\\n  \\nAfter suite installation, you must create a listener for the external access port that\\'s passed during suite install, for the service\\n \\nitom-ingress-controller-svc. \\n. Also, configure \\npulsar proxy, des node port, data broker svc, di receiver, di admin \\nand \\ndata access\\n ports. For\\nmore information, see \\nUpdate the load balancer configuration after suite install\\n.\\nUpdate the \"password reset\" email text\\nTo use the \"Forgot your Password\" feature, \\n you must manually modify the \"Forgotten Password Email Body\" as mentioned in the \\nPost\\nupgrade configurations of OMT\\n.\\nStart Agent Metric Collector (AMC)\\nIf you have used Agent Metric Collector, during an upgrade, the Agent Metric Collector (AMC) configuration files - Credential,\\nTarget, and Collector configuration files, get created and deployed. The custom collection configuration, if present, also gets\\nconfigured. You must enable the Agent Metric Collector after the Operations Bridge upgrade. For more information, see \\nEnable\\nAgent Metric Collector\\n. \\nUpgrade Data Flow Probe (DFP)\\nIf you have used Data Flow Probe,  depending on your upgrade path, it may be necessary to also upgrade or install the Data\\nFlow Probe (DFP). For more information, see \\nUpgrade Data Flow Probe\\n.\\nClean up local registry images after upgrade\\nAfter you perform an upgrade, unwanted images may remain in the local registry which may block future upgrades. It\\'s\\nrecommended to \\nuse OMT\\'s \\ncleanRegistry\\n script\\n to identify and delete unwanted images from the local registry to release some\\nspace.\\nReconfigure Real User Monitor (RUM)\\nAfter you perform an upgrade, you must run the \\nidl_config.sh\\n script to import the \\ncert.pem\\n certificate.\\nRun the command: \\n./idl_config.sh -cacert <PATH>/cert.pem -chart <path/to/charts/opsbridge-suite-<version>.tgz> -namespace <name space>\\nFor example: \\n./idl_config.sh -cacert /root/cert.pem -chart /depot/opsbridge-suite-<version>.tgz -namespace opsb\\nFor more information, see \\nRUM integration with OPTIC Data Lake\\n.\\nEvent forwarding to OPTIC DL\\nTo ensure that the flex table is created, connect to the vertica node and execute the query and make sure that the \\nopr_event\\n is\\nlisted in the result.\\nSELECT table_name, table_schema FROM v_catalog.tables where is_flextable = \\'t\\';\\nFor example,\\ntable_name       table_schema\\n\"opr_event\"      \"mf_shared_provider_default\"\\nIf the \\nopr_event\\n table isn\\'t listed, and the data migration job status failed then trigger the data migration process again to\\nensure that the \\nopr_event\\n table is listed. For more information, see \\nopr_event table missing after upgrade\\n.\\nUpdate event forwarding content pack\\nIf you have integrated classic OBM, you must update the event forwarding content pack for data forwarding to \\nopr_event\\n after\\nperforming an upgrade. For more information, see \\nUpdate Event Forwarding Content Pack\\n.\\nDelete itom-monitoring-sis-adapter-app pod\\nRun the command to delete the \\nitom-monitoring-sis-adapter-app\\n pod:\\nkubectl delete deployment itom-monitoring-sis-adapter-app -n YOUR_NAMESPACE\\nContainerized Operations Bridge 2022.11\\nPage \\n502\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe29ec24929927ed411b81b92ec7cf8'}>,\n",
              "  <Document: {'content': '2\\n. \\nRun \\nProbeGateway.sh\\n with the argument restart.\\nFor example: \\n/opt/UCMDB/DataFlowProbe/bin/ProbeGateway.sh restart\\nStarts the Data Flow Probe on a Linux server.\\nVerify the Data Flow Probe Connection\\nNote:\\n Make sure you use the Internet Explorer browser or the UCMDB local client.\\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Data Flow Probe Setup\\nIf the Data Flow Probe is correctly connected, the domain for which it\\'s created is visible under the \\nDomains and Probes\\n root\\nnode. For example, \\nDefaultDomain (Default)\\n. Under this domain, you will find two nodes: \\nCredentials\\n and \\nData Flow\\nProbes\\n.\\nUnder \\nData Flow Probe\\n, the newly created Data Flow Probe with the given Data Flow Probe name gets displayed.\\nVerify that the status of the probe is \\'Connected\\'.\\nConfigure topology streaming from RTSM to OPTIC Data Lake\\nFollow the steps to forward topology from OBM\\'s RTSM to OPTIC Data Lake:\\n1\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Adapter Management >\\nPulsar Push Adapter > Configuration Files > PulsarPushAdapter/settings.json  \\nIf you are using the UCMDB Local Client go to \\nData Flow Management > Adapter Management > Pulsar Push\\nAdapter > Configuration Files > PulsarPushAdapter/settings.json.\\n2\\n. \\nTo check if you have configured the Pulsar Push Adapter, change the value of \"\\ntest.connection.topic\\n\"\\nto \\n\"persistent://public/default/mf_shared_cmdb_entity_configuration_item_raw\":\\nThis Pulsar topic exists in OPTIC Data Lake. You will use this later to test the communication from RTSM to Pulsar.\\nUpgrade of classic OBM integrated with the OpsBridge suite\\nIf you have upgraded or plan to upgrade a classic OBM system that is forwarding topology to OPTIC Data Lake, you will need to\\nupgrade the DFP with respect to the OBM RTSM version. For DFP upgrade steps, see \\nUpgrade Data Flow Probe\\n.\\nPost upgrade steps\\nFollow these steps after upgrading DFP:\\nConfigure Data Flow Probe\\n1\\n. \\nFor Linux only: After the installation, open the \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n file. Verify that the \\nappilog.agent.probe.integrationsOnlyProbe\\n parameter gets set to \\nfalse\\n.\\n2\\n. \\nIf OBM isn\\'t using TLS, edit the \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n file and change the \\nappilog.agent.pro\\nbe.protocol\\n to HTTP\\n3\\n. \\nIf you enable TLS in OBM, enable TLS in the Data Flow Probe:\\na\\n. \\nOpen the file \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n.\\nb\\n. \\nChange the property \\nserverPortHttps\\n from 8443 to 443.\\n4\\n. \\nIf you enable TLS in OBM, import the corresponding certificates to the \\nJava Keystore\\n of the Data Flow Probe: \\na\\n. \\nTo get the certificate, run the following command on the OBM system:\\nOBM on Windows: \\n%TOPAZ_HOME%\\\\bin\\\\opr-cert-mgmt.bat -export \"OBM Webserver CA Certificate\" PEM \"C:\\\\certificate.pem\"\\nOBM on Linux:\\necho | openssl s_client -showcerts -servername ${HOSTNAME} -connect ${HOSTNAME}:443 2>/dev/null | sed -n \\'/-----BEGIN CERTIFICATE\\nImportant: \\nBy default, DFP tries to start on port 80. If port 80 isn\\'t available, you may not see the newly\\ncreated Data Flow Probe. In such a case, go to \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n  and set\\nthe value of  \\nappilog.agent.callhome.port\\n to an available port (For example: \\nappilog.agent.callhome.port = 8081\\n) and\\nrestart Data Flow Probe.\\nNote\\n: Use the Internet Explorer browser or the UCMDB Local Client.\\nContainerized Operations Bridge 2022.11\\nPage \\n511\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '52435928559dc007017d2e139966687'}>,\n",
              "  <Document: {'content': '-----/,/-----END CERTIFICATE-----/p\\' > /root/certificate.pem\\nImportant: \\n{HOSTNAME}\\n would be the FQDN of the OBM server. \\nb\\n. \\nIf OBM and DFP are on separate systems, copy \\ncertificate.pem \\nto the Data Flow Probe system.\\nFor example: \\nOBM on Linux:\\n \\nscp /root/certificate.pem <DFP Node>:/root\\n \\nc\\n. \\nRun the following command to import the OBM certificate to the Data Flow Probe (DFP) system:\\nOn Linux:\\n/opt/UCMDB/DataFlowProbe/bin/jre/bin/keytool -import -trustcacerts -file /root/certificate.pem -alias obmcert -keystore  /opt/UCMDB/DataF\\nlowProbe/bin/jre/lib/security/cacerts  \\n  \\nOn Windows:\\nC:\\\\UCMDB\\\\DataFlowProbe\\\\bin\\\\jre\\\\bin\\\\keytool.exe -import -trustcacerts -file C:\\\\certificate.pem -alias obm_smperfqa02 -keystore  C:\\\\UCMD\\nB\\\\DataFlowProbe\\\\bin\\\\jre\\\\lib\\\\security\\\\cacerts \\n \\nWhen prompted for password:\\nIf DFP was never started, enter \"\\nchangeit\\n\".\\nIf DFP was already started, then enter the password specified in the \\nSet Up Truststore Password\\n screen of the\\nDFP install wizard.\\nRestart the Data Flow Probe\\nFollow the steps:\\nOn Windows:\\n1\\n. \\nGo to \\n<Data Flow Probe install folder>/bin\\n2\\n. \\nRun \\ngateway.bat.\\nFor example: \\n<Data Flow Probe install folder>/bin/gateway.bat restart\\nStarts the Data Flow Probe on a Windows server.\\nOr\\nUse the stop/start menu entries.\\nOn  Linux:\\n1\\n. \\nGo to\\n /opt/UCMDB/DataFlowProbe/bin\\n2\\n. \\nRun \\nProbeGateway.sh\\n with the argument restart.\\nFor example: \\n/opt/UCMDB/DataFlowProbe/bin/ProbeGateway.sh\\n \\nrestart\\nStarts the Data Flow Probe on a Linux server.\\nVerify the Data Flow Probe Connection\\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Data Flow Probe Setup\\nIf the Data Flow Probe is correctly connected, the domain for which it\\'s created is visible under the \\nDomains and Probes\\n root\\nnode. For example, \\nDefaultDomain (Default)\\n. Under this domain, you will find two nodes: \\nCredentials\\n and \\nData Flow\\nProbes\\n.\\nUnder \\nData Flow Probe\\n, the newly created Data Flow Probe with the given Data Flow Probe name gets displayed.\\nVerify that the status of the probe is \\'Connected\\'.\\nConfigure topology streaming from RTSM to OPTIC Data Lake \\nFollow the steps to forward topology from OBM\\'s RTSM to OPTIC Data Lake:\\n1\\n. \\nOn OBM go to \\nAdministration > RTSM Administration > Data Flow Management > Adapter Management >\\nPulsar Push Adapter > Configuration Files > PulsarPushAdapter/settings.json \\nNote:\\n Make sure you use the Internet Explorer browser or the UCMDB Local Client.\\nImportant\\n: By default, DFP tries to start on port 80. If DFP and OBM (OBM isn\\'t using TLS) are on the same\\nmachine, OBM may occupy port 80 and you may not see the newly created Data Flow Probe. In such a case, go\\nto \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n  and set the value of  \\nappilog.agent.callhome.port\\n to an\\navailable port (For example: \\nappilog.agent.callhome.port = 8081\\n) and restart Data Flow Probe.\\nImportant:\\nUse the Internet Explorer browser or the UCMDB Local Client.\\nTopology integration from an external UCMDB instance into OPTIC DL is currently not supported, but only\\nthrough (classic) OBM/RTSM.\\nContainerized Operations Bridge 2022.11\\nPage \\n512\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bd544202468c72fb27334c0dfad0de25'}>,\n",
              "  <Document: {'content': 'Upgrade Data Flow Probe\\nDepending on your upgrade path, it may be necessary to also upgrade the Data Flow Probe (DFP).\\nUpgrade with the OBM capability selected\\nIf you are planning to forward topology from the containerized OBM to OPTIC Data Lake, you will need to upgrade the DFP with\\nrespect to the OBM RTSM version. For DFP upgrade steps, see \\nUpgrade Data Flow Probe\\n.\\nPost upgrade tasks\\nFollow these steps after upgrading DFP:\\nConfigure the Data Flow Probe \\n1\\n. \\nFor Linux only: After the installation, open the \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n file. Verify if the \\napp\\nilog.agent.probe.integrationsOnlyProbe\\n parameter gets set to \\nfalse\\n.\\n2\\n. \\nIf you enable TLS in OBM, enable TLS in the Data Flow Probe:\\na\\n. \\nOpen the file \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n.\\nb\\n. \\nChange the property \\nserverPortHttps\\n from 8443 to 443.\\n3\\n. \\nIf you installed the Data Flow Probe on a host running the containerized operations bridge single server evaluation\\ndeployment:\\na\\n. \\nOpen the file \\n<Data Flow Probe install folder>/conf/DataFlowProbe.properties\\n.\\nb\\n. \\nSet \\nappilog.agent.callhome.enabled\\n to false.\\nc\\n. \\nSet \\nappilog.agent.callhome.port\\n to 8081.\\n4\\n. \\nImport the corresponding certificates to the\\n Java Keystore\\n of the Data Flow Probe: \\na\\n. \\nTo get the certificate and integrate with DFP, run the following command on the master node of the OBM system:\\necho | openssl s_client -showcerts -servername ${HOSTNAME} -connect ${HOSTNAME}:443 2>/dev/null | sed -n \\'/-----BEGIN CERTIFICATE\\n-----/,/-----END CERTIFICATE-----/p\\' > /root/certificate.pem\\nImportant:\\n{HOSTNAME}\\n would be the FQDN of the external access host.    \\nb\\n. \\nCopy  \\ncertificate.pem \\nto the Data Flow Probe system.\\nFor example\\n: scp /root/certificate.pem <DFP Node>:/root \\nc\\n. \\nRun the following command to import the OBM certificate to the Data Flow Probe (DFP) system:\\nOn Linux:\\n/opt/UCMDB/DataFlowProbe/bin/jre/bin/keytool -import -trustcacerts -file /root/certificate.pem -alias obmcert -keystore  /opt/UCMDB/DataF\\nlowProbe/bin/jre/lib/security/cacerts  \\n  \\nOn Windows:\\nC:\\\\UCMDB\\\\DataFlowProbe\\\\bin\\\\jre\\\\bin\\\\keytool.exe -import -trustcacerts -file C:\\\\certificate.pem -alias obm_smperfqa02 -keystore  C:\\\\UCMD\\nB\\\\DataFlowProbe\\\\bin\\\\jre\\\\lib\\\\security\\\\cacerts \\n \\nWhen prompted for password:\\nIf DFP was never started, enter \"\\nchangeit\\n\".\\nIf DFP was already started, then enter the password specified in the \\nSet Up Truststore Password\\n screen of the\\nDFP install wizard.\\nRestart the Data Flow Probe\\nFollow the steps:\\nOn Windows:\\n1\\n. \\nGo to \\n<Data Flow Probe install folder>/bin\\n2\\n. \\nRun \\ngateway.bat.\\nFor example: \\n<Data Flow Probe install folder>/bin/gateway.bat restart\\nStarts the Data Flow Probe on a Windows server.\\nOr\\nUse the stop/start menu entries.\\nOn  Linux:\\n1\\n. \\nGo to \\n/opt/UCMDB/DataFlowProbe/bin\\nImportant: \\nThe version of the Data Flow Probe must be the same as the RTSM version\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n510\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b42159ccb2f9559240705d5e4b740a10'}>,\n",
              "  <Document: {'content': 'Important: \\nNote down the time when you start the data collection. You may later use this time to collect the\\nmissed data for the upgrade period. For more information, see, \\nAMC collector configuration \\n page.\\n\\ue91b\\n\\ue91b\\nNote: \\nRun the command to check the metrics collector log:\\nkubectl logs -n <namespace> <oa-metric-collector-pod> -c <oa-metric-collector>\\nFor example:\\nkubectl logs -n opsb itom-monitoring-oa-metric-collector-7f447bf6bc-fqcv2 -c itom-monitoring-oa-metric-collector\\nRun the command to check the discovery collector log:\\nkubectl logs -n <namespace> <oa-discovery-collector-pod> -c <oa-discovery-collector>\\nFor example:\\nkubectl logs -n opsb itom-monitoring-oa-discovery-collector-7767f8b5d-7mzkr -c itom-monitoring-oa-discovery-collector\\n \\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n509\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a9e7f13a9faaef45505e693c31b8580'}>,\n",
              "  <Document: {'content': 'If you are using the UCMDB Local Client go to \\nData Flow Management > Adapter Management > Pulsar Push\\nAdapter > Configuration Files > PulsarPushAdapter/settings.json.\\n2\\n. \\nTo check if you have configured the Pulsar Push Adapter, change the value of \"\\ntest.connection.topic\\n\" to\\n\"persistent://public/default/mf_shared_cmdb_entity_configuration_item_raw\\n\":\\nThis Pulsar topic exists in OPTIC Data Lake. You will use this later to test the communication from RTSM to Pulsar.\\nContainerized Operations Bridge 2022.11\\nPage \\n513\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '584e62020abf2ca093a8134078f16b93'}>,\n",
              "  <Document: {'content': \"3\\n. \\nRestart the \\nomi\\n and \\nitom-ucmdb\\n pods.\\nUpdate the capability password keys\\nIf you have changed or reset the \\nOpsBridge admin password\\n, update the \\nidm_opsbridge_admin_password\\n. \\nFollow these steps:\\n1\\n. \\nRun the following command to get into the IdM pod:\\nkubectl exec -it $(kubectl get pod -n <namespace> -ocustom-columns=NAME:.metadata.name |grep idm|head -1) -n <namespace> -c idm sh\\nwhere, \\n<namespace>\\n is the actual namespace where you've deployed IdM. \\n2\\n. \\nRun the command to update the secrets: \\nupdate_secret <secret key> <password created for DB to upgrade suite>\\nFor example: \\nIf you have deployed the suite with OPTIC Reporting capability \\nupdate_secret MA_DB_USER_PASSWORD_KEY <password created for DB to upgrade suite>\\nupdate_secret CM_DB_PASSWD_KEY <password created for DB to upgrade suite>\\nFor example: \\nIf you have deployed the suite with AEC capability \\nupdate_secret \\nBVD_DB_USER_PASSWORD_KEY\\n <password created for DB to upgrade suite>\\nFor example: If you have changed or reset the \\nOpsBridge admin password\\nupdate_secret idm_opsbridge_admin_password <current OpsBridge admin password>\\nImportant\\n: If you have ever changed or reset the \\nOpsBridge admin password\\n, and skipped this step, the\\nadmin account gets locked after the upgrade. If the admin account gets locked after the upgrade, apply the\\nworkaround for the known issue \\nOCTCR19S1319389\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n516\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '17ac87f56ac8b025d74178b38135543'}>,\n",
              "  <Document: {'content': \"Upgrade storage provisioner chart\\nOn the master (control plane) node, run the following command to install the storage provisioner chart:\\nhelm upgrade <lpv chart release name> <new lpv chart name> -n core  \\nwhere:\\n<lpv chart release name>:\\n Name of the local storage provision chart deployment name. For example: \\nlpv\\n. You can find the\\nrelease name using the command \\nhelm list -A\\n. \\n<new lpv chart name>:\\n \\nitom-kubernetes-local-storage-provisioner-<chart version>.tgz\\n. The file is under \\n$K8S_HOME/charts\\n on your\\nfirst master (control plane) node. For example: \\n$CDF_HOME/charts/itom-kubernetes-local-storage-provisioner-2.3.3-xxx.tgz\\n.\\nFor example: \\nhelm upgrade lpv $CDF_HOME/charts/itom-kubernetes-local-storage-provisioner-2.3.3-xxx.tgz -n core\\nIf you've used \\nlpv.yaml\\n in your existing installation, upgrade with the following command:\\nhelm upgrade <helm_deployment_name> <chart_name> -n core [-f <YAML file>]\\nExample:\\nhelm upgrade lpv $CDF_HOME/charts/itom-kubernetes-local-storage-provisioner-2.3.3-xxx.tgz -n core -f ./lpv.yaml\\nExample without YAML:\\nhelm upgrade lpv $CDF_HOME/charts/itom-kubernetes-local-storage-provisioner-2.3.3-xxx.tgz -n core\\nVerify the storage provisioner chart upgrade\\nWait about 2 minutes (observe the last column of the \\nkubectl\\n command output) for the provisioning process to complete and\\nthen, check for the provisioner pods. \\nkubectl get pod -n core | grep local-volume\\nExample:\\n# kubectl get pod -n core | grep local-volume\\nlocal-volume-provisioner-9gh5j              1/1     Running      0          2m46s\\nlocal-volume-provisioner-9m669              1/1     Running      0          3m10s\\nlocal-volume-provisioner-d4vvp              1/1     Running      0          2m21s\\nlocal-volume-provisioner-kdkqh              1/1     Running      0          3m21s\\nlocal-volume-provisioner-zfhct              1/1     Running      0          2m41s\\nlocal-volume-provisioner-zp55w              1/1     Running      0          3m21s\\nUntil each pod restarts, run the following command once a minute to get  the latest age of the pods:\\nkubectl get pod -n core | grep -e RESTARTS -e local-volume\\nNote\\n:\\n You must wait for an additional \\n3 mins\\n after the storage provisioner chart upgrade before you\\nperform the application upgrade.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n514\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3b3e05a89550b45d2b9e437de6abd8f1'}>,\n",
              "  <Document: {'content': 'Update secrets\\n This topic mentions the additional secrets that are required when you are performing \\nN-2\\n upgrade.\\nAdd the following secrets to the values.yaml, secrets section. Pass the base64 encoded password to the same parameters.\\nPassword \\nDescription\\nBTCD_DB_\\nPASSWD_K\\nEY\\nPassword for NOM metric transformation and Hyperscale Observability.\\nHyperscale\\nObservability\\nOPTIC_DAT\\nALAKE_INT\\nEGRATION_\\nPASSWOR\\nD\\nIDM Integration user password of the providing deployment when using Shared OPTIC DL. \\nIf you plan to use Shared OPTIC DL, retrieve the password as mentioned below and pass the same in\\nthe secrets section of values.yaml.\\nTo retrieve the decoded password, execute the command:\\nkubectl -n <namespace of the providing application> get secret <providing_suite_secret> --template={{.data.idm_integration_admin_password}}\\nExample:\\nkubectl -n <nom-helm> get secret <nom-secret> --template={{.data.idm_integration_admin_password}}\\nShared\\nOPTIC\\nReporting\\nUpdate the passwords in Base64 format\\nThe samples/values.yaml\\n file will have all the required secret keys. You must update respective passwords in the base64 format.\\nExample:  \\n   \\nIf the password for \\nidm_opsbridge_admin_password\\n is Testing@123, follow the below example to update the password in base64\\nformat:          \\necho –n Testing@123 | base64 \\nCopy the output value and update it in the \\nvalues.yaml\\n  \\nfor the same key\\n idm_opsbridge_admin_password.\\nExample:\\nidm_opsbridge_admin_password: VGVzdGluZ0AxMjM=\\nCheck password complexity\\n1\\n. \\nRun the following command on the control plane or bastion node to verify the password complexity for \\nucmdb_uisysadmin_p\\nassword\\n: \\npass=$(kubectl get secret opsbridge-suite-secret -n <suite-namespace> --template={{.data.ucmdb_uisysadmin_password}} | base64 -d);regex=\\'\"<>~;{}`!@#^*$\\';if [[ $pass =~ [$regex] ]] || echo $pass | grep -q \"\\'\" ; then echo Password complexity is incorrect. Please change the password.; else echo Password complexity is correct.; fi \\nFor example: \\npass=$(kubectl get secret opsbridge-suite-secret -n opsb-helm --template={{.data.ucmdb_uisysadmin_password}} | base64 -d);regex=\\'\"<>~;{}`!@#^*$\\';if [[ $pass =~ [$regex] ]] || echo $pass | grep -q \"\\'\" ; then echo Password complexity is incorrect. Please change the password.; else echo Password complexity is correct.; fi\\nPassword complexity is incorrect. Please change the password.\\nIf the command output is \\nPassword complexity is correct.\\n then \\ndon\\'t\\n update the \\nucmdb_uisysadmin_password\\n. \\nIf you see the output \\nPassword complexity is incorrect\\n, you must update the password. The password policy requires the\\npassword to include at least one upper case, one lower case, one numeric, one of these special characters (\\n,/\\\\:._?&%=+-[]\\n()|\\n) and \\nexactly 8 characters\\n long and base64 encoded. \\nExample to convert to base64:\\necho -n \"Abcdef&1\" | base64\\n2\\n. \\nRun the following command to update the \\nucmdb_uisysadmin_password:\\nhelm upgrade <releasename> <chart> -n  <suite namespace> --reuse-values --set secrets.ucmdb_uisysadmin_password=<newpassword in base64 format>\\nFor example: \\nhelm upgrade deployment01 ./opsbridge-suite-202x.xx.x.tgz -n opsb-helm --reuse-values --set secrets.ucmdb_uisysadmin_password=QWJjZGYmMTI= \\nContainerized Operations Bridge 2022.11\\nPage \\n515\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'abce087a95c9ce803bd45de9640fa1dd'}>,\n",
              "  <Document: {'content': 'Update the values file\\nUpdating values.yaml for upgrading Operations Bridge 2022.05 to 2022.11\\n1\\n. \\nRun the following command to retrieve the \\nvalues.yaml\\n from the existing chart:\\nhelm get values <helm deployment name> -n <suite namespace> > <VALUES_FILE_NAME>\\nFor example: \\n \\nhelm get values deployment01 -n opsb-helm > /var/tmp/values2022_05_0.yaml\\nYou may run the command \\nhelm list -A to \\nlist all the deployment names and the suite namespaces, and use the same in \\nhel\\nm get values\\n command.\\n2\\n. \\nThe suite zip (\\nopsbridge-suite-chart-2022.11.x.zip\\n) contains the \\nvalues.yaml \\nfile \\nspecific to the applicable Kubernetes\\nprovider,\\n under \\nsamples\\n directory. Refer to the same and update the retrieved values.yaml.\\n    \\n \\n3\\n. \\nIf you are upgrading an environment deployed using K3S, and  if the \\nstorageClasses\\n section parameters are\\n empty\\n or \\nnull\\nthen, update the \\nstorageClasses\\n section parameters to \\n\"local-path\",\\n as shown in the example before proceeding with the\\nupgrade.\\n \\n    # Used to inject storageClasses into PVC creation.\\n    # User can change any/all to match the actual storageClasses used in their environment.\\n    storageClasses:\\n      # All 4 OPSB PVCs are created using \"default-rwx\", omi-artemis PVC is created using \"default-rwo\".\\n      default-rwx: \"local-path\"\\n      default-rwo: \"local-path\"\\n4\\n. \\nUCMDB is now a separate section, the parameter reference will be referred as \\nucmdbserver.deployment.database.<parameter>\\n ucmdbserver:\\n  deployment:\\n    replicaCount: 2                                  # If OBM HA is disabled, Update this value to 1.\\n    database:\\n      dbName: rtsm\\n      user: rtsm\\n      userPasswordKey: RTSM_DB_USER_PASSWORD_KEY           # [DO NOT CHANGE]\\n5\\n. \\nIf you are upgrading an Azure deployment with OBM capability\\n, then you must have the following sections and\\nupdate the value for the load balancer IP:\\n # Required only for Azure. Private IP for Azure Load Balancer configuration\\n  expose:\\n    internalLoadBalancer:\\n      ip:\\n # Required only for Azure. Private IP for Azure Load Balancer configuration\\n  global:\\n    loadBalancer:\\n      ip:\\n \\nYou can reconfigure the existing installation after a successful upgrade. Follow the steps in \\nAdd/Remove capabilities\\n.\\nUpdating values.yaml for upgrading Operations Bridge 2021.11 to 2022.11\\n1\\n. \\nRun the following command to retrieve the \\nvalues.yaml\\n from the existing chart:\\nhelm get values <helm deployment name> -n <suite namespace> > <VALUES_FILE_NAME>\\nFor example: \\n \\nhelm get values deployment01 -n opsb-helm > /var/tmp/values2021_11_0.yaml\\nYou may run the command \\nhelm list -A to \\nlist all the deployment names and the suite namespaces, and use the same in \\nhel\\nm get values\\n command.\\n2\\n. \\nThe suite zip (\\nopsbridge-suite-chart-2022.11.x.zip\\n) contains the \\nvalues.yaml \\nfile \\nspecific to the applicable Kubernetes\\nprovider,\\n under \\nsamples\\n directory. Refer to the same and update the retrieved values.yaml. \\nIn 2022.11 values.YAML, all the capabilities have changed from \\ntags\\n to \\nglobal.services\\n.  \\nCloud Monitoring\\n capability is renamed to\\n Hyperscale Observability\\n in 2022.11\\n.\\nImportant:\\n Do not enable\\n \\nany new capability\\n \\nwhile you upgrade \\nfrom Operations Bridge. \\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n517\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9026298b5a3601f1a0677a2b7dd38529'}>,\n",
              "  <Document: {'content': '  #Password for External OBMs RTSM user.The username will be provided in Helm values.yaml under global.amc.rtsmUsername\\n  OBM_RTSM_PASSWORD:\\n  #Password for External OBMs user with Content Packs upload permissions. The username will be provided in Helm values.yaml under global.monitoringService.obmUsername\\n  OBM_USER_PASSWORD_KEY:\\n  #UCMDB Master Key.This key is not related to any configured UCMDB schema or database password. Its value can be anything as it is used for encryption.You can provide any value for this encryption key but its length must be of 32 chars only\\n  #The password must meet the following requirements:\\n  #\"Length must be exactly 32\"\\n  #\"Must contain 1 or more upper case characters\"\\n  #\"Must contain 1 or more lower case characters\"\\n  #\"Must contain 1 or more digit (0-9) characters\"\\n  #\"Must contain 1 or more special characters in: :/._+-[]\"\\n  ucmdb_master_key:\\n  \\n  #System Administrator Password used for OBM JMX and UCMDB sysadmin user\\n  #The password must meet the following requirements:\\n  #\"Length must be at least 8 characters\"\\n  #\"Length cannot exceed 64 characters\"\\n  #\"Must contain 1 or more upper case characters\"\\n  #\"Must contain 1 or more lower case characters\"\\n  #\"Must contain 1 or more digit (0-9) characters\"\\n  #\"Must contain 1 or more special characters in: -+\"?/.,<>:;[]{}`~!@#%^&*()_=|$\"\\n  sys_admin_password:\\n  \\n  #Password for External Universal Discovery user.The username will be provided in Helm values.yaml under global.cms.udUsername\\n  UD_USER_PASSWORD:\\n  #Password for smtpServer user. Used for report scheduling with pdf-print tool.\\n  schedule_mail_password_key:\\n  # IDM Integration User password of the providing deployment when shared optic DL is used.\\n  OPTIC_DATALAKE_INTEGRATION_PASSWORD:\\nOptional\\n. This step applies to Operations Bridge and Network Operations Manager integration. If you are using the\\nintegration in your exisitng deployment, add the following in 2022.11\\n \\nvalues.yaml\\n file\\n# Tune the performance of itom-nom-metric-transformation pod using the below configurations.\\nnommetricstransform:\\n  deployment:\\n    database:\\n      dbName: btcd                              # OK to change for EXTERNAL Postgres\\n      user: btcd                                # OK to change for EXTERNAL Postgres\\n      userPasswordKey: BTCD_DB_PASSWD_KEY       # [DO NOT CHANGE]\\n # Used to inject storageClasses into PVC creation.\\n    # User can change any/all to match the actual storageClasses used in their environment.\\n    storageClasses:\\n      # All 4 OPSB PVCs are created using \"default-rwx\", omi-artemis PVC is created using \"default-rwo\".\\n      default-rwx:                    # set it to \"cdf-nfs\" if using NFSProvisioner capability for PV creation\\n      default-rwo:                    # set it to \"cdf-nfs\" if using NFSProvisioner capability for PV creation\\nIf you are upgrading an environment deployed using K3S, and  if the \\nstorageClasses\\n section parameters are\\n empty\\n or \\nn\\null\\n then, update the \\nstorageClasses\\n section parameters to \\n\"local-path\",\\n as shown in the example before proceeding with\\nthe upgrade.\\n    # Used to inject storageClasses into PVC creation.\\n    # User can change any/all to match the actual storageClasses used in their environment.\\n    storageClasses:\\n      # All 4 OPSB PVCs are created using \"default-rwx\", omi-artemis PVC is created using \"default-rwo\".\\n      default-rwx: \"local-path\"\\n      default-rwo: \"local-path\"\\nYou can reconfigure the existing installation after a successful upgrade. Follow the steps in \\nAdd/Remove capabilities\\n.\\nImportant:\\n \\nDon\\'t change the default values for the \\nStorageClass\\n, if you have not changed the default\\nStorageClass\\n value in 2021.11\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n519\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'af14d923080019026eedb38da644ea49'}>,\n",
              "  <Document: {'content': 'Upgrade using CLI\\nUpgrade suite using helm upgrade command:\\nhelm upgrade <helm deployment name> <chart> -n <suite namespace> -f <values.yaml> \\nWhere:\\n<helm deployment name>: \\nDeployment name where your Operations Bridge suite is installed. \\n<suite namespace>: \\nNamespace where your Operations Bridge suite is installed.\\n<values.yaml>: \\nGive the full path to the \\nvalues.yaml\\n file restored from the current deployment as mentioned in the\\ntopic \\nUpdate the values file\\n.\\n<chart>: \\nThe absolute path to the suite chart package. Example: \\nopsbridge-suite-20xx.xx.x.tgz\\n. chart file is available under \\nc\\nharts\\n directory mentioned in the topic \\nDownload, unzip and verify the upgrade package\\n.\\nFor example:\\nhelm upgrade opsb-helm /root/2022.05/chart/opsbridge-suite-chart/charts/opsbridge-suite-20xx.xx.0.tgz --namespace opsb-helm -f values.yaml \\nRelease \"opsb-helm\" has been upgraded. Happy Helming!\\nNAME: opsb-helm\\nLAST DEPLOYED: Fri Sep 16 14:16:28 20xx\\nNAMESPACE: opsb-helm\\nSTATUS: deployed\\nREVISION: 8\\nTEST SUITE: None\\nNOTES:\\n**********************************************************************************\\n** WARNING:                                                                     **\\n**                                                                              **\\n** 1. If you used a values.yaml file to install Opsbridge Suite you must manage **\\n** access to this file carefully as it contains sensitive information.          **\\n** Micro Focus recommends restricting access by leveraging file permissions,    **\\n** by moving the file to a secure location or by simply deleting the file.      **\\n** Failing to secure values.yaml, you may exposing the system to increased      **\\n** security risks. You understand and agree to assume all associated            **\\n** risks and hold Micro Focus harmless for the same.                            **\\n** It remains at all times the Customer\\'s sole responsibility to                **\\n** assess its own regulatory and business requirements. Micro Focus does not    **\\n** represent or warrant that its products comply with any specific legal        **\\n** or regulatory standards applicable to Customer                               **\\n** in conducting Customer\\'s business.                                           **\\n**                                                                              **\\n**********************************************************************************\\n**********************************************************************************\\n** NOTE:                                                                        **\\n**                                                                              **\\n** 1. Please run the post installation script to create load balancer to access **\\n** the Opsbridge UI                                                             **\\n**********************************************************************************\\nThank you for deploying Opsbridge Suite 2.4.0+20221100.149\\nInstalled Component:\\n        Reporting\\n    Automatic Event Correlation\\n    Stakeholder Dashboard\\n    Hyperscale Observability\\n    Agentless Monitoring\\nDatabase Used:\\n    External Vertica, Host:  vertmyd055sverti-01.example.net\\n    External oracle, Host: myd-hvm.example.net for BVD/IDM/Autopass/OBM.\\nConfiguration:\\n    Required PVC will be created during Installation as long as corresponding PV are available.\\n    Agent Metric Collection is enabled, OBM host: Multi2-OBM.example.net\\nShown below are important URLs for you:\\nReporting UI (Business Value Dashboard):\\n    https://myd-hvm.example.net:443/bvd\\nLicense Management UI:\\n    https://myd-hvm.example.net:443/autopass\\nUser Management UI:\\n    https://myd-hvm.example.net:443/idm-admin\\nContainerized Operations Bridge 2022.11\\nPage \\n520\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cbda1a71c313f805a34cffb42a46a8a2'}>,\n",
              "  <Document: {'content': 'You must replace the \\ntags\\n section by copying the \\nglobal.services\\n section from 2022.11 values.yaml. All the\\nCapabilities deployment is disabled by default, you must uncomment the specific line to deploy by removing the\\nleading \"#\", under the \\nglobal.services \\nsection depending on the capabilities you had deployed in 2021.11.\\nThe following sections have changed in 2022.11:\\n global:\\n  # [REQUIRED] Externally accessible hostname/FQDN (Load balancer OR Master Node)\\n  externalAccessHost:\\n  # [REQUIRED] Externally accessible port (Load balancer OR Master Node). External Access Port along with External Access Host is used to access Opsbridge Suite.\\n  externalAccessPort:\\n  # Every Service here represents a functionality Opsbridge Suite offers. Users can enable one or multiple components.\\n  # opticReporting: Reporting component contains OPTIC Data Lake, BVD and Collection Service, if \"true\" reporting will be installed.\\n  # automaticEventCorrelation: Automatic Event Correlation component contains Automatic Event Correlation and OPTIC Data Lake.\\n  # stakeholderDashboard: Stakeholder dashboard contains BVD component.\\n  # obm: Operations Bridge Manager component.\\n  services:\\n# User must uncomment the capabilities they want to deploy and comment the ones they do no want to deploy\\n    automaticEventCorrelation:\\n#      deploy: true\\n    stakeholderDashboard:\\n#      deploy: true\\n    obm:\\n#      deploy: true\\n    hyperscaleObservability:\\n#      deploy: true\\n    agentlessMonitoring:\\n#      deploy: true\\n    anomalyDetection:                           # Anomaly Detection is a technology preview capability. Enable this capability to try a technology preview of the new Anomaly Detection Configurator and OPTIC DL Source Configurator user experience interfaces.\\n#      deploy: true\\n    opticReporting:\\n#      deploy: true\\nUCMDB is now a separate section, the parameter reference will be referred as \\nucmdbserver.deployment.replicaCount\\n ucmdbserver:\\n  deployment:\\n    replicaCount: 1                                  # If OBM HA is disabled, Update this value to 1.\\n    database:\\n      dbName: rtsm\\n      user: rtsm\\n      userPasswordKey: RTSM_DB_USER_PASSWORD_KEY           # [DO NOT CHANGE]\\n2022.11 values.yaml lists all the secrets, you must include these secrets and update the corresponding base64\\nconverted passwords for all applicable secrets depending on the existing capabilities in your environment.\\n#Secrets Password must be provided in Base64 encoded format.\\nsecrets:\\n  #Admin Password for IDM admin user. This password will be used to log into IDM UI.\\n  #The password must meet the following requirements:\\n  #\"Length must be at least 8 characters\"\\n  #\"Length cannot exceed 64 characters\"\\n  #\"Must contain 1 or more upper case characters\"\\n  #\"Must contain 1 or more lower case characters\"\\n  #\"Must contain 1 or more digit (0-9) characters\"\\n  #\"Must contain 1 or more special characters in: -+\"?/.,<>:;[]{}`~!@#%^&*()_=|$\"\\n  idm_opsbridge_admin_password:\\n  #Verica DBA and RO User passwords\\n  ITOMDI_DBA_PASSWORD_KEY:\\n  ITOMDI_RO_USER_PASSWORD_KEY:\\n  #Postgres/Oracle Db Passwords for different users\\n  AUTOPASS_DB_USER_PASSWORD_KEY:\\n  BVD_DB_USER_PASSWORD_KEY:\\n  CM_DB_PASSWD_KEY:\\n  IDM_DB_USER_PASSWORD_KEY:\\n  MA_DB_USER_PASSWORD_KEY:\\n  SNF_DB_USER_PASSWORD_KEY:\\n  OBM_MGMT_DB_USER_PASSWORD_KEY:\\n  OBM_EVENT_DB_USER_PASSWORD_KEY:\\n  RTSM_DB_USER_PASSWORD_KEY:\\n  BTCD_DB_PASSWD_KEY:\\nImportant:\\n Do not enable\\n \\nany new capability\\n \\nwhile you upgrade \\nfrom Operations Bridge. \\nDON\\'T\\n uncomment \\ndeploy:\\n \\nfalse\\n for \\nopticDataLake,\\n retain the default options as it is\\n. \\n\\ue91b\\n\\ue91b\\nImportant:\\n Please set \\nucmdbserver.deployment.replicaCount\\n to \\n1,\\n if you have containerized\\n OBM\\ncapability with HA deployed\\n.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n518\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62442ee3901a92250ac1714b7f89954b'}>,\n",
              "  <Document: {'content': 'Hyperscale Observability:\\n    Download ops-monitoring-ctl CLI from:\\n    Linux: https://opsb.itom-toolkit-khush.itombyok.internal:443/staticfiles/ops-monitoring-ctl/linux/ops-monitoring-ctl\\n    Windows: https://opsb.itom-toolkit-khush.itombyok.internal:443/staticfiles/ops-monitoring-ctl/windows/ops-monitoring-ctl.exe\\nGrafana UI:\\n    https://opsb.myexample.itombyok.internal:5443/grafana\\nPerformance Troubleshooter:\\n    https://opsb.myexample.itombyok.internal:443/dashboard\\nContainerized Operations Bridge 2022.11\\nPage \\n521\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e876f16446859b7808c1c0da1449131'}>,\n",
              "  <Document: {'content': \"-h\\n, \\n-?\\n, \\n--h\\nelp\\nDisplay the script usage or help.\\n-s\\n, \\n--\\nsilent\\n, \\n--su\\nppress\\nOption to run the script without prompting for variable input. The passwords gets pulled from\\nenvironment variables: \\nVERTICA_DBA_PASS, VERTICA_RW_PASSWD, VERTICA_RO_PASSWD\\n.\\n-w\\n, \\n--dbap\\nass\\nWith this option, you may specify the database administrator password on the command line. \\nIf you use \\n-w\\n without a password and \\nVERTICA_DBA_PASS\\n is already given in the environment variable,\\nthen the \\ndbinit\\n script uses the environment variable. \\n-c\\n, \\n--tlscrt\\nYou must type the path of the file that contains the base64 encoded server certificate along with the\\ncertificate file name. If you don't give this option, the Vertica TLS remains as configured earlier.\\nYou must use this option with \\n--tlskey\\n, to enable TLS communication with Vertica.\\n-k\\n, \\n--\\ntlskey\\nYou must type the path of the file that contains the base64 encoded server key certificate along with\\nthe key file name. \\n-e\\n, \\n--tlsenf\\norce\\nOption to enforce TLS. The default is \\ntrue\\n. When you set \\ntlsenforce\\n to \\ntrue\\n, Vertica gets configured to\\nreject non TLS communication. When set to \\nfalse\\n, then Vertica will accept non TLS communication. \\nYou can use this option without the \\n--tlscrt\\n and \\n--tlskey\\n options to disable or enable TLS enforcement.\\n-t\\n, \\n--tlsonl\\ny\\nOption to apply only the TLS changes to Vertica. You must use \\n--tlscrt\\n and \\n--tlskey \\noptions or \\n--\\ntlsenforce\\n option with this option.\\nThe script performs only the TLS configurations if you use the \\ntlsonly\\n option and the other settings in\\nVertica remain the same. \\n-v,--\\nverbose\\nOption to view the detailed output on the console that includes all the SQL queries invoked.\\n-n,--nochan\\nge\\nOption to view the SQL queries that are invoked when the script runs. This option doesn't change\\nVertica. It displays only the queries. \\nOption\\nDescription\\nFor example:\\nTo configure the server key and certificate for TLS communication along with other configurations:\\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce true\\nNote that the \\n--tlsonly \\noption isn't required in the command while you perform other configurations using the script.\\nTo configure the server key and certificate for TLS communication and to disable non TLS communication with Vertica: \\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce true --tlsonly\\nTo configure the server key and certificate for TLS communication and to allow non TLS communication with Vertica: \\n./dbinit.sh --tlscrt /tmp/servercert.crt --tlskey /tmp/servercert.key --tlsenforce false --tlsonly\\nIf you have a Vertica system already configured for TLS communication, to disable non TLS connections to Vertica:\\n./dbinit.sh --tlsenforce true --tlsonly\\n8\\n. \\nIf you have changed the TLS settings, make sure to restart the Vertica database to apply the TLS settings.\\nRelated topics\\nWhile you upgrade, you may see the \\nUser available location does not exist on node\\n error. See the troubleshooting topic \\nOPTIC\\nDL Vertica Plugin fails to upgrade with an error\\n to resolve this error. \\nNote\\n: It isn't recommended to set the  \\ntlsenforce\\n to \\nfalse\\n.\\nImportant\\n: If you want to use a new CA signed certificate, you must regenerate the certificate and key files\\nand update the configuration in Vertica. To update the configuration, run \\ndbinit\\n again with the new certificate\\nand key file using the \\n--tlscert\\n and \\n--tlskey\\n options.\\nContainerized Operations Bridge 2022.11\\nPage \\n526\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '43564c486cced5073bdc2b476abfcbf7'}>,\n",
              "  <Document: {'content': \"Disable the Agent Metric Collector\\nAfter the upgrade, OOTB configurations run automatically. For AMC to discover nodes and collect metrics, OBM should be\\naccessible. After suite upgrade only OBM connection will be available. If AMC not disabled, collection will be failing. Therefore,\\nyou need to disable the agent metric collection before you upgrade. You can disable all collection configurations or OOTB\\ncollection configurations based on the requirements.\\nRun the following command in the folder containing the \\nops-monitoring-ctl\\n tool to list all the collection configurations:\\nOn Linux:\\n./ops-monitoring-ctl get collector\\nOn Windows:\\nops-monitoring-ctl.exe get collector\\nSample output:\\nRun the command to disable the Agent Metric collection:\\nOn Linux:\\n./ops-monitoring-ctl disable collector -n <content name> \\nOn Windows:\\nops-monitoring-ctl.exe  disable collector -n <content name> \\nHere, replace <content name> with the name of the content. For example:\\n./ops-monitoring-ctl disable collector -n agent-collector-sysinfra-001\\nVerify that the collection is disabled\\nRun the following command in the folder containing the \\nops-monitoring-ctl\\n tool to verify the status of collection configurations:\\nOn Linux:\\n./ops-monitoring-ctl get collector-status -o yaml\\nOn Windows:\\nops-monitoring-ctl.exe get collector-status -o yaml\\nSample output:\\n# ops-monitoring-ctl.exe get collector-status -o yaml\\nName: agent-collector-sysinfra_new_custom\\nmetric:\\ntimestamp: 1650360045\\ndurationInMilliSec: 1\\nstate: Metric collection is disabled on 19 Apr 22 14:50 IST\\nsummary: Metric collection is disabled\\ndiscovery:\\ntimestamp: 1650360045\\ndurationInMilliSec: 1\\nstate: Discovery disabled on 19 Apr 22 14:50 IST\\nsummary: collection status disabled\\nConfigure agent metric collector parameter\\nYou can find these parameters in \\nvalues.yaml\\n file.\\nglobal.isAgentMetricCollectorEnabled\\n. The default value is true.\\nglobal.autoStartAgentMetricCollector\\n. The default value is true.\\nTo check the current status, run the following command:\\nhelm get values <deployment> -n <namespace> -o json | jq '.global.autoStartAgentMetricCollector'\\nNote\\n: To \\ndisable custom collection\\n, use the aforementioned command and replace name of the collection\\nconfiguration with custom configuration.\\nContainerized Operations Bridge 2022.11\\nPage \\n522\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b4683e7cb09656191903eab37609ec9'}>,\n",
              "  <Document: {'content': 'Back up the external certificates\\nTo corroborate if you have imported external certificates into OPTIC Data Lake with the \\nidl_config.sh \\nscript, for example as part\\nof configuring OBM or to \\nestablish trust between OPTIC Data Lake’s Pulsar and OBM’s Data Flow Probe to sync topology\\n,\\nexecute one of the following commands on a Kubernetes control plane node. If the root user is required, start an interactive\\nroot shell before executing the command by running \\nsudo su\\n first.\\nWhen upgrading from version 2020.10 Patch 2 or older:\\nif kubectl -n <SUITE_NAMESPACE> get secret receiver-secret -o json | \"$CDF_HOME/bin/jq\" -e \\'.data | keys | length\\' &>/dev/null; then echo \"Certificates found.\"; else echo \"Certificates NOT found.\"; fi\\nWhen upgrading from version 2021.05 or newer:  \\nif kubectl -n $(helm list -A | awk \\'/opsbridge-suite-/{print $2}\\') get cm opsb-ca-certificate api-client-ca-certificates -o json | \"$CDF_HOME/bin/jq\" -e \\'[.items[].data] | values | add | keys | length\\' &>/dev/null; then\\n  echo \"Certificates found.\"\\nelse\\n  echo \"Certificates NOT found.\"\\nfi\\nIf you see the message \"Certificates NOT found\", you can skip this backup step. If you believe trust had been established\\nbefore but was lost, you will need to reconfigure it on the OPTIC Data Lake side after the upgrade. The steps for\\nreconfiguration depend on which product was integrated. For an example with classic OBM, see \\nConfigure the suite with OBM\\ncertificates\\n.\\nIf you see the message \"Certificates found\", create a backup of the certificates with the following steps:\\n1\\n. \\nLog into the Kubernetes control plane (master) or bastion node as root user or a \\nSUDO\\n user.\\n2\\n. \\nRun the following commands substituting \\n<SUITE_NAMESPACE>\\n appropriately, If the super user is needed, execute \\nsudo\\nsu\\n before running the commands. \\nTo upgrade from \\n2020.10 Patch 2 or older\\n execute:\\nCERTS=\"$(kubectl -n <SUITE_NAMESPACE> get secret receiver-secret -o jsonpath=\\'{.data}\\')\"\\necho \"authorizedClientCAs:\" >/tmp/backup_extcerts.yaml\\nmapfile -t KEYS < <(echo \"$CERTS\" | \"$CDF_HOME/bin/jq\" -r \\'. | keys[]\\')\\nfor key in \"${KEYS[@]}\"; do\\n  echo \"  ${key}: |\" >>/tmp/backup_extcerts.yaml\\n  echo \"$CERTS\" | \"$CDF_HOME/bin/jq\" -r \".[\\\\\"$key\\\\\"]\" | base64 -d | sed \\'/^$/d; s/^/    /; $a\\\\\\' >>/tmp/backup_extcerts.yaml\\ndone\\nTo upgrade from \\n2021.05 or newer\\n execute:\\nNS=$(helm list -A | awk \\'/opsbridge-suite-/ { print $2}\\')\\n# delete older backup if it exists\\nrm -f /tmp/backup_extcerts.yaml\\n# back up trusted server CAs\\nCERTS=\"$(kubectl -n \"$NS\" get cm opsb-ca-certificate -o jsonpath=\\'{.data}\\')\"\\nif [ -n \"$CERTS\" ]; then\\n  echo \"caCertificates:\" >>/tmp/backup_extcerts.yaml\\n  mapfile -t KEYS < <(echo \"$CERTS\" | \"$CDF_HOME/bin/jq\" -r \\'. | keys[]\\')\\n  for key in \"${KEYS[@]}\"; do\\n    echo \"  ${key}: |\" >>/tmp/backup_extcerts.yaml\\n    echo \"$CERTS\" | \"$CDF_HOME/bin/jq\" -r \".[\\\\\"$key\\\\\"]\" | sed \\'/^$/d; s/^/    /; $a\\\\\\' >>/tmp/backup_extcerts.yaml\\n  done\\nfi\\n# back up trusted client CAs removing duplicates\\nCERTS=\"$(kubectl -n \"$NS\" get cm api-client-ca-certificates -o jsonpath=\\'{.data}\\')\"\\nMD5S=()\\nif [ -n \"$CERTS\" ]; then\\n  echo \"authorizedClientCAs:\" >>/tmp/backup_extcerts.yaml\\n  mapfile -t KEYS < <(echo \"$CERTS\" | \"$CDF_HOME/bin/jq\" -r \\'. | keys[]\\')\\n  for key in \"${KEYS[@]}\"; do\\n    cert=\"$(echo \"$CERTS\" | \"$CDF_HOME/bin/jq\" -r \".[\\\\\"$key\\\\\"]\")\"\\n    md5=\"$(openssl x509 -noout -modulus -in <(echo \"$cert\") | openssl md5 | cut -d \"=\" -f 2 | xargs)\"\\n    if grep -Fqx \"${md5}\" <(printf \\'%s\\\\n\\' \"${MD5S[@]}\") &>/dev/null; then\\n      echo \"Skipping duplicate certificate with subject: $(openssl x509 -noout -subject -in <(echo \"$cert\"))\"\\n      continue\\n    fi\\n    MD5S+=(\"$md5\")\\n    echo \"  ${key}: |\" >>/tmp/backup_extcerts.yaml\\n    echo \"$cert\" | sed \\'/^$/d; s/^/    /; $a\\\\\\' >>/tmp/backup_extcerts.yaml\\n  done\\nfi\\n# include certificates that were manually added to the receiver (if they are not duplicates)\\nMANUAL_CERTS=\"$(kubectl -n \"$NS\" get secret receiver-secret -o jsonpath=\\'{.data}\\')\"\\nif [ -n \"$MANUAL_CERTS\" ]; then\\n  if [ -z \"$CERTS\" ]; then\\n    echo \"authorizedClientCAs:\" >>/tmp/backup_extcerts.yaml\\n  fi\\n  mapfile -t KEYS < <(echo \"$MANUAL_CERTS\" | \"$CDF_HOME/bin/jq\" -r \\'. | keys[]\\')\\n  for key in \"${KEYS[@]}\"; do\\n    cert=\"$(echo \"$MANUAL_CERTS\" | \"$CDF_HOME/bin/jq\" -r \".[\\\\\"$key\\\\\"]\" | base64 -d)\"\\n    if ! md5=\"$(openssl x509 -noout -modulus -in <(echo \"$cert\") | openssl md5 | cut -d \"=\" -f 2 | xargs)\"; then\\n      echo \"Invalid certificate in receiver\\'s secret\\'s key $key\"\\n      continue\\n    fi\\nContainerized Operations Bridge 2022.11\\nPage \\n527\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20b8b387198ff73bf7fb2867efb0a5e9'}>,\n",
              "  <Document: {'content': \"Upgrade OPTIC DL Vertica plugin\\nIf you are upgrading from the previous version to this version, you may choose to upgrade the OPTIC DL Vertica Plugin. The\\ndata flow isn't impacted if there is a mismatch between the OPTIC DL Vertica Plugin and the application versions. However,\\nthis version of OPTIC DL Vertica Plugin consists of important fixes and it's recommended that you upgrade the plugin.\\nThe database configuration tool in this version has the \\ngenconfig\\n option to create the configurations YAML file. This YAML file\\ncontains all the input values for the tool and can be edited when required. If you choose not to install the RPM, you won't be\\nable to use the database configuration tool. It's recommended to use the database configuration tool over the\\nmanual environment variables update.\\nPerform these upgrade steps on the same Vertica node where you have already installed the OPTIC DL Vertica Plugin (this\\nmeans the Vertica node that has the \\n/usr/local/itom-di-pulsarudx\\n folder):\\n1\\n. \\nLog on to the Vertica database as the database administrator and run the following command:\\n/opt/vertica/bin/vsql -U <database administrator user> -w <dbadmin password> -d <database name> -f /usr/local/itom-di-pulsarudx/sql/uninst\\nall.sql\\n2\\n. \\nStop and start the Vertica database. This step cleans the older libraries from Vertica Memory space. Run the following\\ncommands:\\n/opt/vertica/bin/admintools -t stop_db -d <database name> -p <dbadmin password> -F\\n/opt/vertica/bin/admintools -t start_db -d <database name> -p <dbadmin password> -F\\n3\\n. \\nOn the Vertica node, switch as root user and run the following command to get the RPM name:\\nrpm -qa | grep itom-di-pulsarudx\\nNote down the RPM name. In case of a rollback, you must perform the rollback to this version.\\n4\\n. \\nRun the following command to upgrade the RPM:\\nrpm -Uvh itom-di-pulsarudx-<RPM version>\\nMake sure that the \\n<RPM version>\\n is greater than the installed RPM name that you noted down in step 3.\\n5\\n. \\nAs a root user and go to the location \\n/usr/local/itom-di-pulsarudx/\\nbin.\\n6\\n. \\nGo to the location \\n/usr/local/itom-di-pulsarudx/conf\\n and check if you have the \\ndbinit_conf.yaml\\n file. This means that you have\\nalready run \\ndbinit.sh\\n with the \\ngenconfig\\n option and the file is created in the default location.  \\nIf the file doesn't exist, run the following command. This tool creates the \\ndbinit_conf.yaml\\n file in the location \\n/usr/local/itom-di-\\npulsarudx/conf\\n. \\nFor more information, see \\nCreate the variables file\\n.\\n./dbinit.sh genconfig <options>\\nOptions usage:\\n \\n./dbinit.sh genconfig [-h] [-f|--filepath] [-y|--yes] [-p false| --postload false]\\nOption\\nDescription\\n-h\\nDisplay the script usage or help.\\nUsage\\n: \\n./dbinit.sh genconfig -h\\n-y\\n, \\n--yes\\nForce overwrite. The script prompts you to overwrite an existing configuration file. This option will\\ncause the file to be overwritten.\\nUsage\\n: \\n./dbinit.sh genconfig -y \\nOR\\nUsage\\n: \\n./dbinit.sh genconfig --yes\\n-f\\n, \\n--filepath\\nThe default configuration file location - \\n/usr/local/itom-di-pulsar-udx/conf/dbinit_conf.yaml\\n. However, if you\\nwant to generate a configuration file in a different location you can use this option.\\nUsage\\n: \\n./dbinit.sh genconfig -f /tmp/my_conf.yaml \\nOR\\nUsage\\n: \\n./dbinit.sh genconfig --filepath /tmp/my_conf.yaml\\n-p false\\n, \\n--p\\nostload false\\nThe configuration file will have a Postload resource pool configured by default. If you don't require this\\nresource pool, you can use this option.\\nUsage\\n: \\n./dbinit.sh genconfig -p false \\nOR\\nUsage\\n: \\n./dbinit.sh genconfig --postload false\\n7\\n. \\nRun the following command to perform the user creation, grants, install and create the resource pool. While upgrading,\\nthis command checks for the Vertica read write and read only users and gives an error if any of the users aren't created. It\\nalso creates resource pools if it's not created earlier according to the variable values.\\nMake sure to run this command as the root user: \\n./dbinit.sh <option>\\nYou can retain the same values as prompted or change the values and press \\nenter\\n. You must type the database\\nadministrator password when prompted. For the \\nreadWriteUser\\n and \\nreadOnlyUser\\n, \\ndbinit.sh\\n prompts for passwords unless\\nyou use the \\n--silent\\n option.\\nOptions usage\\n: \\ndbinit.sh [-h|-?|--help] [-c|--tlscrt] [-k|--tlskey] [-e|--tlsenforce] [-t|--tlsonly] [-s|--silent|--suppress] [-w|--dbapass [password]]\\n [-v|--verbose] [-n|--nochange]\\nOption\\nDescription\\nImportant\\n: You must edit the file with the values required for the upgrade. Make sure give the same\\nvariable values that you used before the upgrade. Don't change the values for the Vertica parameters,\\nResource pool names, and the Kubernetes cluster provider parameters. \\nContainerized Operations Bridge 2022.11\\nPage \\n525\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3b566a2096b195ef03a23bea33be15c1'}>,\n",
              "  <Document: {'content': 'Upgrade Vertica\\nYou must upgrade Vertica to the supported version before you upgrade the OPTIC DL Vertica Plugin. If not, the \\ndbinit.sh\\n script\\ndisplays an error/warning according to the installed Vertica version. \\nPrerequisites\\n1\\n. \\nYou must take a complete backup of your existing Vertica database before the upgrade. If the upgrade fails, you can\\nreinstall the previous version of Vertica and restore your database to that version. \\n2\\n. \\nOnce you take a complete backup, you must stop the \\ncron\\n scheduling of backup. Run the \\ncrontab -e\\n command as the\\nVertica database administrator. Remove the \\ncrontab\\n entry for the scheduling database. Save the changes. \\nFollow these steps to upgrade Vertica: \\n1\\n. \\nYou must perform this step if you plan to upgrade a major or minor Vertica version (for example, from Vertica 9.x.x or\\nfrom Vertica 10.0.x to 10.1.1). Skip this step for Vertica patch version upgrades. Identify the Vertica node where the \\n/usr/lo\\ncal/itom-di-pulsarudx\\n directory exists. Log on to that node as the database administrator and run the following command:\\n/opt/vertica/bin/vsql -U <database administrator user> -w <dbadmin password> -d <database name> -f /usr/local/itom-di-pulsarudx/sql/uninst\\nall.sql\\n2\\n. \\nRun the following command to stop the database using \\nadmintools\\n:\\n/opt/vertica/bin/admintools -t stop_db -d <database name> -p <dbadmin password> -F\\n3\\n. \\nOn the Vertica node, run the following command as the root user:\\nrpm -Uvh <Vertica rpm file >\\n/opt/vertica/sbin/update_vertica --rpm <Vertica  rpm file> --dba-user <database administrator user>\\n4\\n. \\nLog on to the Vertica database as the database administrator.\\n5\\n. \\nRun the following command to start the database using \\nadmintools\\n:\\n/opt/vertica/bin/admintools -t start_db -d <database name> -p <dbadmin password> -F\\nWait for the database to start.\\n6\\n. \\nAfter you perform a major or minor version Vertica upgrade or before you upgrade the suite, make sure you upgrade the\\nOPTIC DL Vertica Plugin using the \\ndbinit.sh\\n script.\\nRelated topics \\nSee the Vertica documentation for more information on \\nupgrade Vertica\\n and \\nupgrade Vertica on AWS\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n524\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3ab946cbd1d4e0516577f6b48876f8c'}>,\n",
              "  <Document: {'content': \"Example:\\nhelm get values deployment01 -n opsb-helm -o json | jq '.global.autoStartAgentMetricCollector'\\nSet \\nautoStartAgentMetricCollector \\nto\\n \\nfalse\\n. autoStartAgentMetricCollector \\nstarts only the out of the box regular and historic collections\\nimmediately after the upgrade\\n. \\nYou must set it to\\n \\nfalse\\n \\nto prevent AMC from starting before OBM is accessible.\\nYou can set the value for these two parameter via helm \\nvalues.yaml\\n file or \\nAppHub\\nContainerized Operations Bridge 2022.11\\nPage \\n523\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a1736005934969e59212898dfa5e9dcf'}>,\n",
              "  <Document: {'content': '    if grep -Fqx \"${md5}\" <(printf \\'%s\\\\n\\' \"${MD5S[@]}\") &>/dev/null; then\\n      echo \"Skipping duplicate certificate with subject: $(openssl x509 -noout -subject -in <(echo \"$cert\"))\"\\n      continue\\n    fi\\n    MD5S+=(\"$md5\")\\n    echo \"  ${key}: |\" >>/tmp/backup_extcerts.yaml\\n    echo \"$cert\" | sed \\'/^$/d; s/^/    /; $a\\\\\\' >>/tmp/backup_extcerts.yaml\\n  done\\nfi\\n3\\n. \\nThe\\n /tmp/backup_extcerts.yaml\\n file now contains the external certificates that are imported into OPTIC Data Lake. \\nThe\\nformat of this file is only recognized by a suite with version 2021.05 or newer\\n. When you execute the \\nhelm upgra\\nde\\n command, you must pass this file in the command line using \\n-f. \\nSee\\n upgrade\\n command syntax in \\nUpgrade\\ncontainerized capabilities\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n528\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4e9af876a8ac5d7f17b756240eaa54f6'}>,\n",
              "  <Document: {'content': 'When you deploy the suite product (OpsB,DCA,or NOM), you can find the current version of BVD installation with external\\nOracle database in the directory where you have the restored data.\\nData migration for external postgres\\nTo migrate data:\\n1\\n. \\nRun the following command to get a list of all namespaces:\\nkubectl get ns\\n2\\n. \\nSearch for the Opsbridge namespace in the output. In this example, \\nopsbridge-tim7t\\n is the Opsbridge namespace.\\nNAME         STATUS AGE\\ncore         Active 32d\\ndefault         Active 32d\\nkube-public     Active 32d\\nkube-system     Active 32d\\nopsbridge-tim7t Active 32d\\n3\\n. \\nOn the machine where the earlier version of BVD is available, run the following command to set the replicas to \\n0\\n for all\\nthe \\nBVD PODs\\n.\\nkubectl -n <OPSBRIDGE NAMESPACE> scale --replicas=0 deployment/bvd-explore-deployment deployment/bvd-controller-deployment deployment/bvd-www-deployment deployment/bvd-quexserv deployment/bvd-receiver-deployment\\n4\\n. \\nOn the external Postgres system connected to earlier version of BVD, use the following commands to create a BVD dump\\nfile:\\nsu - postgres\\npg_dump -U postgres bvd_database_name>bvd_dump_file_name.pgsql\\nBefore importing data, ensure that you create databases for BVD in an external Postgres database. Create all the user names\\nand databases in the current version of BVD, same as earlier version of BVD. Refer the \\nPrepare\\n section of install\\ndocumentation.\\n1\\n. \\nCopy bvd.dump file from the earlier version of the external Postgres system to the current version of the external\\nPostgres system.\\nscp /tmp/bvd.dump root@ExternalPostgresVMforLatestBVDVersion:/tmp/<bvd database name.dump>\\n2\\n. \\nRestore the database on the external Postgres system for the current version of BVD using the following shell commands.\\nsu - postgres\\npsql bvd_database_name<bvd_dump_file_name.pgsql \\nWhen you deploy the suite product (OpsB,DCA,or NOM), you can find the current version of BVD installation with external\\nPostgres database in the directory where you have the restored data. \\nSet up database connection\\nAfter the upgrade, set up a database connection for BVD to ensure that the dashboard can display all the data elements.\\n1\\n. \\nAccess side navigation panel, click \\n \\n \\n \\nAdministration\\n >\\n \\nDashboards & Reports \\n> \\nPredefined Queries.\\n2\\n. \\nOn the \\nPredefined Queries\\n page, click \\n \\n and select \\nDB Connection Settings\\n.\\n3\\n. \\nEnter the required information in the \\nSET UP DB CONNECTION\\n form. The details you enter here should match\\nthe information with the earlier version of the BVD database connection.\\nNote: To access predefined queries in the legacy UI\\n:\\n  \\nOn the upper-right side of the masthead \\n \\nAdministration\\n \\n> Predefined Queries.\\nContainerized Operations Bridge 2022.11\\nPage \\n530\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62daa712e00b0930a35bf015eb6a9ab1'}>,\n",
              "  <Document: {'content': \"Migrate Stakeholder Dashboards from 2019.11 version\\nThis version of containerized OpsBridge doesn't support a direct, in place upgrade of BVD component of any containerized\\nOperations Bridge version of 2019.11 and earlier versions. However, you can migrate data for the following databases of BVD\\nfrom containerized Operations Bridge 2019.11:\\nExternal Oracle \\nExternal Postgres \\nData migration for external oracle\\nTo migrate data:\\n1\\n. \\nRun the following command to get a list of all the namespaces.\\n kubectl get ns\\n2\\n. \\nSearch for the OpsBridge namespace in the output. In the following example, \\nopsbridge-tim7t\\n is the namespace.\\nNAME         STATUS AGE\\ncore         Active 32d\\ndefault         Active 32d\\nkube-public     Active 32d\\nkube-system     Active 32d\\nopsbridge-tim7t Active 32d\\n3\\n. \\nOn the machine where BVD (earlier version) is available, run the following command to set the replicas to \\n0\\n for all the\\nBVD PODs\\n.\\nkubectl -n <OPSBRIDGE NAMESPACE> scale --replicas=0 deployment/bvd-explore-deployment deployment/bvd-controller-deployment deployment/bvd-www-deployment deployment/bvd-quexserv deployment/bvd-receiver-deployment\\n4\\n. \\nTo export data to the current version of BVD external Oracle system and create a dump file in external Oracle system\\nthat's connected to the earlier version of BVD, do the following:\\na. Log into Oracle with \\nsysdba\\n permission and run the following SQL commands:\\nsu – <oracle user name> SQL> CREATE DIRECTORY <directory name> AS '<Target directory path to place the dump file>' \\nb. Give BVD database with access to this directory.\\nSQL> GRANT read,write ON DIRECTORY <directory name> TO <bvd database name>;  \\nc. Give export access to the BVD database.\\nSQL> GRANT DATAPUMP_EXP_FULL_DATABASE TO <bvd database>; SQL> exit;\\nd. Create a backup of the BVD data base using \\nexpdp\\n shell command.\\nexpdp bvddatabase/databasepassword@SID DIRECTORY=<directory name> DUMPFILE=<directory name>_sch_bvd.dump LOGFILE=bvd_lg.log SCHEMAS=<bvd database name>\\n \\nOnce you create the BVD dump file, import the file into an external Oracle database system. Before importing data, ensure\\nthat you create the databases in the external Oracle database.\\nAll the user names and databases that you create in the current version of BVD should be the same as in the earlier\\nversion of BVD. Refer the \\nPrepare\\n section of install documentation.\\nPerform the following steps in the Oracle database system to connect to the current version of BVD.\\n1\\n. \\nAssign the folder that contains the exported dump file.\\nsu - <oracle user name>SQL> CREATE DIRECTORY <directory name> AS '<full path to the directory>';\\n2\\n. \\nGive BVD database with the access to the directory.\\nSQL> GRANT read,write ON DIRECTORY <directory name> TO <bvd database>;\\n3\\n. \\nGive BVD database with full import access.\\nSQL> GRANT DATAPUMP_IMP_FULL_DATABASE TO <bvd database>;SQL> exit;\\n4\\n. \\nTo restore the backup of the BVD database, run the \\nimpdp\\n utility on the shell.\\nimpdp bvddatabasename/bvddbpassword@<Oracle SID> DIRECTORY=bvdbackupdb DUMPFILE=<bvd dump file name.dump> LOGFILE=<import bvddatabase>.log SCHEMAS=<bvd databasename>\\nContainerized Operations Bridge 2022.11\\nPage \\n529\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '51d10108773f311a22ddc55fc546495a'}>,\n",
              "  <Document: {'content': ' \\n \\n4\\n. \\nClick \\nTEST CONNECTION\\n to ensure the database connection. \\n5\\n. \\nClick \\nSAVE SETTINGS\\nOnce you have set up the database connection, you need to run all the queries once in the upgraded BVD to enable\\ndashboards to show all the data elements. \\n1\\n. \\nOn the \\nPredefined Queries\\n page, you can see the  list of all the data or parameter queries. \\n2\\n. \\nClick the edit button to view all the details of the query.\\n3\\n. \\nClick \\nRUN \\nunder Query*.\\n4\\n. \\nClick \\nSAVE DATA QUERY\\n. \\n5\\n. \\nEdit and run all the queries listed under \\nPredefined Queries\\n. \\nOnce you perform all the preceding steps, view the dashboards to verify whether you can see all the dashboard elements.\\nFor example, when you run a data query called \\nMulticategory \\nand save the query, you will see the updated values on the\\ndashboard. When you hover over \\nMulticategory\\n under \\nData Channel\\n, you will see when was the latest update made.\\n \\n \\nContainerized Operations Bridge 2022.11\\nPage \\n531\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '190e7f906fb3a944569d074585c301d6'}>,\n",
              "  <Document: {'content': 'Update Event Forwarding Content Pack\\nIf you have classic OBM integrated, you must update the event forwarding content pack for data forwarding to \\nopr_event\\n after\\nan upgrade.\\nUpdate event forwarding content pack\\n1\\n. \\nOn the control plane (master) node, in the \\nintegration-tools \\ndirectory, execute the following command:\\n./get-obm-setup-tool.sh\\n2\\n. \\nGo to \\nobm-configurator-interim\\n folder.\\n3\\n. \\nCopy the \\nCOSO_Data_Lake_Event_Integration_CP-4.09.zip\\n content pack file to the OBM system.\\n4\\n. \\nTo use the OBM content pack UI, import the content pack from the OBM system.\\n5\\n. \\nEnable the event forwarding rule.\\nRelated topics\\nFor more information on getting integration tools, see \\nGet Integration Tools\\n.\\nFor more information on importing content packs, see \\nImport Content Packs\\n.\\nFor more information on enabling the event forwarding rule, see \\nEvent Forwarding\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n532\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a154263af25a3636f8f1d2f3abb0c086'}>,\n",
              "  <Document: {'content': 'Create PV for OBM\\nIf you have deployed Operations Bridge with OBM capability, you must create PV by following the instructions mentioned in\\nthis topic. You can follow the section applicable to your deployment and ignore the rest.\\nEmbedded Kubernetes\\n1\\n. \\nExecute the following commands to create volume for \\nArtemis\\n on the NFS server.\\ncd <itom_platform_foundation_standard_202x.xx.xxxx>/cdf/scripts/\\n./setupNFS.sh /var/vols/itom/opsbvol\\nX\\n true 1999 1999\\nchmod -R 755 /var/vols/itom/opsbvol\\nX\\n \\nWhere: \\n X is an unused volume number, for example, if you have volumes till\\n opsbvol6\\n, this can be \\nopsbvol7\\n.\\n2\\n. \\n Create a YAML file \\nArtemisPV.yaml\\n on the first control plane node with the required data. You must enter the \\nNFS server\\nname\\n and the \\npath\\n where you have created the NFS volumes applicable to your environment. \\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol\\nX\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol\\nX\\n    server: \\nyournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n \\nWhere: \\n \\nX\\n is an unused volume number, for example, if you have volumes till\\n opsbvol6\\n, this can be \\nopsbvol7\\n.\\n3\\n. \\nTo create a persistent volume, run the following command:\\nkubectl create -f ArtemisPV.yaml\\n4\\n. \\nTo verify volume creation, run the following command:\\nkubectl get pv\\nAWS\\nFollow these steps to create the PVs for \\nArtemis\\n manually. On AWS, the \\nEFS Service\\n provides NFS in specific regions. See\\nthe \\nAWS documentation\\n and adjust the mount path if you use a different one.\\n1\\n. \\nTo create the PV, connect to the bastion machine within your VPC with SSH. Run the following commands to create\\nvolumes for the suite by replacing the NFS DNS with the \\nEFS\\n \\nDNS\\n name:\\n \\nsudo su\\nNFS_FQDN=<EFS_NFS_DNS>\\nfor i in X; do\\n    mkdir \"/mnt/efs/var/vols/itom/opsbvol$i\"\\n    chown 1999:1999 \"/mnt/efs/var/vols/itom/opsbvol$i\"\\n    chmod g+w+s \"/mnt/efs/var/vols/itom/opsbvol$i\"\\n    kubectl apply -f - <<<\"\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol$i\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/opsbvol$i\\n    server: $NFS_FQDN\\n  persistentVolumeReclaimPolicy: Retain\\n  storageClassName: opsb-default\\nTip:\\n You must copy the \\nsetupNFS.sh\\n script onto your NFS server if it\\'s not present there already.\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n534\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a5907440f27d2559c8403f9f1e15fd26'}>,\n",
              "  <Document: {'content': '  volumeMode: Filesystem\\n\"\\ndone\\n  \\nWhere: \\n \\nX\\n is an unused volume number, for example, if you have volumes till\\n opsbvol6\\n, this can be \\nopsbvol7\\n.\\n2\\n. \\nYou can then exit the root prompt with \\nCtrl+D\\n. \\n3\\n. \\nTo verify Persistent Volume creation, run the command: \\nkubectl get pv\\nAzure\\nFollow these steps to create the PVs manually:\\n1\\n. \\nLog on to the bastion node.\\n2\\n. \\nRun the following commands to create the PV directories:\\ncd /mnt/nfs/<fileshare name>\\nsudo mkdir -p var/vols/itom/opsbvolX\\nWhere: \\n \\nX\\n is an unused volume number, for example, if you have volumes till\\n opsbvol6\\n, this can be \\nopsbvol7\\nsudo chown -R <user id>:<group id> /mnt/nfs/<fileshare name>\\nsudo chmod g+w+s var/vols/itom/opsbvol*\\n3\\n. \\nRun the following commands by replacing the File Share with the Azure File Share name:\\n \\nsudo su\\nFILE_SHARE=<FILE_SHARE_NAME>\\nfor i in X; do\\n    mkdir \"/mnt/nfs/$FILE_SHARE/var/vols/itom/opsbvol$i\"\\n    chown 1999:1999 \"/mnt/nfs/$FILE_SHARE/var/vols/itom/opsbvol$i\"\\n    chmod g+w+s \"/mnt/nfs/$FILE_SHARE/var/vols/itom/opsbvol$i\"\\n    kubectl apply -f - <<<\"\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: opsbvol$i\\nspec:\\n  accessModes:\\n    - ReadWriteOnce\\n  azureFile:\\n    secretName: azure-secret\\n    secretNamespace: core\\n    shareName: $FILE_SHARE/var/vols/itom/opsbvol$i\\n  capacity:\\n    storage: 10Gi\\n  mountOptions:\\n    - dir_mode=0775\\n    - uid=1999\\n    - gid=1999\\n    - mfsymlinks\\n    - nobrl\\n    - noserverino\\n  persistentVolumeReclaimPolicy: Retain\\n  storageClassName: opsb-default\\n  volumeMode: Filesystem\\n\"\\ndone\\n \\nWhere: \\n \\nX\\n is an unused volume number, for example, if you have volumes till\\n opsbvol6\\n, this can be \\nopsbvol7\\n4\\n. \\nYou can then exit the root prompt with \\nCtrl+D\\n. \\n5\\n. \\nTo verify Persistent Volume creation, run the command:\\n \\nkubectl get pv\\nImportant\\n: the value \\n1999:1999\\n represents the \\nuser:group\\n ids that containers use to run as part of the suite.\\nIf you change them, ensure that you update the values when you configure your YAML file or the AppHub UI.\\nImportant\\n: the value \\n1999:1999\\n represents the \\nuser:group\\n ids used by the containers that run as part of the suite.\\nIf you change them, ensure that you update the values when you configure your YAML file or the AppHub UI.\\nNote\\n: The default value for \\n<user id>:<group id>\\n is \\n1999:1999\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n535\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5766f09a506bf1fefe9f205f403b0895'}>,\n",
              "  <Document: {'content': \"Enable or Disable Data Migration\\nIf you want to restore historical data from backup to \\nopr_event\\n, use the helm variable \\nValues.global.oprEventFlex\\n.\\nData migration to opr_event\\nThe data migration process takes 4 minutes to complete for 15000 records. Therefore, if you don't want to wait, you can\\nrestore the history data from the backup at a later time and rerun the helm upgrade.\\nThe default option for the helm variable \\nValues.global.oprEventFlex\\n is \\nTrue\\n. Use the \\nFalse\\n option to disable data migration.\\nIf you have classic OBM integrated into the suite, before the upgrade, you might need to stop event forwarding by performing\\nthe following steps while upgrading the suite:\\n1\\n. \\nDisable the \\nCOSO Data Lake Event Integration Rule\\n in OBM and proceed with the upgrade.\\n2\\n. \\nOnce the upgrade is complete, run the classic configuration tool again and enable the \\nCOSO Data Lake Event\\nIntegration Rule\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n533\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6e36754e171c99cb6cb583f718ea885f'}>,\n",
              "  <Document: {'content': '2\\n. \\nClick \\n \\nto grant the certificate.\\nComponents and their supported versions\\nThe components and their supported versions that are required for reporting are:\\nComponent\\nSupported version\\nClassic OBM \\n2020.05 and higher \\nContainerized OBM\\n2021.08\\nOperations Agent \\n12.14  and higher \\nBusiness Process Monitor (BPM) with \\nBPM9.53_LG2022_Patch\\n9.53\\nOBM MP for BPM\\n1.02\\nTask 1: Deploy the BPM aspect and configure the Data Receiver endpoint\\nThe BPM aspect is available with the OBM MP for BPM. Download the OBM MP for BPM from the Market Place.\\nInstall the OBM MP for BPM\\nFollow the steps:\\n1\\n. \\nDownload the \\nOBM_MP_for_Business_Process_Monitor_01.00.007.zip\\n from \\nMarket Place\\n and extract its contents.\\n2\\n. \\nIn the OBM UI, go to \\nAdministration \\n>\\n Setup and Maintenance \\n>\\n Content Packs\\n.\\n3\\n. \\nClick \\nImport Content pack Definition and Content\\n. \\n4\\n. \\nGo to the \\nOBM_MP_for_Business_Process_Monitor_01.00.007\\n > \\nopr \\n> \\ncontent\\n > \\nen_US\\n, select the\\nOBM_Management_Pack_for_Businees_Process_Monitor.zip\\n file and click \\nImport\\n.\\nDeploy the BPM aspect and configure the Data Receiver endpoint \\nFollow the steps:\\n1\\n. \\nIn the OBM UI, go to \\nAdministration > Monitoring > Management Templates & Aspects\\n.\\n2\\n. \\nIn the Configurations Folder pane, click \\nBusiness Process Monitor > BPM OPTIC Data Lake Integration\\n.\\n3\\n. \\nIn Management Templates & Aspects pane, select the aspect and then click  \\n \\n \\nAssign and Deploy\\n item.\\n4\\n. \\nIn the Configuration Item tab, click the CI of the BPM node (Operations Agent node) on which you want to deploy the\\nAspect, and then click \\nNext\\n.\\n5\\n. \\nIn the Required Parameters tab, enter the OPTIC Data Lake receiver URL in the following format: \\nhttps://<externalAccessHost\\n>:30001\\nHere, \\nexternalAccessHost\\n is the value that you set in the \\nvalues.yaml\\n file. For more information, see the \\nConfigure\\nValues.yaml\\n page.\\n6\\n. \\nClick \\nNext\\n and then click \\nFinish\\n.\\nVerify the deployment status\\nRun the command on BPM to validate if the BPM policies are installed:\\nOn Linux: \\n/opt/OV/bin/ovpolicy -list\\nOn Windows:\\n ovpolicy -list\\nThe BPM policies are listed as follows:\\nTask 2: Configure BPM to send data to OPTIC Data Lake\\nNote: \\nSkip these steps if you have already installed the BPM Management Pack.\\n\\ue916\\n\\ue916\\nNote:\\n \\nYou can also execute the \\nmpinstall.sh \\n(on Linux) or \\nmpinstall.vbs\\n (on Windows) present in\\nthe \\nOBM_MP_for_Business_Process_Monitor_01.00.007 \\nfile to install the OBM MP for BPM.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n538\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e50c5e39a77ea6284309d7190ede54d'}>,\n",
              "  <Document: {'content': \"Integrate BPM\\nBusiness Process Monitor (BPM) is an application monitoring software that provides you with information about user\\nexperience, availability, and performance of applications by running synthetic transactions.\\nYou can integrate BPM with the Operations Bridge suite to view the BPM data on Business Value Dashboard (BVD) and\\nPerformance Dashboard (PD).\\nFollow the steps to integrate BPM with containerized Operations Bridge:   \\nSynthetic Transaction Reports give you with information about end user experience, availability, and performance of\\napplications.\\nBusiness Process Monitor (BPM) enables you to run synthetic transactions and collect metrics. This section gives you\\ninformation to send the metrics collected by BPM to OPTIC Data Lake and generate synthetic transaction reports on Business\\nValue Dashboard (BVD).\\nPrerequisites\\nOPTIC Reporting\\n capability\\nRun the command on the master (control plane) node to check if you have installed the \\nOPTIC Reporting \\ncapability:\\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting:\\nFor example:\\nhelm get values opsb -n opsbs | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true\\nTo add the \\nOPTIC Reporting\\n capability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page. \\nOperations Bridge Manager (OBM). For installation steps, see Install.\\nConfigure a secure connection between OBM and OPTIC Data Lake:\\nTo configure classic OBM, see \\nConfigure classic OBM\\nTo configure containerized OBM, see \\nConfigure a secure connection between containerized OBM and OPTIC Data\\nLake\\nValidate the connection between BVD and OPTIC Data Lake. See \\nValidate the connection between BVD and OPTIC Data\\nLake\\n.\\nBusiness Process Monitor. For installation steps, see \\nInstallation tasks\\n.\\nApply the Business Process Monitor patch \\nBPM9.53_LG2022_Patch\\n.\\n \\nFor patch installation instructions, see \\nPatch\\nInstallation Instructions\\n.\\nOBM Management Pack for Business Process Monitor (OBM MP for BPM). Download the OBM MP for BPM from \\nMarket Place\\nand install it. Steps to install are present later in this document.\\nOperations Agent.  Install and integrate Operations Agent on the BPM with OBM.\\nTo stream BPM data into the OPTIC Data Lake, you must integrate the Operations Agent which is on the BPM with OBM.\\nPerform the following steps to check if you have installed Operations Agent:\\n1\\n. \\nLog on to the BPM node.\\n2\\n. \\nRun the following command:\\nOn Linux\\n: \\n/opt/OV/bin/opcagt -version\\nOn Windows\\n: \\nopcagt -version\\nThe version of the Operations Agent appears. Make sure that the version is 12.14 or higher.\\nInstall and integrate Operations Agent with OBM\\nRun the following commands if you want to \\ninstall and integrate\\n Operations Agent (on a BPM) with OBM:\\nOn Linux: \\n./oainstall.sh -i -a -s <OBM load balancer or gateway server>\\nOn Windows: \\ncscript oainstall.vbs -i -a -s <OBM load balancer or gateway server> \\nGrant the Operations Agent certificate request\\nFollow the steps:\\n1\\n. \\nOn OBM, go to \\nAdministration > SETUP AND MAINTENANCE > Certificate Request\\nNote:\\n Aggregate tables of BPM aren't populated for the DI receiver endpoint. In order to populate them, use\\nthe Data Enrichment Service (DES) endpoint.\\n\\ue916\\n\\ue916\\nNote:\\nFor containerized OBM, \\n<OBM load balancer or gateway server> \\nis the  FQDN of the external access host. \\nFor more information, see \\nOperations Agent Install\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n537\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4dfdba8ea08ef5c53604da378fc3792f'}>,\n",
              "  <Document: {'content': 'Integrate\\nThis section contains information about the products that you can integrate with containerized Operations Bridge.\\nIntegrate OpsBridge with Monitoring Service Edge\\nIntegrate BPM\\nIntegrate DCA with Operations Bridge on OpenShift\\nIntegrate DCA with Operations Bridge on AWS\\nIntegrate DCA with Operations Bridge on-premises\\nIntegrate Network Reports\\nIntegrate OBM\\nIntegrate RUM\\nIntegrate SiteScope\\nIntegrate Prometheus Alert Manager\\nIntegrate Azure Monitor Alert Manager\\nRelated topics\\nSupport matrix\\nContainerized Operations Bridge 2022.11\\nPage \\n536\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '541bf7edb73c66ee15cdbe1fb6157228'}>,\n",
              "  <Document: {'content': \"Integrate Classic OBM\\nBenefits of integrating OBM and containerized Operations Bridge\\nThe integration of OBM with Operations Bridge adds the following capability to OBM:\\nView OPTIC Data Lake based out of the box reports on BVD\\nView metrics in OPTIC Data Lake on Performance Dashboard\\nAutomatic Event Correlation \\nFollow the steps to integrate classic OBM with Containerized Operations Bridge\\nThe topic provides the steps to configure a classic OBM for correlating events and forwarding them to OPTIC Data Lake. \\nTasks for configuring classic OBM\\nCreate the \\nobm-configurator.jar\\n tool and install the OBM CA certificate on the suite by using the \\nIntegration Tools\\n. \\nConfigure OBM and create \\nconfigureSuite.zip\\n by executing \\nobm-configurator.jar. \\nThe \\nconfigureSuite.zip \\ncontains\\nthe\\n setup-obm.sh\\n. \\nConfigure the suite by extracting \\nconfigureSuite.zip \\non the control plane (master) node and executing \\nsetup-obm.sh\\n.\\nCreate the obm-configurator.jar tool\\n On the control plane (master) node, in the \\nintegration-tools \\ndirectory, execute the following command:\\n./get-obm-setup-tool.sh\\nThe \\nobm-configuration.jar\\n tool is created in the same directory where \\nget-obm-setup-tool.sh\\n resides.\\nHere's a sample output after running the command: \\n./get-obm-setup-tool.sh\\ninfo: Logfile: /tmp/get-obm-setup-tool-2021-10-11-16-56-35.log\\ninfo: Working dir: /root/tools2\\ninfo: AEC Namespace:  opsb-helm\\ninfo: COSO Namespace: opsb-helm\\ninfo: Getting files from the suite ...\\ninfo: Fetching suite certificates ...\\ninfo: Copying file issue_ca.crt from container idm to /root/tools2/obm-configurator-interim/issue_ca.crt ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied suite certificates\\ninfo: Fetching IDL configuration script ...\\ninfo: Name of the file: /artifacts/idl-config-1.5.0.zip\\ninfo: Copying file idl-config-1.5.0.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/idl-config-1.5.0.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully fetched IDL configuration script\\ninfo: Fetching OBM IDL configuration script ...\\ninfo: Name of the file: /artifacts/opr-config-idl-11.20.004.010.zip\\ninfo: Copying file opr-config-idl-11.20.004.010.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/opr-config-idl-11.20.004.010.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully fetched OBM configuration script\\ninfo: Fetching 'COSO_Data_Lake_Event_Integration' ...\\ninfo: Name of the file: /artifacts/COSO_Data_Lake_Event_Integration_CP-3.01.zip\\ninfo: Copying file COSO_Data_Lake_Event_Integration_CP-3.01.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/COSO_Data_Lake_Event_Integration_CP-3.01.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied 'COSO_Data_Lake_Event_Integration'\\ninfo: Fetching 'COSO_Data_Lake_AEC_Integration_CP' ...\\ninfo: Name of the file: /artifacts/COSO_Data_Lake_AEC_Integration_CP-3.04.zip\\ninfo: Copying file COSO_Data_Lake_AEC_Integration_CP-3.04.zip from container itom-static-files-provider to /root/tools2/obm-configurator-interim/COSO_Data_Lake_AEC_Integration_CP-3.04.zip ...\\ntar: Removing leading `/' from member names\\ninfo: Successfully copied 'COSO_Data_Lake_AEC_Integration_CP'\\ninfo: Fetching OBM Setup Tool ...\\ninfo: Name of the file: /artifacts/obm-configurator.jar\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\nIf an existing OBM is connected to OPTIC Data Lake and BVD, refer \\nConnected Servers\\n.\\nNote\\n: The\\n obm-configurator.jar\\n tool configures a classic OBM for correlating events and forwarding the events\\nto OPTIC Data Lake. This tool can't change the configuration of a configured classic OBM or Operations Bridge\\nsuite.\\nContainerized Operations Bridge 2022.11\\nPage \\n544\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'adce97d85f38235d942ba13ecb8a8338'}>,\n",
              "  <Document: {'content': 'The Performance Troubleshooting component enables you to troubleshoot network issues by comparing performance\\nmetrics. It is a containerized service that is integrated with a non-containerized deployment of NNMi. It is cross-launched from\\nNNMi in context of nodes, interfaces, incidents, layer 2 connections, and MPLS Smart Plugin (SPI) objects.\\nPerformance Troubleshooting has the ability to use both NNM iSPI Performance for Metrics and OPTIC Data Lake as the data\\nsource. If you want to use NNM iSPI Performance for Metrics as the data source, it is necessary to have NNM iSPI Performance\\nfor Metrics in your environment to utilize the features of this capability.\\nPerformance Troubleshooting with NPS as data source\\nPerformance Troubleshooting with OPTIC Data Lake as data source\\nContainerized Operations Bridge 2022.11\\nPage \\n542\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eb9fa7d0dd5f7b51757d375abba41f45'}>,\n",
              "  <Document: {'content': \"Configure new BPM Instance\\nDuring Instance creation, under the Data Push Settings section, enter the following:\\n1\\n. \\nOn BPM UI, select the BPM host machine in the BPM tree and click \\n \\nCreate New Instance\\n button in the Browse tab\\ntoolbar.\\n2\\n. \\nSelect the \\nSend data via Operations\\n \\nAgent\\n check box.\\n3\\n. \\nThe Data receiver URL is automatically populated:\\nhttp://localhost:30005/bsmc/rest/genericdata\\nWhen a BPM script gets executed, a copy of the metric data gets processed by the policies deployed to the Operations\\nAgent and forwarded to OPTIC Data Lake.\\nConfigure existing BPM Instance\\nSelect the instance for which you want to configure the OPTIC Data Lake integration:\\n1\\n. \\nClick the \\nConfiguration\\n tab on the top right corner. \\n2\\n. \\nSelect the \\nSend data via the Operations\\n \\nAgent\\n check box.\\n3\\n. \\nIn the \\nData Push Settings\\n section, select the \\nSend data via the Operations Agent\\n check box.  The Data receiver\\nURL is automatically populated:\\nhttp://localhost:30005/bsmc/rest/genericdata\\nWhen a BPM script gets executed, a copy of the metric data gets processed by the policies deployed to the Operations\\nAgent and forwarded to OPTIC Data Lake.\\n(Optional) Add the tenant id to BPM instances\\nFollow the steps to update the tenant id:\\n1\\n. \\nOn BPM, go to:\\nOn Windows: \\nC:\\\\ProgramData\\\\MF\\\\BPM\\\\config\\nOn Linux:\\n /opt/MF/BPM/config/\\n2\\n. \\nMake sure that you have replaced the \\ncoso_sample_attributes.cfg\\n file with the corresponding file in the\\nBPM9.53_LG2022_Patch\\n.\\n3\\n. \\nOpen the \\ntopaz_agent_ctrl.cfg\\n file. The \\ntopaz_agent_ctrl.cfg\\n file contains a list of all the instances on a BPM.\\n4\\n. \\nFor each instance, add the tenant id in the \\nTenantId\\n parameter. \\nFor example:\\n \\nIn this example, \\nSite19\\n has the \\nTenantId \\n1.\\n5\\n. \\nRestart the BPM service:\\nOn Linux: \\n1\\n. \\nGo to \\n/opt/MF/BPM/bin \\n2\\n. \\nExecute the script to stop BPM service: \\n./stopBpmDaemon.sh\\n file \\n3\\n. \\nExecute the script to start BPM service: \\n./startBpmDaemon.sh\\nYou can use the data in OPTIC Data Lake to generate reports using either BVD or any other Business Intelligence tools of your\\nchoice. You can also use the metrics to generate dashboards using Performance Dashboards. For configuration steps,\\nsee \\nConfigure Performance Dashboards\\n.\\nThe local client allows you to launch the BPM UI from your local system. Use this option if your browser\\ndoesn't support Java Applets. For details, see \\nHow to Access BPM Admin\\n.\\nNote: \\n To add BPM instances, see the \\nInstance definition page\\n.\\n\\ue916\\n\\ue916\\nNote: \\nThe \\nTenantId\\n can have up to 80 characters and can include both letters and numbers.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n539\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d7e17d7d4480bab614bfbd24c06587'}>,\n",
              "  <Document: {'content': 'Integrate network reports\\nYou can stream metrics collected by Network Node Manager i (NNMi), Network Automation (NA), NNM iSPI for Traffic, NNM iSPI\\nfor Quality Assurance, NNM iSPI for MPLS, and NNM iSPI for Multicast into OPTIC Data Lake that is deployed with containerized\\nOperations Bridge by integrating NOM OPTIC Reporting. You can use this data in OPTIC Data Lake to view network reports on\\nOperations Bridge Business Value Dashboards (BVD). This document covers the instructions to integrate NNMi, NA, and iSPI\\nwith containerized Operations Bridge for NOM OPTIC Reporting.\\nBefore you proceed with the integration, refer to \\nOperations Bridge - Sizing the deployment\\n to ensure that you meet the\\nrequirements for the integration.\\nBenefits of integrating containerized Operations Bridge and NOM OPTIC Reporting\\nThe integration of containerized Operations Bridge with NOM OPTIC Reporting adds the following capabilities to your current\\ndeployment:\\nMonitor large-scale physical and virtual networks by streaming network fault, availability, and performance metrics to the\\nOperations Bridge OPTIC Data Lake.\\nView the NNMi data (component health, interface health, and custom collected metrics) on BVD of containerized\\nOperations Bridge and also generate custom reports.\\nView the NNM iSPI for QA data (Probes, CBQoS, and Ping_Pair_Latency)\\n \\non BVD of containerized Operations Bridge and\\nalso generate custom reports.\\nView the NNM iSPI Traffic data for traffic health summary report to view flow exporting interfaces, applications, and sites\\non BVD of containerized Operations Bridge.\\nCreate custom reports for NNM iSPI for MPLS and NNM iSPI for Multicast data.\\nSend NA data to containerized Operations Bridge OPTIC Data Lake. You can make use of this data to generate custom\\nreports.\\nYou can integrate NOM with the following versions of Operations Bridge: \\nOperations Bridge\\nIntegration architecture\\nNOM OPTIC Reporting is a containerized reporting service that is integrated with a non-containerized deployment of NA, NNMi,\\nand iSPIs. It supports data from the following:\\nNNM iSPI Performance for Metrics\\nNNM iSPI Performance for Quality Assurance\\nNNM iSPI Performance for Traffic\\nNNM iSPI for MPLS\\nNNM iSPI for IP Multicast\\nIt provides out-of-the-box reports based on this data. You can also create custom reports in BVD or any other Business\\nIntelligence tool, as required.\\nNOM OPTIC Reporting components\\nThe Operations Bridge OPTIC Data Lake component enables you to store Performance and Compliance data reports available\\nin OPTIC Data Lake from NA, NNMi, and iSPIs. These reports help you predict resource utilization, detect problems, and take\\ncorrective actions before critical business availability is impacted. You can either use out-of-the-box reports or create custom\\nreports as per your requirement.\\nThe Business Value Dashboards component enables you to view custom dashboards and reports from NA, NNMi, and iSPIs.\\nNetwork executives can use these dashboards to get an insight into the near real-time status of the network. You can also\\ncreate new Dashboards using the metrics available in NNMi and NA. \\nContainerized Operations Bridge 2022.11\\nPage \\n541\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fe8e162c4887df5051470cb8c4da2e69'}>,\n",
              "  <Document: {'content': \"Upgrade your existing content to new BPM content\\nPerform the below steps only if you are upgrading from a version older than 2022.05: \\n1\\n. \\nGet the \\nBPMContentUpgrade.zip\\n onto the master node by running the below command:\\nwget https://<externalAccessHost>:<externalAccessPort>:/staticfiles/contrib/bpm/BPMContentUpgrade.zip --no-check-certificate\\n2\\n. \\nUnzip the file: \\nunzip BPMContentUpgrade.zip\\n3\\n. \\nGive the script execute permission: \\nchmod +x BPMContentUpgrade.sh\\n4\\n. \\nRun the script: \\nBPMContentUpgrade.sh\\n5\\n. \\nGive the input for Administration Endpoint URL in the format:\\nhttps://<OPTIC DL_Administration_Endpoint_URL>:<Port number>/urest/v2/itom-data-ingestion-administration\\nFor example: \\nhttps://mymaster.example.com:30004/urest/v2/itom-data-ingestion-administration\\n6\\n. \\nCheck the log \\nBPMContentUpgrade.log\\n for any errors.\\n7\\n. \\nUpgrade the BPM content\\n. Run the command: \\n./ops-content-ctl upgrade content -n OpsB_BPM_Content -v 202x.xx.xxx\\nFor example: \\n./ops-content-ctl upgrade content -n OpsB_BPM_Content -v 2022.05.003\\n8\\n. \\nOn BPM follow the steps to use the current version of the BPM content pack:\\n1\\n. \\nStop the BPM services.\\n2\\n. \\nGo to \\nC:\\\\ProgramData\\\\MF\\\\BPM\\\\config\\n on Windows and \\n/opt/MF/BPM/config \\non Linux and edit the \\ncoso_sample_attributes.cfg\\n file\\nas below:\\n1\\n. \\nReplace all \\napplication_cmdb_global_id\\n with \\ncmdb_global_id\\n.\\n2\\n. \\nReplace all \\napplication_cmdb_id\\n with \\ncmdb_id\\n.\\n3\\n. \\nSave the file.\\n4\\n. \\nStart the BPM services.\\nNote:\\n You can get the URL from the Infrastructure settings on OBM. Run the following command on the\\nmaster node to get the Port number:\\nkubectl get svc -n $(helm list -A | grep opsbridge | awk '{print $2}')|grep itom-di-administration-nodeport\\n\\ue916\\n\\ue916\\nNote:\\n When you have given the correct \\nadmin endpoint\\n and the script fails, the errors will appear in the\\nlog file, otherwise the log file will be empty.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n540\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19c0ed5230095c27cfd681f2271fdf44'}>,\n",
              "  <Document: {'content': 'Integration scenarios\\nThe integration prerequisites and procedure will vary for the following scenarios:\\nIntegrate Network reports\\nIf you have installed Operations Bridge and you want to integrate with NOM OPTIC Reporting for the first time, then deploy\\nNOM reporting using the existing OPTIC Data Lake available from your Operations Bridge installation.  This will allow you to\\nhave network metrics and the Operations metrics in the same OPTIC Data Lake for reporting.\\nTo deploy NOM and integrate network reports:\\n1\\n. \\nInstall NOM. See \\nInstall\\n.\\n2\\n. \\nDepending on your setup, complete one or both integrations:\\nTo integrate with OPTIC Reporting capability in NOM with NNMi. See \\nIntegrate OPTIC Reporting with NNMi and Smart\\nPlugins (SPIs)\\n.\\nTo integrate with OPTIC Reporting capability in NOM with NA. See \\nIntegrate OPTIC Reporting with NA\\nUpgrade Network Reports within an existing integration\\nIf you have an existing installation of Operations Bridge and NOM OPTIC Reporting in a shared OPTIC DL scenario. You must\\nupgrade both Operations Bridge and NOM OPTIC Reporting installation.\\nTo upgrade Network reports:\\n1\\n. \\nUpgrade NOM. See \\nUpgrade\\n.\\n2\\n. \\nDepending on your setup, complete one or both integrations:\\nTo integrate with OPTIC Reporting capability in NOM with NNMi. See \\nIntegrate OPTIC Reporting with NNMi and Smart\\nPlugins (SPIs)\\n.\\nTo integrate with OPTIC Reporting capability in NOM with NA. See \\nIntegrate OPTIC Reporting with NA\\nRelated topics\\nTo upgrade the Operations bridge, see \\nUpgrade\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n543\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd42c07c640105f9977f975cfaf99f591'}>,\n",
              "  <Document: {'content': 'For example, run the following command after changing to the \\nintegration-tools\\n directory:\\ncd integration-tools/obm-configurator-interim\\n./idl_config.sh -cacert /tmp/obm_ca.crt -chart path/to/charts/\\nopsbridge-suite-2021.05.tgz -namespace opsb-suite\\nInstall OBM CA certificate in Operations Bridge using the AppHub\\na\\n. \\nChange the name of the \\nobm_ca.crt\\n file to a unique name that qualifies your classic OBM system. The name must start\\nwith \"client\" and end with the \".crt\" extension. Ensure that the filename must not be more than 20 characters.\\nFor example, \\nclient-abc-obm.crt\\nb\\n. \\nOn the AppHub UI, choose \\nDeployments\\n > \\nEdit \\nand click the \\nSecurity \\ntab. Refer to \\nReconfigure a deployment\\n topic.\\nc\\n. \\nUpload the new \\nUpload OPTIC Data Lake Client Authentication Certificates\\n certificate by using the option \\nClick\\nhere \\nor drag and add files for \\nUpload OPTIC Data Lake Client Authentication Certificates\\n.\\nd\\n. \\nClick on \\nVERIFY CERTIFICATE\\n. Make sure that the validation is successful.\\ne\\n. \\nClick on the \\nDatabases\\n tab and click on \\nVERIFY\\n for each of the databases. Then click \\nREDEPLOY\\n.\\nf\\n. \\nAfter some time, run the command to verify the pods which aren\\'t running:\\nkubectl get pods --all-namespaces -o wide | awk -F \" *|/\" \\'($3!=$4 || $5!=\"Running\") && $5!=\"Completed\" {print $0}\\'\\nConfigure the OBM system\\nThe \\nobm-configurator.jar\\n file is the setup tool for a classic OBM system. You must execute the tool on a classic OBM\\nGateway system. This tool configures OBM with the Operations Bridge suite and creates the \\nconfigureSuite.zip\\n file in the\\nsame directory.\\nYou can use the tool to:\\nEstablish trust between OBM and OPTIC Data Lake \\nConfigure event forwarding\\nConfigure Automatic Event Correlation (AEC)\\nPerform the following steps:\\n1\\n. \\nTo copy the \\nobm-configurator.jar\\n file to the OBM Gateway system, run the following command :\\nscp integration-tools/obm-configurator.jar root@<obm_system>:/var/tmp\\nIf you have installed the OBM Gateway system on Windows, manually copy the tool to the system.\\n2\\n. \\nExecute the \\nobm-configurator.jar\\n tool on the OBM Gateway system. Enter passwords for \\nadmin\\n and \\nZIP file\\nencryption\\n when prompted.\\nUse the following syntax when executing the \\nobm-configurator.jar\\n with only the required parameters:\\n/opt/HP/BSM/JRE/bin/java -jar/var/tmp/obm-configurator.jar --endpoint-id <id> --suite-service-hostname <host> --obm-ca-cert-alias <cert-alias>\\nImportant:\\n \\nYou must upload the obm_ca.crt in AppHub UI and reconfigure the deployment as mentioned in\\nthis section. If you skip this step and later try to upgrade using AppHub, the certificates will not be present in\\nAppHub and the integration will not work.\\n\\ue91b\\n\\ue91b\\nNote: \\nYou can\\'t use the \\nobm-configuration.jar \\ntool to configure a containerized OBM system.\\n\\ue916\\n\\ue916\\nNotes: \\nThe event forwarding from the OBM to OPTIC Data Lake is enabled immediately after you configure OBM. It\\'s\\npossible that OBM immediately tries to forward events to the configured OPTIC Data Lake, while OPTIC Data\\nLake and the suite are still not configured. This might result in some warning events that mention that the\\nevent forwarding to OPTIC Data Lake has failed.\\nWhen you rerun the tool, it might abort because the suite certificates are already installed when the tool was\\nexecuted previously. In such a situation, add the \\n--force\\n parameter. The inclusion of the --force parameter in\\nthe command ensures that the tool execution proceeds even when the suite certificates are installed. Ensure\\nthat you rerun the tool with this parameter only when the installed certificates are from the current suite,\\nwhich was installed when the tool was run previously.\\nContainerized Operations Bridge 2022.11\\nPage \\n546\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e0c7685219e569f49731866f05be192e'}>,\n",
              "  <Document: {'content': \"Execute the command as a \\nsudo\\n \\nuser\\n if you aren't using the \\nroot user\\n.\\nThe required \\nparameters are as follows:\\n--endpoint-id\\n: Defines the identifier used to register the OBM system in the suite. This parameter should be a readable\\nstring and is used to identify the OBM system when checking the registered OBM systems in the suite. \\n--suite-service-hostname\\n: Defines the FQDN of the system on which OBM can reach the \\nitom-di-receiver-svc\\n service. This\\nis typically the FQDN of the control plane (master) node.\\n--obm-ca-cert-alias\\n: After you install additional trusted certificates on OBM, it's recommended to use only the OBM CA\\ncertificate to configure the suite. To prevent an import of all trusted certificates from OBM into the suite, ensure that\\nyou specify the CA certificate alias by using the parameter, \\n--obm-ca-cert-alias\\n.\\nPassword parameters: You're prompted for passwords if you haven't specified them on the command line.\\nThe following optional parameters are updated with the default settings or operations if they're not specified:\\n--configuration-type\\n: Defines the type of configuration especially if event correlation isn't desired. By default, the\\nvalue is AEC. You can set it to \\nFORWARDING\\n (to configure only event forwarding) or \\nTRUST_ONLY\\n (to\\nexchange certificates only). The valid options are as follows:\\nTRUST_ONLY: Exchanges certificates to establish trust between OBM and OPTIC Data Lake. Choose this\\noption if you want to configure OPTIC Reporting.\\nFORWARDING: Configures the classic OBM and the suite for event forwarding. Choose this option if you\\nwant to configure OPTIC Reporting, specifically the Event reports.\\nAEC: Configures OBM and the Operations Bridge suite for event forwarding and Automatic Event\\nCorrelation. By default, the option is set to AEC. Choose this option if you want to configure OPTIC Reporting\\nand AEC.\\n--integration-user\\n: Gets set to \\nOBM_event_submit_user\\n--obm-url\\n: Gets set to https://localhost:443, if not specified. If you haven't used TLS, you can specify the OBM\\nHTTP URL.\\n--itom-di-receiver-port\\n: You can use this parameter to overwrite the port, especially when you install the Operations\\nBridge suite on a cloud platform. This isn't a required option. The port gets automatically detected during tool\\ncreation.\\n--itom-di-administration-port\\n: You can use this parameter to overwrite the port, especially when you install the\\nOperations Bridge suite on a cloud platform. This isn't a required option. The port gets automatically detected\\nduring tool creation.\\n--itom-di-data-access-port\\n: You can use this parameter to overwrite the port, especially when you install the\\nOperations Bridge suite on a cloud platform. This isn't a required option. The port gets automatically detected\\nduring tool creation.\\n--force\\n: You can use this parameter to allow the tool execution to proceed when the suite certificates are already\\ninstalled after the tool was run previously. Although this isn't a required option, the tool execution might abort\\nbecause the suite certificates are already installed when the tool was executed previously.\\nExamples\\nEstablish trust between OBM and OPTIC Data Lake using basic authentication for OBM:\\nNote:\\nIf you add BVD after integrating through \\nobm-configurator.jar\\n tool fails due to duplicate servers then\\nupdate the DNS entry for the server. Run the following command:\\nopr-connected-server.bat -username -password -update -dns itom-di-receiver-svc\\nTo find the ID of the connected server, run the following command:\\nopr-connected-server.bat -list -username -password\\n\\ue916\\n\\ue916\\nImportant\\n: In cloud deployments, the default ports for DI Receiver, DI Data Access, and DI\\nAdministration are 30001, 30003, and 30004 respectively. If you didn't use the\\n Cloud Automation\\nToolkit\\n provided by Micro Focus and instead provisioned your cloud infrastructure manually using\\ndifferent ports, then specify these ports explicitly with the corresponding parameters that are mentioned.\\nNote\\n: The \\nTRUST_ONLY\\n is a subset of \\nFORWARDING\\n, which in turn is a subset of AEC.\\nThis means that if you specify AEC, \\nobm-configurator.jar\\n establishes trust between OBM and\\nOPTIC Data Lake, and then configures event forwarding and also configures Automatic Event\\nCorrelation.\\nContainerized Operations Bridge 2022.11\\nPage \\n547\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '289222311ad867562f95d8e7c64e1e6b'}>,\n",
              "  <Document: {'content': 'info: Copying file obm-configurator.jar from container itom-static-files-provider to /root/tools2/obm-configurator.jar ...\\ntar: Removing leading `/\\' from member names\\ninfo: Successfully copied OBM setup tool\\ninfo: Getting deployment information ...\\ninfo: Creating files package ...\\ninfo: Creating package of files ...\\n/root/tools2\\ninfo: Creating tool jar ...\\ninfo: Unpacking tool package ...\\ninfo: Updating tool files ...\\ninfo: Creating the updated tool package ...\\n/root/tools2\\ninfo: Cleanup ...\\nSuccessfully created OBM setup tool in file /root/tools2/obm-configurator.jar\\nRemove old certificates from the OBM trust store\\n(Optional) If the classic OBM is connected to an earlier instance of OPTIC Data Lake, remove the old suite certificates from OBM\\ntrust store before executing the \\nobm-configurator.jar \\nfile. Refer to the Remove the OBM Configuration page in related\\ntopics and perform the steps, as required.\\nInstall an OBM CA certificate\\nPerform the following steps to install an OBM CA certificate on the Operations Bridge suite:\\n1\\n. \\nTo get the list of trusted certificates, run the following command on the classic OBM gateway server:\\nOn Linux\\n/opt/OV/bin/ovcert -list\\nOn Windows\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcert\" -list\\n2\\n. \\nFrom the list of certificates, locate the OBM\\n Trusted Certificate\\n.\\nFor example, \\nCA_297819c4-f266-75c1-0518-a723aacc1fde_2048\\n3\\n. \\nTo export the trusted certificate to a file, run the following command:\\nOn Linux\\n/opt/OV/bin/ovcert -exporttrusted -file obm_ca.crt -alias \"CA_297819c4-f266-75c1-0518-a723aacc1fde_2048\" -ovrg server\\nOn Windows\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcert\" -exporttrusted -file obm_ca.crt  -alias \"CA_297819c4-f266-75c1-0518-a723aacc1fde_2048\" -ovrg server\\n4\\n. \\nTo install an OBM CA certificate, do one of the following options:\\nInstall OBM CA certificate in Operations Bridge using the CLI\\na\\n. \\nCopy the \\nobm_ca.crt\\n file to the control plane (master) node of the Operations Bridge suite.\\nb\\n. \\nOn the control plane (master) node, install the OBM CA certificate using the\\n idl_config.sh\\n tool:\\nidl_config.sh -cacert <cert_file> -chart <chart> -namespace <namespace> [-release <release>]\\nTip: \\nThe OBM Trusted Certificate is usually with a * in the Trusted Certificates section.\\n\\ue917\\n\\ue917\\nNote: \\nYou can find the\\n idl_config.sh\\n tool in the \\nobm-configurator-interim \\ndirecto\\nry, which is in\\nthe \\nintegration-tools\\n direc\\ntory.\\n\\ue916\\n\\ue916\\nImportant:\\n \\nIf you have used an existing Shared OPTIC Data Lake, you must enter the Providing\\ndeployment\\'s chart name and Providing deployment application namespace.\\n  \\nFor example, if you have used\\nNOM\\'s Shared OPTIC DL, then you must provide NOM chart name and NOM application namespace.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n545\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '531f1627d171f4fd765080ec35347b7e'}>,\n",
              "  <Document: {'content': \"Compatibility chart\\nThe following table lists the components of Operations Bridge that are required to view Performance Dashboards (PD) with\\nOBM:\\nData source\\nMinimum version required\\nOperations Agent\\nManagement Pack\\nOperations Agent\\n12.14 or higher\\nOBM Management Pack for Infrastructure 2020.08\\nOperations Agent (Agent Metric\\nCollector)\\n12.00 and higher\\nNA\\nSiteScope 2020.10 and higher\\n12.14 and higher\\nOBM Management Pack for SiteScope Metric\\nStreaming 2020.05\\nBPM 9.53\\n12.14 and higher on the BPM\\nserver\\nOBM Management Pack for Business Process Monitor\\nNote: \\nTo graph BPM data that's present in OPTIC Data Lake on the Performance Dashboard:\\nThere must be one Containment relationship between the \\nBusiness Transaction Flow \\n(\\nBTF\\n) CI and the\\nBusiness Application (BA) CI.\\nIf there is a \\nCiCollection\\n (\\nCiC\\n) CI between the \\nBTF\\n and \\nBA\\n CI, then there must be one Containment\\nrelationship between the \\nBTF\\n and \\nCiC\\n CI.  \\nIf you choose to model additional relationships between the \\nBTF\\n CI and other \\nCiC \\nor BA CIs, use a different\\nrelationship such as Dependency.\\nContainerized Operations Bridge 2022.11\\nPage \\n554\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '432b19703ed4bece1e0bc8e2cc6d13b2'}>,\n",
              "  <Document: {'content': \"the certificate for the OBM administration user is in \\nPKCS12\\n format\\nthe certificate and key for the integration user is in \\nPEM \\nformat\\nClient Certificates\\nTool Parameter\\nFormat\\nDescription\\n--admin-client-cert\\nPKCS12\\nOBM admin user client certificate and key chain\\n--client-cert\\nPEM\\nIntegration user client certificate\\n--client-key\\nPEM\\nIntegration user client key\\nConfigure the Operations Bridge suite\\nThe obm-configurator.jar tool creates the \\nconfigureSuite.zip\\n file. Perform the following steps to use the\\ncreated \\nconfigureSuite.zip\\n file and configure the suite:\\n1\\n. \\nTo copy the \\nconfigureSuite.zip\\n file to the \\n/var/tmp \\ndirectory of the control plane (master) node, run the following\\ncommand:\\nscp configureSuite.zip root@<suite_master_hostname>:/var/tmp\\nFor example:\\nscp configureSuite.zip root@suitemaster.example.com:/var/tmp\\nIf you have installed the OBM system on Windows, manually copy the package to the control plane (master) node.\\n2\\n. \\nOn the control plane (master) node, extract the package:\\ncd /var/tmp\\nunzip configureSuite.zip -d configureSuite\\n3\\n. \\nTo run the \\nsetup-obm.sh\\n tool in the \\n/var/tmp/configureSuite\\n directory, run the following command.\\ncd configureSuite\\nbash setup-obm.sh -chart <path>\\nwhere \\nchart\\n is the path to either a directory containing the chart or a path to a \\ngzipped TAR\\n file.\\nFor example:\\ncd configureSuite\\nbash setup-obm.sh -chart /path/to/opsbridge-suite-2020.10.0-149.tgz\\nThe \\nsetup-obm.sh\\n tool does the following in the Operations Bridge suite:\\nNote:  \\nIt's recommended that the certificates should have a valid Subject Alternative Name (SAN). You\\ncan run the following command to verify if a certificate has a SAN: \\nopenssl x509 -noout -ext subjectAltName -in <certificate file>\\nFor example:\\nopenssl x509 -noout -ext subjectAltName -in server.crt\\nThe sample output after running the command is as follows:\\nX509v3 Subject Alternative Name:\\n    DNS:omi, DNS:omi-0, DNS:omi-0.omisvc, DNS:omi-0.omisvc.opsb-helm, DNS:omi-0.omisvc.opsb-helm.svc, DNS:omi-0.omi\\nsvc.opsb-helm.svc.cluster.local, DNS\\n:omi-0.omisvc.opsb-helm.svc.cluster.local.\\nImportant:\\n \\nIf you have used an existing Shared OPTIC Data Lake, you must enter the Providing\\ndeployment's chart name with the absolute path.\\n  \\nFor example, if you have used NOM's Shared OPTIC DL,\\nthen you must provide NOM chart name.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n549\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fa8bbfc9ae277617b69d3346d343bb5b'}>,\n",
              "  <Document: {'content': 'Browser after some time for a new event with the \\nAutomatically Correlated Event: … \\ntitle.\\nIf the event is visible in the browser, it indicates that you have configured Automatic Event Correlation correctly. \\nThe following image shows that AEC is configured.\\nRelated topics\\nIntegration Tools\\nOBM Configurator Tool\\nReconfigure a deployment\\nRemove the OBM Configuration\\nContainerized Operations Bridge 2022.11\\nPage \\n552\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '384dd5657ed024dfbfe7b3bd12f019d8'}>,\n",
              "  <Document: {'content': 'Installs the OBM CA certificate in the suite.\\nConfigures the OBM system as a data source and receiver for AEC events (if you set the \\n--configuration-type\\n to \\nAEC\\n).\\nVerify the OBM configuration\\nTo verify the OBM configuration:\\nVerify import of all OPTIC Data Lake certificates\\nEnsure that all OPTIC Data Lake certificates are imported into OBM.\\n1\\n. \\nRun the following command on OBM:\\nOn Linux\\n/opt/OV/bin/ovcert -list\\nOn Windows\\n\"%OvInstallDir%\\\\bin\\\\win64\\\\ovcert\" -list\\nThe command lists all trusted certificates. Make sure that the certificate starting with \\nMF CDF\\n exists in the trusted list.\\nHere\\'s a sample output after running the command:\\nVerify event forwarding\\n1\\n. \\nTo verify the configuration, run the following command on the OBM Gateway server and send a test event to OPTIC Data\\nLake:\\nOn Windows:\\nGo to\\n \\n%TOPAZ_HOME%\\\\opr\\\\support\\n and run the command, \\nsendEvent.bat -j -t TestEvent -s normal\\nHere\\'s a sample output after running the command:\\nNote\\n: The following certificate verification step is valid for on-premises installations only. The certificate names\\nvary in AWS and Azure environments.\\nNote\\n: The certificate name starting with \\nMF CDF\\n must exist for on-premises installations, and this name might\\nvary in cloud deployments such as in AWS, Azure, or OpenShift environments.  In cloud deployments, the\\ncertificate names are dependent on the cloud environments that you\\'re using.\\nNote\\n: The events that occur after you activate the Event Forwarding Rule, flow into OPTIC Data Lake.\\nContainerized Operations Bridge 2022.11\\nPage \\n550\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '760bb85b45ff4301e6f1b63bbca34998'}>,\n",
              "  <Document: {'content': '/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--configuration-type TRUST_ONLY \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--admin-user obmadmin\\nConfigure event forwarding from OBM to OPTIC Data Lake using basic authentication for the OBM administration\\nuser and the event integration user:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm\\\\\\n--configuration-type FORWARDING \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--admin-user obmadmin \\nConfigure Automatic Event Correlation where OBM uses client authentication for the OBM administration user\\nand the event integration user (default configuration type is AEC):\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--integration-user obm_integration_user \\\\\\n--admin-client-cert admin.user_cert.p12 \\\\\\n--client-cert integration.user_cert.pem \\\\\\n--client-key integration.user.key.pem \\nConfigure Automatic Event Correlation where you have set OBM without TLS. In this case, you must specify the\\nOBM URL:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--configuration-type AEC \\\\\\n--suite-service-hostname opsbsuite.company.com \\\\\\n--obm-ca-cert-alias CA_319fbf5a-119d-46b4-9260-1f4d881ff17d_2048 \\\\\\n--admin-user obmadmin --integration-user obm_integration_user \\\\\\n--obm-url \"http://obm.company.com:8081\"\\nFor Windows, replace \\n/opt/HP/BSM/JRE/bin/java\\n with \\n%TOPAZ_HOME%\\\\JRE\\\\bin\\\\java.exe\\nFor more examples and a detailed description of all possible tool parameters, see the\\n OBM Configurator Tool\\ntopic.\\nSpecial certificate handling\\nDepending on the certificates that OBM uses, it might be necessary to specify the alias of the certificates, which are as follows:\\nOBM Web Certificate\\nWhen you use a CA-signed certificate to access the web (for example, accessing the OBM web services), you must specify\\nthe alias of the installed web certificate using the \\n--web-cert-alias\\n parameter. Run the following command to find the alias:\\nOn Linux\\n/opt/HP/BSM/bin/opr-cert-mgmt.sh -list\\nOn Windows\\n%TOPAZ_HOME%\\\\bin\\\\opr-cert-mgmt.bat -list\\nOBM CA Certificate\\nWhen you install additional trusted certificates on OBM, it\\'s recommended that you use only the OBM CA certificate\\nto configure the suite. To prevent an import of all trusted certificates from OBM into the Operations Bridge suite, you must\\nspecify the CA certificate alias using the \\n--obm-ca-cert-alias\\n parameter. Run the following command to find the alias of the\\nOBM CA certificate: \\nOn Linux\\n/opt/OV/bin/ovcert -list -ovrg server\\nOn Windows\\n%OvInstallDir%\\\\bin\\\\win64\\\\ovcert.exe -list -ovrg server\\nOBM Client Certificates\\nIf you use client certificates to access OBM, you must specify the certificate files for the setup. When using client\\ncertificates, make sure that:\\nContainerized Operations Bridge 2022.11\\nPage \\n548\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58c105d24212159d54a08de6bee9dda7'}>,\n",
              "  <Document: {'content': 'On Linux:\\nGo to \\n/opt/HP/BSM/opr/support\\n and run the command,\\n ./sendEvent.sh -j -t TestEvent -s normal\\nHere\\'s a sample output after running the command:\\n2\\n. \\nFrom the OBM menu, choose \\nWorkspaces \\n> \\nOperations Console\\n and click \\nEvent Perspective\\n.\\n \\nCheck if the test\\nevent is listed and verify if the \\nState\\n of the event shows as, \\nForwarded\\n.\\n3\\n. \\nYou can also check the \\nopr_event\\n table in the \\nmf_shared_provider_default \\nschema to verify if the event has reached\\nOPTIC Data Lake.\\nOn a system that has the vsql command, such as a Vertica node, run the command:\\n/opt/vertica/bin/vsql -U dbadmin -c \"select node_hint,title,timestamp from mf_shared_provider_default.opr_event where title ilike \\'testEvent\\' limit 10;\"\\nYou are prompted \\nto enter the password for the \\ndbadmin\\n user. You can specify a different user such as the user, \\n<vertica_r\\nouser> \\nthat you created in the \\nPrepare Vertica database\\n section.\\nVerify AEC\\nWait for five minutes before verifying the AEC configuration because it can take up to five minutes for the configuration from\\nthe previous steps to be applied.\\n1\\n. \\nTo send a test event to OPTIC Data Lake, run the following command on the OBM Gateway server:\\nOn Windows\\n\"%TOPAZ_HOME%\\\\opr\\\\support\\\\sendEvent.bat\" -j -t \"Test Start\" -eh AutoCorrelationTest:Start -nx second -t \"Test End\" -eh AutoCorrelationTest:End\\nHere\\'s the sample output after running the command:\\nOn Linux\\n/opt/HP/BSM/opr/support/sendEvent.sh -j -t \"Test Start\" -eh AutoCorrelationTest:Start -nx second -t \"Test End\" -eh AutoCorrelationTest:End \\nHere\\'s the sample output after running the command on the Linux platform:\\n2\\n. \\nFrom the OBM menu, choose \\nWorkspaces \\n>\\n Operations Console \\nand click \\nEvent Perspective. \\nCheck the OBM Event\\nContainerized Operations Bridge 2022.11\\nPage \\n551\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '68464a390700aca2fcca9b5e06dfc46a'}>,\n",
              "  <Document: {'content': 'Configure Performance Dashboards\\nPerformance dashboard supports graphing Operations Agent, Business Process Monitoring (BPM), and SiteScope metrics from\\nthe OPTIC Data Lake.\\nTo enable graphing of these metrics, install the hotfix \\nHF_PD_11.00_011\\n (available through Micro Focus Software Support) on\\nOperations Bridge Manager 2020.05 (Gateway and DPS systems ) and then integrate it with containerized Operations Bridge\\nsuite.\\nFollow the steps:\\nPrerequisites\\nConfigure the data sources of your choice:\\nTo configure the Agent metric collector, see \\nConfigure System Infrastructure Reports using Agent Metric Collector\\n.\\nTo configure the metric streaming policies, see \\nConfigure System Infrastructure Reports using metric streaming policies\\n.\\nTo configure SiteScope, see \\nConfigure System Infrastructure Reports using SiteScope\\n.\\nTo configure BPM, see \\nConfigure synthetic transaction reports using BPM\\n.\\nTask 1: Install the hotfix\\nFollow the steps:\\n1\\n. \\nContact Support to get the hotfix. Then extract the \\nHF_PD_11.00_011.tar\\n file contents to a folder.\\n2\\n. \\nMake sure that you have set \\n$TOPAZ_HOME\\n.\\nOn Linux\\n: If TOPAZ_HOME isn\\'t set, then set to OBM installed folder as shown below:\\nexport TOPAZ_HOME=/opt/HP/BSM\\n3\\n. \\nTo install this hotfix, go to the location where you extracted the \\nHF_PD_11.00_011.tar\\n file and run \\nInstall-OBM-PD\\n script.\\n \\nOn Windows\\n: \\nRun Install-OBM-PD.bat\\n \\nOn Linux\\n:\\n Run Install-OBM-PD.sh \\n4\\n. \\nCheck for OBM status to see if all services are started and then launch the OBM.\\n5\\n. \\nFollow the steps to import the \\nOBMContentPack-Performance_Dashboard_Meta_Model_Configuration.zip\\n:\\n1\\n. \\n    In the OBM go to\\n Administration -> Setup and Maintaince -> Content Packs\\n.\\n2\\n. \\n    Select \\nImport Content Pack definitions and content\\n.\\n3\\n. \\n    Import the attached \"OBMContentPack-Performance_Dashboard_Meta_Model_Configuration.zip\" Content Pack.\\n4\\n. \\n    In Performance Perspective UI, click the \\nclear cache\\n menu option before launching/creating a dashboard.\\nTask 2: View metrics\\nFollow the steps:\\n1\\n. \\nOn OBM, go to \\nWorkspaces > Operations Console > Performance Perspective\\n. Performance Perspective appears.\\n2\\n. \\nThe nodes (of CIs) that are monitored by the data collectors (Agent, SiS, or BPM) are listed.\\n3\\n. \\nIn the left pane, select a node and right-click it.\\n4\\n. \\nSelect \\nShow\\n and then click \\nProperties\\n. The properties window appears. The ‘\\nMonitored By\\n’ field displays the data\\ncollector that\\'s monitoring the node.\\n5\\n. \\nIn the performance pane, click \\nNew Dashboards\\n. An empty dashboard appears.\\n1\\n. \\nClick on the chart title, a menu appears.\\n2\\n. \\nClick \\nEdit\\n.\\n3\\n. \\nType a title for the chart. Select a data source (OPTIC Data Lake), class name, metric name, and instance name from\\nthe respective drop-down. Enter a label in the Label box.\\n4\\n. \\n(Optional) Click \\nAdd Metrics\\n, to add another metric class or metrics.\\n6\\n. \\nClick \\nSave\\n to save the dashboard.\\nNote:\\n  If you are using OBM 2020.10 (classic or containerized), or a higher version you don\\'t need the hotfix. \\n\\ue916\\n\\ue916\\nNote:\\n Apply the hotfix only if you are using OBM 2020.05.\\n\\ue916\\n\\ue916\\nNote:\\nIn the case of a distributed OBM setup, you must apply the hotfix on all Gateway servers.\\nThe backup of the original war is available at \\n%ovinstalldir%\\\\newconfig\\\\OVPM\\\\backup\\n folder\\nThe hotfix installation logs are available at:\\n \\nOn Windows\\n: \\n\\'%TOPAZ_HOME%\\\\log\\\\pmi\\\\Install_PDHotfix.log\\'\\n \\nOn Linux\\n: \\n\\'$TOPAZ_HOME/log/pmi/Install_PDHotfix.log\\' \\nContainerized Operations Bridge 2022.11\\nPage \\n553\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3e2b3683d8776a62297a4e7dcf240c73'}>,\n",
              "  <Document: {'content': \"3\\n. \\nIn the Receiver Endpoint section, complete the following information:\\n1\\n. \\nOptional\\n. Select the \\nUse HTTP(S) proxy server to connect to receiver\\n check box to specify proxy settings.\\nEnter the host name of the proxy system, the proxy port number, and the proxy user name and the password\\nassociated with the proxy user.\\n2\\n. \\nEnter the Endpoint URL. Depending on your BVD and OBM versions, this URL has one of the following formats:\\nhttps://\\n<external_access_host>\\n:<\\nPort\\n>/bvd-receiver/api/submit/\\n<API_key>\\n (BVD container deployment, OBM classic\\ndeployment)\\nhttps://bvd-receiver.<\\nnamespace\\n>.svc.cluster.local:4000/bvd-receiver/api/submit/<\\nAPI_key\\n>\\n (BVD container\\ndeployment, OBM container deployment)\\nhttp(s)://\\n<Hostname>\\n:<\\nPort\\n>/api/submit/\\n<API_key>\\n (BVD 10.12 or earlier)\\n<external_access_host>\\n is the FQDN of the host which you specified as \\nEXTERNAL_ACCESS_HOST\\n in the \\ninstall.propertie\\ns\\n file during the ITOM Platform installation. Usually, this is the master node's FQDN.\\n<\\nnamespace\\n>\\n is the namespace assigned to your Operations Bridge Suite deployment. \\n<\\nHostname\\n>\\n is the Fully Qualified Domain Name (FQDN) of the BVD server and \\nPort\\n is the port assigned to the\\nBVD receiver during the configuration (default: 12224 or 12225).\\nTo find out the value for \\nAPI_key\\n, log in to BVD as an administrator. Navigate to \\nAdministration > System\\nSettings\\n and copy the key.\\nExamples:\\nhttps://receiver.example.com/bvd-receiver/api/submit/e7a1dfba6dbe4eaf89c6a8421cb5892b\\nhttps://receiver.example.com:12225/api/submit/e7a1dfba6dbe4eaf89c6a8421cb5892b\\n3\\n. \\nClick \\nimport the certificate\\n to import the BVD TLS certificate either directly from the server or to upload the\\nlocally available certificate file.\\n4\\n. \\nOptional\\n. In the BVD Configuration section, enter a comma separated list of tags. Tag the data channels  to separate\\ndata from incoming streams and to create more specific data channels. For example, if you have separate OBM\\nservers for different regions and you want separate dashboards for each region, you can add a tag that identifies the\\nregion for this OBM server location.\\n5\\n. \\nIn the Test Connection section, click \\nRun Test\\n to check that the specified connection attributes are correct. If you\\nsee any error message, correct the connection information, and retest the connection.\\n6\\n. \\nMake sure to select the  \\nActivate after save\\n check box if you want to enable the server connection immediately.\\n7\\n. \\nClick \\nCreate\\n to save this BVD connection.\\n2\\n. \\nAccess \\nAdministration > Setup and Maintenance > BVD Data Forwarding\\n. In the right pane, click \\nCreate\\n. Also,\\nclick \\n \\nNew\\n.\\n3\\n. \\nIn the \\nGeneral\\n section, complete the following information:\\n1\\n. \\nEnter a display name and (optional) a description for the forwarding rule.\\n2\\n. \\nOptional\\n. Enter a comma separated list of tags.\\nYou can tag data channels to separate data from incoming streams and to create more specific data channels. For\\nexample, if you have separate \\nOBM\\n servers for different regions and you want separate dashboards for each region,\\nyou can add a tag that identifies the region for this \\nOBM\\n server location.\\nThe tags you enter gets added to the data channel after the tags specified for the BVD Connected Server.\\n4\\n. \\nIn the \\nTarget BVD Server\\n section, select \\nt\\nhe connected server that will receive data from OBM.\\n5\\n. \\nOptional\\n. In the \\nEvent Status\\n section, choose one or multiple monitoring dashboards from which you want to forward\\ndata to BVD.\\nCaution:\\n  If you change the monitoring dashboard name, the data channel doesn't get updated. Instead, BVD creates a\\nnew data channel with the changed monitoring dashboard name. Widgets that use the old data channel in BVD won't\\nreceive data from \\nOBM\\n anymore and you need to update  the new data channel.\\n6\\n. \\nOptional\\n. In the \\nKPI Status\\n section, choose the one or multiple views from which you want to forward KPI status data.\\nClick next to the view name to choose specific KPIs. If no individual KPIs are selected, system forwards KPI status data\\nfor all CIs that are associated with the chosen view.\\n7\\n. \\nOptional\\n. In the \\nPerformance Dashboard Data\\n section, choose one or multiple public favorites of your performance\\ndashboards for which you want to forward data to BVD.\\n8\\n. \\nOptional\\n. Clear the check box \\nActivate after save\\n if you want the status of the rule to be inactive after clicking \\nSave\\n.\\nYou can activate the rule at a later point in time.\\n9\\n. \\nClick \\nSave\\n to save the BVD data forwarding rule.\\nContainerized Operations Bridge 2022.11\\nPage \\n556\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c40e4c57b478e6445f12651d2cee685'}>,\n",
              "  <Document: {'content': 'Integrate BVD with OBM\\nYou can configure OBM to send Event Status, KPI Status and Performance Dashboard data to \\nBVD\\n. Use the \\nBVD\\n Data\\nForwarding rules manager on the \\nOBM\\n server to specify which data you want to forward.\\nData forwarding\\nForward the following \\nOBM\\n data to \\nBVD\\n:\\nEvent Status data:\\n From the specified OBM monitoring dashboard, event status data gets collected and forwarded.\\nKPI Status data:\\n The KPI status data is, data collected from all CIs that are associated with a view  and the KPI set that you\\nspecify. If you don\\'t  specify a KPI set, all KPIs of the chosen view are forwarded.\\nPerformance Dashboard data:\\n The performance dashboard data is, data collected from your public favorites in \\nOBM\\n. To\\nforward Performance Dashboard data, save your performance dashboard charts as favorites with the \\nShare as Public\\n option\\nenabled before including this data in a rule.\\nBVD Data Channels for \\nOBM\\n Data\\nWhen you send BVD data to \\nOBM\\n by using forwarding rules, BVD creates data channels that uniquely identify the data you are\\nsending.\\nEach data channel consists of tags and dimensions (dims). Tags are static labels and dimensions are names that are\\nassociated with a specific value. For more information, see the Create custom integrations section.\\nThe data channels are structured differently depending on the data you choose to forward:\\nEvent Status\\n<tags connected server><tags forwarding rule><dim monitoring dashboard><dim widget label><dim widget type>\\nKPI Status\\n<tags connected server><tags forwarding rule><dim view name><dim CI name><dim KPI name>\\nPerformance Dashboard\\n<tags connected server><tags forwarding rule><metricName><instanceName><dSName><systemName><className>\\n<tags connected server>\\n are all tags that are specified when adding a BVD Connected Server.\\n<tags forwarding rule>\\n are all tags that are specified when creating a BVD Data Forwarding Rule.\\nEnable Performance Dashboard Data Forwarding on the \\nOBM\\n Server\\nTo enable performance dashboard data forwarding to BVD, follow these steps:\\n1\\n. \\nIn the Performance Perspective, select the charts that you want to forward and save them as favorites:\\n1\\n. \\nIn \\nOBM\\n, access \\nWorkspaces > Operations Console > Performance Perspective\\n.\\n2\\n. \\nIn the \\nView Explorer\\n, select a view and then the CI for which you want enable data forwarding.\\n3\\n. \\nIn the \\nPerformance\\n pane, choose a performance dashboard from the drop-down list.\\n4\\n. \\nClick the title of the chart you want to save as favorite and click \\nAdd to Favorite\\n.\\nChoose to add the favorite to the default page, user-defined favorite page, or create a new user defined favorite\\npage. Click \\nSave.\\n2\\n. \\nOpen the favorite in the \\nPerformance\\n pane, access the menu and click \\nSave\\n. Check the \\nShare as Public\\n option and\\nclick \\nSave\\n.\\nForward \\nOBM\\n Data to BVD\\nTo forward \\nOBM\\n data to BVD, follow the steps:\\n1\\n. \\nAdd your BVD instance as a Connected Server in OBM as follows:\\n1\\n. \\nIn the central Connected Servers pane, click \\nNew\\n and select \\nBusiness Value Dashboard\\n. Also, you can click \\nNew\\nin the Business Value Dashboard area in the right pane.\\nThe Create BVD Connected Server panel opens.\\n2\\n. \\nIn the General section, enter a display label, an identifier (a unique internal name if you want to replace the\\nautomatically generated one), and, optionally, a description of the specified connection.\\nNote\\n: When you forward more data to BVD than the database could handle, you will get the message \"\\nThe BVD\\ndata receiver is throttled\\n\" from the receiver. To avoid this and to retain DB health status, reduce the number\\nof days for the data records that are stored in DB. This will automatically delete records older than the configured\\ntime span. To configure the number of days to keep data records, access \\nAdministration > System settings\\n> Aging\\n. Refer to \\nModify the settings\\n page to get more information.\\nContainerized Operations Bridge 2022.11\\nPage \\n555\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c3afd1aa2d5373fed745a95ee4c8c93'}>,\n",
              "  <Document: {'content': \"3\\n. \\nIn the Receiver Endpoint section, complete the following information:\\n1\\n. \\nOptional\\n. Select the \\nUse HTTP(S) proxy server to connect to receiver\\n check box to specify proxy settings.\\nEnter the host name of the proxy system, the proxy port number, and the proxy user name and the password\\nassociated with the proxy user.\\n2\\n. \\nEnter the Endpoint URL. Depending on your BVD and OBM versions, this URL has one of the following formats:\\nhttps://\\n<external_access_host>\\n:<\\nPort\\n>/bvd-receiver/api/submit/\\n<API_key>\\n (BVD container deployment, OBM classic\\ndeployment)\\nhttps://bvd-receiver.<\\nnamespace\\n>.svc.cluster.local:4000/bvd-receiver/api/submit/<\\nAPI_key\\n>\\n (BVD container\\ndeployment, OBM container deployment)\\nhttp(s)://\\n<Hostname>\\n:<\\nPort\\n>/api/submit/\\n<API_key>\\n (BVD 10.12 or earlier)\\n<external_access_host>\\n is the FQDN of the host which you specified as \\nEXTERNAL_ACCESS_HOST\\n in the \\ninstall.propertie\\ns\\n file during the ITOM Platform installation. Usually, this is the master node's FQDN.\\n<\\nnamespace\\n>\\n is the namespace assigned to your Operations Bridge Suite deployment. \\n<\\nHostname\\n>\\n is the Fully Qualified Domain Name (FQDN) of the BVD server and \\nPort\\n is the port assigned to the\\nBVD receiver during the configuration (default: 12224 or 12225).\\nTo find out the value for \\nAPI_key\\n, log in to BVD as an administrator. Navigate to \\nAdministration > System\\nSettings\\n and copy the key.\\nExamples:\\nhttps://receiver.example.com/bvd-receiver/api/submit/e7a1dfba6dbe4eaf89c6a8421cb5892b\\nhttps://receiver.example.com:12225/api/submit/e7a1dfba6dbe4eaf89c6a8421cb5892b\\n3\\n. \\nClick \\nimport the certificate\\n to import the BVD TLS certificate either directly from the server or to upload the\\nlocally available certificate file.\\n4\\n. \\nOptional\\n. In the BVD Configuration section, enter a comma separated list of tags. Tag the data channels  to separate\\ndata from incoming streams and to create more specific data channels. For example, if you have separate OBM\\nservers for different regions and you want separate dashboards for each region, you can add a tag that identifies the\\nregion for this OBM server location.\\n5\\n. \\nIn the Test Connection section, click \\nRun Test\\n to check that the specified connection attributes are correct. If you\\nsee any error message, correct the connection information, and retest the connection.\\n6\\n. \\nMake sure to select the  \\nActivate after save\\n check box if you want to enable the server connection immediately.\\n7\\n. \\nClick \\nCreate\\n to save this BVD connection.\\n2\\n. \\nAccess \\nAdministration > Setup and Maintenance > BVD Data Forwarding\\n. In the right pane, click \\nCreate\\n. Also,\\nclick \\n \\nNew\\n.\\n3\\n. \\nIn the \\nGeneral\\n section, complete the following information:\\n1\\n. \\nEnter a display name and (optional) a description for the forwarding rule.\\n2\\n. \\nOptional\\n. Enter a comma separated list of tags.\\nYou can tag data channels to separate data from incoming streams and to create more specific data channels. For\\nexample, if you have separate \\nOBM\\n servers for different regions and you want separate dashboards for each region,\\nyou can add a tag that identifies the region for this \\nOBM\\n server location.\\nThe tags you enter gets added to the data channel after the tags specified for the BVD Connected Server.\\n4\\n. \\nIn the \\nTarget BVD Server\\n section, select \\nt\\nhe connected server that will receive data from OBM.\\n5\\n. \\nOptional\\n. In the \\nEvent Status\\n section, choose one or multiple monitoring dashboards from which you want to forward\\ndata to BVD.\\nCaution:\\n  If you change the monitoring dashboard name, the data channel doesn't get updated. Instead, BVD creates a\\nnew data channel with the changed monitoring dashboard name. Widgets that use the old data channel in BVD won't\\nreceive data from \\nOBM\\n anymore and you need to update  the new data channel.\\n6\\n. \\nOptional\\n. In the \\nKPI Status\\n section, choose the one or multiple views from which you want to forward KPI status data.\\nClick next to the view name to choose specific KPIs. If no individual KPIs are selected, system forwards KPI status data\\nfor all CIs that are associated with the chosen view.\\n7\\n. \\nOptional\\n. In the \\nPerformance Dashboard Data\\n section, choose one or multiple public favorites of your performance\\ndashboards for which you want to forward data to Stakeholder Dashboard.\\n8\\n. \\nOptional\\n. Clear the check box \\nActivate after save\\n if you want the status of the rule to be inactive after clicking \\nSave\\n.\\nYou can activate the rule at a later point in time.\\n9\\n. \\nClick \\nSave\\n to save the BVD data forwarding rule.\\nContainerized Operations Bridge 2022.11\\nPage \\n558\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '188ee0bc4035d9c140d72c2a94bf064b'}>,\n",
              "  <Document: {'content': \"Example: Sending JSON Data to BVD\\nSending Data from Data Center East\\nIn this example, Data Center East sends two sets of JSON data to the BVD receiver. In both sets, the data fields \\nhost\\nand \\nmetricName\\n uniquely identify the value. The fields are therefore selected as dimensions (dims) and included in the\\nURL. Once received by the BVD server, the JSON data creates two data channels:\\n \\nHost A\\n \\n \\nCPU load\\n and \\n \\nHost B\\n \\n \\nDisk util\\n.\\nLessons learned:\\n Pick the fields in your data that uniquely identify the values you want to send to BVD and include\\nthe fields as dimensions in the HTTP post request.\\nNote\\n: If you send data to BVD from an application that's \\nnot\\n part of the suite container deployment (for example a\\nclassically installed \\nOBM\\n), define the receiver URL as follows:\\nhttps://<external_access_host>/bvd-receiver/api/submit/<API key>\\nIf you send data to BVD from an application that's also installed as a suite container, define the receiver URL as follows:\\nhttps://bvd-receiver\\n.<namespace>.svc.cluster.local:4000/bvd-receiver/api/submit/<API_key>\\n<namespace>\\n is the namespace assigned to your suite deployment. You can check the namespace by accessing\\nSUITE > Management\\n in the Management Portal.\\nSending Additional Data From Data Center East\\nThe primary location of Data Center East is in New York City, with backup servers located in Boston. Both locations\\nsend the same set of JSON data. To differentiate the data from the two locations without modifying the JSON data, you\\ncan add an additional dimension \\nloc\\n with the corresponding value to the URL. The modified URL updates the data\\nchannels to\\n \\nHost A\\n \\n \\nCPU load\\n \\n \\nNYC\\n and \\n \\nHost B\\n \\n \\nDisk util\\n \\n \\nBoston\\n.\\nIn this example, the dimension \\nloc\\n added to the URL.\\nLessons learned:\\n Directly assign values to your dimensions by adding dim=value pairs to the HTTP post request.\\nSending Data From Data Center West\\nContainerized Operations Bridge 2022.11\\nPage \\n562\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e8058271faa9e9927c94d866d4b2144'}>,\n",
              "  <Document: {'content': \"A second data center, Data Center West, starts sending data similar to the JSON data sent by Data Center East. The\\ndata from Data Center West uses the same data channels as the data from East. To distinguish the data from the two\\ncenters, you must add the origin to the data. You can do this by adding tags to the URL. Tags are static labels that you\\ncan attach to your data to create more specific data channels.\\nIn this example, added the tags \\neast\\n and \\nwest\\n to the URL. The tags precede the dims in the data channels.\\nLessons learned:\\n Attach tags to your data to create specific data channels.\\nAssociating Data Channels with Widgets\\nOnce \\nBVD\\n has received the data, it creates the corresponding data channels. You can then associate a data channel\\nwith your widget in the widget's properties. In this example, for the \\nSparkline \\nwidget, associate the following data\\nchannel: \\n \\n \\neast\\n \\n \\nHost A\\n \\n \\nCPU load\\n \\n \\nNYC\\n.\\nBy default, the widget consumes data from the \\nvalue\\n data field. In this example, the current value is 42. If the field that\\nholds the values you are interested in has a different name (for example, \\nmetricVal\\n), select that name in the Data\\nField property of the widget.\\nLessons learned:\\n Connect your data to a widget by selecting the corresponding data channel in the widget's\\nproperties.\\nContainerized Operations Bridge 2022.11\\nPage \\n563\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1c204a6dec0de9dd119e5898f9a9398f'}>,\n",
              "  <Document: {'content': 'Integrate BVD with OBM\\nYou can configure OBM to send Event Status, KPI Status and Performance Dashboard data to \\nStakeholder Dasboard\\n. Use the\\nBVD\\n Data Forwarding rules manager on the \\nOBM\\n server to specify which data you want to forward.\\nData forwarding\\nForward the following \\nOBM\\n data to \\nStakeholder Dasboard\\n:\\nEvent Status data:\\n From the specified OBM monitoring dashboard, event status data gets collected and forwarded.\\nKPI Status data:\\n The KPI status data is, data collected from all CIs that are associated with a view  and the KPI set that you\\nspecify. If you don\\'t  specify a KPI set, all KPIs of the chosen view are forwarded.\\nPerformance Dashboard data:\\n The performance dashboard data is, data collected from your public favorites in \\nOBM\\n. To\\nforward Performance Dashboard data, save your performance dashboard charts as favorites with the \\nShare as Public\\n option\\nenabled before including this data in a rule.\\nBVD Data Channels for \\nOBM\\n Data\\nWhen you send BVD data to \\nOBM\\n by using forwarding rules, BVD creates data channels that uniquely identify the data you are\\nsending.\\nEach data channel consists of tags and dimensions (dims). Tags are static labels and dimensions are names that are\\nassociated with a specific value. For more information, see the Create custom integrations section.\\nThe data channels are structured differently depending on the data you choose to forward:\\nEvent Status\\n<tags connected server><tags forwarding rule><dim monitoring dashboard><dim widget label><dim widget type>\\nKPI Status\\n<tags connected server><tags forwarding rule><dim view name><dim CI name><dim KPI name>\\nPerformance Dashboard\\n<tags connected server><tags forwarding rule><metricName><instanceName><dSName><systemName><className>\\n<tags connected server>\\n are all tags that are specified when adding a BVD Connected Server.\\n<tags forwarding rule>\\n are all tags that are specified when creating a BVD Data Forwarding Rule.\\nForward Performance Dashboard Data to Stakeholder Dashboard\\nTo enable performance dashboard data forwarding to BVD, follow these steps:\\n1\\n. \\nIn the Performance Perspective, select the charts that you want to forward and save them as favorites:\\n1\\n. \\nIn \\nOBM\\n, access \\nWorkspaces > Operations Console > Performance Perspective\\n.\\n2\\n. \\nIn the \\nView Explorer\\n, select a view and then the CI for which you want enable data forwarding.\\n3\\n. \\nIn the \\nPerformance\\n pane, choose a performance dashboard from the drop-down list.\\n4\\n. \\nClick the title of the chart you want to save as favorite and click \\nAdd to Favorite\\n.\\nChoose to add the favorite to the default page, user-defined favorite page, or create a new user defined favorite\\npage. Click \\nSave.\\n2\\n. \\nOpen the favorite in the \\nPerformance\\n pane, access the menu and click \\nSave\\n. Check the \\nShare as Public\\n option and\\nclick \\nSave\\n.\\nForward \\nOBM\\n Data to Stakeholder Dashboard\\nTo forward \\nOBM\\n data to Stakeholder Dashboard, follow the steps:\\n1\\n. \\nAdd your BVD instance as a Connected Server in OBM as follows:\\n1\\n. \\nIn the central Connected Servers pane, click \\nNew\\n and select \\nBusiness Value Dashboard\\n. Also, you can click \\nNew\\nin the Business Value Dashboard area in the right pane.\\nThe Create BVD Connected Server panel opens.\\n2\\n. \\nIn the General section, enter a display label, an identifier (a unique internal name if you want to replace the\\nautomatically generated one), and, optionally, a description of the specified connection.\\nNote\\n: When you forward more data to Stakeholder Dashboard than the database could handle, you will get the\\nmessage \"\\nThe BVD data receiver is throttled\\n\" from the receiver. To avoid this and to retain DB health status,\\nreduce the number of days for the data records that are stored in DB. This will automatically delete records older\\nthan the configured time span. To configure the number of days to keep data records, access \\nAdministration >\\nSystem settings > Aging\\n. Refer to \\nModify the settings\\n page to get more information.\\nContainerized Operations Bridge 2022.11\\nPage \\n557\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b050b82823690ff551095440bca3250f'}>,\n",
              "  <Document: {'content': \"https://bvd.example.com/bvd-\\nreceiver/api/submit/47a648e9065d465012e541288b5a345e/dims/viewName,ciName=abc/tags/obm,kpi?\\ndims=kpiName,location=nyc&tags=bvd\\nHowever, if you specify the same dimension or tag more than once, the value of the last query parameter overwrites the\\nvalues of the previous parameters. The value of the last query parameter appears multiple times as data channel.\\nExample\\nhttps://bvd.example.com/bvd-receiver/api/submit/47a648e9065d465012e541288b5a345e/dims/location=boston?\\ndims=location=nyc&tags=east&dims=location=atlanta\\nIn this example, the dim \\nlocation\\n will have the value \\natlanta\\n. Because dimensions are accumulated, the value \\natlanta\\n appears\\nthree times as data channel.\\nJSON data arrays\\nYou can submit multiple JSON objects in a single web service call by adding them to an array.\\nArray:\\n[\\n{\\n      a: 1,\\n      b: 2\\n   },\\n{\\n      c: 3,\\n      d: 4\\n   }\\n]\\nNested JSON data\\nIf the input contains nested data, \\nBVD\\n automatically flattens the data by renaming nested name value pairs to include the\\nnames of the parent elements, separated by slashes (/), for example:\\nNested JSON data:\\nFlattened JSON data:\\n{\\n   a: 1,\\n   b: 2,\\n   c: {\\n      x: 6,\\n      y: 7\\n   }\\n}\\n{\\n   a: 1,\\n   b: 2,\\n   c/x: 6,\\nc/y: 7\\n}\\nData storage\\nBVD\\n stores only a specific number of data records per channel. The records are only kept if they 're related to a widget.\\nContainerized Operations Bridge 2022.11\\nPage \\n561\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '980e20fd49492bb962e034331127b1a0'}>,\n",
              "  <Document: {'content': \"Create custom integrations\\nBVD\\n expects to receive your data as HTTP post requests in JavaScript Object Notation (JSON) format.\\nLet your JSON input contains flat data, consisting of name value pairs. If you must send nested data, \\nBVD\\n automatically\\nflattens the data. You can also send JSON data in arrays. This enables you to send multiple data objects in a single web service\\ncall.\\nLearn More\\nSending dimensions and tags in the receiver URL\\nThe \\nBVD\\n receiver URL should look something like this:\\nURL with dimensions only:\\nhttps://<external_access_host>/bvd-receiver/api/submit/<API key>/dims/<dims>[,<dims=value>]\\nURL with tags only:\\nhttps://<external_access_host>/bvd-receiver/api/submit/<API key>/tags/<tags>\\nURL with both dimensions and tags:\\nhttps://<external_access_host>/bvd-receiver/api/submit/<API key>/dims/<dims>[,<dims=value>]/tags/<tags>\\nThe tags can also precede the dims:\\nhttps://<external_access_host>/bvd-receiver/api/submit/<API key>/tags/<tags>/dims/<dims>[,<dims=value>]\\nIf the application sending the data is also installed as a suite container, define the receiver URL as follows:\\nhttps://bvd-receiver\\n.<namespace>.svc.cluster.local:4000/bvd-receiver/api/submit/<API_key>\\n<external_access_host>\\n \\nThe fully qualified domain name of the host which you specified as \\nEXTERNAL_ACCESS_HOST\\n in the \\ninstall.properties\\n file\\nduring the Container Deployment Foundation installation. Usually, this is the master node's FQDN.\\n<namespace>\\n \\nThe namespace assigned to your suite deployment. You can check the namespace by accessing \\nSUITE > Management\\nin the Management Portal.\\n<API_key>\\n \\nIdentifies your \\nBVD\\n instance. You can find the API key in \\n \\nAdministration\\n \\n> Settings\\n.\\n<tags>\\n \\nStatic labels that you can attach to your data to create more specific data channels.\\n<dims>\\n \\nThe names in your JSON name value pairs. Select and combine dimensions (dims) that uniquely identify your data.\\n<dims=value>\\n \\nThe names and values in your JSON name value pairs. Directly assign values with names to improve data identification.\\nUse this option, for example, if you have separate servers of the same data source for different locations and you want\\nseparate dashboards for each location. These name value pairs don't have to be part of the JSON input. If they're, the\\nvalues in the URL will overwrite the values in the JSON input.\\nSending dims and tags as HTTP parameters\\nYou can also submit the dims and tags as HTTP parameters of the URL.\\nExample\\nhttps://bvd.example.com/bvd-receiver/api/submit/47a648e9065d465012e541288b5a345e?\\ndims=viewName,ciName,kpiName,location=nyc&tags=obm,kpi\\nSending dims and tags in the receiver URL and as HTTP parameters\\nYou can combine the receiver URL and HTTP parameters to send dims and tags. Define the dims and tags as part of the URL\\npath first, then add additional dims and tags as HTTP parameters.\\nExample\\nContainerized Operations Bridge 2022.11\\nPage \\n560\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e49c1cd21cd57ecd80c0c848d0638e4'}>,\n",
              "  <Document: {'content': 'Verifying metrics forwarding from OBM to BVD\\nMetrics Forwarding from Performance Graphing in OBM \\n9.2x \\nand \\n10.0x\\n:\\n1\\n. \\nIn \\nOBM\\n, open Infrastructure Settings:\\nAdmin > Platform > Setup and Maintenance > Infrastructure Settings\\nIn the \\nApplications\\n list, select \\nPerformance Graphing\\n.\\n2\\n. \\nSet the option \\nTrace Level\\n to 2.\\n3\\n. \\nAccess the \\novpmtrace.0.txt\\n file available at the following location:\\nWindows: \\n%ovdatadir%\\\\shared\\\\server\\\\log\\nLinux: \\n/var/opt/OV/shared/server/log\\n4\\n. \\nThe log file contains trace messages that indicate that \\nPerformance Graphing\\n is forwarding the data to the endpoint.\\nThe following are samples from the log file:\\ncom.hp.pm.core.configuration.PostDataTask:run() -> JSON data to post ...\\ncom.hp.pm.core.configuration.PostDataTask:postDashboardData() -> \\nPost data to service dashboard endpoint is success\\nMetrics Forwarding from Performance Dashboard in OBM 10.1x:\\n1\\n. \\nIn \\nOBM\\n, open Infrastructure Settings:\\nAdministration > Setup and Maintenance > Infrastructure Settings\\nIn the \\nApplications\\n list, select \\nPerformance Dashboard\\n.\\n2\\n. \\nAccess the \\nbvd.log\\n file available at the following location:\\nWindows: \\n<OMi_HOME>\\\\log\\\\pmi\\nLinux: \\n/opt/HP/BSM/log/pmi\\nThe log file contains trace messages that indicate that \\nPerformance Dashboard\\n is forwarding the data to the endpoint.\\nThe following are samples (\\ntrace level set to INFO\\n) from the log file:\\ncom.hp.pm.core.configuration.bvd.PostDataTask:postDashboardData() \\n-> BVD - Post data to endpoint is success\\n3\\n. \\nTo enable debugging or tracing, edit the \\nlog4j.properties\\n file and set all \\nlog4j.category\\n variables as \\nDEBUG\\n or \\nTRACE\\n:\\nWindows: \\n%TOPAZ_HOME%\\\\conf\\\\core\\\\Tools\\\\log4j\\\\pmi\\\\log4j.properties\\nLinux: \\n$TOPAZ_HOME/conf/core/Tools/log4j/pmi/log4j.properties\\nThe \\nPerformance Dashboard\\n log file is available at the following location:\\nWindows: \\n%TOPAZ_HOME%\\\\log\\\\pmi\\\\pmi.log\\nLinux: \\n$TOPAZ_HOME/log/pmi/pmi.log\\nThe \\nPerformance Dashboard\\n - BVD integration log file is available at the following location:\\nWindows: \\n%TOPAZ_HOME%\\\\log\\\\pmi\\\\bvd.log\\nLinux: \\n$TOPAZ_HOME/log/pmi/bvd.log\\nContainerized Operations Bridge 2022.11\\nPage \\n559\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f04e67ec112e302f11fc5ac14af3cefc'}>,\n",
              "  <Document: {'content': '1\\n. \\nGo to \\nAdministration\\n > \\nService Health\\n > \\nKPI Assignment\\n.\\n2\\n. \\nSelect \\nConfiguration Item\\n as the CI Type.\\n3\\n. \\nSelect a KPI Assignment and click \\n \\nto edit it.\\n4\\n. \\nAdd \\nCompliance\\n and \\nVulnerability\\n to the KPI configuration:\\nBusiness Rule as \\n \\nWorst Status Rule\\nCalculated Based on as HIs and Child KPIs\\nRelated HIS as \\nUnresolved most critical events: Compliance\\n (for compliance) or \\nUnresolved most critical\\nevents: Vulnerability\\n (for Vulnerability)\\n5\\n. \\nClick \\nSave\\n.\\nKPI status propagation\\nTo enable status propagation for the compliance and vulnerability KPI, edit the KPI and set the status to default as follows:\\n1\\n. \\nIn OBM, go to \\nAdministration\\n > \\nService health\\n > \\nKPI definition\\n.\\n2\\n. \\nSelect \\nCompliance\\n3\\n. \\nIn the \\nRules\\n > \\nApplication rule\\n option, make sure that the value of \\nWorst Status Rule\\n is set to \\nDefault\\n.\\nIntegrate DCA and OBM\\nThis integration is supported with only containerized OBM deployment. You can integrate only one instance of OBM with DCA.\\nFollow the steps to integrate DCA and OBM:\\n1\\n. \\nFrom the DCA console, go to \\nSETTINGS\\n > \\nIntegrations\\n >\\nOperations Bridge Integrations\\n.\\n2\\n. \\nClick \\n \\nnext to \\nOperations Bridge Integrations\\n.\\n3\\n. \\nIn the \\nOperations Bridge Configuration\\n area, specify the following information:\\nName: This can be a combination of alphanumeric characters and special symbols with a maximum length of 100\\ncharacters.\\nFully Qualified Host Name: Fully qualified host name of the OBM server.\\nPort: 443\\nUsername: An user with admin access or who has permissions to create events. Ensure the username is existing in\\nthe OBM server before integrating with DCA.\\nPassword: Password of the OBM user.\\nTrusted Certificate: Select the trusted certificate file (\\n<filename>.crt\\n) on your local system.\\n4\\n. \\nClick \\nOK\\n. OBM server with host name specified is listed in the \\nOperations Bridge Integrations\\n page.\\nEnable OBM integration\\nFollow the steps to enable event logging in OBM when ever a compliance or vulnerability scan is performed in DCA:\\n1\\n. \\nOn the DCA console, go to \\nSETTINGS\\n > \\nIntegrations\\n > \\nOperations Bridge Integrations\\n.\\n2\\n. \\nClick the toggle switch to enable OBM server integration with DCA.\\nValidate integration\\nTo test OBM integration with DCA, complete the following steps:\\n1\\n. \\nFrom the DCA console, go to \\nSETTINGS\\n > \\nIntegrations\\n > \\nOperations Bridge Integrations\\n.\\n2\\n. \\nClick the Setting icon next to the OBM server and select \\nTest Connection\\n.\\nIf the integration is successful, a banner \"Test connection successful for <\\nOBM server name specified in DCA\\n>\" is\\ndisplayed at the top of the console.\\nEdit OBM integration\\nYou cannot edit the Fully Qualified Host Name field. If you want to change the Fully Qualified Host Name, delete the existing\\nintegration and then create a new integration with the desired Fully Qualified Host Name.\\nTo edit the OBM and DCA integration, complete the steps:\\n1\\n. \\nFrom the DCA console, go to \\nSETTINGS\\n > \\nIntegrations\\n > \\nOperations Bridge Integrations\\n.\\n2\\n. \\nClick the setting icon next to the OBM server and select \\nEdit\\n.\\n3\\n. \\nMake changes as required and click \\nOK\\n to save changes.\\nDelete OBM integration\\nTo delete the OBM and DCA integration, complete the steps:\\n1\\n. \\nFrom the DCA console, go to \\nSETTINGS\\n > \\nIntegrations\\n > \\nOperations Bridge Integrations\\n.\\n2\\n. \\nClick the setting icon next to the OBM server and select \\nDelete\\n.\\nPost integration tasks\\nContainerized Operations Bridge 2022.11\\nPage \\n566\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '10e724afd6ee7f235d97282de30a98f6'}>,\n",
              "  <Document: {'content': 'Where:\\n<aws_hostname> is the hostname of the AWS instance where OBM is installed\\n<\\nfilename\\n>.crt is the filename of the certificate\\n3\\n. \\nCopy the <\\nfilename\\n>.crt file from OBM to the system where you are launching DCA.\\nExport CA certificate from OBM on-premises\\nFollow these steps to export CA certificate from a containerized OBM:\\n1\\n. \\nLog in to OBM master node as administrator user.\\n2\\n. \\nRun the following command:\\nkubectl -n $(kubectl get ns | grep opsb | cut -d\" \" -f1) get secret nginx-default-secret -o json | jq --raw-output \\'.data.\"tls.crt\"\\' | base64 -d | sed \"1,/END/d\" >> /tmp/<\\n3\\n. \\nCopy the <\\nfilename\\n>.crt file from OBM to the system where you are launching DCA.\\nFollow these steps to export CA certificate from a non-containerized OBM:\\n1\\n. \\nLog into OBM master node as administrator user.\\n2\\n. \\nGo to the folder /opt/HP/BSM/bin. Example: \\ncd /opt/HP/BSM/bin\\n.\\n3\\n. \\nRun the following command:\\n./opr-cert-mgmt.sh -export \"OBM Webserver CA Certificate\" PEM /tmp/<filename>.crt\\n.\\nFor example, OBM 2019.11 and later: \\n./opr-cert-mgmt.sh -export \"OBM Webserver CA Certificate\" PEM /tmp/ca-cert.crt\\n. The\\ncertificate is generated under the /tmp folder.\\nFor example, OBM 2019.05: \\n./opr-cert-mgmt.sh -export \"OMi Webserver CA Certificate\" PEM /tmp/ca-cert.crt\\n. The certificate is\\ngenerated under the /tmp folder.\\n4\\n. \\nCopy the <\\nfilename\\n>.crt file from OBM to the system where you are launching DCA.\\nAdd event sub categories\\nFollow these steps for a containerized OBM:\\n1\\n. \\nIn OBM, go to \\nAdministration\\n > \\nSetup and Maintenance\\n > \\nInfrastructure Settings\\n.\\n2\\n. \\nScroll down and find \\nOPERATIONS MANAGEMENT - Health Indicators For Unresolved And Unassigned Events.\\n3\\n. \\nSet the value of the Event Subcategories to \\nCompliance; Vulnerability.\\nKPI status propagation\\nTo enable status propagation for the compliance and vulnerability KPI, edit the KPI and set the status to default as follows:\\n1\\n. \\nIn OBM, go to \\nAdministration\\n > \\nService health\\n > \\nKPI definition\\n.\\n2\\n. \\nSelect \\nCompliance\\n and click \\nEdit\\n.\\n3\\n. \\nIn the \\nRules\\n > \\nApplication rule\\n option, set the value of \\nWorst Status Rule\\n to \\nDefault\\n.\\nFor OBM 2019.05\\nAfter you install OBM version 2019.05 and DCA, make sure that the following prerequisites are met:\\nAdd event sub-categories\\nFollow these steps for a non-containerized OBM:\\n1\\n. \\nGo to \\nAdministration\\n > \\nSetup and Maintenance\\n > \\nInfrastructure Settings\\n.\\n2\\n. \\nScroll down and find \\nOPERATIONS MANAGEMENT - Health Indicators For Unresolved And Unassigned Events.\\n3\\n. \\nSet the value of the Event Subcategories to \\nCompliance; Vulnerability.\\n4\\n. \\nClick \\nSave\\n.\\nOBM is ready to receive events with subcategory fields set to Compliance or Vulnerability. The following indicators are\\ncreated automatically when you add \\nCompliance\\n and \\nVulnerability\\n in \\nEvent Subcategories\\n:\\nUnassigned most critical events: Compliance\\nUnresolved most critical events: Compliance\\nUnassigned most critical events: Vulnerability\\nUnresolved most critical events: Vulnerability\\nThese indicators can be viewed \\nAdministration\\n > \\nService Health\\n > \\nIndicator Definitions\\n.\\nCreate the compliance and vulnerability KPIs\\nFollow the steps to create compliance and vulnerability KPIs in non-containerized OBM version 2019.05:\\n1\\n. \\nGo to \\nAdministration\\n > \\nService Health\\n > \\nKPI Definitions\\n.\\n2\\n. \\nClick \\n \\nNew.\\n3\\n. \\nSet the display name as \\nCompliance\\n or \\nVulnerability\\n.\\n4\\n. \\nSet Domain as \\nSystem\\n.\\n5\\n. \\nSelect the RULE NAME as \\n \\nWorst Status Rule\\n.\\n6\\n. \\nSet the value of \\nTrend declines if\\n to \\nValue decreases\\n.\\n7\\n. \\nSet Status presentation as \\nStatus Icon\\n.\\n8\\n. \\nSelect default values for other fields.\\n9\\n. \\nClick \\nCreate\\n.\\nUpdate the KPI assignment\\nTo update the compliance and vulnerability KPIs in non-containerized OBM version 2019.05:\\nContainerized Operations Bridge 2022.11\\nPage \\n565\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b49bf4ff2a197eb2959841b3e92b2538'}>,\n",
              "  <Document: {'content': \"Integrate DCA with Operations Bridge Manager\\nThis topic provides information for integrating Data Center Automation (DCA) with Operations Bridge Manager (OBM), one of\\nthe core components of Operations Bridge.\\nYou can integrate DCA 2022.05 with OBM 2022.05 (containerized and non containerized) when both products are deployed on-\\npremises.\\nYou can integrate DCA 2022.05 with OBM 2022.05 (containerized) when both products are deployed on AWS.\\nYou can integrate DCA 2022.05 with OBM 2022.05 (non-containerized) when both products are deployed on Red Hat OpenShift\\n(on-premises).\\nBenefits of integrating DCA and OBM\\nThe integration of DCA with OBM adds the following capability to OBM:\\nThe compliance and vulnerability status from DCA are sent as events to OBM. At OBM, these events set KPIs on the specific\\ninstances to which the events belong. This enables you to view the KPI status of the node on the OBM health dashboard and\\nevents in the event browser.\\nIntegration architecture\\n1\\n. \\nThe compliance and vulnerability scan policies of DCA generate the status events.\\n2\\n. \\nDCA Event Services, post the events securely to the OBM event rest interface.\\n3\\n. \\nData Processing Service on OBM sets the compliance and vulnerability KPIs based in the events received and the status is\\nreflected in the dashboards.\\n4\\n. \\nClick the URL attribute (that is a part of the event) to view the same event on the DCA console and get additional details\\nabout the event or resolve the event.\\nIntegration prerequisites\\nFor DCA\\nTo have vulnerability events shown with their proper status in OBM, you must first import the patch metadata in DCA. For\\nmore details, see \\nImport patch metadata\\n.\\nFor OBM 2019.11 and later versions\\nThe KPIs, KPI assignments, and indicators required for the integration are bundled with OBM and are installed along with OBM.\\nAfter you install OBM and DCA, make sure that the following prerequisites are met:\\nExport CA certificate from OBM on AWS\\nFollow these steps to export CA certificate from a containerized OBM installed on AWS:\\n1\\n. \\nLog in to the bastion node where OBM is installed.\\n2\\n. \\nRun the following command:\\nHOSTNAME=<aws_hostname>\\nopenssl s_client -showcerts -servername ${HOSTNAME} -connect ${HOSTNAME}:443 2>/dev/null | sed -n '/-----BEGIN CERTIFICATE-----/,/-----END CERTIFICATE-----/p' > /tmp/<\\nContainerized Operations Bridge 2022.11\\nPage \\n564\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2a9bc8076449b60566cbf4cab79cee5'}>,\n",
              "  <Document: {'content': 'Oracle Enterprise Linux\\nServer 6\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nOracle Enterprise Linux 6\\nX86_64\\nOracle Enterprise Linux\\nServer 7\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nOracle Enterprise Linux 7\\nX86_64\\nOracle Solaris 11\\nhost_node\\nMANAGED\\nunix\\nSUNOS\\nSunOS 5.11\\nOracle Solaris 11 SPARC\\nhost_node\\nMANAGED\\nunix\\nSUNOS\\nSunOS 5.11\\nWindows 7\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows 7 X86_64\\nWindows 8.1\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows 8.1 X86_64\\nWindows Server 2012\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows Server 2012 x64\\nWindows Server 2012 R2\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows Server 2012 R2 x64\\nWindows Server 2016\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows Server 2016 x64\\nWindows Server 2019\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows Server 2019 x64\\nUnprovisioned server\\nhost_node\\nUNPROVISIONED\\nunix\\nLINUX\\nLinux Service OS\\nServer\\ntype\\nhost_servertype\\nos_family\\nextended_os_family\\nos_description\\nImport resources using the DCA console\\nTo import OBM nodes as resources in DCA, complete the following steps:\\n1\\n. \\nFrom the DCA console, go to the Resource List page (\\nResource Management\\n >\\nResources\\n).\\n2\\n. \\nClick \\n \\n. The Import page appears.\\n3\\n. \\nClick \\n \\nnext to \\nFile\\n.\\n4\\n. \\nIn the \\nOpen\\n dialog box, go to the location where you saved the CSV file.\\n5\\n. \\nSelect the CSV file, and then click \\nOpen\\n.\\n6\\n. \\nClick \\nOK\\n. The resources are imported into DCA (it can take up to a few seconds for the newly imported resources to be\\nindexed for search).\\nAfter the import is successful, you can view these imported resources on the Resources page (\\nResource Management\\n >\\nResources\\n).\\nAdd DCA resources as monitored nodes in OBM\\nFollow the steps:\\n1\\n. \\nGo to \\nAdministration > Setup and Maintenance > Monitored node\\n.\\n2\\n. \\nIn the Node Views pane, expand \\nPredefined Node Filters\\n and click \\nMonitored Nodes\\n. All monitored nodes are listed.\\n3\\n. \\nClick \\n \\n.\\n4\\n. \\nSelect \\nComputer\\n and then select the node type. Create New Monitored Nodes window opens.\\n5\\n. \\nIn the \\nPrimary DNS Name\\n box, add the Fully Qualified Domain Name (FQDN) of the node.\\n6\\n. \\nIn the \\nIP Addresses\\n box, click a row in the IP Address column. The IP address of the node is displayed automatically.\\n7\\n. \\nClick \\nOk.\\n The node is added to the node list.\\nCreate a resource group and associate resources and policies in DCA\\nFollow the steps:\\n1\\n. \\nOn the DCA console, go to the Resource Group List page (\\nResource Management \\n> \\nResource Groups\\n).\\n2\\n. \\nClick \\n \\n.\\n3\\n. \\nType the name and add a description for the resource group.\\n4\\n. \\nOptional\\n. Associate a parent resource group, this enables you to set an existing resource group as the parent group of this\\nnew resource group.\\n5\\n. \\nClick \\nDone\\n. The Resource Group Details page appears.\\nYou can view the child resource group details only when you view a parent resource group.\\n6\\n. \\nClick \\n \\nnext to Resources section to attach resources to the group.\\n7\\n. \\nOn the Resources List page, select the resources and click \\nAdd\\n.\\n8\\n. \\nSet a maintenance schedule for this resource group.\\n9\\n. \\nAttach a policy to the resource group.\\nBased on the scheduled time, the compliance scan is run.\\nPerform compliance scan on resources in DCA\\nOnly the on-demand and scheduled scans trigger events in OBM, the test scans do not trigger any events.\\nContainerized Operations Bridge 2022.11\\nPage \\n569\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2d7e1bfac9fddb7c880fcaca4ac8f46f'}>,\n",
              "  <Document: {'content': 'You can perform scan on resources in the following ways:\\nOn demand scans\\nScheduled scans\\nOn demand scans\\nUse this option to scan resources at any time you want. You can either scan resources individually or in bulk (resources or\\nresource groups). Follow the steps to scan resources individually:\\n1\\n. \\nOn the DCA console, go to \\nResource Management \\n> \\nResources\\n. All available resources are listed.\\n2\\n. \\nSelect a resource to display the Resource Details page.\\n3\\n. \\nClick \\nAction\\n and then click \\nScan\\n. The Scan page opens.\\n4\\n. \\nSelect a policy.\\n5\\n. \\nClick \\nScan\\n.\\nFollow the steps to scan resources in bulk:\\n1\\n. \\nOn the DCA console, go to \\nResource Management \\n> \\nResources\\n. All available resources are listed.\\n2\\n. \\nClick \\nAction\\n and then click \\nScan\\n. The Scan page opens.\\n3\\n. \\nSelect a policy. The Resource selection page opens.\\n4\\n. \\nClick \\nScan\\n.\\nThe status of the scan is displayed in the Jobs page and the result of the scan is displayed in the Resource list page.\\nFollow the steps to scan resource group:\\n1\\n. \\nOn the DCA console, go to \\nResource Management \\n> \\nResources Groups\\n. All available resource groups are listed.\\n2\\n. \\nClick a resource a group.\\n3\\n. \\nClick \\nAction\\n and then click \\nScan\\n. The Scan page opens.\\n4\\n. \\nSelect a policy. The Resource selection page opens.\\n5\\n. \\nClick \\nScan\\n.\\nThe status of the scan is displayed in the Jobs page and the result of the scan is displayed in the Resource list page.\\nSchedule the scan\\nUse this option to schedule the scan. You can schedule a scan only for resource groups.\\n1\\n. \\nOn the DCA console, go to \\nResource Management \\n> \\nResources Groups\\n. All available resources are listed.\\n2\\n. \\nClick a resource a group.\\n3\\n. \\nClick \\nMaintenance Schedules\\n.\\n4\\n. \\nThe ADD MAINTENANCE SCHEDULE pane opens. Enter details in the pane.\\nThis schedules jobs for all the resources associated with the policies in the resource group.\\nValidate DCA - OBM integration\\nTo validate the DCA - OBM integration, make sure:\\n1\\n. \\nCompliance and vulnerability scan status for a particular resource type in DCA,  appears as events that reconcile to a\\ncorresponding monitored node in the OBM event browser.\\n2\\n. \\nThe OBM dashboard (For example: 360\\n0\\n view) displays compliance and vulnerability KPIs for the corresponding monitored\\nnode.\\n3\\n. \\nYou are able to cross-launch from to DCA by clicking the URL specified in the custom message attribute (\\ndca_url\\n). \\nView events in OBM event browser\\nCompliance and vulnerability events with the same status are sent just once (without any duplicates) to OBM. However, an\\nevent is sent to OBM every time a scan fails; even if the scan had failed previously.\\nTo view the events:\\n1\\n. \\nLog into OBM.\\n2\\n. \\nGo to \\nWorkspaces\\n and click \\nEvent perspective\\n.\\n3\\n. \\nUnder \\nBrowse Views\\n tab, select the required view from the drop-down.\\nView KPIs in health dashboard\\nTo view the KPIs:\\n1\\n. \\nLog into OBM.\\n2\\n. \\nGo to \\nWorkspaces\\n > \\nDashboards\\n and click a dashboard. For example: 360\\n0\\n view.\\n3\\n. \\nSelect the node to view the KPIs.\\nContainerized Operations Bridge 2022.11\\nPage \\n570\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '892822c3f317539fbb99fddcee19ef5a'}>,\n",
              "  <Document: {'content': 'To view the DCA events in OBM event browser, make sure that the target resources or nodes are available both in DCA and\\nOBM, and they have the same FQDN. If the target resources or nodes in DCA are not monitored by OBM, events do not\\nreconcile to any node and hence no KPI is set.\\nTo sync the nodes between DCA and OBM, you can import OBM nodes as resources in DCA or add DCA resources as monitored\\nnodes in OBM.\\nSync DCA and OBM nodes\\nTo sync DCA and OBM nodes (or resources) you can either import OBM nodes as resources in DCA or add DCA resources as\\nmonitored nodes in OBM.\\nImport OBM nodes as resources in DCA\\nFollow the steps:\\n1\\n. \\nCreate credentials.\\n2\\n. \\nCreate a CSV file.\\n3\\n. \\nImport resources using the DCA console.\\nCreate a credential\\nFollow these steps:\\n1\\n. \\nFrom the DCA console, go to the Credentials List page (\\nSettings\\n > \\nCredentials\\n).\\n2\\n. \\nClick \\n \\n.\\n3\\n. \\nThe Add Credential area appears. You can add a credential in one of the following ways:\\n1\\n. \\nUsername Password Based Credential\\nProvide values for the following fields:\\nName\\n: Enter a name for the credential. This is a string used to identify the credential.\\nUsername\\n: Enter the user name that will be used by DCA to login on the resource.\\nPassword\\n: Enter a password for the user.\\nConfirm Password\\n: Re-enter the password.\\n2\\n. \\nKey Pair Based Credential\\nYou can generate a private key using RSA or Prime256v1 ECDSA.\\nPrerequisites:\\n1\\n. \\nGenerate a key pair.\\nExample\\nFor RSA:\\nopenssl genrsa -out key1.pem\\nThis command generates a RSA private key file \\nkey1.pem\\nFor Prime256v1 ECDSA:\\nopenssl ecparam -name prime256v1 -out prime256v1.pem\\nopenssl ecparam -in prime256v1.pem -genkey -noout -out key1.pem\\nThis command generates a Prime256v1 ECDSA private key file \\nkey1.pem\\n2\\n. \\nTo extract open-ssh compatible public key from the private key, run the following command:\\nssh-keygen -y -f key1.pem\\n3\\n. \\nCopy the output from the previous command and paste it into the following file of the target server:\\n~/.ssh/authorized_keys\\nIf the path does not exist, create the directory and the file in the given location.\\n \\nKey Pair Based Credential\\nProvide values for the following fields:\\nName\\n: Enter a name for the credential. This is a string used to identify the credential.\\nUsername\\n: Enter the user name that will be used by DCA to login on the resource.\\nPrivate key\\n: Click \\n \\nto upload the Private key. This Private key is the \\nkey1.pem\\n generated in the prerequisites.\\nPassphrase\\n: \\n(Optional)\\n Enter a passphrase for the certificate.\\nConfirm Passphrase\\n: \\n(Optional)\\n Re-enter the passphrase.\\n4\\n. \\nClick \\nSave\\n.\\nThe credential that you created is displayed as a tile on the Credentials page. You must add these credentials under the\\ncredential_id\\n attribute in the CSV file.\\nCreate a CSV file\\nFollow the steps:\\n1\\n. \\nIn the header row, type the UCMDB CI type attributes of the resources separated by commas.You can add any number of\\nCI type attributes from UCMDB.\\nNote: Monitored nodes on OBM are referred as resources in DCA. The terms resources and nodes are used\\ninterchangeably to mean the same in this topic.\\nContainerized Operations Bridge 2022.11\\nPage \\n567\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'eea7109238108b57b9eac4b0ef2875b3'}>,\n",
              "  <Document: {'content': \"2\\n. \\nIn the subsequent rows, type the corresponding values of the resources separated by commas. You can skip a value by\\nadding two consecutive commas between the values.\\nCommas are not allowed within the values as they are used to separate two values.\\nIf you want to map credentials to the imported resources, note down the credential IDs from the Credentials page\\n(\\nSettings\\n > \\nCredentials\\n). Type the credential IDs under the \\ncredential_id\\n attribute in the CSV file.\\nTo access the Credentials page, you must associate the Manage Credentials permission set with the Administrator\\nrole.\\nYou can skip credential mapping for select resources by not providing a value for the credential IDs of those\\nresources.\\nCSV file attributes\\nThe following table lists the CSV file headers and the expected values.\\nHeader\\nDescription\\ntype\\nUCMDB Configuration Item\\nname\\nHost name\\nhost_servertype\\nManaged or Unprovisioned\\nos_family\\nWindows or Unix\\nextended_os_family\\nNT, LINUX, or sunOS\\ndisplay_label\\nThe value shown in the UI for a given resource. It can be user defined or the same as the host\\nname.\\nos_description\\nCharacteristics of hardware or device operating system running on a node. It may also include the\\nversion and patch information.\\ndescription\\nUser defined\\ncredential_id\\nRefers to the ID of the credential that you want to associate with the resource. You can see the\\ncredential ID under the credential name on the Credential List page (Settings > Credentials).\\nSample CSV file\\nYou can also find a sample_resources.csv file on the DCA server in the following location:\\n<DCA_ROOT_NFS_PATH>/content/dca/api/resourceDiscovery\\nThe CSV file created for previous DCA releases is not compatible with this version. Remove the column 'dca_resource_type'\\nfrom the CSV file before importing it to DCA 2018.08 and later versions.\\ntype,name,host_servertype,os_family,extended_os_family,display_label,os_description,description,credential_id\\nhost_node,testres.hpeswlab.net,MANAGED,unix,LINUX,testres.hpeswlab.net,Red Hat Enterprise Linux Server 6 X86_64,ItsLinuxOS,89baa815-cf60-40de-a5d6-b7a6fa9206e6\\nhost_node,testres1.hpeswlab.net,MANAGED,unix,LINUX,testres1.hpeswlab.net,Red Hat Enterprise Linux Server 7 X86_64,ItsLinuxOS,89baa815-cf60-40de-a5d6-b7a6fa9206e6\\nhost_node,testres2.hpeswlab.net,UNPROVISIONED,unix,LINUX,testres2.hpeswlab.net,Linux Service OS,ItsLinuxOS,89baa815-cf60-40de-a5d6-b7a6fa9206e6\\nhost_node,testresource1.hpeswlab.net,MANAGED,windows,NT,testresource1.hpeswlab.net,Windows Server 2012 x64,ItsWindowsOS,693414a8-cece-4b1b-b188-56a82e26a3b6\\nhost_node,testresource2.hpeswlab.net,MANAGED,windows,NT,testresource2.hpeswlab.net,Windows Server 2012 R2 x64,ItsWindowsOS,693414a8-cece-4b1b-b188-56a82e26a3b6\\nhost_node,testresource3.hpeswlab.net,MANAGED,unix,SUNOS,testresource3.hpeswlab.net,SunOS 5.11 X86,ItsSolarisOS,693414a8-cece-4b1b-b188-56a82e26a3b6\\nhost_node,testresource4.hpeswlab.net,MANAGED,unix,SUNOS,testresource4.hpeswlab.net,SunOS 5.11 X86,ItsSolarisOS,693414a8-cece-4b1b-b188-56a82e26a3b6\\nResource attributes\\nThe following table lists the mandatory headers and corresponding values for the discovery of various servers:\\nServer\\ntype\\nhost_servertype\\nos_family\\nextended_os_family\\nos_description\\nRed Hat Enterprise Linux\\nServer 5\\nnode\\nMANAGED\\nunix\\nLINUX\\nRed Hat Enterprise Linux\\nServer 5 X86_64\\nRed Hat Enterprise Linux\\nServer 6\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nRed Hat Enterprise Linux\\nServer 6 X86_64\\nRed Hat Enterprise Linux\\nServer 7\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nRed Hat Enterprise Linux\\nServer 7 X86_64\\nRed Hat Enterprise Linux\\nServer 8\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nRed Hat Enterprise Linux\\nServer 8 X86_64\\nCentOS 7\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nCentOS 7 X86_64\\nSUSE Linux Enterprise Server\\n11\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nSuSE Linux Enterprise Server\\n11 X86_64\\nSUSE Linux Enterprise Server\\n12\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nSuSE Linux Enterprise Server\\n12 X86_64\\nSUSE Linux Enterprise Server\\n15\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nSuSE Linux Enterprise Server\\n15 X86_64\\nUbuntu Server 14.04\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nUbuntu Server 14.04 X86_64\\nUbuntu Server 16.04\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nUbuntu Server 16.04 X86_64\\nOracle Enterprise Linux\\nServer 5\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nOracle Enterprise Linux 5\\nX86_64\\nContainerized Operations Bridge 2022.11\\nPage \\n568\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b05b6e50bcf5b8170754b1c64b5eab7f'}>,\n",
              "  <Document: {'content': 'View compliance and vulnerability scan status in OBM\\nThe integration of DCA and OBM enables compliance and vulnerability scan intelligence from DCA to be integrated as event\\ndriven key performance indicators (KPIs) in OBM. The compliance and vulnerability status from DCA are sent as events to OBM.\\nAt OBM, these events set KPIs on the specific instances to which the events belong. This enables you to view the KPI status of\\nthe node on the OBM health dashboard and events in the event browser. Two types of Event Driven KPIs are enabled through\\nthis integration:\\nCompliance status KPI: Compliance status KPI is set when a compliance life cycle event is generated. It indicates the state\\nof an entity with respect to the standard regulations like PCI, HIPAA, FISMA, CIS, and ISO.\\nVulnerability status KPI: Vulnerability status KPI is set when a system is not compliant and thereby causing a potential\\nvulnerability (as published by National Vulnerability Database).\\nThese events are displayed on the OBM event browser.\\nThe indicators, KPIs and KPI assignments that are required for the Operations Bridge and DCA integration are included in the\\n\\'OMi Content Pack\\', which is shipped as a part of OBM and is installed by default. \\nProblem statement\\nHow do I view performance, availability, compliance, and vulnerability status in a single dashboard?\\nWorkflow\\nRole\\nLocation\\nPrivileges required\\nroot\\nDCA\\nAdministrator\\nTo view compliance and vulnerability scan status in OBM health dashboards, complete the following tasks:\\n1\\n. \\nExport CA certificate from OBM\\n2\\n. \\nIntegrate OBM and DCA\\n3\\n. \\nImport OBM nodes as resources in DCA\\n4\\n. \\nCreate a resource group and associate policies and resources\\n5\\n. \\nPerform compliance scan on resources\\n6\\n. \\nView events in OBM events browser\\n7\\n. \\nView events in KPI health dashboard\\nExport CA certificate from OBM\\nTo export CA certificate from a containerized OBM:\\n1\\n. \\nLog in to OBM master node as administrator user.\\n2\\n. \\nRun the following command:\\nkubectl  -n $(kubectl get ns | grep opsb | cut -d\" \" -f1) get secret nginx-default-secret -o json | jq --raw-output \\'.data.\"tls.crt\"\\' | base64 -d | sed \"1,/END/d\"  >> /tmp/<\\n3\\n. \\nCopy the <\\nfilename\\n>.crt file from OBM to the system where you are launching DCA.\\nIntegrate OBM and DCA\\nTo integrate OBM with DCA, complete the following steps:\\n1\\n. \\nFrom the DCA console, go to \\nSETTINGS\\n > \\nIntegrations\\n >\\nOperations Bridge Integrations\\n.\\n2\\n. \\nClick \\n \\nnext to \\nOperations Bridge Integrations\\n.\\n3\\n. \\nIn the \\nOperations Bridge Configuration\\n area, specify the following information:\\nName: This can be a combination of alphanumeric characters and special symbols with a maximum length of 100\\ncharacters.\\nFully Qualified Host Name: Fully qualified hos name of the OBM server.\\nPort: This can be any positive integer between 0 and 65535.\\nUsername: Username of the OBM server with root access. Ensure this Username is existing in the OBM server before\\nintegrating with DCA.\\nPassword: Password of the OBM user.\\nTrusted Certificate: Select the trusted certificate file (\\ncacert.pem\\n) on your local system.\\n4\\n. \\nClick \\nOK\\n. OBM server with hostname specified is listed in the \\nOperations Bridge Integrations\\n page.\\nNote:\\n Monitored nodes on OBM is referred as resources in DCA. The terms resources and nodes are used\\ninterchangeably to mean the same in this topic.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n572\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '45729cfa426dcfec869e7bcf44188ebe'}>,\n",
              "  <Document: {'content': \"os_family\\nWindows or Unix\\nextended_os_family\\nNT, LINUX, or sunOS\\ndisplay_label\\nThe value shown in the UI for a given resource. It can be user defined or the same as the host\\nname.\\nos_description\\nCharacteristics of hardware or device operating system running on a node. It may also include the\\nversion and patch information.\\ndescription\\nUser defined\\ncredential_id\\nRefers to the ID of the credential that you want to associate with the resource. You can see the\\ncredential ID under the credential name on the Credential List page (Settings > Credentials).\\nHeader\\nDescription\\nSample CSV file\\nThe CSV file created for previous DCA releases is not compatible with this version. Remove the column 'dca_resource_type'\\nfrom the CSV file before importing it to DCA 2018.08 and later versions.\\ntype,name,host_servertype,os_family,extended_os_family,display_label,os_description,description,credential_id\\nhost_node,testres.hpeswlab.net,MANAGED,unix,LINUX,testres.hpeswlab.net,Red Hat Enterprise Linux Server 6 X86_64,ItsLinuxOS,89baa815-cf60-40de-a5d6-b7a6fa9206e6\\nhost_node,testres1.hpeswlab.net,MANAGED,unix,LINUX,testres1.hpeswlab.net,Red Hat Enterprise Linux Server 7 X86_64,ItsLinuxOS,89baa815-cf60-40de-a5d6-b7a6fa9206e6\\nhost_node,testres2.hpeswlab.net,UNPROVISIONED,unix,LINUX,testres2.hpeswlab.net,Linux Service OS,ItsLinuxOS,89baa815-cf60-40de-a5d6-b7a6fa9206e6\\nhost_node,testresource1.hpeswlab.net,MANAGED,windows,NT,testresource1.hpeswlab.net,Windows Server 2012 x64,ItsWindowsOS,693414a8-cece-4b1b-b188-56a82e26a3b6\\nhost_node,testresource2.hpeswlab.net,MANAGED,windows,NT,testresource2.hpeswlab.net,Windows Server 2012 R2 x64,ItsWindowsOS,693414a8-cece-4b1b-b188-56a82e26a3b6\\nhost_node,testresource3.hpeswlab.net,MANAGED,unix,SUNOS,testresource3.hpeswlab.net,SunOS 5.11 X86,ItsSolarisOS,693414a8-cece-4b1b-b188-56a82e26a3b6\\nhost_node,testresource4.hpeswlab.net,MANAGED,unix,SUNOS,testresource4.hpeswlab.net,SunOS 5.11 X86,ItsSolarisOS,693414a8-cece-4b1b-b188-56a82e26a3b6\\nYou can also find a sample_resources.csv file on the DCA server in the following location:\\n<DCA_ROOT_NFS_PATH>/content/dca/api/resourceDiscovery\\nResource attributes\\nThe following table lists the mandatory headers and corresponding values for the discovery of various servers:\\nServer\\ntype\\nhost_servertype\\nos_family\\nextended_os_family\\nos_description\\nRed Hat Enterprise Linux\\nServer 5\\nnode\\nMANAGED\\nunix\\nLINUX\\nRed Hat Enterprise Linux\\nServer 5 X86_64\\nRed Hat Enterprise Linux\\nServer 6\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nRed Hat Enterprise Linux\\nServer 6 X86_64\\nRed Hat Enterprise Linux\\nServer 7\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nRed Hat Enterprise Linux\\nServer 7 X86_64\\nRed Hat Enterprise Linux\\nServer 8\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nRed Hat Enterprise Linux\\nServer 8 X86_64\\nCentOS 7\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nCentOS 7 X86_64\\nSUSE Linux Enterprise Server\\n11\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nSuSE Linux Enterprise Server\\n11 X86_64\\nSUSE Linux Enterprise Server\\n12\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nSuSE Linux Enterprise Server\\n12 X86_64\\nSUSE Linux Enterprise Server\\n15\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nSuSE Linux Enterprise Server\\n15 X86_64\\nUbuntu Server 14.04\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nUbuntu Server 14.04 X86_64\\nUbuntu Server 16.04\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nUbuntu Server 16.04 X86_64\\nOracle Linux Server 5\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nOracle Linux\\n 5 X86_64\\nOracle Linux Server 6\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nOracle Linux\\n 6 X86_64\\nOracle Linux Server 7\\nhost_node\\nMANAGED\\nunix\\nLINUX\\nOracle Linux\\n 7 X86_64\\nOracle Solaris 11\\nhost_node\\nMANAGED\\nunix\\nSUNOS\\nSunOS 5.11\\nOracle Solaris 11 SPARC\\nhost_node\\nMANAGED\\nunix\\nSUNOS\\nSunOS 5.11\\nWindows 7\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows 7 X86_64\\nWindows 8.1\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows 8.1 \\nX86_64\\nWindows Server 2012\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows Server 2012 x64\\nWindows Server 2012 R2\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows Server 2012 R2 x64\\nWindows Server 2016\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows Server 2016 x64\\nWindows Server 2019\\nhost_node\\nMANAGED\\nwindows\\nNT\\nWindows Server 2019 x64\\nUnprovisioned server\\nhost_node\\nUNPROVISIONED\\nunix\\nLINUX\\nLinux Service OS\\nImport resources using the DCA console\\nTo import OBM nodes as resources in DCA, complete the following steps:\\n1\\n. \\nFrom the DCA console, go to the Resource List page (\\nResource Management\\n >\\nResources\\n).\\n2\\n. \\nClick \\n \\n. The Import page appears.\\n3\\n. \\nClick \\n \\nnext to \\nFile\\n.\\n4\\n. \\nIn the \\nOpen\\n dialog box, browse to the location where you saved the CSV file.\\nContainerized Operations Bridge 2022.11\\nPage \\n574\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ff78a4fe5617776220126de7ae470cef'}>,\n",
              "  <Document: {'content': 'Import OBM nodes as resources in DCA\\nFor importing OBM nodes as resources through DCA, follow these steps:\\n1\\n. \\nCreate credentials.\\n2\\n. \\nCreate a CSV file.\\n3\\n. \\nImport resources using the DCA console.\\nCreate a credential\\nBefore you create a CSV file, you must first create credentials as these credentials need to be added under\\nthe \\ncredential_id\\n attribute in the CSV file. To add a credential, follow these steps:\\n1\\n. \\nFrom the DCA console, go to the Credentials List page (\\nSettings\\n > \\nCredentials\\n).\\n2\\n. \\nClick \\n \\n.\\n3\\n. \\nThe Add Credential area appears. You can add a credential in one of the following ways:\\n1\\n. \\nUsername Password Based Credential\\nProvide values for the following fields:\\nName\\n: Enter a name for the credential. This is a string used to identify the credential.\\nUsername\\n: Enter the user name that will be used by DCA to login on the resource.\\nPassword\\n: Enter a password for the user.\\nConfirm Password\\n: Re-enter the password.\\n2\\n. \\nKey Pair Based Credential\\nYou can generate a private key using RSA or Prime256v1 ECDSA.\\nPrerequisites:\\n1\\n. \\nGenerate a key pair.\\nExample\\nFor RSA:\\nopenssl genrsa -out key1.pem\\nThis command generates a RSA private key file \\nkey1.pem\\nFor Prime256v1 ECDSA:\\nopenssl ecparam -name prime256v1 -out prime256v1.pem\\nopenssl ecparam -in prime256v1.pem -genkey -noout -out key1.pem\\nThis command generates a Prime256v1 ECDSA private key file \\nkey1.pem\\n2\\n. \\nTo extract open-ssh compatible public key from the private key, run the following command:\\nssh-keygen -y -f key1.pem\\n3\\n. \\nCopy the output from the previous command and paste it into the following file of the target server:\\n~/.ssh/authorized_keys\\nIf the path does not exist, create the directory and the file in the given location.\\n \\nKey Pair Based Credential\\nProvide values for the following fields:\\nName\\n: Enter a name for the credential. This is a string used to identify the credential.\\nUsername\\n: Enter the user name that will be used by DCA to login on the resource.\\nPrivate key\\n: Click \\n \\nto upload the Private key. This Private key is the \\nkey1.pem\\n generated in the prerequisites.\\nPassphrase\\n: \\n(Optional)\\n Enter a passphrase for the certificate.\\nConfirm Passphrase\\n: \\n(Optional)\\n Re-enter the passphrase.\\n4\\n. \\nClick \\nSave\\n.\\nThe credential that you created is displayed as a tile on the Credentials page.\\nCreate a CSV file\\nBefore you create a CSV file, you must first\\n \\ncreate credentials\\n \\nas these credentials need to be added under the \\ncre\\ndential_id\\n attribute in the CSV file\\n.\\n1\\n. \\nIn the header row, type the UCMDB CI type attributes of the resources separated by commas.You can add any number of\\nCI type attributes from UCMDB.\\n2\\n. \\nIn the subsequent rows, type the corresponding values of the resources separated by commas. You can skip a value by\\nadding two consecutive commas between the values.\\nCommas are not allowed within the values as they are used to separate two values.\\nIf you want to map credentials to the imported resources, note down the credential IDs from the Credentials page\\n(\\nSettings\\n > \\nCredentials\\n). Type the credential IDs under the \\ncredential_id\\n attribute in the CSV file.\\nTo access the Cedentials page, you must associate the Manage Credentials permission set with the Administrator\\nrole.\\nYou can skip credential mapping for select resources by not providing a value for the credential IDs of those\\nresources.\\nCSV file attributes\\nThe following table lists the CSV file headers and the expected values.\\nHeader\\nDescription\\ntype\\nUCMDB Configuration Item\\nname\\nHost name\\nhost_servertype\\nManaged or Unprovisioned\\nContainerized Operations Bridge 2022.11\\nPage \\n573\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '42b6b4f47917886ceabe1e8c9d6592a7'}>,\n",
              "  <Document: {'content': 'Related topics\\nFor information on features of DCA, see \\nDCA get started\\nFor information on features of OBM, see \\nOBM get started\\nFor information about how to troubleshoot integrations, see \\nTroubleshoot integrations\\n.\\nFor information about integration log files, see \\nHow to find the log files\\n.\\nFor more information on how to import patch metadata, see \\nImport patch metadata\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n571\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '426ac037784d91409d825c29c39b7e87'}>,\n",
              "  <Document: {'content': 'Integrate OBR with OBM\\nThis section provides step-by-step instructions to perform on OBR and OBM systems to integrate OBR with OBM container and\\nview OBR reports on the OBM Dashboard user interface. You can launch OBR reports in the context of a Configuration Item (CI)\\nor Business View from the OBM Dashboard user interface. Integrating OBR with OBM Dashboard enriches the component\\ngallery and provides a convenient way to view all the OBM and OBR reports in one place, without launching OBR. Follow these\\nsteps to integrate OBR with OBM container and view OBR reports on OBM Dashboard:\\n1\\n. \\nEnable Global ID on OBM System\\n2\\n. \\nCreate a User in OBM\\n3\\n. \\nCreate a User in OBR and configure preferences\\n4\\n. \\nConfigure OBM/OBR LW-SSO Authentication\\n5\\n. \\nConfigure OBR FQDN and OBM FQDN in OBR\\n6\\n. \\nConfigure SAP BusinessObjects Trusted Authentication\\n7\\n. \\nDisable Clickjacking\\n8\\n. \\nGenerate the Report Component XML in OBR\\n9\\n. \\nLoad the report component to OBM Dashboard\\n10\\n. \\nCreate OBM Dashboard Page and Add the Report Component\\nStep 1: Enable Global ID on OBM container\\nFollow these steps to enable global ID on OBM:\\n1\\n. \\nOn your OBM system, change the Global ID Generator settings using the following link:\\nhttps://<OBM external access host>/jmx-console/\\nThe UCMDB search field appears.\\n2\\n. \\nType \\nUCMDB;service=Multiple CMDB Instances Services\\n in search and select the same from the search drop-down\\nlist.\\nThe UCMDB:service=Multiple CMDB Instances Services page appears.\\n3\\n. \\nClick \\nsetAsGlobalIdGenerator\\n.\\nFigure 1.1 (a) SetAsGlobalIdGenerator\\n4\\n. \\nType \\n1\\n as the value for \\ncustomerID\\n and \\ndbTimeout\\n .\\nFigure 1.1 (b) SetAsGlobalIdGenerator\\n5\\n. \\nClick \\nInvoke\\n.\\nFigure 1.1 (c) SetAsGlobalIdGenerator\\nOBM is set as the Global ID Generator.\\nStep 2: Create a User in OBM\\nCreate a user account in OBM with permissions to create and view pages in OBM Dashboard. The same OBM username needs\\nto be created as a user in OBR with permission to view OBR reports.\\nIn this document, an existing OBM user account \\nadmin\\n is used as an example user.\\nStep 3: Create a User in OBR and Configure Preferences\\nOBR uses SAP BusinessObjects for user management. To create a user in OBR, perform the following steps:\\nContainerized Operations Bridge 2022.11\\nPage \\n581\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e8f0c2f641798aae689437b977e43847'}>,\n",
              "  <Document: {'content': 'KPI =\\nCompliance\\nSeverity =\\nNormal \\nDCA status: Compliant\\nEvent description:  Resource is Compliant.\\nKPI =\\nCompliance\\nSeverity =\\nWarning \\nDCA status: Non compliant - within RSLO\\nEvent description:  Resource is Not Compliant, server is Within Remediation Service Level Objective.\\nKPI =\\nCompliance\\nSeverity =\\nCritical \\nDCA status: Non compliant out of RSLO\\nEvent description:  Resource is Not Compliant, server is Out of Remediation Service Level Objective.\\nOBM events\\nDCA events\\nVulnerability KPI\\nOBM events\\nDCA events\\nKPI = none\\nSeverity =\\nCritical \\nDCA status: Failed\\nEvent description:  There was an error running the <policy_name> policy on the resource. For more\\ninformation, check the job details.\\nA critical exception event will be generated each time DCA fails to measure compliance/vulnerability for\\nvarious reasons, like incorrect credentials, system error, etc.\\nKPI =\\nVulnerability \\nSeverity =\\nNormal \\nDCA status: Non vulnerable\\nEvent description:  Resource is Not Vulnerable with no security threats.\\nKPI =\\nVulnerability\\nSeverity =\\nWarning \\nDCA status: Vulnerable - low risk\\nEvent description:  Resource is Vulnerable with low risk security threats.\\nKPI =\\nVulnerability\\nSeverity =\\nMajor \\nDCA status: Vulnerable - medium risk\\nEvent description:  Resource is Vulnerable with medium risk security threats.\\nKPI =\\nVulnerability\\nSeverity =\\nCritical \\nVulnerable - high risk\\nEvent description:  Resource is Vulnerable with high risk security threats.\\nView KPIs in health dashboard\\nFollow the steps to view the events:\\n1\\n. \\nLog on to OBM.\\n2\\n. \\nGo to \\nWorkspaces\\n > \\nDashboards\\n and click a dashboard.  \\n3\\n. \\nSelect the node to view the events.\\nRelated topics\\nFor information on how to import resources in DCA, see \\nDiscover resources\\nFor information on how to perform compliance scans, see \\nCompliance scans\\nFor information on how to create and edit resource groups, see \\nCreate resource groups\\nContainerized Operations Bridge 2022.11\\nPage \\n577\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '37ff8e7e026609241d3667a936773f2f'}>,\n",
              "  <Document: {'content': 'If you are an LDAP user, do not perform Steps 1 to 6. Perform only step 7. For more information, see the \\nConfigure LDAP\\nAuthentication for OBR\\n topic in the OBR documentation.\\n1\\n. \\nLog on to \\nSAP BusinessObjects Central Management Console (CMC)\\n using the following link as an administrator:\\nhttp://<System_FQDN>:8443/BOE/CMC\\nwhere \\n<System_FQDN>\\n is the fully qualified domain name of the system where SAP BusinessObjects is installed.\\nFigure 3.1: Log on screen of SAP BusinessObjects Central Management Console\\n2\\n. \\nSelect \\nUsers and Groups\\n from the drop-down box.\\nFigure 3.2: CMC Users and Groups screen\\n3\\n. \\nSelect the \\nUser List\\n and click \\nCreate New User\\n icon as shown in figure 1.3.\\nFigure 3.3: Creating a new user\\nContainerized Operations Bridge 2022.11\\nPage \\n582\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e3d8c1e49a2fb3216728bb1fd4ea8490'}>,\n",
              "  <Document: {'content': 'RTSM\\nThis page is still under development. No published version is available at this time.\\nContainerized Operations Bridge 2022.11\\nPage \\n579\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '855d76e36707def7c23fb6abf4f7cf20'}>,\n",
              "  <Document: {'content': '4\\n. \\nEnter the user details in the \\nNew User\\n window as shown in figure 1.4 (a).\\nThe SAP BusinessObjects username must be the same as the Account Name in OBM.\\n1\\n. \\nCheck \\nPassword never expires\\n under Enterprise Password Settings.\\n2\\n. \\nClick \\nCreate & Close\\n.\\nFigure 3.4(a): Create New User Screen\\nThe newly created user appears in the \\nUser List\\n as shown in the following figure:\\n5\\n. \\nTo add the OBR user to the Administrator group, perform the following steps:\\n1\\n. \\nSelect the user you created and click the \\nAdd a member to a user group\\n icon as shown below.\\nFigure 3.5(a): Add a member\\n2\\n. \\nTo move Administrators from \\nAvailable Groups\\n to \\nDestination Group(s)\\n, select \\nAdministrators\\n, click \\n>\\n, then\\nclick \\nOK\\n as shown in figure 2.5 (b).\\nFigure 3.5(c): Join Group\\nContainerized Operations Bridge 2022.11\\nPage \\n583\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '838035258b03959924df1fdf3cd97a3'}>,\n",
              "  <Document: {'content': 'Integrate Service Manager with OBM\\nYou can integrate Service Manager (SM) with Operations Bridge Manager (OBM) for the following capabilities:\\nForward OBM events and their updates automatically or manually to Service Manager as an incident.\\nView the events that are forwarded, including detailed information about the corresponding Service Manager incident on\\nOBM Event Browser.\\nLaunch extended Incident Details view from the event record.\\nLaunch extended Event Details from the incident record.\\nYou can use any one of the following Global ID Generators:\\nRTSM\\nUCMDB\\nContainerized Operations Bridge 2022.11\\nPage \\n578\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a03d8cc13be2e69ee4489b40bc701b0b'}>,\n",
              "  <Document: {'content': '6\\n. \\nTo verify User and Group configuration, perform the following steps:\\n1\\n. \\nDouble-click \\nadmin\\n, the user you created from the list of users.\\n2\\n. \\nSelect \\nMember Of\\n and check if \\nAdministrators\\n is listed on the right side as shown in figure 2.6.\\nFigure 3.6: Member of a group\\n7\\n. \\nTo ensure the proper functioning of the Drill Up/Drill Down functionality in reports while accessing them from the OBM\\nDashboard console, you must set the user preferences as follows:\\n1\\n. \\nLog on to SAP BusinessObjects BI Launch pad with the user credentials created in CMC from the following link:\\nhttps://<Host_Name>:8443/BOE/BI\\nwhere \\n<Host_Name>\\n is the name of the server on which SAP BusinessObjects is installed.\\nWhile logging on to the SAP BusinessObjects BI Launch pad for the first time, make sure to change the password.\\n2\\n. \\nClick \\nPreferences\\n in the upper right corner as shown in figure 1.7 (a).\\nFigure 3.7(a): Preferences\\n3\\n. \\nIn the \\nGeneral\\n tab, ensure that the default preferences are selected.\\nFigure 3.7(b): Preferences\\nContainerized Operations Bridge 2022.11\\nPage \\n584\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c21ad16592bd8df7466f78eba3b1c74e'}>,\n",
              "  <Document: {'content': 'UCMDB\\nThis page is still under development. No published version is available at this time.\\nContainerized Operations Bridge 2022.11\\nPage \\n580\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '80eafa86be49f4670e903194b7f9ea1a'}>,\n",
              "  <Document: {'content': 'Cross launch to view DCA scan job details is supported through the drill down URL custom message attribute \\ndca_url\\n in the\\nOBM events browser. To launch DCA, go to the event details and click the URL provided. The URL includes protocol, FQDN of\\nthe computer that hosts DCA, the communication port, and the root URL path: \\n<protocol>://<host>:\\n<port>/<prefix>/<target URL>\\nFollow the steps:\\n1\\n. \\nOn OBM, go to \\nWorkspaces\\n >\\nOperations Console\\n.\\n2\\n. \\nIn the Event Perspective, click the arrow and then select a node. All events are listed.\\n3\\n. \\nSelect an event, go to the \\nCustom Attributes\\n tab and then click the URL. The Job Details or Resource Details page\\nappears on DCA, depending on the event type.\\nEvent fields\\nThe mandatory fields for an event are \\nTitle\\n and \\nSeverity\\n. However, for the purpose of event Integration from DCA, it is\\nrecommended to set the below mentioned fields.\\nEvent Field\\nXML Attribute Name\\nData\\nType\\nDescription\\nTitle\\ntitle\\nString\\nLiteral\\nTitle of the event.\\nDescription\\ndescription\\nString\\nLiteral\\nDescription of the event.\\nSeverity\\nseverity\\nString\\nLiteral\\nThe severity of the event:\\nCritical, Major, Minor, Warning, Normal\\nTime Created\\ntime_created\\nDate\\nand\\ntime\\nTime at which the event was observed at source.\\nCategory\\ncategory\\nString\\nLiteral\\nTop level categorization:\\nException, Compliance, Vulnerability\\nSub Category\\nsub_category\\nString\\nLiteral\\nSub level categorization:\\nException, Compliance, Vulnerability\\nETI\\neti_hint\\nString\\nLiteral\\nType of event.\\nFor example: CPULoad:Bottlenecked. If this is used properly, key\\nand close-key-pattern correlation doesn’t need to be used,\\nbecause the default eti based good-bad correlation will cover it.\\nNode\\nnode_hints[node_dns_name]\\nString\\nLiteral\\nPrimary DNS name of the node.\\nRelated CI\\nrelated_ci_hints[hint]\\nString\\nLiteral\\nTo identify the event CI.\\nFor example: <application>@@<node>\\nSource CI\\nsource_ci[target_id]\\nString\\nLiteral\\nThe source CI, which represents DCA in the RTSM.\\nDrilldown URL\\ncustom_attribute_list[<CA\\nname>]\\nString\\nLiteral\\n<protocol>://<host>:<port>/<prefix>/<target URL>\\nPolicies\\ncustom_attribute_list[<CA\\nname>]\\nString\\nLiteral\\nA comma separated list of policy names to which the resource is\\nsubscribed to.\\nFor example: Policy1, Policy2.\\nlog_only\\nkey\\nclose_key_pattern\\nMapping DCA scan status to OBM event attributes\\nCompliance KPI\\nOBM events\\nDCA events\\nKPI = None\\nSeverity =\\nCritical\\nDCA status: Failed\\nEvent description:  There was an error running the <policy_name> policy on the resource. For more\\ninformation, check the job details.\\nA critical exception event will be generated each time DCA fails to measure compliance/vulnerability for\\nvarious reasons, like incorrect credentials, system error, etc.\\nContainerized Operations Bridge 2022.11\\nPage \\n576\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9ad7accea7b5714e1f1243c33a1297d3'}>,\n",
              "  <Document: {'content': '5\\n. \\nSelect the CSV file, and then click \\nOpen\\n.\\n6\\n. \\nClick \\nOK\\n. The resources are imported into DCA (it can take up to a few seconds for the newly imported resources to be\\nindexed for search).\\nAfter the import is successful, you can view these imported resources on the Resources page (\\nResource Management\\n >\\nResources\\n).\\nCreate a resource group and associate policies and resources\\n1\\n. \\nFrom the DCA console, go to the Resource Group List page (\\nResource Management \\n> \\nResource Groups\\n).\\n2\\n. \\nClick \\n \\n.\\n3\\n. \\nType the name and add a description for the resource group.\\n4\\n. \\nOptional\\n. Associate a parent resource group enables you to set an existing resource group as the parent group of this new\\nresource group.\\n5\\n. \\nClick \\nDone\\n. The Resource Group Details page appears.\\nYou can view the child resource group details only when you view a parent resource group.\\n6\\n. \\nClick \\n \\nnext to Resources section to attach resources to the group.\\n7\\n. \\nFrom the Resources List page, select the resources and click \\nAdd\\n.\\n8\\n. \\nSet a maintenance schedule for this resource group.\\n9\\n. \\nAttach a policy to the resource group.\\nBased on the scheduled time, the compliance scan is run.\\nPerform compliance scan on resources\\nOnly the on-demand and scheduled scans trigger events in OBM, the test scans do not trigger any events.\\nYou can perform scan on resources in the following ways:\\non demand scans\\nscheduled scans\\nOn demand scans\\nUse this option to scan resources at any time you want. You can either scan resources individually or in bulk (resources or\\nresource groups). Follow the steps to scan resources individually:\\n1\\n. \\nOn the DCA console, go to \\nResource Management \\n> \\nResources\\n. All available resources are listed.\\n2\\n. \\nSelect a resource to display the Resource Details page.\\n3\\n. \\nClick \\nAction\\n and then click \\nScan\\n. The Scan page opens.\\n4\\n. \\nSelect a policy.\\n5\\n. \\nClick \\nScan\\n.\\nFollow the steps to scan resources in bulk:\\n1\\n. \\nOn the DCA console, go to \\nResource Management \\n> \\nResources\\n. All available resources are listed.\\n2\\n. \\nClick \\nAction\\n and then click \\nScan\\n. The Scan page opens.\\n3\\n. \\nSelect a policy. The Resource selection page opens.\\n4\\n. \\nClick \\nScan\\n.\\nThe status of the scan is displayed in the Jobs page and the result of the scan is displayed in the Resource list page.\\nFollow the steps to scan resource group:\\n1\\n. \\nOn the DCA console, go to \\nResource Management \\n> \\nResources Groups\\n. All available resource groups are listed.\\n2\\n. \\nClick a resource a group.\\n3\\n. \\nClick \\nAction\\n and then click \\nScan\\n. The Scan page opens.\\n4\\n. \\nSelect a policy. The Resource selection page opens.\\n5\\n. \\nClick \\nScan\\n.\\nThe status of the scan is displayed in the Jobs page and the result of the scan is displayed in the Resource list page.\\nSchedule the scan\\nUse this option to schedule the scan. You can schedule a scan only for resource groups.\\n1\\n. \\nOn the DCA console, go to \\nResource Management \\n> \\nResources Groups\\n. All available resources are listed.\\n2\\n. \\nClick a resource a group.\\n3\\n. \\nClick Maintenance Scheduled.\\n4\\n. \\nThe ADD MAINTENANCE SCHEDULE pane opens. Fill all the fields….\\n5\\n. \\nClick \\nAction\\n and then click \\nScan\\n. The Scan page opens.\\n6\\n. \\nSelect a policy. The Resource selection page opens.\\n7\\n. \\nClick \\nScan\\n.\\nView events in OBM event browser\\nFollow the steps to view the events:\\n1\\n. \\nLog on to OBM.\\n2\\n. \\nGo to Workspaces and click Event perspective. Event perspective opens\\n3\\n. \\nUnder Browse Views tab, select the required View from the drop-down.\\nCross launch DCA\\nContainerized Operations Bridge 2022.11\\nPage \\n575\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c1a6dd6f80b3298ac1c235da42930fe3'}>,\n",
              "  <Document: {'content': 'Containerized Operations Bridge 2022.11\\nPage \\n591\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c13eaab8dfe79bcfd5d2557b30c96d6'}>,\n",
              "  <Document: {'content': 'Step 9: To load the report component to OBM Dashboard\\nPerform the following steps to load the report component to OBM Dashboard :\\n1\\n. \\nFrom the OBR system, copy the report component file \\n*.uim.xml\\n file.\\n2\\n. \\nOn the OBM container system, run the following commands to paste the report component file:\\n1\\n. \\nTo get the namespace of the suite:\\nkubectl get ns\\n2\\n. \\nTo get the OBM pod name:\\nkubectl get pods -n <namespace>\\n3\\n. \\nTo paste the file: \\nkubectl cp /opt/<Component XML file>.xml <namespace>/<OBM pod>:/opt/HP/BSM/conf/uimashup/import/toload/Co\\nmponents -c omi\\nFor example, \\nkubectl cp /opt/SM System Inventory.uim.xml opsbridge1/omi-0:/opt/HP/BSM/conf/uimashup/import/toload/Components -c\\nomi\\n4\\n. \\nTo verify the XML, log on to the OBM pod: \\nkubectl exec -it <OBM pod> -n <namespace> -c omi bash\\n and go to the location\\nwhere you have pasted the file.\\n3\\n. \\nLoad the XML (\\n*.uim.xml\\n) file using the opr-jmxClient utility.\\n1\\n. \\nOn the OBM pod, go to the following location for the opr-jmxClient utility:\\ncd /opt/HP/BSM/opr/support\\n2\\n. \\nRun the command: \\n/opr-jmxClient.sh -s localhost:4447 -r -b \"Foundations:service=UIMDataLoader\" -m loadComponentsGallery -a 1 tru\\ne\\nThe component is visible on the OBM system in the following location:\\n/opt/HP/BSM/conf/uimashup/import/toload/Components\\nAfter a successful upload, the component is visible in the following location:\\n/opt/HP/BSM/conf/uimashup/import/loaded/Components\\n4\\n. \\nTo verify the availability of the component in the OBM Dashboard console:\\n1\\n. \\nLog on to the OBM user interface.\\n2\\n. \\nClick \\nWorkspaces > My Workspace > Component Gallery\\n.\\nFigure 9.2: Components Gallery\\nThe component must be available within the category.\\n5\\n. \\nTo verify the wiring, Click Wiring as shown in Figure 4.9.\\nBy default, all reports are wired on CIChange and ViewChange events. If the report does not support any events, clear the\\nContainerized Operations Bridge 2022.11\\nPage \\n589\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '68931565ae4966925e605a7ca29f42ce'}>,\n",
              "  <Document: {'content': '4\\n. \\nClick the \\nWeb Intelligence\\n tab, and select the \\nSynchronize drill on report blocks\\n check-box.\\nFigure 3.7(c): Preferences\\nStep 4: Configure OBM/OBR LW-SSO Authentication\\nUsing Lightweight Single Sign-on (LW-SSO), you can enable an OBM Dashboard user to access OBR reports with the same user\\ncredentials.\\nAs SAP BusinessObjects is a third-party application, Single Sign-on (SSO) cannot be directly achieved with OBM using LW-SSO. \\nFor OBM Dashboard, SSO is set up first between the OBR and OBM using LW-SSO as explained in this section of steps.\\nThen, SSO is set up between the OBR and SAP BusinessObjects using SAP BusinessObjects Trusted Authentication as\\nexplained in \\nStep 6: Configure SAP BusinessObjects Trusted Authentication\\n.\\nBefore setting up LW-SSO, ensure that OBR is in the \\nLocal Intranet\\n zone on all clients accessing OBM and OBR. To do this, open\\nInternet Explorer and go to \\nInternet Options > Security\\n. Click \\nLocal Intranet > Sites >Advanced\\n and add OBR to the\\nzone.\\nTo configure LW-SSO, perform the following steps:\\n1\\n. \\nCopy the LW-SSO token from Operations Bridge Suite Management Portal:\\n1\\n. \\nLaunch the IDM administration Portal and log on as an administrative user. For example, \\nhttps://<external_hostname>:54\\n43\\n2\\n. \\nClick \\nAdministration\\n > \\nIdM Administration\\n. Click \\nSystem Settings\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n585\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a4d3c5799b2cd18f7585a4e1c2e1f003'}>,\n",
              "  <Document: {'content': 'Find the Document ID of a Report\\n1\\n. \\nLog on to the SAP BusinessObjects BI Launchpad \\nhttps://<BO_System_FQDN>:8443/BOE/BI\\n2\\n. \\nClick \\nDocument List\\n and navigate to the folder that contains the report.\\n3\\n. \\nSelect a report and click \\nProperties\\n.\\n4\\n. \\nCopy the \\nCUID\\n:\\nContainerized Operations Bridge 2022.11\\nPage \\n592\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5e26be15f4d9ea690c62413b13dfcfb2'}>,\n",
              "  <Document: {'content': \"Integrate RUM\\nReal User Monitoring (RUM) is an application monitoring software that provides information about end user behavior,\\navailability, and performance of applications by monitoring real user traffic. It monitors applications on the web and on cloud\\nand enables fast and targeted problem resolution. \\nYou can integrate RUM with the Operations Bridge suite to view the RUM data on Business Value Dashboard (BVD) and\\nPerformance Dashboard (PD).\\nTo integrate RUM with containerized Operations Bridge, see \\nConfigure Real User Monitor (RUM)\\n.\\nNote: \\nFor streaming data to OPTIC Data Lake, it's not required to integrate Operations Bridge Manager (OBM)\\nor Operations Agents (OA) with the RUM engine.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n593\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f99ac865dd6402c1bd8bb148b222a36'}>,\n",
              "  <Document: {'content': 'check-box to disable the wiring.\\nFigure 9.3: Edit Component\\nStep 10: Create a OBM Dashboard Page and Add the Report Component\\nYou must create an OBM Dashboard page and add the OBR report as a component on the page.\\nTo create an OBM Dashboard page, perform the following steps:\\n1\\n. \\nOn the OBM user interface, click \\nNew page\\n.\\nFigure 10.1\\n2\\n. \\nSplit the page as per the requirement.\\nFigure 10.2\\n3\\n. \\nClick Components and drag-drop the components, such as View Explorer, to trigger the events.\\n4\\n. \\nDrag and drop the required OBR components.\\nThe OBR report can be viewed on the OBM Dashboard page.\\n5\\n. \\nSave the page to view it from the OBM Dashboard user interface.\\nIf you get a \\ncertificate error \\nas shown in the following image, import the certificate from your browser, and re-launch the\\nbrowser.\\nIn Internet Explorer, if your browser does not provide a save option for the import certificate settings, import the certificate\\nevery time you close or re-launch your browser.\\nContainerized Operations Bridge 2022.11\\nPage \\n590\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8574df76bf9535b2fb9e0a7a373f0a2d'}>,\n",
              "  <Document: {'content': '3\\n. \\nFrom HPSSO, click \\nCreation Domain\\n. Type the OBR and OBM domain name and value.\\nThe HPSSO supports a single domain. Make sure that OBR and OBM are hosted on the same domain.\\n4\\n. \\nClick \\nSave\\n.\\n5\\n. \\nClick \\nInitial String\\n. Select the \"Show value\" checkbox.\\n6\\n. \\nCopy the \\nValue\\n and note it down in a text file.\\n2\\n. \\nTo configure LW-SSO in OBR, perform the following steps:\\n1\\n. \\nLog on to OBR Administration Console from the following link:\\nhttp://<OBR_Server_FQDN>:21411/OBRApp/\\nwhere, \\n<OBR_Server_FQDN>\\n is the name of the server on which OBR is installed.\\n2\\n. \\nGo to \\nAdditional Configurations > Security\\n in the left pane.\\n3\\n. \\nClick \\nSecurity\\n and the \\nLW-SSO\\n tab appears.\\n4\\n. \\nCopy the values from the Token Creation Key (InitString) field in OBM (This is the InitString you have copied from\\nOBM to a text file.) and paste them into the \\nInit String\\n field.\\n5\\n. \\nCheck the \\nEnabled\\n option.\\n6\\n. \\nIn the \\nDomain\\n field, enter the OBR domain.\\n7\\n. \\nIn the \\nExpiration Period\\n field, enter the recommended value of \\n60\\n minutes for LW-SSO configuration.\\n8\\n. \\nIn the \\nProtected Domains\\n field, add the OBMdomain name. Type the multiple protected domain names with\\ncomma-separated without space.\\n1\\n. \\nEven if OBR and OBM are hosted in the same domain, add the domain name to the \\nProtected Domain\\n field.\\n2\\n. \\nEnsure \\n<PMDB_HOME>\\\\PMDB\\\\data\\\\config.prp, bo.cms\\n is set to fully qualified domain names of the OBR system.\\n3\\n. \\nIn OBM integration with OBR, if OBM is HTTPS enabled, add/edit the following parameters to the \\nconfig.prp\\n file:\\nadmin.ssl=true\\nbo.ssl=true\\nRestart the \\nHPE_PMDB_Platform_Administrator\\n service.\\n9\\n. \\nClick \\nSave\\n to save the configuration.\\nFigure 3.3\\nThe following confirmation message appears:\\nLW-SSO\\n Configuration saved successfully. Restart the \\nHPE_PMDB_Platform_Administrator\\n service for these\\nchanges to take effect\\nContainerized Operations Bridge 2022.11\\nPage \\n586\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '54ac9e81bc0fb43c784d3c8d19adcdc8'}>,\n",
              "  <Document: {'content': '10\\n. \\nRestart the \\nHPE_PMDB_Platform_Administrator\\n service from the Windows services list.\\nStep 5: Configure OBR FQDN and OBM FQDN in \\nOBR\\n1\\n. \\nOn the OBR system, go to the following location:\\nWindows\\n: %PMDB_HOME%\\\\adminServer\\\\webapps\\\\OBRApp\\\\WEB-INF\\\\classes\\nLinux\\n: cd $PMDB_HOME/adminServer/webapps/OBRApp/WEB-INF/classes\\n2\\n. \\nOpen lwssofmconf.xml and add the following entries after </protectedDomains>:\\nFigure 5: Add OBR and OBM FQDNs\\n3\\n. \\nSave the changes to the file.\\n4\\n. \\nRestart the \\nHPE_PMDB_Platform_Administrator\\n service.\\nStep 6: Configure SAP BusinessObjects Trusted Authentication\\nTo set up SSO between the OBR Administration Console and SAP BusinessObjects, perform the following steps:\\n1\\n. \\nOn the OBR Administration Console, go to \\nAdditional Configurations > Security > BO Trusted Authentication\\n.\\nFigure 6.1\\n2\\n. \\nCheck the \\nEnabled\\n option.\\n3\\n. \\nEnter a string of your choice in the \\nShared Secret\\n box.\\nSAP BusinessObjects Trusted Authentication works based on a shared secret mechanism between the OBR Administration\\nConsole and SAP BusinessObjects. The string you copied from OBM is the shared secret.  This string is the same shared\\nsecret across the OBR Administration Console and SAP BusinessObjects.\\nTo verify if the same shared secret is also configured in SAP BusinessObjects, log on to SAP BusinessObjects CMC.\\n4\\n. \\nClick \\nSave\\n to save the configuration.\\n5\\n. \\nRestart the \\nHPE_PMDB_Platform_Administrator\\n service from the Windows services list, to apply the changes made in\\nConfigure OMi 10 (OBM)/ LW-SSO Authentication\\n and \\nConfigure SAP BusinessObjects Trusted Authentication\\n steps.\\nOn a Linux host, log on as a root user and run the following command:\\nsystemctl stop HPE_PMDB_Platform_Administrator.service\\nsystemctl start HPE_PMDB_Platform_Administrator.service \\nStep 7: Disable Clickjacking\\nDisable ClickjackFilterSameOrigin on SAP BusinessObjects system\\n1\\n. \\nOn your SAP BusinessObjects system, go to the following directory:\\nOn Linux\\n: $PMDB_HOME/BOWebServer/webapps/BOE/WEB-INF\\nOn Windows\\n: %PMDB_HOME%\\\\BOWebServer\\\\webapps\\\\BOE\\\\WEB-INF\\n2\\n. \\nOpen the \\nweb.xml\\n file.\\n3\\n. \\nGo to \\nClickjackFilterSameOrigin\\n filter:\\nContainerized Operations Bridge 2022.11\\nPage \\n587\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '49c55f8c254261c531b95c57ac2d160a'}>,\n",
              "  <Document: {'content': '4\\n. \\nComment the element as shown here:\\n5\\n. \\nRestart the \\nBusinessObjects\\n service.\\nOn Linux\\n: SAPBOBJEnterpriseXI40\\nOn Windows\\n: Business Objects Webserver\\n6\\n. \\nWait for five minutes.\\nStep 8: Generate the Report Component XML in OBR\\nGenerate the component XML file using the ComponentGenerator command on the OBR host and load it to the OBM.\\nPerform the following steps to generate the report component XML file:\\n1\\n. \\nLog on to the OBR system.\\n2\\n. \\nOpen a command-line window (for Windows) or a shell prompt (for Linux).\\n3\\n. \\nRun the following commands to see the ComponentGenerator syntax:\\nFor Windows\\n: \\n%PMDB_HOME%\\\\bin\\\\ComponentGenerator\\nFor Linux\\n: \\n$PMDB_HOME/bin/ComponentGenerator\\nFigure 8.1: (Windows)\\n4\\n. \\nRun the following command to generate the XML file:\\nFor Windows:\\n \\n%PMDB_HOME%\\\\bin\\\\ ComponentGenerator –c <categoryName> -d <documentId > -n <componentName> -l <outputDir> -f\\n<optional Parameter>\\nFor Linux: \\n$PMDB_HOME/bin/ ComponentGenerator.sh –c <categoryName> -d <documentId > -n <componentName> -l <outputDir> -f <o\\nptional Parameter>\\nCategory Name = This is the Category to be created in Component Gallery in OBM Dashboard\\nDocument Id = This is the report’s unique document ID – see \\nFinding the Document ID of a Report\\nFile Location = This is the directory where the component XML file will be created\\nComponent Name =  The Component name to be created for the report in OBM Dashboard (note the use of quotes here)\\nOptional Parameter = Use non zero value if the report does not accept view or CIID as the parameter.\\nThe above command generates \\n<Component Category><componentName>.uim.xml\\n file in on the Desktop.\\nExample\\nThe following is an example command for System Management Inventory:\\n%PMDB_HOME%\\\\bin\\\\ComponentGenerator -c OBR -d AfHfjvp01_pHrwWbfzGNaTY -l C:\\\\Users\\\\Administrator\\\\Desktop -n \"SM System Inventory\"\\nThe command displays the following result:\\nFigure 8.2: SM System Inventory\\nContainerized Operations Bridge 2022.11\\nPage \\n588\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5356843b3f2950b4dbfb4516904bfdaa'}>,\n",
              "  <Document: {'content': \"Integrate SiteScope metrics with OPTIC DL\\nSiteScope is agentless monitoring software that monitors the availability and performance of system infrastructures, such as\\nservers, network devices and services, applications, operating systems, and various other enterprise components.\\nYou can integrate SiteScope with the Operations Bridge suite to view the SiteScope data on Business Value Dashboard (BVD)\\nand Performance Dashboard (PD)\\nFollow the steps to integrate SiteScope with the Operations Bridge suite:\\nTo view system infrastructure reports, you must send the performance metrics collected by the SiteScope to OPTIC Data\\nLake. You can send metrics for any SiteScope monitor type to OPTIC Data Lake for custom reporting and Performance\\nDashboards.  Business Value Dashboard (BVD) uses the data from a specific set of monitors to populate the System\\nInfrastructure Reports. The section '\\nList of Monitors\\n' on this page, provides a complete list of monitors that are used to\\npopulate the System Infrastructure Reports on BVD.\\nPrerequisites\\nOPTIC Reporting \\ncapability\\nRun the command on the master (control plane) node to check if you have installed the \\nOPTIC Reporting\\n capability: \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticReporting:\\nFor example:\\nhelm get values opsb -n opsbs | grep opticReporting -A 1\\nopticReporting:\\n    deploy: true\\nTo add the \\nOPTIC Reporting\\n capability, follow the instructions listed on the \\nAdd/Remove capabilities\\n page. \\nOperations Bridge Manager (OBM). For installation steps, see \\nInstall\\n.\\nConfigure a secure connection between OBM and OPTIC Data Lake:\\nTo configure classic OBM, see \\nConfigure classic OBM\\nTo configure containerized OBM, see \\nConfigure a secure connection between containerized OBM and OPTIC Data\\nLake\\nValidate the connection between BVD and OPTIC Data Lake. See \\nValidate the connection between BVD and OPTIC Data\\nLake\\n.\\nSiteScope. For installation steps, see \\nInstall\\n.\\nInstall and integrate Operations Agent on the SiteScope server with OBM.\\nTo stream SiteScope data into the OPTIC Data Lake, you must integrate the Operations Agent which is on the SiteScope\\nserver with OBM.\\nPerform the following steps to check if Operations Agent is installed:\\n1\\n. \\nLog on to the SiteScope server:\\nOn Linux as root\\nOn Windows as Administrator\\n2\\n. \\nRun the following commands:\\nOn Linux:\\ncd /opt/OV/bin\\n./opcagt -version\\nOn Windows:\\n%ovinstalldir%\\\\bin\\nopcagt -version\\nThe version of the Operations Agent is displayed. Make sure that the version is 12.14.\\nInstall and integrate Operations Agent with OBM\\nRun the following commands if you want to install and integrate Operations Agent (on a SiteScope server) with OBM:\\nOn Linux:\\n \\n./oainstall.sh -i -a -s <OBM load balancer or gateway server> \\nOn Windows:\\n \\ncscript oainstall.vbs -i -a -s <OBM load balancer or gateway server> \\nGrant the SiteScope server certificate\\nFollow the steps:\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nNote:\\nFor containerized OBM 2019.11, \\n<OBM load balancer or gateway server> \\nis the  FQDN of the external access host. \\nFor more information, see \\nOperations Agent Install\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n594\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a8ef1360264ccf4e17815eb43e40575d'}>,\n",
              "  <Document: {'content': '1\\n. \\nOn OBM, go to \\nAdministration > SETUP AND MAINTENANCE > Certificate Request\\n2\\n. \\nClick \\n \\nto grant the certificate.\\nComponents and their supported versions\\nComponent\\nSupported version\\nClassic OBM \\n2020.05 and higher\\nContainerized OBM\\n2021.05 and higher\\nOperations Agent \\n12.14 and higher\\nSiteScope\\n2020.10 and higher\\nSiteScope Metric Streaming OPTIC\\nData Lake content\\n2020.05\\nTask 1: Deploy the metrics streaming aspect\\nThe SiteScope Metrics Steaming policy is available with the SiteScope Metric Streaming OPTIC Data Lake content. \\nInstall the SiteScope Metric Streaming OPTIC Data Lake content\\nFollow the steps:\\n1\\n. \\nDownload the SiteScope Metric Streaming OPTIC Data Lake content from the \\nMarket Place\\n.\\n2\\n. \\nOn OBM, go to \\nAdministration\\n >\\nSETUP AND MAINTENANCE\\n > \\nContent Packs\\n.\\n3\\n. \\nClick \\nImport\\n. Import Content Pack window appears.\\n4\\n. \\nBrowse to the location where you have saved the SiteScope Metric Streaming OPTIC Data Lake content and then click\\nImport\\n.\\n5\\n. \\nThe required aspect gets imported. Click \\nClose\\n.\\nDeploy the SiteScope Metrics Streaming Aspect\\nFollow the steps:\\n1\\n. \\nOn OBM, go to \\nAdministration > Monitoring > Management Templates & Aspects\\n.\\n2\\n. \\nIn the Configurations Folder pane, click \\nSiteScope Metric Streaming\\n.\\n3\\n. \\nIn Management Templates & Aspects pane, right-click the \\nSiteScope Metric Streaming\\n aspect and then click \\nAssign and Deploy\\n item.\\n4\\n. \\nIn the Configuration Item tab, click the CI of the SiteScope server (Operations Agent node) on which you want to deploy\\nthe Aspect, and then click \\nNext\\n.\\n5\\n. \\nIn the Required Parameters tab, enter the OPTIC Data Lake receiver URL in the following format: \\nhttps://<externalAccessHost\\n>:30001/itomdi/receiver\\n6\\n. \\nClick \\nNext \\nand then click \\nFinish\\n.\\nValidate the installation of the SiteScope policies\\nRun the command on the SiteScope server:\\nOn Linux:\\n \\n/opt/OV/bin/ovpolicy -list\\nOn Windows:\\n \\novpolicy -list\\nThe SiteScope policies are listed as follows:\\nTask 2: (Optional) Add the tenant_id\\nA tenant id enables you to configure multiple tenants.\\nFollow the steps to update the tenant id:\\n1\\n. \\nOn SiteScope server, go to:\\nOn Windows:\\n \\n\"<SITESCOPE_HOME>\\\\templates.applications\\\\COSO_tenant.properties\"\\nNote: \\nSiteScope supports ingestion of tenant id into OPTIC Data Lake\\nonly from version 2020.10 and higher.\\n\\ue916\\n\\ue916\\nContainerized Operations Bridge 2022.11\\nPage \\n595\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'adc2bc3af1bade7ccad9114f07d991b7'}>,\n",
              "  <Document: {'content': \"1\\n. \\nSelect the \\nMonitors\\n context. In the monitor tree, expand the group directory that contains the monitor, and select the\\nmonitor. For the complete list of monitors that are required to populate BVD Reports, see '\\nList of Monitors\\n' section on\\nthis page.\\n2\\n. \\nIn the right pane, click the \\nProperties\\n tab, and select \\nSearch/Filter Tags\\n.\\n3\\n. \\nSelect the OPTIC Data Lake tag with the value as OPTIC Data Lake and then click \\nSave\\n.\\nOption 2: Assign the OPTIC Data Lake tag to a group of monitors\\nFollow the steps:\\n1\\n. \\nOn \\nMonitors\\n context, right-click \\nSiteScope\\n root (or the group or monitor in the monitor tree).\\n2\\n. \\nSelect \\nGlobal Search and Replace\\n from the context menu. The Global Search and Replace window opens.\\n3\\n. \\nSelect the \\nMonitor\\n option and click \\nNext\\n.\\n4\\n. \\nIn the \\nSelect Subtype\\n tab, select the monitors for which you want to assign the OPTIC Data Lake tag.\\nFor Agentless system infrastructure reporting, select the monitors shown in the following image:\\n \\n \\n5\\n. \\nClick \\nNext\\n.\\n6\\n. \\nIn the \\nReplace Mode\\n tab. Select the \\nReplace\\n option.\\nContainerized Operations Bridge 2022.11\\nPage \\n597\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '112de7d1f0b0ad09f5d14e821c7041c6'}>,\n",
              "  <Document: {'content': '14\\n. \\nClick \\nApply\\n.\\n15\\n. \\nThe Summary page displays the result. Click \\nFinish\\n.\\n \\n \\nList of monitors\\nFor SiteScope reports, enable the following Agentless Infrastructure Monitors:\\nCPU Monitor\\nMemory Monitor\\nNetwork Bandwidth Monitor\\nDynamic Disk Space Monitor\\nMicrosoft Windows Resources Monitor\\nUNIX Resources Monitors\\nTable name\\nMonitor type\\nopsb_agentless_node\\nCPU, Memory, Windows Resources, UNIX Resources\\nopsb_agentless_cpu\\nCPU\\nopsb_agentless_disk\\nWindows Resources\\nopsb_agentless_filesys\\nDynamic Disk Space, UNIX Resources \\nopsb_agentless_netif\\nNetwork Bandwidth, Windows Resources, UNIX Resources \\nopsb_agentless_generic\\nAll SiteScope monitors\\nFor information about specific monitors that populate the SiteScope raw tables, see \\nSource of SiteScope raw tables\\n.\\nAfter you enable the monitors, metrics from the nodes are sent to OPTIC Data Lake. After completing the aforementioned\\nconfigurations, it would take about 30 minutes for you to see the last hour data in the System Resource Top 3 report on BVD. \\nYou can also use these metrics to generate dashboards using Performance Dashboards. For configuration steps, see \\nConfigure\\nPerformance Dashboards\\n.\\nTask 6: \\n(Optional) \\nEnable downtime\\nContainerized Operations Bridge 2022.11\\nPage \\n599\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8f81c1d21cb69a5a81575e2ebf81e15'}>,\n",
              "  <Document: {'content': 'On Linux:\\n \\n/opt/HP/SiteScope/templates.applications/COSO_tenant.properties\\n2\\n. \\nIn the \\nCOSO_tenant.properties\\n file, add a tenant id to the \\n_tenantIdForCOSO\\n parameter.\\nTask 3: Restart SiteScope\\nUse one of the following options to restart SiteScope:\\nOn Windows:\\n1\\n. \\nOpen the Services Window.\\n2\\n. \\nSelect the SiteScope service and click \\nStop\\n.\\n3\\n. \\nSelect the SiteScope service and click \\nStart\\n.\\nIt takes a few minutes for the SiteScope server to start. \\nOn Linux:\\n1\\n. \\nOpen a terminal window on the server where you have installed SiteScope.\\n2\\n. \\nRun the stop command:\\n /opt/HP/SiteScope/stop\\n3\\n. \\nRun the start command: \\n/opt/HP/SiteScope/start\\nIt takes a few minutes for the SiteScope server to start. \\nTask 4: Create monitors\\nCPU Monitor\\n. See \\nCPU Monitor\\n.\\nMemory Monitor\\n. See \\nMemory Monitor\\n.\\nNetwork Bandwidth Monitor\\n. See \\nNetwork Bandwidth Monitor\\n.\\nDynamic Disk Space Monitor\\n. See \\nDynamic Disk Space Monitor\\n.\\nMicrosoft Windows Resources Monitor\\n. See \\nMicrosoft Windows Resources Monitor\\n.\\nUNIX Resources Monitors\\n. See \\nUNIX\\n Resources Monitor\\n.\\nTask 5: Enable monitors\\nYou must add the OPTIC Data Lake tag to monitors to enable them to stream data into OPTIC Data Lake.\\nFollow the steps:\\nI. Add the OPTIC Data Lake tag\\n1\\n. \\nIn the SiteScope UI, select the \\nPreferences\\n context.\\n2\\n. \\nSelect \\nSearch/Filter Tags.\\n Follow the steps to create the \\nCOSO tag\\n if it doesn\\'t exist:\\n1\\n. \\nClick \\n \\nNew tag\\n. \\n2\\n. \\nIn the \\nTag\\n name box enter \\'COSO\\' (It must be in upper case).\\n3\\n. \\nTo add a Value, click \\n \\n. A new row gets created.\\n4\\n. \\nIn the \\nValue Name\\n column, double-click and enter \\'COSO\\' (It must be in upper case).\\n5\\n. \\nClick \\nOk\\n. The tag gets created and available for assignments.\\nII. Assign the OPTIC Data Lake tag to monitors \\nAssign the OPTIC Data Lake tag to each monitor individually or a group of monitors.\\nOption 1: Assign the OPTIC Data Lake tag to a single monitor\\nFollow the steps:\\nNote:\\nThe tenant_id can have a maximum of 80 characters.\\nIf a \\ntenant_id\\n isn\\'t configured or if the \\ntenant_id\\n  has more than 80 characters, the following warning message\\ngets logged in \\nerror.log\\n file:\\n\"tenant_id isn\\'t configured. Configure the tenant_id and make sure that you don\\'t exceed 80 characters.\\nPlease restart the SiteScope after updating the tenant_id.\"\\nTip: \\nThe section \\'\\nList of Monitors\\n\\' on this page, provides a complete list of Agentless Infrastructure Monitors.\\nUse only the Internet Explorer browser or the SiteScope local client to view the UI.\\nContainerized Operations Bridge 2022.11\\nPage \\n596\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ad2c2e4e528b44c559ac1640b25ac4fd'}>,\n",
              "  <Document: {'content': '7\\n. \\nClick \\nNext\\n.\\n8\\n. \\nIn the \\nChoose Changes\\n tab, select the OPTIC Data Lake tag as shown in the image:\\n9\\n. \\nClick \\nNext.\\n10\\n. \\nIn the Affected Objects tab, you can see the list of monitors that are tagged with the OPTIC Data Lake tag.\\n11\\n. \\nClick \\nNext\\n.\\n12\\n. \\nIn the \\nReview summary\\n tab, you can review the changes.\\n13\\n. \\nClear the \\nVerify monitor properties with remote server\\n check box.\\nContainerized Operations Bridge 2022.11\\nPage \\n598\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4ccd671e292502a973c73e93df1ca6b6'}>,\n",
              "  <Document: {'content': 'SiteScope sends the downtime flag to the OPTIC Data Lake. For more information see, \\nConfigure downtime\\n.\\nRelated topics\\nFor details about the metrics collected by SiteScope, see \\nSystem Infrastructure schema tables\\n.\\nFor information about specific monitors that populate the SiteScope raw tables, see \\nSource of SiteScope raw tables\\n.\\nContainerized Operations Bridge 2022.11\\nPage \\n600\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'de0fb9273af2142791212866ea0bf1a'}>,\n",
              "  <Document: {'content': 'Install Monitoring Service Edge on K3S using a script\\nThis topic lists the prerequisites and the steps required for deploying the \\nMonitoring Service Edge\\n onto a single node K3s\\nenvironment using the \\ninstall.sh\\n script. The script will prompt various parameters that are required for installation.  Using the\\nparameters and values given, the script either creates a new \\nvalues.yaml\\n file or updates the existing \\nvalues.yaml\\n file that is used\\nduring helm installation. Though the script deploys the edge chart, it allows you to add/modify user configurable parameters\\nbefore the deployment. \\nPrerequisites \\nBefore you start running the i\\nnstall.sh\\n, you must keep the following details ready:\\nCredentials of  Agent Metric Collector integration user created on external OBM RTSM. See \\nCreate an Agent Metric\\nCollector integration user\\n.\\n \\nOpsBridge environment details\\nNamespace to install Edge chart, for example: \\nmonitoring-edge\\nDeployment name for helm installation, for example: \\nmonitoring-edge\\nIf you use the script to download the images from Docker Hub and you need to use a proxy to access the Internet. Set the\\nHTTP proxy environment variables before running the installation script. \\nFor example,\\nexport http_proxy=http://web-proxy.example.com:8080\\nexport no_proxy=\"127.0.0.1, localhost, *.example.net\"\\nexport https_proxy=https://web-proxy.example.com:8080\\nor\\nexport https_proxy=http://web-proxy.example.com:8080\\nDownload the script\\n1\\n. \\nDownload the \\nmonitoring-service-edge-chart-<version>.zip\\n from the \\nMicro Focus Software Licenses and Downloads\\n website. \\n2\\n. \\nRun the following command to unzip the \\nmonitoring-service-edge-chart-<version>.zip file\\n.\\nunzip monitoring-service-edge-chart-<version>.zip\\nThe unzipped file will have the following directories and files under \\nmonitoring-service-edge-chart\\n:\\nDirectories/files\\nDescription\\nomt\\nOMT_External_K8s_20xx.xx-<version>.zip\\napphub-chart-values.yaml\\ncharts\\nmonitoring-service-edge-<version>.tgz.\\n  \\nDon\\'t extract this\\n.\\noffline_images\\nallimages.tar\\nimage-set.json\\nsamples\\nvalues.yaml\\ngenericPV.yaml\\nitom-edge-scc.yaml\\nopenshift\\nvalues.yaml\\nscripts\\ninstall.sh\\nupgrade.sh\\nExecute the script\\nImportant:\\n \\n1\\n. \\nMake sure that you have K3s version equal to or lower than \\n\"v1.23.6+k3s1\"\\n installed in your environment\\nbefore running the script. \\nRun the following command to install a specific version of K3s:\\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=<Supported-Version> sh\\nExample:\\ncurl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.23.6+k3s1 sh \\nFor steps to install K3s, see \\nK3s documentation\\n.\\n2\\n. \\nThe script installs OMT 2022.11\\n3\\n. \\nMake sure that you \\nIntegrate OpsBridge with OBM\\n before running the script.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n606\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '225fe476107774a0604cd844f61ac270'}>,\n",
              "  <Document: {'content': '4\\n. \\nRun the command in connected OBM to export the CA certificate\\n/opt/OV/bin/ovcert -exporttrusted -file <filename> -alias  CA_<coreid of the connected OBM>_<ASYMMETRIC_KEY_LENGTH>-ovrg server\\nExample:\\n/opt/OV/bin/ovcert -exporttrusted -file obm_ca.crt -alias \"CA_3df9d650-8e49-75be-1eb0-cfb204d74adf_2048\" -ovrg server\\nTask 3: Establish trust from OpsBridge to OBM (connected to Monitoring Service Edge)\\n1\\n. \\nCopy the obm_ca.crt file that\\'s generated in the previous step to OpsBridge and run the command:\\n./idl_config.sh -cacert <cert_file> -chart <chart> -namespace <namespace> [-release <release>]\\nExample:\\n./idl_config.sh -cacert /tmp/obm_ca.crt -chart path/to/charts/opsbridge-suite-2021.11.tgz -namespace opsb-suite\\nTask 4: Establish trust from OBM (connected to Monitoring Service Edge) to OpsBridge \\n1\\n. \\nCopy the jar file generated in task 1 to the connected OBM and run the following command:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id my_obm \\\\\\n--configuration-type TRUST_ONLY \\\\\\n--suite-service-hostname <OpsBridge> \\\\\\n--obm-ca-cert-alias <CA certificate id> \\\\\\n--admin-user <obmadmin> --integration-user <obm_integration_user>\\nExample:\\n/opt/HP/BSM/JRE/bin/java -jar obm-configurator.jar --endpoint-id chk_certcomm \\\\\\n--configuration-type TRUST_ONLY \\\\\\n--suite-service-hostname mycomp.mysystem.net \\\\\\n--obm-ca-cert-alias CA_3df9d650-8e49-75be-1eb0-cfb204d74adf_2048 \\\\\\n--admin-user admin --integration-user admin\\nVerify the configurations\\nRun the following command to verify the configurations on OBM.\\n \\n/opt/OV/bin/ovcert -list\\nNote\\n: You can find the\\n idl_config.sh\\n tool in the \\nobm-configurator-interim \\ndirectory in the \\nintegration-\\ntools\\n directory. The certificates get loaded into config map api-client-ca-certificate in OpsBridge\\nenvironment.\\nContainerized Operations Bridge 2022.11\\nPage \\n605\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2c239cb8c75ef1e8f7ce1dbd875b58e6'}>,\n",
              "  <Document: {'content': \"Integrate OpsBridge with OBM\\nThis topic gives you steps to establish trust between the connected OBM and the OpsBridge deployment. \\nThe trust establishment enables the flow of metrics, events, and topology to OpsBridge deployment. \\nThe supported versions of OBM are:\\n2022.05\\n2021.11\\n2021.05\\n2020.10 \\nThe OBM 2020.10 has the following limitations:\\nDoesn't support HTTP proxy for topology synchronization\\nDoesn't support the Performance Dashboard (PD) proxy graphing\\nTask 1: Download the integration tool\\nThe OBM integration tools are required to:\\n1\\n. \\nConfigure a secure connection between OBM and OPTIC Data Lake (DL).\\n2\\n. \\nConfigure event forwarding from OBM to OPTIC DL (Both Reporting and Automatic Event Correlation capabilities use event\\nforwarding)\\n3\\n. \\nConfigure Automatic Event Correlation\\nFollow the steps on the master node to get the integration tools:\\n1\\n. \\nThe \\nopsb-obm-integration-tools.zip\\n file is present in the \\nstatic-files-provider\\n container of the \\nopsb-resource-bundle\\n pod.\\nTo get the zip file, run the following command on the master (control plane) node:\\nwget https://<externalAccessHost>:<externalAccessPort>/staticfiles/opsb-obm-integration-tools.zip --no-check-certificate\\nFor example: \\nwget https://myhost.mycompany.net:443/staticfiles/opsb-obm-integration-tools.zip --no-check-certificate\\n2\\n. \\nRun the following commands to extract the contents of \\nopsb-obm-integration-tools.zip\\n to the \\nintegration-tools\\n directory: \\nunzip opsb-obm-integration-tools.zip -d integration-tools\\ncd integration-tools\\n3\\n. \\nRun the command to get the files necessary to configure the OBM and OPTIC Data Lake integration\\n./get-obm-setup-tool.sh\\nThe \\nobm-configuration.jar\\n tool is created in the same directory where \\nget-obm-setup-tool.sh\\n resides.\\nTask 2: Export trusted certificates issued by OBM connected to Monitoring Service Edge\\nRun the commands to export the certificates on the OBM. The following commands apply to both Linux and Windows:\\n1\\n. \\nRun the command to get the ASYMMETRIC_KEY_LENGTH\\novconfget sec.cm ASYMMETRIC_KEY_LENGTH\\n2\\n. \\nRun the command to get the \\ncoreid\\n of the OBM\\novcoreid -ovrg server\\n3\\n. \\nFind CA certificate in the connected OBM\\n/opt/OV/bin/ovcert -list\\nTip: \\nRun the following command to get the \\n<externalAccessHost> and <externalAccessPort>\\n:\\nhelm get values -n $(helm list -A | grep opsbridge | awk '{print $2,$1}') | grep -i externalAccess\\nCommand output:\\nexternalAccessHost: mycomp.mysystem.net\\nexternalAccessPort: 443\\nContainerized Operations Bridge 2022.11\\nPage \\n604\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2000567d0300a53034a1cc85937d51dc'}>,\n",
              "  <Document: {'content': 'Example Output:\\n# kubectl create -n monitoring-edge -f genericPV.yaml\\npersistentvolume/edgevol1 created\\npersistentvolume/edgevol2 created\\npersistentvolume/edgevol3 created\\npersistentvolume/edgevol4 created\\nContainerized Operations Bridge 2022.11\\nPage \\n603\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '67fb0f5f3681f9cea969c80fe0026ad2'}>,\n",
              "  <Document: {'content': \"Integrate OpsBridge with Monitoring Service Edge\\nMonitoring Service Edge is a service that enables the flow of system infrastructure, management, and custom metrics from\\nthe Operations Agent nodes to OpsBridge. This service is available after you install the Edge chart.\\nMonitoring Service Edge collects metrics from the Operations Agent (OA) nodes connected to an OBM. Therefore, it must be\\nconnected to the same\\n \\nOBM that the OA nodes are connected. So you must integrate the OBM with the OpsBridge deployment\\nbefore deploying the Edge chart. You can deploy the Edge chart onto a single node \\nK3s\\n environment using the \\ninstall.sh\\n script\\nor manually on MF Kubernetes with CDF environment. After installing the Edge chart, you must establish trust between\\nMonitoring Service Edge and OBM. This enables the Agent Metric Collector in the Monitoring Service Edge to collect metrics\\nfrom the Operations Agent nodes and forward them to OPTIC DL on the  OpsBridge deployment.\\nThis section gives you information to:\\nIntegrate OpsBridge with OBM\\nInstall Monitoring Service Edge on K3s using a script\\nInstall Monitoring Service Edge on OpenShift\\nInstall Monitoring Service Edge on MF Kubernetes with OMT\\n \\nEstablish trust between Monitoring Service Edge and OBM\\nPrerequisites\\u200b\\u200b\\n You must have the following information to perform an Edge deployment, irrespective of whether it's K3s, OpenShift, or\\nOMTK8s):\\nOBM tenant FQDN, port, protocol\\nAgent Metric Collector credentials. See \\nCreate an Agent Metric Collector integration user\\n.\\nOpsBridge FQDN and port\\nProxy information to connect from Edge to OpsBridge.(proxy host, port, credentials). This is required only if you need a\\nproxy between Edge and OpsBridge.\\nTo add the Edge data broker port to OBM. It's recommended to add this port to OBM before the Edge installation. \\nSystem requirement\\nIf you install in a cluster, the master (control plane) node and worker node hosts must use the same operating system. The\\nfollowing operating systems are supported:\\nOperating System\\nArchitecture Type\\nVersions\\nRed Hat Enterprise Linux\\nx86_64\\n7.x  where (x>=8) and 8.x where (x>=3)\\nCentOS\\nx86_64\\n7.x where (x>=8) and 8.x where (x>=3)\\nOracle Enterprise Linux\\nx86_64\\n7.x where (x>=8) and 8.x where (x>=3)\\nSizing guide\\nThe sizing calculator spreadsheet is useful to plan the provisioning of systems for \\nMonitoring Service Edge Chart\\ndeployment and understand the implications of various choices you make.\\nClick \\nhere\\n and download the sizing calculator (excel file) to your system. Then open the excel file and enable editing. \\nContainerized Operations Bridge 2022.11\\nPage \\n601\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8a7a1acdac398a47d8148ea6aa525476'}>,\n",
              "  <Document: {'content': 'Create Persistent volumes for Edge\\nTo install the Edge you need to create Persistent Volumes (PVs).\\nYou can consider PVs as a mapping to a storage volume, for example, an NFS server volume in this case. When you create a\\npersistent volume, the storage volume becomes available in Kubernetes. The cluster components use this storage. The cluster\\ncomponents directly do not use a PV for their storage. They use something called a PVC (Persistent Volume Claim). You can\\nthink of PVC as an intermediary between the cluster components and the actual storage volume. The PVC will then bind to the\\nPV, or in simpler words, use the PV. The cluster components then use the PVC instead.\\nThis makes the cluster components (say a Pod) access a storage volume in the following flow:\\nPod -> PVC -> PV -> Storage Volume(ex. NFS)\\nTo create Persistent volumes, perform the following tasks:\\nThe monitoring-service-edge-chart-<version>.zip contains the genericPV.yaml file under samples folder. You can edit the\\nsame file as applicable.  \\nImportant:  Provide the FQDN of the NFS server and the NFS directory path that you used while configuring the NFS server.\\nDo not edit any names or labels or change any indentation in the yaml file.\\nYou can update the required values and maintain the yaml syntax.\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: edgevol1\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/edgevol1\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: edgevol2\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/edgevol2\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: edgevol3\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/edgevol3\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\n  volumeMode: Filesystem\\n---\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n  name: edgevol4\\nspec:\\n  accessModes:\\n    - ReadWriteMany\\n  capacity:\\n    storage: 10Gi\\n  nfs:\\n    path: /var/vols/itom/edgevol4\\n    server: yournfsserver.example.net\\n  persistentVolumeReclaimPolicy: Retain\\nTo create persistent volumes, run the following command:\\nkubectl create -f genericPV.yaml \\nContainerized Operations Bridge 2022.11\\nPage \\n602\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '373a0f5b309cc26721d8111fac724d77'}>,\n",
              "  <Document: {'content': 'Confirm Password:\\nEnable Monitoring\\nDo you want to enable prometheus and grafana (true/false) [false]:\\n \\nEnter the namespace for installing apphub chart (NOTE: If the namespace does not exist, then the script will\\ncreate the namespace) [omt] : \\nomt namespace does not exit... so creating...\\nnamespace/omt created\\nStarting the apphub chart installation\\nEnter the external access port for apphub[30443] : 30443\\n+ helm install apphub /opt/cdf/charts/apphub-1.22.0+20221100.75.tgz --namespace omt -f ./apphub-chart-values.yaml\\nCreate namespace\\nEnter the namespace where you want to install the Edge chart (NOTE: If the namespace doesn\\'t exist, then the\\nscript will create the namespace) [monitoring-edge]:\\n Enter the namespace, if the entered namespace doesn\\'t exist then\\nthe script will create a new namespace.\\nCreate values.yaml file \\nDo you want to use already existing values.yaml file for installation [true/false] [false]:\\n Enter \\'true\\' if you already\\nhave \\nvalues.yaml\\n created, then you need to give the absolute path to the \\nvalues.yaml\\nGive the absolute path of the values.yaml YAML file: \\n<absolute path>\\nThe script prompts you for the following information and creates the \\nvalues.yaml\\n file to use during installation:  \\nEnter the external access hostname/FQDN:\\n Enter the FQDN of the external access hostname/FQDN (Load balancer or\\ncontrol plane Node).\\nEnter the external access port [31443]:\\n Enter the port to use for the external access host. This port must be available on\\nthe node. This is the port for IDM.\\nDo you want to use containerized OBM [true/false] [false]\\n: Enter true if you\\'re accessing containerized OBM.\\nEnter the OBM hostname:\\n Enter the FQDN of the OBM load balancer or gateway server.  This is the OBM server,\\nto which the Agent Metric Collector registers itself and from which the Operations Agent nodes list is retrieved.\\nEnter the OBM port [443]:\\n Enter the port of the OBM server. \\nEnter the protocol used by components to access OBM and RTSM (If OBM is configured to be accessed using\\nhttp, set this parameter to http) [https]:\\n Enter the protocol used to access OBM and RTSM. \\nEnter the Agent Metric Collector integration user created on OBM RTSM: \\nUse lowercase to enter the external OBM\\nusername that you created in \"Create an Agent Metric Collector integration user\".  This is to pull metrics from Operations\\nAgents for \\nOPTIC Reporting\\n. \\nEnter the OBM data broker node port (The external access port within the OMT cluster used by the data broker\\ncomponent of the agent metric collector. This port is for external \\nOBM to agent metric collector\\ncommunication.) [31382]: \\nPress \\nEnter\\n to accept the default port 31382. The external access port within the OMT cluster,\\nwhich gets used by the data broker component of the agent metric collector. This port is for external OBM to agent metric\\ncollector communication. \\nEnter the OBM server port (The BBC port used by the OBM server for incoming connections. The Agent Metric\\nCollector uses this port to communicate with OBM.) [383]: \\nPress \\nEnter\\n to accept the default port 383.  The OBM server\\nuses the \\nBBC \\nport for incoming connections. The Agent Metric Collector uses this port to communicate with OBM. The default\\nImportant:\\n If you want to use \\nprometheus and grafana\\n then only set it to set it true as this will require\\nhigher system resources (CPU and Memory utilization).\\n\\ue91b\\n\\ue91b\\nNote:\\n Supported port range for chart install over K3s is 30000-32767\\n\\ue917\\n\\ue917\\nImportant: \\nAlthough the script refers to OpsBridge, it is applicable to both on-premises and SaaS setup.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n608\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '526cf002d3907d8cb202eceb444edad8'}>,\n",
              "  <Document: {'content': \"port used by OBM is 383, therefore don't change this setting unless you have changed the default \\nBBC \\nport on the OBM\\nserver.\\nEnter the external access host for Operations Bridge: \\nEnter your Operations Bridge endpoint host name. You will get\\nthis upon registering for Operations Bridge.\\nEnter the external access port for Operations Bridge: \\nEnter your Operations Bridge endpoint port. You will get this upon\\nregistering for Operations Bridge.\\nDo you want to enter proxy details to be used to connect to Operations Bridge over the internet\\n \\n[true/false]\\n[false]: \\nEnter 'true' if you want to use proxy details to connect to \\nOpsBridge\\n.\\nEnter the proxy scheme to connect to Operations Bridge over the internet [https/http] [https]\\n:\\nEnter the proxy host to be used to connect to Operations Bridge over the internet: \\nProxy host for connecting to\\nOpsBridge\\n.\\nEnter the proxy port to be used to connect to Operations Bridge over the internet: \\nProxy port  for connecting to\\nOpsBridge\\n.\\nEnter the proxy user to be used to connect to Operations Bridge over the internet: \\nProxy username for connecting\\nto \\nOpsBridge\\n.\\nEnter password for OBM RTSM user in Plain Text: \\nEnter the OBM RTSM user password.\\nConfirm Password: \\nConfirm OBM RTSM user password\\nEnter password for OpsBridge Proxy in Plain Text: \\nEnter the SaaS Proxy user password.\\nConfirm Password: \\nConfirm the above entered password\\nDeploy\\nDo you want to proceed with the installation (true/false) [true]: \\nAccept the default if you want to proceed with the\\ninstallation. You can pause at this instruction and navigate the location of \\nvalues.yaml\\n in another shell to validate and update\\nthe file as required. \\nIf you enter '\\ntrue'\\n, the script will prompt for the following information: \\nEnter the helm deployment name for helm installation [monitoring-edge]:\\n Enter the helm deployment name under\\nwhich you want to install the helm chart. \\nExample output:\\n+ helm install monitoring-edge ../charts/monitoring-service-edge-1.2.0+20221100.21.tgz --namespace monitoring-edge -f values.yaml\\nNAME: monitoring-edge\\nLAST DEPLOYED: Thu Jul  7 00:15:17 2022\\nNAMESPACE: monitoring-edge\\nSTATUS: deployed\\nREVISION: 1\\nTEST SUITE: None\\nNOTES:\\n**********************************************************************************\\n** WARNING:                                                                     **\\n**                                                                              **\\n** If you used a values.yaml file to install Monitoring Service Edge  **\\n** you must manage access to this file carefully as it contains sensitive       **\\n** information.                                                                 **\\n** Micro Focus recommends restricting access by leveraging file permissions,    **\\n** by moving the file to a secure location or by simply deleting the file.      **\\n** Failing to secure values.yaml, you may exposing the system to increased      **\\n** security risks. You understand and agree to assume all associated            **\\n** risks and hold Micro Focus harmless for the same.                            **\\n** It remains at all times the Customer's sole responsibility to                **\\n** assess its own regulatory and business requirements. Micro Focus does not    **\\n** represent or warrant that its products comply with any specific legal        **\\n** or regulatory standards applicable to Customer                               **\\n** in conducting Customer's business.                                           **\\n**                                                                              **\\n**********************************************************************************\\nThank you for deploying Monitoring Service Edge 1.2.0+20221100.21\\nBelow are the Installation Summary:\\nShown below are important URLs for you:\\nUser Management UI:\\n    https://myexamplehost.net:31443/idm-admin\\nOpsBridge Details:\\nImportant: \\nVerify your  \\n<directory where you unzipped the monitoring-service-edge-chart-<version>.zip>/monitoring-servi\\nce-edge-chart/scripts/values.yaml\\n before proceeding with the installation.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n609\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7de204e220e3c70211a37f525fb5f14f'}>,\n",
              "  <Document: {'content': \"The script walks you through the installation steps and will prompt you for any required details.  For each query, the script will\\ndisplay the default value in square brackets.  If you press the \\nENTER\\n key, the script will use the default value. Else, enter an\\nalternate value.\\nRun the following commands: \\ncd <directory where you unzipped the monitoring-service-edge-chart-<version>.zip>/monitoring-service-edge-chart/scripts\\n./install.sh\\nThe script checks for K3s, ff K3s isn't installed in the environment, then the script will exit with the below message:\\n# ./install.sh\\nTerminating the script as K3S is not installed in the environment...\\nEnd user license\\nDo you agree with these terms in EULA (The EULA can be found at https://www.microfocus.com/en-\\nus/legal/software-licensing) (true/false)[false]:\\n Enter '\\ntrue'\\n if you agree with terms in the End User License Agreement.\\nUpload the required installation images \\nThe script uploads required installation images from \\n<directory where you unzipped the monitoring-service-edge-chart-<version>.zip>/mo\\nnitoring-service-edge-chart/offline_images.\\n\\u200b\\nImporting the required images...\\nunpacking docker.io/hpeswitomsandbox/itom-busybox:1.40.0-003 (sha256:8782e68c686efdd77871f0230def2abcf68168642d241e05351d1241dee8fe61)...done\\nunpacking docker.io/hpeswitomsandbox/itom-cdf-deployer:1.13.0-75 (sha256:24b5e486e5831f9770fbcdccb0fbceb2f8cd32f924bee9fe7f2acb54ef26c73d)...done\\nunpacking docker.io/hpeswitomsandbox/itom-credential-manager:1.17.0.13 (sha256:eb4558271cf6621df47b7dd1bdb893171c5b35dd98dd90d6eefca0c570158ec6)...done\\n.\\n.\\nTagging image itom-busybox:1.40.0-003 ... DONE\\nTagging image itom-cdf-deployer:1.13.0-75 ... DONE\\nTagging image itom-credential-manager:1.17.0.13 ... DONE\\n\\u200b\\nInstall OMT\\nThe script installs OMT 2022.11 from the \\n<directory where you unzipped the monitoring-service-edge-chart-<version>.zip>/monitoring-servi\\nce-edge-chart/omt.\\n \\nStarting the OMT installation\\n+ ./install --k8s-provider generic --capabilities Tools=true,Monitoring=false,LogCollection=false,DeploymentManagement=false,ClusterManagement=false --cdf-home /opt/cdf\\nWarning: tools will be copied to /opt/cdf.\\nAre you sure to continue? (Y/N): Y\\nTool copy done! Tools are copied to /opt/cdf.\\nEnter the password for admin user in Plain Text:\\nNote:\\n \\nExecute the i\\nnstall.sh \\nscript from the \\n<directory where you unzipped the monitoring-service-edge-chart-<version>.zip>/m\\nonitoring-service-edge-chart/scripts\\n directory only or else the script will fail.\\nRun the script using \\nsudo \\nwhile running as a non-root user. \\n\\ue916\\n\\ue916\\nNote: \\nProvide the password for OMT admin user, which is used for both grafana and idm UI logins.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n607\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '43872e66043cd7b7f4244a86dee4b867'}>,\n",
              "  <Document: {'content': '    https://SaaS-myexamplehost.com:30010/\\n    Agent Metric Collection is enabled, OBM host: clasicobm-hydra.swinfra.net\\nGrafana UI:\\n    https://myexamplehost.net:30443/grafana\\nPod status\\n \\nVerify pod status using the command:\\nkubectl get pod -n <edge namespace>\\n Example:\\nkubectl get pod -n monitoring-edge\\nEnable Monitoring after deployment:\\nIf you had chosen not to enable Prometheus and Grafana during the edge install, and at a later point of time want to enable\\nthe same, perform these steps:\\n1\\n. \\nCreate a new namespace (\\nomt\\n)\\n2\\n. \\nMake a copy of \\napphub-chart-values.yaml \\nunder /omt directory in a /tmp directory, then update these parameters:\\nexternal access host, external access port and the apphub admin password (Password is in Plain text)\\n        global:   \\n          externalAccessHost:\\n          externalAccessPort:\\n          apphubAdmin:\\n            userPassword:\\n3\\n. \\nExecute the following command to install the apphub chart which is present in the \\n/opt/cdf/charts\\n directory:\\nhelm install apphub </opt/cdf/charts/apphub-1.22.0+20221100.75.tgz> -n <namespace> -f /tmp/apphub-chart-values.yaml\\nUninstall\\nBackup collection configurations before uninstall\\nThis is applicable if you have used AMC for collection in OPTIC Reporting capability.\\nBefore uninstalling Monitoring Service Edge for OpsBridge, you need to backup custom configurations if any:\\nFollow the steps:\\n1\\n. \\nSet up the monitoring CLI for Agent Metric Collection \\n2\\n. \\nRun the following commands to export all custom configurations.\\n1\\n. \\nRun the following command to backup custom credentials, if any: \\nFor Linux:\\n./ops-monitoring-ctl get credentials -n <credential name> -o yaml  -f <file name>\\nFor Windows:\\nops-monitoring-ctl.exe get credentials -n <credential name> -o yaml  -f <file name>\\nExample:\\n./ops-monitoring-ctl get credentials -n custom_amc_obm_basic_auth -o yaml -f custom_amc_obm_basic_auth.yaml\\n2\\n. \\nRun the following command to backup custom targets, if any:\\nFor Linux:\\n./ops-monitoring-ctl get target -n <target name> -o yaml  -f <file name>\\nImportant\\n: Credential files created using the steps above will have the passwords masked. Please add\\nthe passwords before using the file\\nContainerized Operations Bridge 2022.11\\nPage \\n610\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4c8d5754a1dc0e399765b1d5d68071bd'}>,\n",
              "  <Document: {'content': 'For Windows:\\nops-monitoring-ctl.exe  get target -n <target name> -o yaml  -f <file name>\\nExample:\\n./ops-monitoring-ctl get target -n custom_amc_obm_rtsm -o yaml -f custom-amc_obm_rtsm.yaml\\n3\\n. \\nTake a copy of custom file for \\nnodefilter/proxy/ports/hosts\\n if any. For more information, see Modify the collection\\nattributes page.\\n4\\n. \\nRun the following command to backup custom collectors, if any:\\nFor Linux:\\n./ops-monitoring-ctl get coll -n <collector name> -o yaml  -f <file name>\\nFor Windows:\\nops-monitoring-ctl.exe get coll -n <collector name> -o yaml  -f <file name>\\nExample:\\n./ops-monitoring-ctl get coll -n custom-agent-collector-sysinfra -o yaml -f custom-agent-collector-sysinfra.yaml\\n                   Please refer Manage Agent Metric collection page for more information.\\nImportant\\n: Please remove the CreatedBy and CreatedDate fields before using the credential, target, and collector files.\\nUninstall monitoring service edge chart\\nTo uninstall the monitoring service edge chart, run the script with the \\n-u\\n option. If you run uninstall, the edge deployment,\\nnamespace, Apphub deployment, namespace, OMT, PVs and PVCs created by the script will get deleted.\\ncd <directory where you unzipped the monitoring-service-edge-chart-<version>.zip>/monitoring-service-edge-chart/scripts\\n./install.sh -u\\nExample:\\n./install.sh -u\\nUninstalling the edge chart\\nEnter the namespace under which you installed Edge chart [monitoring-edge] :\\nEnter the helm deployment name [monitoring-edge]:\\nUninstalling the deployment:monitoring-edge\\n+ helm uninstall monitoring-edge -n monitoring-edge --no-hooks\\nrelease \"monitoring-edge\" uninstalled\\nDeleting the namespace:monitoring-edge...\\nUninstalling the apphub chart\\nEnter the namespace under which you installed apphub chart [omt] :\\nUninstalling the deployment:apphub\\nDeleting the namespace:omt...\\nUninstalling OMT...\\nUninstall process would remove all OMT components.\\nAre you sure to uninstall OMT? (Y/N): Y\\nUninstall log: /tmp/uninstall.cdf.20220706082145.log\\nUninstallation completed...\\nContainerized Operations Bridge 2022.11\\nPage \\n611\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6f23eb05bd06930c93fd9a0850f9d83c'}>],\n",
              " 'root_node': 'File',\n",
              " 'params': {},\n",
              " 'file_paths': ['drive/My Drive/MF//Opsb_Corpus/opsb_Page_113.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_114.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_115.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_121.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_118.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_120.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_116.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_119.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_117.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_124.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_127.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_122.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_126.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_125.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_123.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_128.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_130.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_129.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_131.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_132.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_133.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_134.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_136.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_135.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_140.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_137.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_139.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_141.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_142.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_138.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_143.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_144.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_145.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_146.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_147.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_150.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_152.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_151.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_149.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_148.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_153.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_154.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_157.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_156.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_155.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_159.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_160.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_158.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_162.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_161.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_165.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_164.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_163.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_168.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_166.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_167.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_171.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_170.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_172.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_169.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_174.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_173.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_175.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_176.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_177.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_178.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_179.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_180.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_182.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_181.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_186.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_184.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_187.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_183.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_185.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_188.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_189.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_190.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_191.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_193.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_192.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_194.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_197.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_195.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_196.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_198.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_199.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_200.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_201.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_202.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_203.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_204.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_205.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_206.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_208.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_207.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_209.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_210.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_211.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_212.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_214.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_213.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_215.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_216.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_217.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_218.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_219.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_220.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_221.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_222.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_223.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_224.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_225.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_227.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_226.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_228.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_229.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_231.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_230.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_233.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_234.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_232.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_236.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_235.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_237.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_238.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_239.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_240.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_241.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_242.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_243.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_244.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_246.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_245.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_247.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_248.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_249.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_251.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_250.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_252.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_253.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_255.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_254.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_257.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_256.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_258.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_259.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_260.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_261.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_263.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_262.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_264.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_265.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_266.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_268.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_267.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_270.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_269.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_273.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_271.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_272.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_274.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_276.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_275.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_278.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_277.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_281.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_279.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_280.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_282.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_284.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_283.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_285.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_286.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_287.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_288.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_289.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_292.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_290.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_291.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_294.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_293.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_295.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_296.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_297.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_301.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_298.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_300.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_299.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_303.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_304.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_306.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_302.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_305.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_309.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_307.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_308.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_310.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_311.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_312.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_315.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_313.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_314.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_316.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_317.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_318.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_321.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_320.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_319.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_322.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_324.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_323.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_325.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_326.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_328.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_329.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_330.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_327.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_331.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_332.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_333.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_334.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_335.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_336.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_337.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_338.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_342.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_339.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_341.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_340.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_344.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_346.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_345.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_343.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_347.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_348.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_349.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_350.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_351.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_352.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_353.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_354.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_355.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_356.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_360.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_357.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_359.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_358.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_362.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_361.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_364.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_363.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_369.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_370.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_366.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_365.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_367.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_368.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_374.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_371.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_372.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_376.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_375.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_373.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_379.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_377.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_378.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_385.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_382.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_386.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_380.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_384.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_381.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_383.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_387.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_392.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_388.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_393.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_389.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_391.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_390.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_397.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_398.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_396.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_399.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_395.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_394.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_402.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_400.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_404.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_403.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_401.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_408.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_407.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_405.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_406.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_414.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_411.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_412.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_413.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_409.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_410.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_421.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_417.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_419.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_416.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_415.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_420.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_418.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_423.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_422.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_426.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_424.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_427.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_425.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_428.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_429.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_430.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_431.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_436.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_434.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_435.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_432.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_433.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_443.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_442.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_437.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_441.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_439.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_438.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_440.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_445.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_444.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_446.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_447.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_448.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_450.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_449.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_451.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_453.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_452.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_454.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_456.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_455.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_457.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_461.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_458.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_459.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_462.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_460.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_464.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_465.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_463.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_466.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_469.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_467.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_468.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_470.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_471.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_475.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_476.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_472.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_478.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_473.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_477.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_474.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_482.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_480.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_479.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_481.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_483.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_488.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_487.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_485.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_484.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_486.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_490.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_491.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_493.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_489.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_492.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_498.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_495.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_497.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_496.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_494.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_499.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_500.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_505.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_507.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_503.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_506.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_501.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_504.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_508.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_502.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_511.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_512.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_510.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_509.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_513.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_516.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_514.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_515.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_517.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_519.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_520.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_518.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_521.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_526.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_522.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_527.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_525.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_524.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_523.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_528.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_530.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_529.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_531.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_532.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_534.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_535.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_533.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_538.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_537.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_536.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_544.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_542.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_539.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_541.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_540.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_543.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_546.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_547.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_545.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_554.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_549.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_552.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_550.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_548.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_551.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_553.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_556.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_555.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_558.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_562.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_563.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_557.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_561.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_560.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_559.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_566.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_565.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_564.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_569.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_570.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_567.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_568.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_572.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_574.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_573.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_571.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_581.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_577.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_582.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_579.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_583.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_578.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_584.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_580.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_576.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_575.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_591.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_589.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_585.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_592.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_593.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_590.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_586.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_587.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_588.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_594.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_595.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_597.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_599.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_596.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_598.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_600.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_606.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_605.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_604.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_603.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_601.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_602.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_608.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_609.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_607.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_610.txt',\n",
              "  'drive/My Drive/MF//Opsb_Corpus/opsb_Page_611.txt'],\n",
              " 'node_id': 'DocumentStore'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#search system will use a Retriever, so we need to initialize it. \n",
        "#A Retriever sifts through all the Documents and returns only the ones relevant to the question. \n",
        "#This tutorial uses the BM25 algorithm\n",
        "\n",
        "from haystack.nodes import BM25Retriever\n",
        "\n",
        "retriever = BM25Retriever(document_store=document_store)\n"
      ],
      "metadata": {
        "id": "O17klHlQWa3o"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the Reader\n",
        "#A Reader scans the texts it received from the Retriever and extracts the top answer candidates. \n",
        "\n",
        "from haystack.nodes import FARMReader\n",
        "\n",
        "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oAk4TO5Ws9P",
        "outputId": "cea7f51e-862e-407b-8944-53e9afebec75"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
            "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
            "INFO:haystack.modeling.model.language_model: * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)\n",
            "INFO:haystack.modeling.model.language_model:Auto-detected model language: english\n",
            "INFO:haystack.modeling.model.language_model:Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.\n",
            "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "\n",
        "querying_pipeline = Pipeline()\n",
        "querying_pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
        "querying_pipeline.add_node(component=reader, name=\"Reader\", inputs=[\"Retriever\"])"
      ],
      "metadata": {
        "id": "VzGWanYaqjIz"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline’s ready, you can now go ahead and ask a question!"
      ],
      "metadata": {
        "id": "oyXi_nD5XQUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = querying_pipeline.run(\n",
        "    query=\"Tell me about Opsb event correlation ?\",\n",
        "    params={\n",
        "        \"Retriever\": {\"top_k\": 10},\n",
        "        \"Reader\": {\"top_k\": 5}\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "lRwQ2B9cw6si",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "9193c8ccb6504e0d84d54358bdbc3b89",
            "ff2f0fd9914b4097bd2a411a394a1ce2",
            "5d3048aeb0f648108bc665a7925c1ef5",
            "de2940844f5943ee83a3b8d68c384684",
            "81a763d0b72a41d2b0dc5ab5b667e7b4",
            "e05eba20f04240f48455fc79bbb0f13c",
            "0d5949a7b40249519427e622f52346a1",
            "b2dbab82e4fa487ca1010bf747ad9cb7",
            "ff9b2831c8c94e40a98da5a40eca89a3",
            "f5026d6d9e364daab5126e2b261687da",
            "2ac3ab887e1145219c91020dd7d5161a"
          ]
        },
        "outputId": "b4472a63-9a70-4c9a-f030-834f5b8e2af6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inferencing Samples:   0%|          | 0/4 [00:00<?, ? Batches/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9193c8ccb6504e0d84d54358bdbc3b89"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "pprint(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYO1Ft7DraU0",
        "outputId": "72749bdb-8551-4529-cb76-69f0105aa2f2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'answers': [<Answer {'answer': 'Automatic Event Correlation', 'type': 'extractive', 'score': 0.26186585426330566, 'context': 'holder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerize', 'offsets_in_document': [{'start': 166, 'end': 193}], 'offsets_in_context': [{'start': 62, 'end': 89}], 'document_ids': ['5fc52982237960a004fc3919c6add461'], 'meta': {}}>,\n",
            "             <Answer {'answer': 'Automatic', 'type': 'extractive', 'score': 0.23116007447242737, 'context': 'es:\\nStakeholder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nCon', 'offsets_in_document': [{'start': 11368, 'end': 11377}], 'offsets_in_context': [{'start': 71, 'end': 80}], 'document_ids': ['5fc52982237960a004fc3919c6add461'], 'meta': {}}>,\n",
            "             <Answer {'answer': 'Automatic Event Correlation\\nAgentless Monitoring', 'type': 'extractive', 'score': 0.17825287580490112, 'context': 'boards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerized OBM\\nAnoma', 'offsets_in_document': [{'start': 5767, 'end': 5815}], 'offsets_in_context': [{'start': 51, 'end': 99}], 'document_ids': ['5fc52982237960a004fc3919c6add461'], 'meta': {}}>,\n",
            "             <Answer {'answer': 'Rule based event correlation', 'type': 'extractive', 'score': 0.16348880529403687, 'context': 'n pattern or identifier. There are two types of\\ncorrelation:\\nRule based event correlation - In this case, you can specify the rules or parameters base', 'offsets_in_document': [{'start': 9252, 'end': 9280}], 'offsets_in_context': [{'start': 61, 'end': 89}], 'document_ids': ['a774b8fc941f72f1a4e02f767573a42c'], 'meta': {}}>,\n",
            "             <Answer {'answer': \"you don't need to specify any rules\", 'type': 'extractive', 'score': 0.14591346681118011, 'context': \"erized OBM.  \\nAutomatic event correlation - In this case, you don't need to specify any rules. The Automatic Event Correlation service\\n(AEC) is a part\", 'offsets_in_document': [{'start': 9614, 'end': 9649}], 'offsets_in_context': [{'start': 58, 'end': 93}], 'document_ids': ['a774b8fc941f72f1a4e02f767573a42c'], 'meta': {}}>],\n",
            " 'documents': [<Document: {'content': \"Parameter group\\nValues\\nDescription\\nbvd.deployment.database.dbName\\nDefault value: \\nbvd\\nGive \\nbvd \\ndatabase name.\\nbvd.deployment.database.user\\nDefault value: \\nbvd\\nGive the user name for \\nbvd\\n database.\\nbvd.deployment.database.userPasswordKey\\nDefault value:\\nBVD_DB_USER_PASSWORD_KEY\\nThis is set while running \\ngen_secrets.sh\\nscript and refers to the password for \\nb\\nvd\\n database (PostgreSQL/Oracle) user.\\nOracle uses this as \\nbvd\\n schema\\npassword. Don't change\\n userPasswordKe\\ny\\n. \\nbvd.smtpServer.host\\nYou must give these parameters\\nto schedule the reports to a specified\\nemail id. Give the \\nsmtpServer\\n host\\nname.\\nbvd.smtpServer.port\\nGive the \\nsmtpServer\\n port.\\nbvd.smtpServer.security\\nGive the value as \\nTLS\\n or \\nSTARTTLS\\n.\\nbvd.smtpServer.user\\nGive the user id for the \\nsmtpServer\\n.\\nbvd.smtpServer.from\\nGive the email id for the \\nsmtpServer use\\nd \\nto send the emails\\nDepending upon the SMTP server\\n'from' and 'user' fields can be the\\nsame or different. \\nbvd.smtpServer.passwordKey\\nDefault value:\\nschedule_mail_password_key\\nThis is set while running \\ngen_secrets.sh\\nscript and refers to the password for \\ns\\nmtpServer\\n (PostgreSQL/Oracle) user.\\nDon't change \\npasswordKey\\n. This is the\\npassword key for the mail Proxy user\\noracleWallet\\nPass the oracle wallet zip's base64\\nfile \\nOBM settings\\nParameter group\\nValues\\nDescription\\nobm.deployment.eventDatabase.dbNa\\nme\\nDefault value: \\nobm_event\\nGive PostgreSQL database name\\nfor the event database.\\nobm.deployment.mgmtDatabase.dbNa\\nme\\nDefault value: \\nobm_mgmt\\nGive PostgreSQL database name\\nfor the management (\\nmgmt\\n)\\ndatabase.\\nobm.deployment.eventDatabase.user\\nDefault value: \\nobm_event\\nGive Oracle user name for the\\nevent database.\\nobm.deployment.mgmtDatabase.user\\nDefault value: \\nobm_mgmt\\nGive Oracle user name for the\\nmanagement database.\\nobm.deployment.mgmtDatabase.userP\\nasswordKey\\nDefault value:\\nOBM_MGMT_DB_USER_PASSWORD_KEY\\nThis is set while running\\ngen_secrets.sh script and refers\\nto the password for \\nmgmt\\ndatabase (PostgreSQL/Oracle)\\nuser. Oracle uses this as  \\nmgmt\\nschema password. Don't change\\nuserPasswordKey\\n. \\nobm.deployment.eventDatabase.userP\\nasswordKey\\nDefault\\nvalue: \\nOBM_EVENT_DB_USER_PASSWORD_KEY\\nThis is set while running\\ngen_secrets.sh script and refers\\nto the password for \\nevent\\ndatabase (PostgreSQL/Oracle)\\nuser. Oracle uses this as\\nevent\\n schema password. Don't\\nchange\\n userPasswordKey\\n. \\nobm.bbc.port\\nDefault value: \\n383\\nThe BBC port used by the OBM\\nserver.\\nImportant\\n:\\n If you are using an existing Shared OPTIC Data Lake, and have changed the \\nbvd\\n database and\\nuser names based on the deployment scenario, ensure that you provide the correct user and database names in\\nthis section. For example, If you are using an existing Shared OPTIC Data Lake from NOM, and your Operations\\nBridge username and database names are  \\nbvd-opsb\\n, provide the same in this section.\\nIf you are planning to Configure Email for Dashboard and Report Schedules using SMTP, \\nbvd.smtpServer.*\\nparameters mentioned in the below table are \\nmandatory\\n.\\n\\ue91b\\n\\ue91b\\nContainerized Operations Bridge 2022.11\\nPage \\n274\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': 10.397209763947675, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fb142c529a18280c9849e337d0e7fb3f'}>,\n",
            "               <Document: {'content': \"Data Lake as under:\\n1\\n. \\nSystem infrastructure metrics from Operations Agent and SiteScope\\n2\\n. \\nSynthetic monitoring metrics from Business Process Monitor (BPM)\\n3\\n. \\nReal user metrics from Real User Monitor (RUM) \\n4\\n. \\nApplication metrics from Management Packs\\n5\\n. \\nTopology information (from OBM/RTSM via Data Flow Probe)\\n6\\n. \\nNetwork Management data (in case of shared OPTIC Data Lake between Operations Bridge and NOM)\\n7\\n. \\nDCA data (in case of shared OPTIC Data Lake between Operations Bridge and DCA)\\n8\\n. \\nCustom Data Ingestion (e.g. using Open Data Ingestion API or Custom Metric Ingestion Tool)\\nData enrichment \\nA component that receives the metrics from data sources, enriches them with CI and downtime information, and forwards\\nthem to OPTIC Data Lake.\\nIt enriches data for the following data collectors:\\nManagement Packs\\nSiteScope\\nFor more information on data enrichment, see \\nEnrich fields in OPTIC Data Lake using Data Enrichment Service\\n.\\nData retention\\nOPTIC Data Lake uses Vertica database to manage such a huge volume of data. Over time, the Vertica database accumulates\\nan enormous amount of data from various data sources. You may need to delete the old and unused data periodically to free\\nup storage space.\\nYou can configure retention profiles per source or content category such that data older than the configured period is\\nautomatically deleted from Vertica database. \\nFor more information about configuring data retention, see \\nCustomize Data Retention\\n.\\nRAW and Aggregated data (e.g. hourly, daily, forecasting) are stored in Vertica to ease the reporting. The data aggregation\\nand forecasting are configured by corresponding taskflows triggered by OPTIC Data Lake.\\nFor more information about Reporting data and task flows, see \\nDataAndTaskFlows\\n.\\nData source resiliency\\nWhen the Optic DL is unavailable and therefore unable to ingest metrics collected by Agent Metric Collector (AMC), AWS\\ncollector, Azure Collector, or Kubernetes collectors, the metrics are temporarily stored in the Store and forward database. After\\nthe OPTIC DL is up and running, the metrics stored in the Store and forward database are streamed to the Optic DL.\\nFor other data sources (Operations Agent, SiteScope, Management Packs, BPM, and RUM), the metrics are stored in their local\\nbuffers during OPTIC DL outage and are streamed to the OPTIC DL after it's up and running.\\nThe buffering mechanism for other data types is as follows:\\nHealth Indicator, KPI, and Event data are buffered in OBM’s relational database.\\nTopology data is forwarded to OPTIC DL using the Data Flow Probe. The Data Flow Probe stores the timestamp of the last\\nsuccessful sync. Once OPTIC DL starts accepting data again, the Data Flow Probe reads the topology data from the RTSM\\nserver and forwards it to OPTIC DL.\\nDowntime data is buffered in OBM's message bus.\\nFor information about configuring resiliency, see \\nConfigure data resiliency\\n. \\nEvent and performance management \\nAll events and metrics collected by different Operations Agent instances are sent to OBM. OBM stores these events and\\nmetrics in its own database. You can view the performance metrics using Performance Dashboards and events in the Events\\nconsole. This consolidated view helps you identify, isolate, and remediate issues before they become showstoppers.\\nData analytics\\nYou can generate meaningful insights from all the data stored in OPTIC Data Lake using analytics.\\nEvent correlation\\nIn a large environment, one of the biggest challenges is how to manage the large number of events that originate from a\\nvariety of sources. The aim is to identify the events that have a significant impact on business services. While it's essential to\\nminimize the number of events that appear, it's even more important to highlight the events that, if not managed\\nappropriately, could cause a breach in service level agreements (SLAs) and generate incidents in your help desk system.\\nEvent correlation entails the grouping of events based on some common pattern or identifier. There are two types of\\ncorrelation:\\nRule based event correlation - In this case, you can specify the rules or parameters based on which events get grouped or\\ncorrelated. You can use two rules to correlate events - one based on topology and the other based on the event stream.\\nRule based correlation is available in both classic and containerized OBM.  \\nAutomatic event correlation - In this case, you don't need to specify any rules. The Automatic Event Correlation service\\n(AEC) is a part of the Automatic Event Correlation capability. It identifies patterns among disparate events and creates a\\nsingle correlated event for a group of events. AEC is available if you install the Automatic Event Correlation capability.\\nAutomatic Event Correlation\\nAutomatic Event Correlation identifies repeated patterns in the incoming event flow over a period of time. It creates a single\\ncorrelated event for all events within a pattern to represent the entire group of events. This grouping or correlation helps to\\nContainerized Operations Bridge 2022.11\\nPage \\n117\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Data Lake as under:\\n1\\n. \\nSystem infrastructure metrics from Operations Agent and SiteScope\\n2\\n. \\nSynthetic monitoring metrics from Business Process Monitor (BPM)\\n3\\n. \\nReal user metrics from Real User Monitor (RUM) \\n4\\n. \\nApplication metrics from Management Packs\\n5\\n. \\nTopology information (from OBM/RTSM via Data Flow Probe)\\n6\\n. \\nNetwork Management data (in case of shared OPTIC Data Lake between Operations Bridge and NOM)\\n7\\n. \\nDCA data (in case of shared OPTIC Data Lake between Operations Bridge and DCA)\\n8\\n. \\nCustom Data Ingestion (e.g. using Open Data Ingestion API or Custom Metric Ingestion Tool)\\nData enrichment \\nA component that receives the metrics from data sources, enriches them with CI and downtime information, and forwards\\nthem to OPTIC Data Lake.\\nIt enriches data for the following data collectors:\\nManagement Packs\\nSiteScope\\nFor more information on data enrichment, see \\nEnrich fields in OPTIC Data Lake using Data Enrichment Service\\n.\\nData retention\\nOPTIC Data Lake uses Vertica database to manage such a huge volume of data. Over time, the Vertica database accumulates\\nan enormous amount of data from various data sources. You may need to delete the old and unused data periodically to free\\nup storage space.\\nYou can configure retention profiles per source or content category such that data older than the configured period is\\nautomatically deleted from Vertica database. \\nFor more information about configuring data retention, see \\nCustomize Data Retention\\n.\\nRAW and Aggregated data (e.g. hourly, daily, forecasting) are stored in Vertica to ease the reporting. The data aggregation\\nand forecasting are configured by corresponding taskflows triggered by OPTIC Data Lake.\\nFor more information about Reporting data and task flows, see \\nDataAndTaskFlows\\n.\\nData source resiliency\\nWhen the Optic DL is unavailable and therefore unable to ingest metrics collected by Agent Metric Collector (AMC), AWS\\ncollector, Azure Collector, or Kubernetes collectors, the metrics are temporarily stored in the Store and forward database. After\\nthe OPTIC DL is up and running, the metrics stored in the Store and forward database are streamed to the Optic DL.\\nFor other data sources (Operations Agent, SiteScope, Management Packs, BPM, and RUM), the metrics are stored in their local\\nbuffers during OPTIC DL outage and are streamed to the OPTIC DL after it's up and running.\\nThe buffering mechanism for other data types is as follows:\\nHealth Indicator, KPI, and Event data are buffered in OBM’s relational database.\\nTopology data is forwarded to OPTIC DL using the Data Flow Probe. The Data Flow Probe stores the timestamp of the last\\nsuccessful sync. Once OPTIC DL starts accepting data again, the Data Flow Probe reads the topology data from the RTSM\\nserver and forwards it to OPTIC DL.\\nDowntime data is buffered in OBM's message bus.\\nFor information about configuring resiliency, see \\nConfigure data resiliency\\n. \\nEvent and performance management \\nAll events and metrics collected by different Operations Agent instances are sent to OBM. OBM stores these events and\\nmetrics in its own database. You can view the performance metrics using Performance Dashboards and events in the Events\\nconsole. This consolidated view helps you identify, isolate, and remediate issues before they become showstoppers.\\nData analytics\\nYou can generate meaningful insights from all the data stored in OPTIC Data Lake using analytics.\\nEvent correlation\\nIn a large environment, one of the biggest challenges is how to manage the large number of events that originate from a\\nvariety of sources. The aim is to identify the events that have a significant impact on business services. While it's essential to\\nminimize the number of events that appear, it's even more important to highlight the events that, if not managed\\nappropriately, could cause a breach in service level agreements (SLAs) and generate incidents in your help desk system.\\nEvent correlation entails the grouping of events based on some common pattern or identifier. There are two types of\\ncorrelation:\\nRule based event correlation - In this case, you can specify the rules or parameters based on which events get grouped or\\ncorrelated. You can use two rules to correlate events - one based on topology and the other based on the event stream.\\nRule based correlation is available in both classic and containerized OBM.  \\nAutomatic event correlation - In this case, you don't need to specify any rules. The Automatic Event Correlation service\\n(AEC) is a part of the Automatic Event Correlation capability. It identifies patterns among disparate events and creates a\\nsingle correlated event for a group of events. AEC is available if you install the Automatic Event Correlation capability.\\nAutomatic Event Correlation\\nAutomatic Event Correlation identifies repeated patterns in the incoming event flow over a period of time. It creates a single\\ncorrelated event for all events within a pattern to represent the entire group of events. This grouping or correlation helps to\\nContainerized Operations Bridge 2022.11\\nPage \\n117\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': 10.044598859369543, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a774b8fc941f72f1a4e02f767573a42c'}>,\n",
            "               <Document: {'content': \"Key capabilities and concepts\\nContainerized Operations Bridge provides the following capabilities:\\nStakeholder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerized OBM\\nAnomaly Detection\\n* The administration and configuration of these capabilities are supported through Monitoring Control Center (MCC). \\nStakeholder Dashboards \\nThe stakeholder dashboard gives you visual information of live data using tables, charts, and widgets. You can stream real-\\ntime data from any data source in JSON format via HTTP post. Stakeholder Dashboards can integrate with a variety of data\\nsources and enable you to represent data in several valuable perspectives. You can create custom dashboards, near real-time\\ndashboards, receive real-time updates, and access information from any device with a browser. \\nOPTIC Reporting \\nThe OPTIC Reporting capability provides you with all the artifacts required for IT infrastructure and event management. It\\nconsists of the following components - Operations Bridge Reporting Content, BVD Reports, Flex Reports and OPTIC Data\\nLake. It consolidates performance metrics, event metrics, and response time data into tables, graphs, and dashboards and\\nenables you to visualize and analyze your IT environment. OPTIC Reporting uses OPTIC One to create reports and visualize the\\nvisualize the real time data of the IT infrastructure. You can create the following reports. \\nBVD reports: BVD reports display visual information of historical or recorded data using tables, charts, and widgets. You\\ncan use Visio to create dashboards and attach data from data queries, which can retrieve data from any Vertica database\\nusing SQL queries. BVD reports are created in OPTIC One. \\nFlex reports: Flex Reports is web-based authoring that allows you to create dashboards and reports (eliminates the use of\\ntools such as visio). You can create and customize reports and dashboards based on your business needs.  Flex reports\\nare created in OPTIC One.\\nBYOBI reports: You can create custom reports using using the Business Intelligence (BI) tool of your choice. \\nYou can send data (performance metrics, events, and topology) from different sources to OPTIC Data Lake. You can represent\\nthis data graphically either on BVD or any other business intelligence tool (BYOBI) of your choice.\\nOperations Bridge offers several out of the box reports that you can view on BVD. These reports are available in the following\\ncategories: \\nSystem Infrastructure reports\\n: Use these reports to view information about the availability and performance of\\nphysical systems in your environment. You can also view information about the average utilization of resources for a\\nchosen period of time. You can use the Agent Metric Collector, Metric Streaming policies, or SiteScope to collect system\\ninfrastructure metrics. You can view historical data collected by AMC in the System Infrastructure reports. \\nSystem Infrastructure reports are downtime aware. The metrics received during planned downtime are excluded from\\ncalculations performed while aggregating data. \\nDowntime aware reports are currently available for system infrastructure metrics collected through AMC and SiteScope.\\nThey're not supported for Metric Streaming policies, BPM, and RUM.\\nEvent reports\\n: Use these reports to view statistics about the events that come into OBM from various data sources like\\nAgent metric collector, SiteScope, RUM, and BPM. Using these reports, you can gain insight into event trends, the severity\\nof events, and event assignments.\\nSynthetic Transaction reports\\n: Use these reports to view information about synthetic polling of end user experience,\\navailability, and performance of applications. You can use Business Process Monitor (BPM) to collect synthetic transaction\\nmetrics. \\nReal User Monitor reports\\n: Use these reports to view information about real end user experience, availability, and\\nperformance of applications. You can use Real User Monitor (RUM) to monitor real user behavior and collect the metrics.\\nAutomatic Event Correlation\\nThe Automatic Event Correlation capability provides you with all the artifacts required for event management, event\\nconsolidation, and noise reduction. This capability consists of OPTIC Data Lake.\\nAutomatic Event Correlation offers the facility to automatically correlate events coming from OBM, by analyzing patterns. This\\nautomatic event correlation (AEC) uses a machine learning algorithm to group related events into a single event and sends the\\ncorrelated event back to OBM. Thus, as an operator viewing the event console in OBM, you will be able to identify key events\\nthat can solve many underlying events.\\nMonitoring Control Center\\nMonitoring Control Center (MCC) is a new section of the Operations Bridge UI that consolidates the administration and\\nconfiguration of monitoring capabilities into a central user experience. MCC provides the following capabilities:\\nAgentless Monitoring\\nHyperscale Observability\\nAgentless Monitoring\\nAgentless Monitoring enables you to gain an overview about the health, availability, and performance of a hybrid set of\\nsystems and applications deployed on-premise and cloud in your infrastructure. The solution provides a single view of the\\nhealth, availability, and performance metrics from multiple infrastructure monitoring solutions, such as SiteScope.\\nContainerized Operations Bridge 2022.11\\nPage \\n114\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Key capabilities and concepts\\nContainerized Operations Bridge provides the following capabilities:\\nStakeholder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerized OBM\\nAnomaly Detection\\n* The administration and configuration of these capabilities are supported through Monitoring Control Center (MCC). \\nStakeholder Dashboards \\nThe stakeholder dashboard gives you visual information of live data using tables, charts, and widgets. You can stream real-\\ntime data from any data source in JSON format via HTTP post. Stakeholder Dashboards can integrate with a variety of data\\nsources and enable you to represent data in several valuable perspectives. You can create custom dashboards, near real-time\\ndashboards, receive real-time updates, and access information from any device with a browser. \\nOPTIC Reporting \\nThe OPTIC Reporting capability provides you with all the artifacts required for IT infrastructure and event management. It\\nconsists of the following components - Operations Bridge Reporting Content, BVD Reports, Flex Reports and OPTIC Data\\nLake. It consolidates performance metrics, event metrics, and response time data into tables, graphs, and dashboards and\\nenables you to visualize and analyze your IT environment. OPTIC Reporting uses OPTIC One to create reports and visualize the\\nvisualize the real time data of the IT infrastructure. You can create the following reports. \\nBVD reports: BVD reports display visual information of historical or recorded data using tables, charts, and widgets. You\\ncan use Visio to create dashboards and attach data from data queries, which can retrieve data from any Vertica database\\nusing SQL queries. BVD reports are created in OPTIC One. \\nFlex reports: Flex Reports is web-based authoring that allows you to create dashboards and reports (eliminates the use of\\ntools such as visio). You can create and customize reports and dashboards based on your business needs.  Flex reports\\nare created in OPTIC One.\\nBYOBI reports: You can create custom reports using using the Business Intelligence (BI) tool of your choice. \\nYou can send data (performance metrics, events, and topology) from different sources to OPTIC Data Lake. You can represent\\nthis data graphically either on BVD or any other business intelligence tool (BYOBI) of your choice.\\nOperations Bridge offers several out of the box reports that you can view on BVD. These reports are available in the following\\ncategories: \\nSystem Infrastructure reports\\n: Use these reports to view information about the availability and performance of\\nphysical systems in your environment. You can also view information about the average utilization of resources for a\\nchosen period of time. You can use the Agent Metric Collector, Metric Streaming policies, or SiteScope to collect system\\ninfrastructure metrics. You can view historical data collected by AMC in the System Infrastructure reports. \\nSystem Infrastructure reports are downtime aware. The metrics received during planned downtime are excluded from\\ncalculations performed while aggregating data. \\nDowntime aware reports are currently available for system infrastructure metrics collected through AMC and SiteScope.\\nThey're not supported for Metric Streaming policies, BPM, and RUM.\\nEvent reports\\n: Use these reports to view statistics about the events that come into OBM from various data sources like\\nAgent metric collector, SiteScope, RUM, and BPM. Using these reports, you can gain insight into event trends, the severity\\nof events, and event assignments.\\nSynthetic Transaction reports\\n: Use these reports to view information about synthetic polling of end user experience,\\navailability, and performance of applications. You can use Business Process Monitor (BPM) to collect synthetic transaction\\nmetrics. \\nReal User Monitor reports\\n: Use these reports to view information about real end user experience, availability, and\\nperformance of applications. You can use Real User Monitor (RUM) to monitor real user behavior and collect the metrics.\\nAutomatic Event Correlation\\nThe Automatic Event Correlation capability provides you with all the artifacts required for event management, event\\nconsolidation, and noise reduction. This capability consists of OPTIC Data Lake.\\nAutomatic Event Correlation offers the facility to automatically correlate events coming from OBM, by analyzing patterns. This\\nautomatic event correlation (AEC) uses a machine learning algorithm to group related events into a single event and sends the\\ncorrelated event back to OBM. Thus, as an operator viewing the event console in OBM, you will be able to identify key events\\nthat can solve many underlying events.\\nMonitoring Control Center\\nMonitoring Control Center (MCC) is a new section of the Operations Bridge UI that consolidates the administration and\\nconfiguration of monitoring capabilities into a central user experience. MCC provides the following capabilities:\\nAgentless Monitoring\\nHyperscale Observability\\nAgentless Monitoring\\nAgentless Monitoring enables you to gain an overview about the health, availability, and performance of a hybrid set of\\nsystems and applications deployed on-premise and cloud in your infrastructure. The solution provides a single view of the\\nhealth, availability, and performance metrics from multiple infrastructure monitoring solutions, such as SiteScope.\\nContainerized Operations Bridge 2022.11\\nPage \\n114\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.Key capabilities and concepts\\nContainerized Operations Bridge provides the following capabilities:\\nStakeholder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerized OBM\\nAnomaly Detection\\n* The administration and configuration of these capabilities are supported through Monitoring Control Center (MCC). \\nStakeholder Dashboards \\nThe stakeholder dashboard gives you visual information of live data using tables, charts, and widgets. You can stream real-\\ntime data from any data source in JSON format via HTTP post. Stakeholder Dashboards can integrate with a variety of data\\nsources and enable you to represent data in several valuable perspectives. You can create custom dashboards, near real-time\\ndashboards, receive real-time updates, and access information from any device with a browser. \\nOPTIC Reporting \\nThe OPTIC Reporting capability provides you with all the artifacts required for IT infrastructure and event management. It\\nconsists of the following components - Operations Bridge Reporting Content, BVD Reports, Flex Reports and OPTIC Data\\nLake. It consolidates performance metrics, event metrics, and response time data into tables, graphs, and dashboards and\\nenables you to visualize and analyze your IT environment. OPTIC Reporting uses OPTIC One to create reports and visualize the\\nvisualize the real time data of the IT infrastructure. You can create the following reports. \\nBVD reports: BVD reports display visual information of historical or recorded data using tables, charts, and widgets. You\\ncan use Visio to create dashboards and attach data from data queries, which can retrieve data from any Vertica database\\nusing SQL queries. BVD reports are created in OPTIC One. \\nFlex reports: Flex Reports is web-based authoring that allows you to create dashboards and reports (eliminates the use of\\ntools such as visio). You can create and customize reports and dashboards based on your business needs.  Flex reports\\nare created in OPTIC One.\\nBYOBI reports: You can create custom reports using using the Business Intelligence (BI) tool of your choice. \\nYou can send data (performance metrics, events, and topology) from different sources to OPTIC Data Lake. You can represent\\nthis data graphically either on BVD or any other business intelligence tool (BYOBI) of your choice.\\nOperations Bridge offers several out of the box reports that you can view on BVD. These reports are available in the following\\ncategories: \\nSystem Infrastructure reports\\n: Use these reports to view information about the availability and performance of\\nphysical systems in your environment. You can also view information about the average utilization of resources for a\\nchosen period of time. You can use the Agent Metric Collector, Metric Streaming policies, or SiteScope to collect system\\ninfrastructure metrics. You can view historical data collected by AMC in the System Infrastructure reports. \\nSystem Infrastructure reports are downtime aware. The metrics received during planned downtime are excluded from\\ncalculations performed while aggregating data. \\nDowntime aware reports are currently available for system infrastructure metrics collected through AMC and SiteScope.\\nThey're not supported for Metric Streaming policies, BPM, and RUM.\\nEvent reports\\n: Use these reports to view statistics about the events that come into OBM from various data sources like\\nAgent metric collector, SiteScope, RUM, and BPM. Using these reports, you can gain insight into event trends, the severity\\nof events, and event assignments.\\nSynthetic Transaction reports\\n: Use these reports to view information about synthetic polling of end user experience,\\navailability, and performance of applications. You can use Business Process Monitor (BPM) to collect synthetic transaction\\nmetrics. \\nReal User Monitor reports\\n: Use these reports to view information about real end user experience, availability, and\\nperformance of applications. You can use Real User Monitor (RUM) to monitor real user behavior and collect the metrics.\\nAutomatic Event Correlation\\nThe Automatic Event Correlation capability provides you with all the artifacts required for event management, event\\nconsolidation, and noise reduction. This capability consists of OPTIC Data Lake.\\nAutomatic Event Correlation offers the facility to automatically correlate events coming from OBM, by analyzing patterns. This\\nautomatic event correlation (AEC) uses a machine learning algorithm to group related events into a single event and sends the\\ncorrelated event back to OBM. Thus, as an operator viewing the event console in OBM, you will be able to identify key events\\nthat can solve many underlying events.\\nMonitoring Control Center\\nMonitoring Control Center (MCC) is a new section of the Operations Bridge UI that consolidates the administration and\\nconfiguration of monitoring capabilities into a central user experience. MCC provides the following capabilities:\\nAgentless Monitoring\\nHyperscale Observability\\nAgentless Monitoring\\nAgentless Monitoring enables you to gain an overview about the health, availability, and performance of a hybrid set of\\nsystems and applications deployed on-premise and cloud in your infrastructure. The solution provides a single view of the\\nhealth, availability, and performance metrics from multiple infrastructure monitoring solutions, such as SiteScope.\\nContainerized Operations Bridge 2022.11\\nPage \\n114\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': 10.043838885144673, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5fc52982237960a004fc3919c6add461'}>,\n",
            "               <Document: {'content': \"Get Integration Tools\\nThe OBM integration tools are required to:\\n1\\n. \\nConfigure a secure connection between OBM and OPTIC Data Lake.\\n2\\n. \\nConfigure event forwarding from OBM to OPTIC Data Lake (Both Reporting and Automatic Event Correlation capabilities\\nuse event forwarding)\\n3\\n. \\nConfigure Automatic Event Correlation\\nFollow the steps on the suite master node to get the integration tools:\\n1\\n. \\nThe \\nopsb-obm-integration-tools.zip\\n file is present in the \\nstatic-files-provider\\n container of the \\nopsb-resource-bundle\\n pod.\\nTo get the zip file, run the following command on the master (control plane) node:\\nwget https://<externalAccessHost>:<externalAccessPort>/staticfiles/opsb-obm-integration-tools.zip --no-check-certificate\\nFor example: \\nwget https://myhost.mycompany.net:443/staticfiles/opsb-obm-integration-tools.zip --no-check-certificate\\n2\\n. \\nRun the following commands to extract the contents of \\nopsb-obm-integration-tools.zip\\n to the \\nintegration-tools\\n directory: \\nunzip opsb-obm-integration-tools.zip -d integration-tools\\ncd integration-tools\\nYou will use the tools in the \\nintegration-tools\\n directory to perform the post-installation tasks.\\nNote\\n: On cloud deployments, perform the tasks on the bastion node instead of the control plane nodes.\\n\\ue916\\n\\ue916\\nTip: \\nRun the following command to get the \\n<externalAccessPort>\\n:\\nhelm get values -n $(helm list -A | grep opsbridge | awk '{print $2,$1}') | grep -i externalAccess\\nCommand output:\\nexternalAccessHost: mycomp.mysystem.net\\nexternalAccessPort: 443\\nContainerized Operations Bridge 2022.11\\nPage \\n294\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': 9.517809480715293, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9a601686daf6759a3e9b6aaad4b23a8e'}>,\n",
            "               <Document: {'content': \"1\\n. \\nIn the Integration studio, select the Integration Point (Pulsar Push Job) and click \\nEdit\\n.\\n2\\n. \\nIn the \\nEdit Integration Job \\nwindow, in the \\nDelta Synchronization \\ntab, and select the \\nScheduler enabled\\ncheck box.\\n3\\n. \\nSelect a specific interval of your choice and click \\nOK\\n.\\n4\\n. \\nClick \\nRefresh.\\nRelated topics\\nTo enable automatic event correlation, see \\nConfigure Automatic Event Correlation\\n.\\nFor more information about the UD content pack installation, see \\nContent Pack Installation\\n.\\nTo customize the data retention, see \\nCustomize data retention\\n.\\nNote:\\nFor every full or delta synchronization, the CI details including attribute changes are sent from RTSM to OPTIC\\nData Lake.\\nIf a CI gets deleted from RTSM, the status of the CI gets updated as 'deleted' but the CI isn't deleted from\\nOPTIC Data Lake. \\nContainerized Operations Bridge 2022.11\\nPage \\n330\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': 8.900911334765585, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1556a69b7573f135b0bb62d45e468ea5'}>,\n",
            "               <Document: {'content': \"Integrate OpsBridge with OBM\\nThis topic gives you steps to establish trust between the connected OBM and the OpsBridge deployment. \\nThe trust establishment enables the flow of metrics, events, and topology to OpsBridge deployment. \\nThe supported versions of OBM are:\\n2022.05\\n2021.11\\n2021.05\\n2020.10 \\nThe OBM 2020.10 has the following limitations:\\nDoesn't support HTTP proxy for topology synchronization\\nDoesn't support the Performance Dashboard (PD) proxy graphing\\nTask 1: Download the integration tool\\nThe OBM integration tools are required to:\\n1\\n. \\nConfigure a secure connection between OBM and OPTIC Data Lake (DL).\\n2\\n. \\nConfigure event forwarding from OBM to OPTIC DL (Both Reporting and Automatic Event Correlation capabilities use event\\nforwarding)\\n3\\n. \\nConfigure Automatic Event Correlation\\nFollow the steps on the master node to get the integration tools:\\n1\\n. \\nThe \\nopsb-obm-integration-tools.zip\\n file is present in the \\nstatic-files-provider\\n container of the \\nopsb-resource-bundle\\n pod.\\nTo get the zip file, run the following command on the master (control plane) node:\\nwget https://<externalAccessHost>:<externalAccessPort>/staticfiles/opsb-obm-integration-tools.zip --no-check-certificate\\nFor example: \\nwget https://myhost.mycompany.net:443/staticfiles/opsb-obm-integration-tools.zip --no-check-certificate\\n2\\n. \\nRun the following commands to extract the contents of \\nopsb-obm-integration-tools.zip\\n to the \\nintegration-tools\\n directory: \\nunzip opsb-obm-integration-tools.zip -d integration-tools\\ncd integration-tools\\n3\\n. \\nRun the command to get the files necessary to configure the OBM and OPTIC Data Lake integration\\n./get-obm-setup-tool.sh\\nThe \\nobm-configuration.jar\\n tool is created in the same directory where \\nget-obm-setup-tool.sh\\n resides.\\nTask 2: Export trusted certificates issued by OBM connected to Monitoring Service Edge\\nRun the commands to export the certificates on the OBM. The following commands apply to both Linux and Windows:\\n1\\n. \\nRun the command to get the ASYMMETRIC_KEY_LENGTH\\novconfget sec.cm ASYMMETRIC_KEY_LENGTH\\n2\\n. \\nRun the command to get the \\ncoreid\\n of the OBM\\novcoreid -ovrg server\\n3\\n. \\nFind CA certificate in the connected OBM\\n/opt/OV/bin/ovcert -list\\nTip: \\nRun the following command to get the \\n<externalAccessHost> and <externalAccessPort>\\n:\\nhelm get values -n $(helm list -A | grep opsbridge | awk '{print $2,$1}') | grep -i externalAccess\\nCommand output:\\nexternalAccessHost: mycomp.mysystem.net\\nexternalAccessPort: 443\\nContainerized Operations Bridge 2022.11\\nPage \\n604\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.\", 'content_type': 'text', 'score': 8.767311203804113, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2000567d0300a53034a1cc85937d51dc'}>,\n",
            "               <Document: {'content': 'Verify AEC on a containerized OBM\\nIt can take up to 5 minutes for the configuration to take effect. Wait for about 5 minutes before verifying the configuration.\\nFollow the steps:\\n1\\n. \\nRun the command on OBM to send a test event to OPTIC Data Lake:\\nkubectl -n opsbridge-coqre exec -it omi-0 -c omi -- bash\\ncd /opt/HP/BSM/opr/support/\\n./sendEvent.sh -j -t \"Test Start\" -eh AutoCorrelationTest:Start -nx second -t \"Test End\" -eh AutoCorrelationTest:End\\nSample output:\\n2\\n. \\nOn OBM, go to \\nWorkspaces > Operations Console > Event Perspective. \\nCheck the OBM event browser after some\\ntime for a new event with the title: “CORRELATED [2 Evts]: …”. If the event is visible in the browser, the Automatic Event\\nCorrelation configuration is correct. \\nSample:\\nContainerized Operations Bridge 2022.11\\nPage \\n331\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': 8.67572254577112, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '23878da8258ca0cfff0830dd5a74f651'}>,\n",
            "               <Document: {'content': 'Verify the install\\n1\\n. \\nVerify that each of the PVCs are bound to a NFS volume.\\nExample: \\n# kubectl get pvc --all-namespaces\\nNAMESPACE        NAME                                                                               STATUS   VOLUME              CAPACITY   ACCESS MODES   STORAGECLASS   AGE\\ncore             db-backup-vol                                                                      Bound    db-backup           5Gi        RWX            cdf-default    29h\\ncore             db-single-vol                                                                      Bound    db-single           5Gi        RWX            cdf-default    29h\\ncore             itom-logging-vol                                                                   Bound    itom-logging        5Gi        RWX            cdf-default    29h\\ncore             itom-vol-claim                                                                     Bound    itom-vol            5Gi        RWX            cdf-default    29h\\ncore             prometheus-itom-prometheus-prometheus-db-prometheus-itom-prometheus-prometheus-0   Bound    itom-monitor        5Gi        RWX            cdf-default    28h\\nopsb-helm   deployment-opsb-helm-configvolumeclaim                                                  Bound    opsbvol3            10Gi       RWX                           27h\\nopsb-helm   deployment-opsb-helm-datavolumeclaim                                                    Bound    opsbvol1            10Gi       RWX                           27h\\nopsb-helm   deployment-opsb-helm-dbvolumeclaim                                                      Bound    opsbvol4            10Gi       RWX                           27h\\nopsb-helm   deployment-opsb-helm-logvolumeclaim                                                     Bound    opsbvol8            10Gi       RWX                           27h\\nopsb-helm   deployment-opsb-helm-omi-artemis-pvc                                                    Bound    opsbvol5            10Gi       RWO                           27h\\nopsb-helm   deployment-opsb-helm-pvc-omi-0                                                          Bound    opsbvol6            10Gi       RWO                           27h\\nopsb-helm   itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-0                               Bound    local-pv-7034901f   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-1                               Bound    local-pv-a248a6b9   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-journal-itomdipulsar-bookkeeper-2                               Bound    local-pv-6c0e5e9f   98Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-0                               Bound    local-pv-f1720991   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-1                               Bound    local-pv-8057665    98Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-bookkeeper-ledgers-itomdipulsar-bookkeeper-2                               Bound    local-pv-bcfc6ca0   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-0                          Bound    local-pv-2b68689f   98Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-1                          Bound    local-pv-16db1ccb   29Gi       RWO            fast-disks     27h\\nopsb-helm   itomdipulsar-zookeeper-zookeeper-data-itomdipulsar-zookeeper-2                          Bound    local-pv-1ca96a82   29Gi       RWO            fast-disks     27h\\n                                      \\n2\\n. \\nRun the following command to see the pod status in a namespace:\\nkubectl get pods -n <application namespace> \\nExample:\\n#  kubectl get pods -n opsb-helm\\nNAME                                                            READY   STATUS      RESTARTS      AGE\\nbvd-controller-deployment-6788dbc75c-7rpjd                      2/2     Running     0             27h\\nbvd-explore-deployment-7b66fb7d6c-2ngm9                         2/2     Running     0             24h\\nbvd-quexserv-5b8894749f-zv6l2                                   2/2     Running     0             24h\\nbvd-receiver-deployment-849c7fddb7-2r4fh                        2/2     Running     0             24h\\nbvd-redis-c6465485c-8mxqh                                       2/2     Running     0             27h\\nbvd-www-deployment-648d9b8b75-qzf4p                             2/2     Running     0             24h\\ncredential-manager-7f47fd64b7-tx6qz                             2/2     Running     0             27h\\nitom-analytics-aec-explained-56cd8d484-grvch                    2/2     Running     0             27h\\nitom-analytics-aec-pipeline-jm-564dc96877-j4nx5                 2/2     Running     0             27h\\nitom-analytics-aec-pipeline-tm-7d7668668b-wjv7p                 1/1     Running     0             27h\\nitom-analytics-auto-event-correlation-job-27554680-z44gd        0/1     Completed   0             26m\\nitom-analytics-auto-event-correlation-job-27554690-9svww        0/1     Completed   0             16m\\nitom-analytics-auto-event-correlation-job-27554700-mk4l4        0/1     Completed   0             6m51s\\nitom-analytics-data-retention-job-27553680-vgll7                0/1     Completed   0             17h\\nitom-analytics-data-retention-job-27554400-xbfcn                0/1     Completed   0             5h6m\\nitom-analytics-datasource-registry-796c948d6c-46r6d             2/2     Running     0             27h\\nitom-analytics-ea-config-b9c4556f6-jhw7l                        2/2     Running     0             27h\\nitom-analytics-event-attribute-reader-5948b55f84-q87q4          2/2     Running     0             27h\\nitom-analytics-flink-controller-7bd6585759-2snnk                2/2     Running     0             27h\\nitom-analytics-opsbridge-notification-8488597bdb-rwhq9          2/2     Running     0             27h\\nitom-analytics-text-clustering-server-58bf4f968d-sr7sn          2/2     Running     0             27h\\nitom-autopass-lms-54bf656cfc-cdkfx                              2/2     Running     0             27h\\nitom-cms-gateway-74fd99fb74-4gvlb                               2/2     Running     0             27h\\nitom-di-administration-77c95bb44b-s7zbw                         2/2     Running     0             27h\\nitom-di-data-access-dpl-559f56c6-mn7tj                          2/2     Running     0             27h\\nitom-di-metadata-server-778fcc756c-6d8tx                        2/2     Running     0             27h\\nitom-di-postload-taskcontroller-758798784c-dnzqw                2/2     Running     0             27h\\nitom-di-postload-taskexecutor-65b96b8b6-64twv                   2/2     Running     0             27h\\nitom-di-receiver-dpl-6f7cbc464d-97q4p                           2/2     Running     1 (27h ago)   27h\\nitom-di-receiver-dpl-6f7cbc464d-9fd9h                           2/2     Running     1 (27h ago)   27h\\nitom-di-scheduler-udx-f9cc5974f-nj9nh                           2/2     Running     0             27h\\nitom-idm-646446589c-bvs5b                                       2/2     Running     0             27h\\nitom-idm-646446589c-nwmf7                                       2/2     Running     0             27h\\nitom-ingress-controller-84f5494f66-49f9q                        2/2     Running     0             27h\\nitom-ingress-controller-84f5494f66-5v7z2                        2/2     Running     0             27h\\nitom-monitoring-admin-69bbb99b8b-bvbcf                          2/2     Running     0             27h\\nitom-monitoring-aws-discovery-collector-845559fdb-8nfl4         4/4     Running     0             27h\\nitom-monitoring-aws-metric-collector-67597f46b6-bzpmd           4/4     Running     0             27h\\nitom-monitoring-azure-discovery-collector-8469b597bc-lq25d      4/4     Running     0             27h\\nitom-monitoring-azure-metric-collector-6c7856f876-c9b5l         4/4     Running     0             27h\\nitom-monitoring-baseline-cfg-preload-job-zmyir-qszjf            0/1     Completed   0             27h\\nitom-monitoring-collection-autoconfigure-job-ucqmt-k85wb        0/1     Completed   0             27h\\nitom-monitoring-collection-manager-7cd8dc96c6-fwv5j             2/2     Running     0             27h\\nitom-monitoring-job-scheduler-5c85486b96-99rzc                  2/2     Running     0             27h\\nitom-monitoring-kubernetes-discovery-collector-c54b5799-qgshw   4/4     Running     0             27h\\nitom-monitoring-kubernetes-metric-collector-744c79949-gw2gw     4/4     Running     0             27h\\nitom-monitoring-oa-discovery-collector-7d8f7cd6c8-4265k         4/4     Running     0             27h\\nitom-monitoring-oa-metric-collector-65df67487f-wzrnz            4/4     Running     0             27h\\nContainerized Operations Bridge 2022.11\\nPage \\n287\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': 8.63603927120098, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '18981246894f40a29877989997bb9aa4'}>,\n",
            "               <Document: {'content': 'Verify the upgrade\\nRun the following command to verify if the pods are in a \"running\" or \"completed\" state:\\nkubectl get pods -n <suite namespace> \\nYou will see the list of pods according to your deployment and the status. \\nFollowing is a sample output:\\n# kubectl get pods -n opsb-helm\\nNAME                                                              READY   STATUS      RESTARTS   AGE\\nbvd-ap-bridge-55bd98c76c-zcz6t                                    2/2     Running     0          114m\\nbvd-controller-deployment-769cdc5c85-wmlm6                        2/2     Running     0          114m\\nbvd-explore-deployment-7bcf6794b-f9qkf                            2/2     Running     3          114m\\nbvd-quexserv-69776d44b9-t9fsh                                     2/2     Running     0          114m\\nbvd-receiver-deployment-55446bbbc9-dx9gs                          2/2     Running     0          114m\\nbvd-redis-75484f6b54-92mqq                                        3/3     Running     0          114m\\nbvd-www-deployment-78b7f5c495-v6hql                               2/2     Running     2          114m\\nips-deployment-5b78b97c8f-584vq                                   2/2     Running     0          114m\\nitom-analytics-auto-event-correlation-job-1606488000-cctdk        0/1     Completed   0          27m\\nitom-analytics-auto-event-correlation-job-1606488600-mx5dx        0/1     Completed   0          17m\\nitom-analytics-auto-event-correlation-job-1606489200-7bkx7        0/1     Completed   0          7m23s\\nitom-analytics-datasource-registry-5f5dc8765b-6hz9f               2/2     Running     0          114m\\nitom-analytics-ea-config-b54865c75-rl47d                          2/2     Running     0          114m\\nitom-analytics-event-attribute-reader-84b744b6b9-k2n4p            2/2     Running     0          114m\\nitom-analytics-opsbridge-notification-65f565895b-wpl47            2/2     Running     0          114m\\nitom-autopass-lms-777c645d5d-v6z8j                                2/2     Running     0          114m\\nitom-cdf-deployer-2020.08-1.2-2.2-7hdt4                           0/1     Completed   0          7h20m\\nitom-cdf-upgrade-deployer-202011-928h7                            0/1     Completed   0          4h38m\\nitom-opsb-amc-collector-1-7c7f46f9cb-clhzn                        2/2     Running     0          19m\\nitom-collect-once-collection-manager-87dd9ffd7-scp7f              2/2     Running     0          114m\\nitom-collect-once-data-broker-75ddf6df44-wzblf                    2/2     Running     0          114m\\nitom-di-administration-784766f897-s54c5                           2/2     Running     0          114m\\nitom-di-data-access-dpl-5f7ffb57fb-phpns                          2/2     Running     0          114m\\nitom-di-dp-job-submitter-dpl-6ccc97ddf7-wv2wt                     2/2     Running     0          114m\\nitom-di-dp-master-dpl-847c847f5f-cntxz                            2/2     Running     0          114m\\nitom-di-dp-worker-dpl-646cb9c4c5-rzc8c                            2/2     Running     0          114m\\nitom-di-metadata-server-6c66df958-24pzs                           2/2     Running     0          114m\\nitom-di-postload-taskcontroller-76cd8b8889-6ffxg                  2/2     Running     0          114m\\nitom-di-postload-taskexecutor-b8c5bd78d-6vvxg                     2/2     Running     0          114m\\nitom-di-receiver-dpl-785c7dfc6c-6wz5h                             2/2     Running     0          110m\\nitom-di-receiver-dpl-785c7dfc6c-xrhfr                             2/2     Running     0          114m\\nitom-di-scheduler-udx-5849b9c5bd-fbkfr                            2/2     Running     0          114m\\nitom-idm-66c8f598d4-stlgt                                         2/2     Running     0          114m\\nitom-idm-66c8f598d4-vdf8h                                         2/2     Running     0          109m\\nitom-ingress-controller-78ff59f77f-6qztl                          2/2     Running     0          114m\\nitom-ingress-controller-78ff59f77f-bz5qb                          2/2     Running     0          113m\\nitom-migrate-vault-secrets-to-k8s-job-c658h                       0/1     Completed   0          115m\\nitom-omi-aec-integration-watcher-1606488000-h76lb                 0/1     Completed   0          27m\\nitom-omi-aec-integration-watcher-1606488600-rms2f                 0/1     Completed   0          17m\\nitom-omi-aec-integration-watcher-1606489200-kfwbl                 0/1     Completed   0          7m23s\\nitom-omi-aec-integration-wi4za-f6988                              0/1     Completed   0          114m\\nitom-omi-di-integration-p5l6g-x6rzp                               0/1     Completed   0          114m\\nitom-opsb-content-administration-7649d4b5dd-k8554                 2/2     Running     0          114m\\nitom-opsb-content-administration-job-a0qop-k9sw4                  0/1     Completed   0          114m\\nitom-opsb-db-connection-validator-job-dr8zf                       0/1     Completed   0          114m\\nitom-opsb-node-resolver-55b6bcd44c-w72kh                          2/2     Running     0          114m\\nitom-opsb-resource-bundle-758c66b866-tvx8z                        1/1     Running     0          114m\\nitom-vault-b8b48ff49-nmh8w                                        1/1     Running     0          114m\\nitomdimonitoring-grafana-97bb8b57b-ztzxn                          2/2     Running     0          114m\\nitomdimonitoring-prometheus-cf4d5c46-wlkjv                        2/2     Running     0          114m\\nitomdimonitoring-prometheus-kube-state-metrics-5f5786db8d-w5vv7   2/2     Running     0          114m\\nitomdimonitoring-prometheus-node-exporter-4lcnz                   1/1     Running     0          108m\\nitomdimonitoring-prometheus-node-exporter-cw6dv                   1/1     Running     0          114m\\nitomdimonitoring-prometheus-node-exporter-j6dns                   1/1     Running     0          108m\\nitomdimonitoring-prometheus-node-exporter-jkx7h                   1/1     Running     0          107m\\nitomdimonitoring-prometheus-node-exporter-l2kmk                   1/1     Running     0          113m\\nitomdimonitoring-prometheus-node-exporter-rfxkm                   1/1     Running     0          107m\\nitomdimonitoring-verticapromexporter-59df7d5785-7drnt             2/2     Running     0          114m\\nitomdipulsar-autorecovery-0                                       1/1     Running     1          114m\\nitomdipulsar-bastion-0                                            2/2     Running     0          114m\\nitomdipulsar-bookkeeper-0                                         2/2     Running     0          109m\\nitomdipulsar-bookkeeper-1                                         2/2     Running     0          111m\\nitomdipulsar-bookkeeper-2                                         2/2     Running     0          114m\\nitomdipulsar-bookkeeper-init-2uppzqu-5m667                        0/1     Completed   0          114m\\nitomdipulsar-broker-5d9457d796-4rdnq                              2/2     Running     0          109m\\nitomdipulsar-broker-5d9457d796-9f5hv                              2/2     Running     0          114m\\nitomdipulsar-broker-5d9457d796-fqf8p                              2/2     Running     0          114m\\nitomdipulsar-proxy-575957cc44-5ltd6                               2/2     Running     0          114m\\nitomdipulsar-proxy-575957cc44-tx422                               2/2     Running     0          114m\\nitomdipulsar-zookeeper-0                                          2/2     Running     0          109m\\nitomdipulsar-zookeeper-1                                          2/2     Running     0          110m\\nitomdipulsar-zookeeper-2                                          2/2     Running     0          114m\\nitomdipulsar-zookeeper-metadata-n7e4b8w-44sgm                     0/1     Completed   0          114m\\nomi-0                                                             2/2     Running     0          2d9h\\nomi-artemis-766fdf6564-sp5jm                                      2/2     Running     0          2d9h\\nContainerized Operations Bridge 2022.11\\nPage \\n501\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': 8.473104318303168, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3526aecf83c3e1767074099c6dd59a13'}>,\n",
            "               <Document: {'content': 'Configures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nDatabase User Name for OBM\\nEvent\\nEnter the \\nOBM Event\\n database username for\\nPostgreSQL/Oracle \\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostna\\nme>\\nDatabase Password (OBM Event)\\nEnter the password for \\nOBM Event\\n database. \\nDatabase Name (OBM Event)\\nEnter the name of the \\nOBM Event\\n database.\\nVerify\\nVerify the connection\\nOpsBridge Database (OBM MGMT)  \\nConfigures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nDatabase User Name for OBM\\nMgmt\\nEnter the \\nOBM Mgmt\\n database username for\\nPostgreSQL/Oracle\\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostnam\\ne>\\n \\nDatabase Password (OBM Mgmt)\\nEnter the password for \\nOBM Mgmt\\n database. \\nDatabase Name (OBM Mgmt)\\nEnter the name of the \\nOBM Mgmt\\n database.\\nVerify\\nVerify the connection\\nOpsBridge Database (RTSM)  \\nConfigures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nDatabase User Name for OBM\\nRTSM\\nEnter the \\nOBM RTSM\\n database username for\\nPostgreSQL/Oracle \\nFor Azure, \\nthe username format is \\n<db_username>@<db_hostnam\\ne>\\nDatabase Password (RTSM)\\nEnter the password for\\n RTSM\\n database. \\nDatabase Name (RTSM)\\nEnter the name of the \\nRTSM \\ndatabase.\\nVerify\\nVerify the connection\\nOpsBridge Database (MONITORINGADMINDB)\\nConfigures external database parameters for the application:\\nConfiguration parameter\\nDescription\\nDefault\\nsetting\\nNote:\\n Not applicable when you select External Oracle.\\n\\ue916\\n\\ue916\\nNote: \\nNot applicable when you select External Oracle.\\n\\ue916\\n\\ue916\\nNote:\\n Not applicable when you select External Oracle.\\n\\ue917\\n\\ue917\\nContainerized Operations Bridge 2022.11\\nPage \\n254\\nThis PDF was generated on 30/11/2022 05:05:56 for your convenience. For the latest documentation, always see\\nhttps://docs.microfocus.com\\n.', 'content_type': 'text', 'score': 8.42573033340469, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5015eb4d876c87a97fc481373f10ddc8'}>],\n",
            " 'no_ans_gap': -1.6335301399230957,\n",
            " 'node_id': 'Reader',\n",
            " 'params': {'Reader': {'top_k': 5}, 'Retriever': {'top_k': 10}},\n",
            " 'query': 'Tell me about Opsb event correlation ?',\n",
            " 'root_node': 'Query'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import print_answers\n",
        "\n",
        "print_answers(\n",
        "    prediction,\n",
        "    details=\"medium\" ## Choose from `minimum`, `medium`, and `all`\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEq_JBDCrlYs",
        "outputId": "82d8a59b-f0a7-4bec-fe74-6feb86e3d5da"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: Tell me about Opsb event correlation ?\n",
            "Answers:\n",
            "[   {   'answer': 'Automatic Event Correlation',\n",
            "        'context': 'holder Dashboards (Business Value Dashboard)\\n'\n",
            "                   'OPTIC Reporting \\n'\n",
            "                   'Automatic Event Correlation\\n'\n",
            "                   'Agentless Monitoring*\\n'\n",
            "                   'Hyperscale Observability*\\n'\n",
            "                   'Containerize',\n",
            "        'score': 0.26186585426330566},\n",
            "    {   'answer': 'Automatic',\n",
            "        'context': 'es:\\n'\n",
            "                   'Stakeholder Dashboards (Business Value Dashboard)\\n'\n",
            "                   'OPTIC Reporting \\n'\n",
            "                   'Automatic Event Correlation\\n'\n",
            "                   'Agentless Monitoring*\\n'\n",
            "                   'Hyperscale Observability*\\n'\n",
            "                   'Con',\n",
            "        'score': 0.23116007447242737},\n",
            "    {   'answer': 'Automatic Event Correlation\\nAgentless Monitoring',\n",
            "        'context': 'boards (Business Value Dashboard)\\n'\n",
            "                   'OPTIC Reporting \\n'\n",
            "                   'Automatic Event Correlation\\n'\n",
            "                   'Agentless Monitoring*\\n'\n",
            "                   'Hyperscale Observability*\\n'\n",
            "                   'Containerized OBM\\n'\n",
            "                   'Anoma',\n",
            "        'score': 0.17825287580490112},\n",
            "    {   'answer': 'Rule based event correlation',\n",
            "        'context': 'n pattern or identifier. There are two types of\\n'\n",
            "                   'correlation:\\n'\n",
            "                   'Rule based event correlation - In this case, you can '\n",
            "                   'specify the rules or parameters base',\n",
            "        'score': 0.16348880529403687},\n",
            "    {   'answer': \"you don't need to specify any rules\",\n",
            "        'context': 'erized OBM.  \\n'\n",
            "                   \"Automatic event correlation - In this case, you don't need \"\n",
            "                   'to specify any rules. The Automatic Event Correlation '\n",
            "                   'service\\n'\n",
            "                   '(AEC) is a part',\n",
            "        'score': 0.14591346681118011}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def opsb_chatbot(querying_pipeline,user_query,top_k):\n",
        "  prediction = querying_pipeline.run(\n",
        "    query=user_query,\n",
        "    params={\n",
        "        \"Retriever\": {\"top_k\": top_k*2},\n",
        "        \"Reader\": {\"top_k\": top_k}\n",
        "    }\n",
        "  )\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "2AurwmlYsEDl"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import print_answers\n",
        "result = opsb_chatbot(querying_pipeline,\"Tell me about OPTIC Reporting capability please\",5)\n",
        "print_answers(\n",
        "    result,\n",
        "    details=\"all\" ## Choose from `minimum`, `medium`, and `all`\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "6f84b3569b60413daf27c63c2aa21da2",
            "4791441ebfc640cfa9568fa2d4050734",
            "45b5b21f42564bcd8b9c44e25cca1f4f",
            "f1a33b30de714ff8a203918c9b26e46d",
            "b55edc76851241a9915d5ca7d9a82599",
            "28a72be7a8614a0494887649b8d278a3",
            "afba4c2f2ddf465f9f09382887c6267d",
            "8d840ca71b4a40cf872b83555d830ac4",
            "5c2f63ce30f24cc486822db52b6d8f96",
            "f7fab93ee2824a8d9ba187165b137626",
            "f30af434954844abaa11b0c4519bee26"
          ]
        },
        "id": "MprfL76usWz0",
        "outputId": "70150c1d-e271-4dad-b004-23e2c64578d9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f84b3569b60413daf27c63c2aa21da2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: Tell me about OPTIC Reporting capability please\n",
            "Answers:\n",
            "[   <Answer {'answer': 'Automatic Event Correlation', 'type': 'extractive', 'score': 0.4425888955593109, 'context': 'holder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerize', 'offsets_in_document': [{'start': 11368, 'end': 11395}], 'offsets_in_context': [{'start': 62, 'end': 89}], 'document_ids': ['5fc52982237960a004fc3919c6add461'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'opticReporting', 'type': 'extractive', 'score': 0.4167580008506775, 'context': 'idge Suite offers. Users can enable one or multiple components.\\n  # opticReporting: Reporting component contains OPTIC Data Lake, BVD and Collection S', 'offsets_in_document': [{'start': 798, 'end': 812}], 'offsets_in_context': [{'start': 68, 'end': 82}], 'document_ids': ['62442ee3901a92250ac1714b7f89954b'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'Automatic Event Correlation', 'type': 'extractive', 'score': 0.35707250237464905, 'context': 'holder Dashboards (Business Value Dashboard)\\nOPTIC Reporting \\nAutomatic Event Correlation\\nAgentless Monitoring*\\nHyperscale Observability*\\nContainerize', 'offsets_in_document': [{'start': 5767, 'end': 5794}], 'offsets_in_context': [{'start': 62, 'end': 89}], 'document_ids': ['5fc52982237960a004fc3919c6add461'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'OPTIC One to create reports and visualize the\\nvisualize the real time data of the IT infrastructure', 'type': 'extractive', 'score': 0.2020414024591446, 'context': 'ent. OPTIC Reporting uses OPTIC One to create reports and visualize the\\nvisualize the real time data of the IT infrastructure. You can create the foll', 'offsets_in_document': [{'start': 12542, 'end': 12641}], 'offsets_in_context': [{'start': 26, 'end': 125}], 'document_ids': ['5fc52982237960a004fc3919c6add461'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'helm', 'type': 'extractive', 'score': 0.12212446331977844, 'context': 'lane) node to check if you have installed \\nOPTIC Reporting\\n capability: \\nhelm get values <helm_deployment_name> -n <suite namespace> | grep opticRepor', 'offsets_in_document': [{'start': 742, 'end': 746}], 'offsets_in_context': [{'start': 73, 'end': 77}], 'document_ids': ['bb57ba209942775573f0535fef17f9a7'], 'meta': {}}>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import print_answers\n",
        "result = opsb_chatbot(querying_pipeline,\"what is BYOBI)\",5)\n",
        "print_answers(\n",
        "    result,\n",
        "    details=\"all\", ## Choose from `minimum`, `medium`, and `all`\n",
        "    max_text_len=None\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "34f26466771340e0bf0621a7583b270d",
            "d03d0b3ffe7346e382a0c6c36101615d",
            "ccffe0a0f4ae4c5cbf3b6654e10a05d2",
            "a7b84123f02245f99c7fdb89f3bd1dfd",
            "6c40c3dc5a7a4808a86879413364e508",
            "f0da2005fdc14898b480e674c2057ebc",
            "fd0658dcba9840dabb4d84e8c5276e7e",
            "5108be4d29774dd9856c32d10fc966d9",
            "d2dbd977e89544ccb3de4e5cac040a71",
            "160ec109e6b44f0c921ffbe466b9dbad",
            "2f3f9440366943b6aa9c506978913022"
          ]
        },
        "id": "3ucVyGlXstsU",
        "outputId": "d98861aa-b3b3-45d9-e520-e3b585e6b17b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inferencing Samples:   0%|          | 0/2 [00:00<?, ? Batches/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34f26466771340e0bf0621a7583b270d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: what is BYOBI)\n",
            "Answers:\n",
            "[   <Answer {'answer': 'Bring your own Business Intelligence', 'type': 'extractive', 'score': 0.9251627922058105, 'context': 't \\nSynthetic Transaction \\nReal User Monitor \\nApplication\\nBring your own Business Intelligence (BYOBI)\\nYou can use third-party Business Intelligence (B', 'offsets_in_document': [{'start': 2449, 'end': 2485}], 'offsets_in_context': [{'start': 57, 'end': 93}], 'document_ids': ['bcd422f5476478db186938529574d177'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'Bring your own Business Intelligence', 'type': 'extractive', 'score': 0.8157068490982056, 'context': 't \\nSynthetic Transaction \\nReal User Monitor \\nApplication\\nBring your own Business Intelligence (BYOBI)\\nYou can use third-party Business Intelligence (B', 'offsets_in_document': [{'start': 876, 'end': 912}], 'offsets_in_context': [{'start': 57, 'end': 93}], 'document_ids': ['bcd422f5476478db186938529574d177'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'business intelligence tool', 'type': 'extractive', 'score': 0.46397078037261963, 'context': 'an represent\\nthis data graphically either on BVD or any other business intelligence tool (BYOBI) of your choice.\\nOperations Bridge offers several out ', 'offsets_in_document': [{'start': 13530, 'end': 13556}], 'offsets_in_context': [{'start': 62, 'end': 88}], 'document_ids': ['5fc52982237960a004fc3919c6add461'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'You can create custom reports using using the Business Intelligence (BI) tool of your choice', 'type': 'extractive', 'score': 0.44487106800079346, 'context': 'in OPTIC One.\\nBYOBI reports: You can create custom reports using using the Business Intelligence (BI) tool of your choice. \\nYou can send data (perform', 'offsets_in_document': [{'start': 13263, 'end': 13355}], 'offsets_in_context': [{'start': 29, 'end': 121}], 'document_ids': ['5fc52982237960a004fc3919c6add461'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'You can create custom reports using using the Business Intelligence (BI) tool of your choice', 'type': 'extractive', 'score': 0.4259227514266968, 'context': 'in OPTIC One.\\nBYOBI reports: You can create custom reports using using the Business Intelligence (BI) tool of your choice. \\nYou can send data (perform', 'offsets_in_document': [{'start': 7662, 'end': 7754}], 'offsets_in_context': [{'start': 29, 'end': 121}], 'document_ids': ['5fc52982237960a004fc3919c6add461'], 'meta': {}}>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import print_answers\n",
        "result = opsb_chatbot(querying_pipeline,\"what is Stakeholder Dashboard)\",5)\n",
        "print_answers(\n",
        "    result,\n",
        "    details=\"all\", ## Choose from `minimum`, `medium`, and `all`\n",
        "    max_text_len=None\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "074bd854a151409b9e1d7a2c83cf8bd1",
            "da76f25ca308457a82290e1a197c8eb2",
            "f32763713f7341fd9dd2985e38ccb874",
            "9463abfdc1994f18851510adce69a389",
            "209dc3b3256045688d3743a97d3e4b0e",
            "31f2e0a46c3048619abab6494458bda4",
            "4de5587107824a99af6cee8a639d2250",
            "451c346561a84c26aed5dda5c915a7ce",
            "917ba160f6044036bc5490e132a20b5f",
            "bfa87fdc65fc41d19268f593b9a25ab3",
            "2645ace625e64b108f9b333187c86e56"
          ]
        },
        "id": "LhL9ZCyhs1H9",
        "outputId": "1c2f5977-97d5-4e20-b63e-87d21647fb02"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "074bd854a151409b9e1d7a2c83cf8bd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: what is Stakeholder Dashboard)\n",
            "Answers:\n",
            "[   <Answer {'answer': 'Business Value Dashboard', 'type': 'extractive', 'score': 0.74318528175354, 'context': 'reate a deployment plan for installing\\nStakeholder Dashboards (Business Value Dashboard) on a single node in K3s environment, you must consider the fo', 'offsets_in_document': [{'start': 540, 'end': 564}], 'offsets_in_context': [{'start': 63, 'end': 87}], 'document_ids': ['755f845a45a1e269f828b79b68cf8983'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'gives you visual information of live data using tables, charts, and\\nwidgets', 'type': 'extractive', 'score': 0.5250613689422607, 'context': 'Dashboard\\n: The stakeholder dashboard gives you visual information of live data using tables, charts, and\\nwidgets. Real-time data can be streamed from', 'offsets_in_document': [{'start': 703, 'end': 778}], 'offsets_in_context': [{'start': 38, 'end': 113}], 'document_ids': ['9e9ce6a04a1e87c3b832a726c62bd185'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'Shared File systems', 'type': 'extractive', 'score': 0.45340245962142944, 'context': '\\nAzure disk, Azure\\nFiles\\nCeph RBD, Ceph\\nFS\\nStakeholder Dashboards\\nShared File systems\\nNFS\\nEFS\\nAzure Files\\nCeph FS\\nAgentless Monitoring\\nShared File Sys', 'offsets_in_document': [{'start': 3511, 'end': 3530}], 'offsets_in_context': [{'start': 66, 'end': 85}], 'document_ids': ['c8f9702a5a058c25d9b2e84934a210c3'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'BVD\\ndatabase', 'type': 'extractive', 'score': 0.4219694137573242, 'context': 'sBridgeInstallScript.sh\\n script, point the installer to the existing BVD\\ndatabase for the Stakeholder Dashboard capability.\\nFollow the instructions to', 'offsets_in_document': [{'start': 1981, 'end': 1993}], 'offsets_in_context': [{'start': 69, 'end': 81}], 'document_ids': ['fbfc458f4c469d5947c86352f4496746'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'BVD\\ndatabase', 'type': 'extractive', 'score': 0.42109057307243347, 'context': 'sBridgeInstallScript.sh\\n script, point the installer to the existing BVD\\ndatabase for the Stakeholder Dashboard capability.\\nFollow the instructions to', 'offsets_in_document': [{'start': 1981, 'end': 1993}], 'offsets_in_context': [{'start': 69, 'end': 81}], 'document_ids': ['cac85fa4fb7db1664b4492502a14e01d'], 'meta': {}}>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = opsb_chatbot(querying_pipeline,\"Agentless Monitoring License requirement ?)\",5)\n",
        "print_answers(\n",
        "    result,\n",
        "    details=\"all\", ## Choose from `minimum`, `medium`, and `all`\n",
        "    max_text_len=None\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "d95789192329418b89cf13603c31fca8",
            "21eb804970f44eb79ee4ce3e2ead9d13",
            "fb7ec0004241406e91da814b9db510c2",
            "304de57a3e5744a0b2efdc236a744e34",
            "24a9992cd00a469ba3ce9a4c6469505d",
            "3368d79c89a54c8e9ff4db38c727567d",
            "6b9359320d5041228c4a761fcb394c4d",
            "1d391a741c7b463baef88e32f212bba7",
            "b8d56c8676a847d387211ea923015261",
            "f25f9bcc621e4d13939a79321e8be7a8",
            "912707223b7147bb96dc15994bd953df"
          ]
        },
        "id": "_9PJ1gR7uVEj",
        "outputId": "357702df-485a-4c40-ac9e-98b8bb9e79e8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.document_stores.memory:InMemoryDocumentStore does not support scale_score for BM25 retrieval. This parameter is ignored.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inferencing Samples:   0%|          | 0/3 [00:00<?, ? Batches/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d95789192329418b89cf13603c31fca8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: Agentless Monitoring License requirement ?)\n",
            "Answers:\n",
            "[   <Answer {'answer': 'Actions', 'type': 'extractive', 'score': 0.11176839470863342, 'context': 'd configuration item topology for infrastructure and applications.\\n5\\n. \\nActions\\n: Actions you can perform on a monitor: Edit, Copy, Alerts, and Delete', 'offsets_in_document': [{'start': 1814, 'end': 1821}], 'offsets_in_context': [{'start': 72, 'end': 79}], 'document_ids': ['2f5f43fb62e7c095563a3fc382d94601'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'Sitescope', 'type': 'extractive', 'score': 0.07851079106330872, 'context': \"ess Monitoring, you don't need to apply for an additional license. The Sitescope license\\nenables you to access Agentless Monitoring.\\nContainerized Ope\", 'offsets_in_document': [{'start': 3210, 'end': 3219}], 'offsets_in_context': [{'start': 71, 'end': 80}], 'document_ids': ['2dd94f007a54e3f6f0beedd2f0ed5002'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'Express (Agentless Monitoring user interface only)\\nPremium\\nUltimate\\nThe following capabilities are available:\\nCapabilities\\nLicense edition\\nComponents\\nAgentless Monitoring\\nExpress*, Premium', 'type': 'extractive', 'score': 0.05437636375427246, 'context': 'Express (Agentless Monitoring user interface only)\\nPremium\\nUltimate\\nThe following capabilities are available:\\nCapabilities\\nLicense edition\\nComponents\\nAgentless Monitoring\\nExpress*, Premium', 'offsets_in_document': [{'start': 106, 'end': 294}], 'offsets_in_context': [{'start': 0, 'end': 188}], 'document_ids': ['2e136bc176bc6b7310799bb0427fb863'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'InstantOn', 'type': 'extractive', 'score': 0.0539034903049469, 'context': 'e a perpetual suite license or use the built-in 60 day trial license (\\nInstantOn\\n) after installation. If you use the\\ntrial license, you will need to ', 'offsets_in_document': [{'start': 130, 'end': 139}], 'offsets_in_context': [{'start': 71, 'end': 80}], 'document_ids': ['2dd94f007a54e3f6f0beedd2f0ed5002'], 'meta': {}}>,\n",
            "    <Answer {'answer': 'Express (Agentless Monitoring user interface only)\\nPremium\\nUltimate\\nThe following capabilities are available:\\nCapabilities\\nLicense edition\\nComponents\\nAgentless Monitoring\\nExpress*, Premium', 'type': 'extractive', 'score': 0.045571405440568924, 'context': 'Express (Agentless Monitoring user interface only)\\nPremium\\nUltimate\\nThe following capabilities are available:\\nCapabilities\\nLicense edition\\nComponents\\nAgentless Monitoring\\nExpress*, Premium', 'offsets_in_document': [{'start': 2024, 'end': 2212}], 'offsets_in_context': [{'start': 0, 'end': 188}], 'document_ids': ['2e136bc176bc6b7310799bb0427fb863'], 'meta': {}}>]\n"
          ]
        }
      ]
    }
  ]
}